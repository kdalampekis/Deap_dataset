{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranformer Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Data Shape: (1280, 32, 8064)\n",
      "Labels Shape: (1280, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import welch\n",
    "from scipy.integrate import simps\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Set your dataset path\n",
    "dataset_path = \"/Users/kostasbekis/Emotion_detection/Deap_dataset/deap/data_preprocessed_python\"\n",
    "\n",
    "# Get all .dat files\n",
    "all_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(\".dat\")]\n",
    "\n",
    "# Load dataset\n",
    "all_data, all_labels = [], []\n",
    "for file in all_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        file_data = pickle.load(f, encoding='latin1')\n",
    "        all_data.append(file_data['data'])  # EEG data shape (40, 8064)\n",
    "        all_labels.append(file_data['labels'])  # Labels shape (4,)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "labels = np.array(all_labels).reshape(-1, 4)  # Shape (1280, 4)\n",
    "data = np.array(all_data).reshape(-1, 40, 8064)  # Shape (1280, 40, 8064)\n",
    "\n",
    "# Use only the first 32 EEG channels\n",
    "eeg_data = data[:, :32, :]\n",
    "\n",
    "print(\"EEG Data Shape:\", eeg_data.shape)  # (1280, 32, 8064)\n",
    "print(\"Labels Shape:\", labels.shape)  # (1280, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Statistical Features Shape: (1280, 128)\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate statistical features\n",
    "def calculate_statistical_features(data):\n",
    "    return np.column_stack([\n",
    "        np.mean(data, axis=1),\n",
    "        np.std(data, axis=1),\n",
    "        kurtosis(data, axis=1),\n",
    "        skew(data, axis=1)\n",
    "    ])\n",
    "\n",
    "# Apply feature extraction to each EEG channel\n",
    "eeg_stat_features = np.hstack([\n",
    "    calculate_statistical_features(eeg_data[:, i, :]) for i in range(eeg_data.shape[1])\n",
    "])\n",
    "\n",
    "print(\"EEG Statistical Features Shape:\", eeg_stat_features.shape)  # (1280, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/44gwyztj0dlbkt4vjvhbq7v80000gn/T/ipykernel_29952/1947442904.py:8: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  return simps(psd[idx_band], dx=freqs[1] - freqs[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Band Power Features Shape: (1280, 160)\n"
     ]
    }
   ],
   "source": [
    "# EEG Frequency Bands\n",
    "FREQ_BANDS = {\"delta\": (0.5, 4), \"theta\": (4, 8), \"alpha\": (8, 12), \"beta\": (12, 30), \"gamma\": (30, 64)}\n",
    "\n",
    "def bandpower(data, sf, band):\n",
    "    low, high = band\n",
    "    freqs, psd = welch(data, sf, nperseg=int(2 * sf / low))\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "    return simps(psd[idx_band], dx=freqs[1] - freqs[0])\n",
    "\n",
    "# Compute power for each band\n",
    "eeg_band_features = np.hstack([\n",
    "    np.array([[bandpower(eeg_data[i, j], 128, FREQ_BANDS[band]) for band in FREQ_BANDS] \n",
    "              for j in range(eeg_data.shape[1])]).flatten()\n",
    "    for i in range(eeg_data.shape[0])\n",
    "])\n",
    "\n",
    "# Reshape to (1280, 160)\n",
    "eeg_band_features = eeg_band_features.reshape(1280, -1)\n",
    "\n",
    "print(\"EEG Band Power Features Shape:\", eeg_band_features.shape)  # (1280, 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 1. Normalize EEG features\n",
    "# ----------------------------------------------------------------\n",
    "# Assuming `eeg_band_features` is your feature tensor \n",
    "# and `labels` is the emotion label tensor (shape: [1280, 4])\n",
    "scaler = StandardScaler()\n",
    "eeg_data_normalized = scaler.fit_transform(eeg_band_features)  # Normalize across the dataset\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "features_tensor = torch.tensor(eeg_data_normalized, dtype=torch.float32)  # Shape: (1280, 160)\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 2. Create multi-class labels (binary for each emotion dimension)\n",
    "# ----------------------------------------------------------------\n",
    "threshold = np.median(labels, axis=0)  # median for each emotion dimension\n",
    "labels_multiclass = (labels > threshold).astype(int)  # 0/1 for each emotion\n",
    "\n",
    "# Convert labels to PyTorch tensor (multi-label, shape: [1280, 4])\n",
    "single_label_array = np.argmax(labels_multiclass, axis=1)  # shape (N,)\n",
    "\n",
    "labels_tensor = torch.tensor(single_label_array, dtype=torch.long)  # shape: (N,)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 3. Train / Validation / Test Split\n",
    "#    (Example: 60% train, 20% val, 20% test)\n",
    "# ----------------------------------------------------------------\n",
    "total_samples = len(features_tensor)\n",
    "train_size = int(0.8 * total_samples)   # 80%\n",
    "val_size   = int(0.1 * total_samples)   # 10%\n",
    "test_size  = total_samples - train_size - val_size\n",
    "\n",
    "# Indices for each subset\n",
    "train_end_idx = train_size\n",
    "val_end_idx = train_size + val_size\n",
    "\n",
    "# Slice features and labels into train/val/test\n",
    "train_data = features_tensor[:train_end_idx]\n",
    "train_labels = labels_tensor[:train_end_idx]\n",
    "\n",
    "val_data = features_tensor[train_end_idx:val_end_idx]\n",
    "val_labels = labels_tensor[train_end_idx:val_end_idx]\n",
    "\n",
    "test_data = features_tensor[val_end_idx:]\n",
    "test_labels = labels_tensor[val_end_idx:]\n",
    "\n",
    "# ----------------------------------------------------------------\n",
    "# 4. Create Datasets & Dataloaders\n",
    "# ----------------------------------------------------------------\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset   = TensorDataset(val_data,   val_labels)\n",
    "test_dataset  = TensorDataset(test_data,  test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset,   batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset,  batch_size=64, shuffle=False)\n",
    "\n",
    "# Now you have train_loader, val_loader, and test_loader.\n",
    "# You can train your model on train_loader, tune hyperparameters on val_loader,\n",
    "# and finally evaluate on test_loader.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, num_heads=8, num_layers=6):\n",
    "        super(EEGTransformer, self).__init__()\n",
    "\n",
    "        # Linear layer to project input_dim (e.g., 160) to hidden_dim (e.g., 128)\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "\n",
    "        # IMPORTANT: Use batch_first=True so the shape is (batch, seq_len, embed_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Final classification layer\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, input_dim)   # Example: (64, 160)\n",
    "        We add a dummy sequence dimension so it becomes (batch_size, 1, input_dim).\n",
    "        \"\"\"\n",
    "        # Add a dummy time dimension of size 1\n",
    "        x = x.unsqueeze(1)  # -> (batch_size, 1, input_dim)\n",
    "\n",
    "        # Project to (batch_size, 1, hidden_dim)\n",
    "        x = self.input_projection(x)\n",
    "        print(f\"Shape after input projection: {x.shape}\")\n",
    "\n",
    "        # Pass through Transformer\n",
    "        x = self.transformer_encoder(x)   # (batch_size, 1, hidden_dim)\n",
    "        print(f\"Shape after transformer: {x.shape}\")\n",
    "\n",
    "        # Since we only have 1 time step, mean-pooling across dim=1\n",
    "        # is effectively the same as x.squeeze(1)\n",
    "        x = x.mean(dim=1)                 # (batch_size, hidden_dim)\n",
    "        print(f\"Shape after pooling: {x.shape}\")\n",
    "\n",
    "        # Classify\n",
    "        x = self.fc(x)                    # (batch_size, num_classes)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 1/20, Loss: 1.4958, Accuracy: 0.2547\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 2/20, Loss: 1.3345, Accuracy: 0.3781\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 3/20, Loss: 1.2619, Accuracy: 0.4250\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 4/20, Loss: 1.1843, Accuracy: 0.4875\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 5/20, Loss: 1.0963, Accuracy: 0.5719\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 6/20, Loss: 1.0043, Accuracy: 0.6203\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 7/20, Loss: 0.8980, Accuracy: 0.6703\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 8/20, Loss: 0.7640, Accuracy: 0.7438\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 9/20, Loss: 0.6443, Accuracy: 0.7922\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 10/20, Loss: 0.5216, Accuracy: 0.8484\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 11/20, Loss: 0.4141, Accuracy: 0.8812\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 12/20, Loss: 0.3080, Accuracy: 0.9125\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 13/20, Loss: 0.2107, Accuracy: 0.9500\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 14/20, Loss: 0.1700, Accuracy: 0.9688\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 15/20, Loss: 0.1391, Accuracy: 0.9641\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 16/20, Loss: 0.1431, Accuracy: 0.9641\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 17/20, Loss: 0.1291, Accuracy: 0.9672\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 18/20, Loss: 0.1248, Accuracy: 0.9656\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 19/20, Loss: 0.1385, Accuracy: 0.9516\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Epoch 20/20, Loss: 0.2040, Accuracy: 0.9250\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "model = EEGTransformer(input_dim=160, hidden_dim=128, num_classes=4)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "# Example train_loader with data of shape [64, 160]\n",
    "train_loader = [\n",
    "    (torch.randn(64, 160), torch.randint(0, 4, (64,))) for _ in range(10)\n",
    "]\n",
    "\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for data, labels in train_loader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(data)             # Forward pass\n",
    "        loss = criterion(outputs, labels) # Compute loss\n",
    "        \n",
    "        loss.backward()                   # Backprop\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Compute accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, \"\n",
    "          f\"Loss: {total_loss/len(train_loader):.4f}, \"\n",
    "          f\"Accuracy: {correct/total:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # evaluation mode\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, labels in test_loader:\n",
    "            data, labels = data.to(device), labels.to(device)\n",
    "            \n",
    "            # If your labels are one-hot, convert them to indices:\n",
    "            if labels.dim() == 2 and labels.size(1) > 1:\n",
    "                labels = torch.argmax(labels, dim=1)\n",
    "            \n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs, dim=1)  # shape: (batch,)\n",
    "            \n",
    "            # Collect\n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "    \n",
    "    # Flatten arrays\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Debug check (optional)\n",
    "    # print(\"all_preds shape:\", all_preds.shape)\n",
    "    # print(\"all_labels shape:\", all_labels.shape)\n",
    "    # print(\"unique preds:\", np.unique(all_preds))\n",
    "    # print(\"unique labels:\", np.unique(all_labels))\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    \n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Shape after input projection: torch.Size([64, 1, 128])\n",
      "Shape after transformer: torch.Size([64, 1, 128])\n",
      "Shape after pooling: torch.Size([64, 128])\n",
      "Confusion Matrix:\n",
      " [[12 39  0 16]\n",
      " [ 6 28  0  6]\n",
      " [ 7  6  0  2]\n",
      " [ 1  4  0  1]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.46      0.18      0.26        67\n",
      "           1       0.36      0.70      0.48        40\n",
      "           2       0.00      0.00      0.00        15\n",
      "           3       0.04      0.17      0.06         6\n",
      "\n",
      "    accuracy                           0.32       128\n",
      "   macro avg       0.22      0.26      0.20       128\n",
      "weighted avg       0.36      0.32      0.29       128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kostasbekis/Emotion_detection/tf_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kostasbekis/Emotion_detection/tf_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kostasbekis/Emotion_detection/tf_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example usage after training\n",
    "cm = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Optional: visualize confusion matrix with seaborn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_confusion_matrix(cm, classes):\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
    "                xticklabels=classes, yticklabels=classes)\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.show()\n",
    "\n",
    "class_names = [\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"]  # Adjust as needed\n",
    "plot_confusion_matrix(cm, class_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 160])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape  # (64, 160)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranformer_paper_based_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def evaluate_model_with_cm(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model on the given data_loader, \n",
    "    compute confusion matrix and classification report.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            \n",
    "            # For single-label 4-class classification:\n",
    "            # outputs is (batch_size, 4)\n",
    "            _, predicted = torch.max(outputs, dim=1)  # (batch_size,)\n",
    "            \n",
    "            all_preds.append(predicted.cpu().numpy())\n",
    "            all_labels.append(batch_y.cpu().numpy())\n",
    "    \n",
    "    # Flatten\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\\n\", cm)\n",
    "    \n",
    "    # Classification report\n",
    "    print(\"\\nClassification Report:\\n\", \n",
    "          classification_report(all_labels, all_preds))\n",
    "    \n",
    "    return cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/44gwyztj0dlbkt4vjvhbq7v80000gn/T/ipykernel_29952/295438526.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x = torch.tensor(x, dtype=torch.float32)\n",
      "/var/folders/0q/44gwyztj0dlbkt4vjvhbq7v80000gn/T/ipykernel_29952/295438526.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y = torch.tensor(y, dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] Train Loss: 0.9848, Train Acc: 0.6445, Val Acc: 0.7266\n",
      "Epoch [2/20] Train Loss: 0.8907, Train Acc: 0.6738, Val Acc: 0.7266\n",
      "Epoch [3/20] Train Loss: 0.8745, Train Acc: 0.6777, Val Acc: 0.7266\n",
      "Epoch [4/20] Train Loss: 0.8680, Train Acc: 0.6816, Val Acc: 0.7266\n",
      "Epoch [5/20] Train Loss: 0.8652, Train Acc: 0.6670, Val Acc: 0.7266\n",
      "Epoch [6/20] Train Loss: 0.8583, Train Acc: 0.6719, Val Acc: 0.7266\n",
      "Epoch [7/20] Train Loss: 0.8577, Train Acc: 0.6787, Val Acc: 0.7266\n",
      "Epoch [8/20] Train Loss: 0.8388, Train Acc: 0.6846, Val Acc: 0.7266\n",
      "Epoch [9/20] Train Loss: 0.8346, Train Acc: 0.6826, Val Acc: 0.7266\n",
      "Epoch [10/20] Train Loss: 0.8349, Train Acc: 0.6787, Val Acc: 0.7266\n",
      "Epoch [11/20] Train Loss: 0.8391, Train Acc: 0.6846, Val Acc: 0.7266\n",
      "Epoch [12/20] Train Loss: 0.8264, Train Acc: 0.6846, Val Acc: 0.7266\n",
      "Epoch [13/20] Train Loss: 0.8259, Train Acc: 0.6768, Val Acc: 0.7266\n",
      "Epoch [14/20] Train Loss: 0.8190, Train Acc: 0.6836, Val Acc: 0.7266\n",
      "Epoch [15/20] Train Loss: 0.8108, Train Acc: 0.6885, Val Acc: 0.7266\n",
      "Epoch [16/20] Train Loss: 0.8153, Train Acc: 0.6895, Val Acc: 0.7188\n",
      "Epoch [17/20] Train Loss: 0.8064, Train Acc: 0.6924, Val Acc: 0.7266\n",
      "Epoch [18/20] Train Loss: 0.8024, Train Acc: 0.6943, Val Acc: 0.7266\n",
      "Epoch [19/20] Train Loss: 0.7931, Train Acc: 0.6953, Val Acc: 0.7031\n",
      "Epoch [20/20] Train Loss: 0.7947, Train Acc: 0.6787, Val Acc: 0.7266\n",
      "Confusion Matrix:\n",
      " [[93  0  0  0]\n",
      " [26  0  0  0]\n",
      " [ 4  0  0  0]\n",
      " [ 5  0  0  0]]\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      1.00      0.84        93\n",
      "           1       0.00      0.00      0.00        26\n",
      "           2       0.00      0.00      0.00         4\n",
      "           3       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.73       128\n",
      "   macro avg       0.18      0.25      0.21       128\n",
      "weighted avg       0.53      0.73      0.61       128\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kostasbekis/Emotion_detection/tf_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kostasbekis/Emotion_detection/tf_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/Users/kostasbekis/Emotion_detection/tf_env/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DEAPDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        \"\"\"\n",
    "        data: np.array of shape (num_samples, seq_len, n_channels)\n",
    "        labels: np.array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.data[idx]      # (seq_len, n_channels)\n",
    "        y = self.labels[idx]    # scalar or integer\n",
    "        # Convert to torch\n",
    "        x = torch.tensor(x, dtype=torch.float32)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "class EEGTransformer(nn.Module):\n",
    "    def __init__(self, input_dim=160, hidden_dim=128, num_classes=4, num_heads=8, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.input_projection = nn.Linear(input_dim, hidden_dim)\n",
    "        self.encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=hidden_dim,\n",
    "            nhead=num_heads,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            self.encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch_size, 160)\n",
    "        # Add a dummy sequence dimension\n",
    "        x = x.unsqueeze(1)      # -> (batch_size, 1, 160)\n",
    "\n",
    "        # Project: (batch_size, 1, hidden_dim)\n",
    "        x = self.input_projection(x)\n",
    "\n",
    "        # Transformer expects (batch_size, seq_len, hidden_dim)\n",
    "        x = self.transformer_encoder(x)   # -> (batch_size, 1, hidden_dim)\n",
    "\n",
    "        # Mean-pool over seq_len (which is 1, so it's basically a squeeze)\n",
    "        x = x.mean(dim=1)                 # -> (batch_size, hidden_dim)\n",
    "\n",
    "        # Classification\n",
    "        x = self.fc(x)                    # -> (batch_size, num_classes)\n",
    "        return x\n",
    "\n",
    "# ----------------------\n",
    "# Main training snippet\n",
    "# ----------------------\n",
    "def train_transformer_deap(train_data, train_labels, val_data, val_labels, device):\n",
    "    # Create datasets\n",
    "    train_dataset = DEAPDataset(train_data, train_labels)\n",
    "    val_dataset   = DEAPDataset(val_data, val_labels)\n",
    "    \n",
    "    train_loader  = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    val_loader    = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "    \n",
    "    model = EEGTransformer(input_dim=160, hidden_dim=128, num_classes=4)\n",
    "    batch_x = torch.randn(32, 160)  # e.g. one batch\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    # Training loop\n",
    "    num_epochs = 20\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "            outputs = model(batch_x)   # shape (B, 4), float\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total   += batch_y.size(0)\n",
    "        \n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc  = correct / total\n",
    "        \n",
    "        # Evaluate on validation set\n",
    "        val_acc = evaluate_accuracy(model, val_loader, device)\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
    "              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Evaluate with confusion matrix\n",
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total   += batch_y.size(0)\n",
    "    return correct / total\n",
    "\n",
    "model = train_transformer_deap(train_data, train_labels, val_data, val_labels, device)\n",
    "cm = evaluate_model_with_cm(model, val_loader, device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neuro-GPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GPTPositionalEncoding(nn.Module):\n",
    "    \"\"\"Optional: learned or sinusoidal positional embeddings.\"\"\"\n",
    "    def __init__(self, d_model, max_len=10000):\n",
    "        super().__init__()\n",
    "        # For simplicity, let's do a learnable embedding\n",
    "        self.pe = nn.Embedding(max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch_size, seq_len, d_model)\n",
    "        We add a positional embedding for each time step.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, d_model = x.shape\n",
    "        positions = torch.arange(0, seq_len, device=x.device).unsqueeze(0)  # (1, seq_len)\n",
    "        # Expand so it matches (batch_size, seq_len)\n",
    "        positions = positions.expand(batch_size, seq_len)\n",
    "        pos_emb = self.pe(positions)  # (batch_size, seq_len, d_model)\n",
    "        return x + pos_emb\n",
    "\n",
    "class GPTDecoderBlock(nn.Module):\n",
    "    \"\"\"A simplified GPT-like decoder block.\"\"\"\n",
    "    def __init__(self, d_model, nhead, dim_feedforward=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(d_model, nhead, dropout=dropout, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(d_model)\n",
    "        \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, dim_feedforward),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim_feedforward, d_model),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, attn_mask=None):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        # Self-attention (causal if attn_mask is set)\n",
    "        attn_out, _ = self.attn(x, x, x, attn_mask=attn_mask)\n",
    "        x = x + self.dropout(attn_out)\n",
    "        x = self.ln1(x)\n",
    "\n",
    "        # Feedforward\n",
    "        mlp_out = self.mlp(x)\n",
    "        x = x + self.dropout(mlp_out)\n",
    "        x = self.ln2(x)\n",
    "        return x\n",
    "\n",
    "class NeuroGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    Simplified GPT-like model for EEG multiclass classification:\n",
    "    1) Input projection\n",
    "    2) Positional encoding\n",
    "    3) Multiple decoder blocks\n",
    "    4) Classification head\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim=32,      # e.g., # of EEG channels or feature dim\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_decoder_layers=4,\n",
    "        num_classes=4,\n",
    "        seq_len=128,\n",
    "        dropout=0.1,\n",
    "        use_causal_mask=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.seq_len = seq_len\n",
    "        self.use_causal_mask = use_causal_mask\n",
    "\n",
    "        # Project input_dim -> d_model\n",
    "        self.input_projection = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        # Positional encoding (optional, but often beneficial for sequences)\n",
    "        self.pos_encoding = GPTPositionalEncoding(d_model, max_len=seq_len)\n",
    "\n",
    "        # Stacked GPT-like decoder layers\n",
    "        self.decoder_blocks = nn.ModuleList([\n",
    "            GPTDecoderBlock(d_model, nhead, dim_feedforward=4*d_model, dropout=dropout)\n",
    "            for _ in range(num_decoder_layers)\n",
    "        ])\n",
    "\n",
    "        # Classification head: we can pool over seq_len or take final token\n",
    "        self.classifier = nn.Linear(d_model, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x shape: (batch_size, seq_len, input_dim)\n",
    "        \"\"\"\n",
    "        # 1) Project to (batch, seq_len, d_model)\n",
    "        x = self.input_projection(x)\n",
    "\n",
    "        # 2) Optional positional encoding\n",
    "        x = self.pos_encoding(x)  # (batch, seq_len, d_model)\n",
    "\n",
    "        # 3) Build an attention mask if using causal approach\n",
    "        attn_mask = None\n",
    "        if self.use_causal_mask:\n",
    "            # A lower-triangular mask: (seq_len, seq_len)\n",
    "            attn_mask = torch.triu(\n",
    "                torch.ones(self.seq_len, self.seq_len, device=x.device), diagonal=1\n",
    "            )\n",
    "            attn_mask = attn_mask.masked_fill(attn_mask==1, float('-inf'))\n",
    "\n",
    "        # Pass through each decoder block\n",
    "        for block in self.decoder_blocks:\n",
    "            x = block(x, attn_mask=attn_mask)\n",
    "\n",
    "        # 4) Pooling or final token for classification\n",
    "        # Option A: mean-pool across time\n",
    "        x = x.mean(dim=1)  # (batch_size, d_model)\n",
    "\n",
    "        # Option B: take the last token (like GPT does)\n",
    "        # x = x[:, -1, :]  # (batch_size, d_model)\n",
    "\n",
    "        # Classification\n",
    "        out = self.classifier(x)  # (batch_size, num_classes)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_neuro_gpt(train_loader, val_loader, device, epochs=10, lr=1e-4):\n",
    "    # Instantiate the model\n",
    "    model = NeuroGPT(\n",
    "        input_dim=160,      # or however many features you have\n",
    "        d_model=128,\n",
    "        nhead=8,\n",
    "        num_decoder_layers=4,\n",
    "        num_classes=4,\n",
    "        seq_len=50,         # or however many time steps\n",
    "        dropout=0.1,\n",
    "        use_causal_mask=False  # set True if you want strictly causal\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()  # single-label 4-class\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_x, batch_y in train_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_x)                # (batch, 4)\n",
    "            loss = criterion(outputs, batch_y)       # batch_y in [0..3]\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "        \n",
    "        train_acc = correct / total\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        \n",
    "        val_acc = evaluate_accuracy(model, val_loader, device)\n",
    "        \n",
    "        print(f\"[Epoch {epoch+1}/{epochs}] \"\n",
    "              f\"Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_accuracy(model, data_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in data_loader:\n",
    "            batch_x = batch_x.to(device)\n",
    "            batch_y = batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            _, predicted = torch.max(outputs, dim=1)\n",
    "            correct += (predicted == batch_y).sum().item()\n",
    "            total += batch_y.size(0)\n",
    "    return correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "[Epoch 1/40] Loss: 1.3883, Train Acc: 0.2838, Val Acc: 0.2550\n",
      "[Epoch 2/40] Loss: 1.3890, Train Acc: 0.2475, Val Acc: 0.2550\n",
      "[Epoch 3/40] Loss: 1.3726, Train Acc: 0.3000, Val Acc: 0.2400\n",
      "[Epoch 4/40] Loss: 1.3662, Train Acc: 0.3450, Val Acc: 0.2600\n",
      "[Epoch 5/40] Loss: 1.3603, Train Acc: 0.3387, Val Acc: 0.2550\n",
      "[Epoch 6/40] Loss: 1.3502, Train Acc: 0.3425, Val Acc: 0.2250\n",
      "[Epoch 7/40] Loss: 1.3475, Train Acc: 0.3463, Val Acc: 0.2200\n",
      "[Epoch 8/40] Loss: 1.3350, Train Acc: 0.3975, Val Acc: 0.2300\n",
      "[Epoch 9/40] Loss: 1.3240, Train Acc: 0.3837, Val Acc: 0.2400\n",
      "[Epoch 10/40] Loss: 1.3110, Train Acc: 0.4100, Val Acc: 0.2650\n",
      "[Epoch 11/40] Loss: 1.2793, Train Acc: 0.4487, Val Acc: 0.2600\n",
      "[Epoch 12/40] Loss: 1.2577, Train Acc: 0.4462, Val Acc: 0.2100\n",
      "[Epoch 13/40] Loss: 1.2436, Train Acc: 0.4575, Val Acc: 0.2500\n",
      "[Epoch 14/40] Loss: 1.2055, Train Acc: 0.4600, Val Acc: 0.2100\n",
      "[Epoch 15/40] Loss: 1.1580, Train Acc: 0.5112, Val Acc: 0.2400\n",
      "[Epoch 16/40] Loss: 1.1336, Train Acc: 0.5175, Val Acc: 0.2300\n",
      "[Epoch 17/40] Loss: 1.0720, Train Acc: 0.5663, Val Acc: 0.2250\n",
      "[Epoch 18/40] Loss: 1.0673, Train Acc: 0.5550, Val Acc: 0.2200\n",
      "[Epoch 19/40] Loss: 1.0491, Train Acc: 0.5600, Val Acc: 0.2450\n",
      "[Epoch 20/40] Loss: 1.0244, Train Acc: 0.5587, Val Acc: 0.2300\n",
      "[Epoch 21/40] Loss: 0.9936, Train Acc: 0.6025, Val Acc: 0.2550\n",
      "[Epoch 22/40] Loss: 0.9240, Train Acc: 0.6350, Val Acc: 0.2600\n",
      "[Epoch 23/40] Loss: 0.8772, Train Acc: 0.6700, Val Acc: 0.2300\n",
      "[Epoch 24/40] Loss: 0.9148, Train Acc: 0.6188, Val Acc: 0.2300\n",
      "[Epoch 25/40] Loss: 0.8354, Train Acc: 0.6575, Val Acc: 0.2750\n",
      "[Epoch 26/40] Loss: 0.7656, Train Acc: 0.7137, Val Acc: 0.2450\n",
      "[Epoch 27/40] Loss: 0.7089, Train Acc: 0.7450, Val Acc: 0.2400\n",
      "[Epoch 28/40] Loss: 0.6578, Train Acc: 0.7712, Val Acc: 0.2550\n",
      "[Epoch 29/40] Loss: 0.6539, Train Acc: 0.7512, Val Acc: 0.2200\n",
      "[Epoch 30/40] Loss: 0.6243, Train Acc: 0.7675, Val Acc: 0.2450\n",
      "[Epoch 31/40] Loss: 0.5767, Train Acc: 0.7850, Val Acc: 0.2450\n",
      "[Epoch 32/40] Loss: 0.4435, Train Acc: 0.8562, Val Acc: 0.2450\n",
      "[Epoch 33/40] Loss: 0.3724, Train Acc: 0.9163, Val Acc: 0.2750\n",
      "[Epoch 34/40] Loss: 0.3241, Train Acc: 0.9100, Val Acc: 0.2600\n",
      "[Epoch 35/40] Loss: 0.2852, Train Acc: 0.9175, Val Acc: 0.2550\n",
      "[Epoch 36/40] Loss: 0.2285, Train Acc: 0.9363, Val Acc: 0.2550\n",
      "[Epoch 37/40] Loss: 0.1725, Train Acc: 0.9650, Val Acc: 0.2600\n",
      "[Epoch 38/40] Loss: 0.1354, Train Acc: 0.9762, Val Acc: 0.2600\n",
      "[Epoch 39/40] Loss: 0.1096, Train Acc: 0.9875, Val Acc: 0.2550\n",
      "[Epoch 40/40] Loss: 0.0954, Train Acc: 0.9900, Val Acc: 0.2500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 3, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# 4. Evaluate on Validation Set\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m     51\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m evaluate_accuracy(model, val_loader, device)\n\u001b[0;32m---> 52\u001b[0m test_acc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_accuracy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mFinal Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[127], line 61\u001b[0m, in \u001b[0;36mevaluate_accuracy\u001b[0;34m(model, data_loader, device)\u001b[0m\n\u001b[1;32m     59\u001b[0m batch_x \u001b[38;5;241m=\u001b[39m batch_x\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     60\u001b[0m batch_y \u001b[38;5;241m=\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 61\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     63\u001b[0m correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (predicted \u001b[38;5;241m==\u001b[39m batch_y)\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/Emotion_detection/tf_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Emotion_detection/tf_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[126], line 99\u001b[0m, in \u001b[0;36mNeuroGPT.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     96\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_projection(x)\n\u001b[1;32m     98\u001b[0m \u001b[38;5;66;03m# 2) Optional positional encoding\u001b[39;00m\n\u001b[0;32m---> 99\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (batch, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# 3) Build an attention mask if using causal approach\u001b[39;00m\n\u001b[1;32m    102\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Emotion_detection/tf_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Emotion_detection/tf_env/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[126], line 17\u001b[0m, in \u001b[0;36mGPTPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m    x: (batch_size, seq_len, d_model)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    We add a positional embedding for each time step.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     batch_size, seq_len, d_model \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m     18\u001b[0m     positions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m0\u001b[39m, seq_len, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# (1, seq_len)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m# Expand so it matches (batch_size, seq_len)\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 3, got 2)"
     ]
    }
   ],
   "source": [
    "# -------------------------------\n",
    "# EXAMPLE USAGE / DEMO\n",
    "# -------------------------------\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "batch_size = 32\n",
    "seq_len = 50\n",
    "input_dim = 160\n",
    "num_classes = 4\n",
    "num_samples = 1000\n",
    "\n",
    "\n",
    "X = np.random.randn(num_samples, seq_len, input_dim)  # random features\n",
    "y = np.random.randint(0, 4, size=(num_samples,))      # random labels [0..3]\n",
    "\n",
    "# Convert to torch tensors\n",
    "features_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "labels_tensor   = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# -------------------------------\n",
    "# 2. Split into Train & Val\n",
    "# -------------------------------\n",
    "train_size = int(0.8 * num_samples)\n",
    "val_size   = num_samples - train_size\n",
    "\n",
    "train_data = features_tensor[:train_size]\n",
    "train_labels = labels_tensor[:train_size]\n",
    "val_data   = features_tensor[train_size:]\n",
    "val_labels = labels_tensor[train_size:]\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_dataset = TensorDataset(train_data, train_labels)\n",
    "val_dataset   = TensorDataset(val_data, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# -------------------------------\n",
    "# 3. Train NeuroGPT\n",
    "# -------------------------------\n",
    "model = train_neuro_gpt(train_loader, val_loader, device, epochs=40, lr=1e-4)\n",
    "\n",
    "# -------------------------------\n",
    "# 4. Evaluate on Validation Set\n",
    "# -------------------------------\n",
    "val_acc = evaluate_accuracy(model, val_loader, device)\n",
    "test_acc = evaluate_accuracy(model, test_loader, device)\n",
    "\n",
    "print(f\"\\nFinal Validation Accuracy: {val_acc:.4f}, {test_acc:4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
