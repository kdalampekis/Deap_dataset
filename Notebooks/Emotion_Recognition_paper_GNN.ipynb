{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Data Shape: (1280, 32, 8064)\n",
      "Labels Shape: (1280, 4)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import welch\n",
    "from scipy.integrate import simps\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "# Set your dataset path\n",
    "dataset_path = \"/Users/kostasbekis/Emotion_detection/Deap_dataset/deap/data_preprocessed_python\"\n",
    "\n",
    "# Get all .dat files\n",
    "all_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(\".dat\")]\n",
    "\n",
    "# Load dataset\n",
    "all_data, all_labels = [], []\n",
    "for file in all_files:\n",
    "    with open(file, 'rb') as f:\n",
    "        file_data = pickle.load(f, encoding='latin1')\n",
    "        all_data.append(file_data['data'])  # EEG data shape (40, 8064)\n",
    "        all_labels.append(file_data['labels'])  # Labels shape (4,)\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "labels = np.array(all_labels).reshape(-1, 4)  # Shape (1280, 4)\n",
    "data = np.array(all_data).reshape(-1, 40, 8064)  # Shape (1280, 40, 8064)\n",
    "\n",
    "# Use only the first 32 EEG channels\n",
    "eeg_data = data[:, :32, :]\n",
    "\n",
    "print(\"EEG Data Shape:\", eeg_data.shape)  # (1280, 32, 8064)\n",
    "print(\"Labels Shape:\", labels.shape)  # (1280, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Statistical Features Shape: (1280, 128)\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate statistical features\n",
    "def calculate_statistical_features(data):\n",
    "    return np.column_stack([\n",
    "        np.mean(data, axis=1),\n",
    "        np.std(data, axis=1),\n",
    "        kurtosis(data, axis=1),\n",
    "        skew(data, axis=1)\n",
    "    ])\n",
    "\n",
    "# Apply feature extraction to each EEG channel\n",
    "eeg_stat_features = np.hstack([\n",
    "    calculate_statistical_features(eeg_data[:, i, :]) for i in range(eeg_data.shape[1])\n",
    "])\n",
    "\n",
    "print(\"EEG Statistical Features Shape:\", eeg_stat_features.shape)  # (1280, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0q/44gwyztj0dlbkt4vjvhbq7v80000gn/T/ipykernel_38858/1947442904.py:8: DeprecationWarning: 'scipy.integrate.simps' is deprecated in favour of 'scipy.integrate.simpson' and will be removed in SciPy 1.14.0\n",
      "  return simps(psd[idx_band], dx=freqs[1] - freqs[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEG Band Power Features Shape: (1280, 160)\n"
     ]
    }
   ],
   "source": [
    "# EEG Frequency Bands\n",
    "FREQ_BANDS = {\"delta\": (0.5, 4), \"theta\": (4, 8), \"alpha\": (8, 12), \"beta\": (12, 30), \"gamma\": (30, 64)}\n",
    "\n",
    "def bandpower(data, sf, band):\n",
    "    low, high = band\n",
    "    freqs, psd = welch(data, sf, nperseg=int(2 * sf / low))\n",
    "    idx_band = np.logical_and(freqs >= low, freqs <= high)\n",
    "    return simps(psd[idx_band], dx=freqs[1] - freqs[0])\n",
    "\n",
    "# Compute power for each band\n",
    "eeg_band_features = np.hstack([\n",
    "    np.array([[bandpower(eeg_data[i, j], 128, FREQ_BANDS[band]) for band in FREQ_BANDS] \n",
    "              for j in range(eeg_data.shape[1])]).flatten()\n",
    "    for i in range(eeg_data.shape[0])\n",
    "])\n",
    "\n",
    "# Reshape to (1280, 160)\n",
    "eeg_band_features = eeg_band_features.reshape(1280, -1)\n",
    "\n",
    "print(\"EEG Band Power Features Shape:\", eeg_band_features.shape)  # (1280, 160)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Features Tensor Shape: torch.Size([1280, 160])\n",
      "Final Labels Tensor Shape: torch.Size([1280, 4])\n"
     ]
    }
   ],
   "source": [
    "# Convert to Tensors\n",
    "features_tensor = torch.tensor(eeg_band_features, dtype=torch.float)\n",
    "labels_tensor = torch.tensor(labels, dtype=torch.float)\n",
    "\n",
    "# Normalize labels between 0 and 1\n",
    "labels_tensor = (labels_tensor - labels_tensor.min()) / (labels_tensor.max() - labels_tensor.min())\n",
    "\n",
    "print(\"Final Features Tensor Shape:\", features_tensor.shape)  # (1280, 160)\n",
    "print(\"Final Labels Tensor Shape:\", labels_tensor.shape)  # (1280, 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class EEG_GNN(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim):\n",
    "        super(EEG_GNN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split indices\n",
    "train_idx, test_idx = train_test_split(np.arange(len(features_tensor)), test_size=0.2, random_state=42)\n",
    "\n",
    "# Masking for train/test\n",
    "train_mask = torch.zeros(len(features_tensor), dtype=torch.bool)\n",
    "test_mask = torch.zeros(len(features_tensor), dtype=torch.bool)\n",
    "train_mask[train_idx] = True\n",
    "test_mask[test_idx] = True\n",
    "\n",
    "# Update graph data with masks\n",
    "graph_data.train_mask = train_mask\n",
    "graph_data.test_mask = test_mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 418100.1562, Test Loss: 144623.5938\n",
      "Epoch 2, Train Loss: 305611.1562, Test Loss: 89893.0156\n",
      "Epoch 3, Train Loss: 228996.9062, Test Loss: 55488.7539\n",
      "Epoch 4, Train Loss: 158911.8281, Test Loss: 38098.8477\n",
      "Epoch 5, Train Loss: 131202.4219, Test Loss: 33126.7305\n",
      "Epoch 6, Train Loss: 116827.2500, Test Loss: 35978.0156\n",
      "Epoch 7, Train Loss: 103684.4531, Test Loss: 41890.4023\n",
      "Epoch 8, Train Loss: 90063.4062, Test Loss: 47769.2812\n",
      "Epoch 9, Train Loss: 82031.9219, Test Loss: 51035.2422\n",
      "Epoch 10, Train Loss: 100394.9141, Test Loss: 50131.5391\n",
      "Epoch 11, Train Loss: 69749.2734, Test Loss: 47963.3242\n",
      "Epoch 12, Train Loss: 74852.5234, Test Loss: 44064.9570\n",
      "Epoch 13, Train Loss: 79805.4141, Test Loss: 38945.1719\n",
      "Epoch 14, Train Loss: 71208.1094, Test Loss: 33286.5625\n",
      "Epoch 15, Train Loss: 56027.4023, Test Loss: 27947.6641\n",
      "Epoch 16, Train Loss: 52513.4023, Test Loss: 23068.7754\n",
      "Epoch 17, Train Loss: 49739.5703, Test Loss: 18644.1328\n",
      "Epoch 18, Train Loss: 39864.9336, Test Loss: 14767.9600\n",
      "Epoch 19, Train Loss: 31203.7070, Test Loss: 11527.5859\n",
      "Epoch 20, Train Loss: 32163.0020, Test Loss: 8962.8242\n",
      "Epoch 21, Train Loss: 31352.4922, Test Loss: 6987.3188\n",
      "Epoch 22, Train Loss: 27116.2812, Test Loss: 5520.7124\n",
      "Epoch 23, Train Loss: 22001.2363, Test Loss: 4475.8101\n",
      "Epoch 24, Train Loss: 24453.0801, Test Loss: 3737.0178\n",
      "Epoch 25, Train Loss: 26176.3984, Test Loss: 3245.6233\n",
      "Epoch 26, Train Loss: 19281.8691, Test Loss: 2884.6843\n",
      "Epoch 27, Train Loss: 20895.4180, Test Loss: 2633.9653\n",
      "Epoch 28, Train Loss: 21857.2227, Test Loss: 2462.7666\n",
      "Epoch 29, Train Loss: 16701.4316, Test Loss: 2339.9084\n",
      "Epoch 30, Train Loss: 18837.8496, Test Loss: 2234.5439\n",
      "Epoch 31, Train Loss: 18162.1426, Test Loss: 2146.3152\n",
      "Epoch 32, Train Loss: 15195.2070, Test Loss: 2063.6238\n",
      "Epoch 33, Train Loss: 15694.0156, Test Loss: 1996.1830\n",
      "Epoch 34, Train Loss: 15509.8711, Test Loss: 1929.1127\n",
      "Epoch 35, Train Loss: 15159.7021, Test Loss: 1869.6121\n",
      "Epoch 36, Train Loss: 15453.4414, Test Loss: 1825.0897\n",
      "Epoch 37, Train Loss: 13337.8203, Test Loss: 1776.3893\n",
      "Epoch 38, Train Loss: 14271.9199, Test Loss: 1749.4508\n",
      "Epoch 39, Train Loss: 13172.7383, Test Loss: 1732.2126\n",
      "Epoch 40, Train Loss: 12647.7959, Test Loss: 1736.9783\n",
      "Epoch 41, Train Loss: 11670.1367, Test Loss: 1757.2605\n",
      "Epoch 42, Train Loss: 9390.1143, Test Loss: 1807.1145\n",
      "Epoch 43, Train Loss: 10006.4346, Test Loss: 1869.9253\n",
      "Epoch 44, Train Loss: 9637.1318, Test Loss: 1941.8285\n",
      "Epoch 45, Train Loss: 9280.0293, Test Loss: 2017.8247\n",
      "Epoch 46, Train Loss: 8811.6797, Test Loss: 2080.7424\n",
      "Epoch 47, Train Loss: 8726.7891, Test Loss: 2134.0603\n",
      "Epoch 48, Train Loss: 8193.1963, Test Loss: 2180.0244\n",
      "Epoch 49, Train Loss: 7669.1104, Test Loss: 2214.8774\n",
      "Epoch 50, Train Loss: 7718.4082, Test Loss: 2244.5740\n",
      "Epoch 51, Train Loss: 8320.6318, Test Loss: 2266.7002\n",
      "Epoch 52, Train Loss: 7623.4502, Test Loss: 2283.1335\n",
      "Epoch 53, Train Loss: 6861.8203, Test Loss: 2293.9058\n",
      "Epoch 54, Train Loss: 6413.7734, Test Loss: 2293.6648\n",
      "Epoch 55, Train Loss: 6597.4385, Test Loss: 2286.7737\n",
      "Epoch 56, Train Loss: 6600.5166, Test Loss: 2266.7610\n",
      "Epoch 57, Train Loss: 6758.1733, Test Loss: 2239.7803\n",
      "Epoch 58, Train Loss: 6128.4062, Test Loss: 2208.0713\n",
      "Epoch 59, Train Loss: 6527.7720, Test Loss: 2160.1738\n",
      "Epoch 60, Train Loss: 5972.4663, Test Loss: 2097.2708\n",
      "Epoch 61, Train Loss: 5364.9990, Test Loss: 2035.9214\n",
      "Epoch 62, Train Loss: 6421.5381, Test Loss: 1980.1272\n",
      "Epoch 63, Train Loss: 5998.6094, Test Loss: 1926.9568\n",
      "Epoch 64, Train Loss: 6284.6963, Test Loss: 1869.9270\n",
      "Epoch 65, Train Loss: 5439.5474, Test Loss: 1816.1411\n",
      "Epoch 66, Train Loss: 5265.8423, Test Loss: 1768.8118\n",
      "Epoch 67, Train Loss: 5586.7798, Test Loss: 1725.2189\n",
      "Epoch 68, Train Loss: 4761.2124, Test Loss: 1680.4965\n",
      "Epoch 69, Train Loss: 4881.4189, Test Loss: 1645.1697\n",
      "Epoch 70, Train Loss: 4796.7969, Test Loss: 1613.1532\n",
      "Epoch 71, Train Loss: 5731.1548, Test Loss: 1582.3049\n",
      "Epoch 72, Train Loss: 4736.4507, Test Loss: 1552.2092\n",
      "Epoch 73, Train Loss: 4563.2061, Test Loss: 1521.1351\n",
      "Epoch 74, Train Loss: 4312.3740, Test Loss: 1493.5992\n",
      "Epoch 75, Train Loss: 4042.8547, Test Loss: 1470.5927\n",
      "Epoch 76, Train Loss: 3883.2373, Test Loss: 1448.2865\n",
      "Epoch 77, Train Loss: 4450.9482, Test Loss: 1419.4260\n",
      "Epoch 78, Train Loss: 4287.8115, Test Loss: 1387.2299\n",
      "Epoch 79, Train Loss: 4124.8105, Test Loss: 1357.8944\n",
      "Epoch 80, Train Loss: 4014.5154, Test Loss: 1330.6301\n",
      "Epoch 81, Train Loss: 3508.8318, Test Loss: 1306.0022\n",
      "Epoch 82, Train Loss: 3985.2332, Test Loss: 1280.7468\n",
      "Epoch 83, Train Loss: 4129.8774, Test Loss: 1259.3123\n",
      "Epoch 84, Train Loss: 4188.0293, Test Loss: 1235.7097\n",
      "Epoch 85, Train Loss: 3517.1594, Test Loss: 1215.1840\n",
      "Epoch 86, Train Loss: 3478.5464, Test Loss: 1197.0713\n",
      "Epoch 87, Train Loss: 3398.9185, Test Loss: 1178.9211\n",
      "Epoch 88, Train Loss: 3845.4392, Test Loss: 1161.1298\n",
      "Epoch 89, Train Loss: 3786.2712, Test Loss: 1145.1676\n",
      "Epoch 90, Train Loss: 3490.9053, Test Loss: 1130.2914\n",
      "Epoch 91, Train Loss: 3454.6021, Test Loss: 1112.8053\n",
      "Epoch 92, Train Loss: 3333.0427, Test Loss: 1098.1252\n",
      "Epoch 93, Train Loss: 3063.5554, Test Loss: 1084.6903\n",
      "Epoch 94, Train Loss: 3302.4832, Test Loss: 1074.3868\n",
      "Epoch 95, Train Loss: 3165.4463, Test Loss: 1065.5057\n",
      "Epoch 96, Train Loss: 3268.4893, Test Loss: 1056.1346\n",
      "Epoch 97, Train Loss: 3033.7043, Test Loss: 1048.9414\n",
      "Epoch 98, Train Loss: 2803.0217, Test Loss: 1041.4629\n",
      "Epoch 99, Train Loss: 3244.9380, Test Loss: 1032.5415\n",
      "Epoch 100, Train Loss: 2584.4065, Test Loss: 1020.9975\n",
      "Epoch 101, Train Loss: 2734.9192, Test Loss: 1009.4866\n",
      "Epoch 102, Train Loss: 3146.5972, Test Loss: 995.2703\n",
      "Epoch 103, Train Loss: 2650.9604, Test Loss: 983.3072\n",
      "Epoch 104, Train Loss: 2708.3894, Test Loss: 969.4161\n",
      "Epoch 105, Train Loss: 3266.8257, Test Loss: 953.0388\n",
      "Epoch 106, Train Loss: 2541.1016, Test Loss: 935.9411\n",
      "Epoch 107, Train Loss: 2851.9124, Test Loss: 921.3066\n",
      "Epoch 108, Train Loss: 2778.0051, Test Loss: 907.6169\n",
      "Epoch 109, Train Loss: 2601.4238, Test Loss: 892.8418\n",
      "Epoch 110, Train Loss: 2392.8367, Test Loss: 876.5650\n",
      "Epoch 111, Train Loss: 2484.0728, Test Loss: 861.3956\n",
      "Epoch 112, Train Loss: 2492.3953, Test Loss: 848.3340\n",
      "Epoch 113, Train Loss: 2303.5718, Test Loss: 833.4606\n",
      "Epoch 114, Train Loss: 2316.6580, Test Loss: 818.0525\n",
      "Epoch 115, Train Loss: 2323.1265, Test Loss: 802.8264\n",
      "Epoch 116, Train Loss: 2335.7908, Test Loss: 790.7148\n",
      "Epoch 117, Train Loss: 2331.4514, Test Loss: 779.7602\n",
      "Epoch 118, Train Loss: 2182.4673, Test Loss: 771.9551\n",
      "Epoch 119, Train Loss: 1952.0576, Test Loss: 765.5242\n",
      "Epoch 120, Train Loss: 2367.9216, Test Loss: 763.3411\n",
      "Epoch 121, Train Loss: 2146.7983, Test Loss: 758.4383\n",
      "Epoch 122, Train Loss: 2121.9106, Test Loss: 754.3069\n",
      "Epoch 123, Train Loss: 1975.0621, Test Loss: 752.3768\n",
      "Epoch 124, Train Loss: 1984.6470, Test Loss: 750.5449\n",
      "Epoch 125, Train Loss: 2055.7268, Test Loss: 750.6311\n",
      "Epoch 126, Train Loss: 2086.5547, Test Loss: 746.5501\n",
      "Epoch 127, Train Loss: 2115.9890, Test Loss: 745.3831\n",
      "Epoch 128, Train Loss: 1894.5239, Test Loss: 745.2155\n",
      "Epoch 129, Train Loss: 2028.5623, Test Loss: 744.1564\n",
      "Epoch 130, Train Loss: 1868.8428, Test Loss: 742.2692\n",
      "Epoch 131, Train Loss: 1887.9752, Test Loss: 740.7950\n",
      "Epoch 132, Train Loss: 1793.2872, Test Loss: 739.2832\n",
      "Epoch 133, Train Loss: 1706.1389, Test Loss: 737.7068\n",
      "Epoch 134, Train Loss: 1803.6877, Test Loss: 735.2460\n",
      "Epoch 135, Train Loss: 1956.9194, Test Loss: 730.1776\n",
      "Epoch 136, Train Loss: 1715.9304, Test Loss: 719.9835\n",
      "Epoch 137, Train Loss: 1842.4197, Test Loss: 708.7344\n",
      "Epoch 138, Train Loss: 1711.1411, Test Loss: 697.9540\n",
      "Epoch 139, Train Loss: 1722.3087, Test Loss: 686.6258\n",
      "Epoch 140, Train Loss: 1504.4751, Test Loss: 674.9436\n",
      "Epoch 141, Train Loss: 1878.3810, Test Loss: 663.3524\n",
      "Epoch 142, Train Loss: 1681.4124, Test Loss: 653.5765\n",
      "Epoch 143, Train Loss: 1868.5552, Test Loss: 644.2327\n",
      "Epoch 144, Train Loss: 1579.0149, Test Loss: 636.7711\n",
      "Epoch 145, Train Loss: 1845.3903, Test Loss: 628.8491\n",
      "Epoch 146, Train Loss: 1528.6722, Test Loss: 621.2021\n",
      "Epoch 147, Train Loss: 1501.3021, Test Loss: 612.9484\n",
      "Epoch 148, Train Loss: 1638.3489, Test Loss: 602.4738\n",
      "Epoch 149, Train Loss: 1653.5541, Test Loss: 593.8214\n",
      "Epoch 150, Train Loss: 1485.3635, Test Loss: 585.8053\n",
      "Epoch 151, Train Loss: 1485.3563, Test Loss: 576.6201\n",
      "Epoch 152, Train Loss: 1333.6049, Test Loss: 567.2000\n",
      "Epoch 153, Train Loss: 1490.8070, Test Loss: 559.0446\n",
      "Epoch 154, Train Loss: 1414.9343, Test Loss: 549.8697\n",
      "Epoch 155, Train Loss: 1517.7604, Test Loss: 541.5298\n",
      "Epoch 156, Train Loss: 1506.1567, Test Loss: 533.8809\n",
      "Epoch 157, Train Loss: 1498.8273, Test Loss: 527.9172\n",
      "Epoch 158, Train Loss: 1434.2070, Test Loss: 523.3835\n",
      "Epoch 159, Train Loss: 1479.4348, Test Loss: 519.5137\n",
      "Epoch 160, Train Loss: 1246.4780, Test Loss: 515.9525\n",
      "Epoch 161, Train Loss: 1172.2725, Test Loss: 513.0509\n",
      "Epoch 162, Train Loss: 1476.4664, Test Loss: 510.1958\n",
      "Epoch 163, Train Loss: 1214.9133, Test Loss: 509.1793\n",
      "Epoch 164, Train Loss: 1287.3867, Test Loss: 508.6222\n",
      "Epoch 165, Train Loss: 1465.5514, Test Loss: 509.4287\n",
      "Epoch 166, Train Loss: 1232.3740, Test Loss: 508.5183\n",
      "Epoch 167, Train Loss: 1177.2303, Test Loss: 506.1921\n",
      "Epoch 168, Train Loss: 1318.6647, Test Loss: 503.3118\n",
      "Epoch 169, Train Loss: 1333.6311, Test Loss: 500.9190\n",
      "Epoch 170, Train Loss: 1272.8625, Test Loss: 498.0470\n",
      "Epoch 171, Train Loss: 1187.4506, Test Loss: 495.6313\n",
      "Epoch 172, Train Loss: 1172.1344, Test Loss: 492.8690\n",
      "Epoch 173, Train Loss: 1201.3514, Test Loss: 489.3248\n",
      "Epoch 174, Train Loss: 1265.9028, Test Loss: 485.0356\n",
      "Epoch 175, Train Loss: 1256.6002, Test Loss: 479.3155\n",
      "Epoch 176, Train Loss: 1256.7437, Test Loss: 473.0259\n",
      "Epoch 177, Train Loss: 1168.8231, Test Loss: 468.4049\n",
      "Epoch 178, Train Loss: 1261.8657, Test Loss: 462.6280\n",
      "Epoch 179, Train Loss: 1209.2174, Test Loss: 458.7124\n",
      "Epoch 180, Train Loss: 1112.6740, Test Loss: 454.7224\n",
      "Epoch 181, Train Loss: 1132.7695, Test Loss: 450.1409\n",
      "Epoch 182, Train Loss: 1150.2241, Test Loss: 446.7879\n",
      "Epoch 183, Train Loss: 1062.4626, Test Loss: 443.9235\n",
      "Epoch 184, Train Loss: 1004.0983, Test Loss: 438.7908\n",
      "Epoch 185, Train Loss: 1040.1849, Test Loss: 434.1714\n",
      "Epoch 186, Train Loss: 1015.1281, Test Loss: 428.6847\n",
      "Epoch 187, Train Loss: 1156.9784, Test Loss: 422.7022\n",
      "Epoch 188, Train Loss: 939.6780, Test Loss: 417.6106\n",
      "Epoch 189, Train Loss: 963.4352, Test Loss: 414.0459\n",
      "Epoch 190, Train Loss: 1110.0046, Test Loss: 410.3763\n",
      "Epoch 191, Train Loss: 961.9683, Test Loss: 406.1729\n",
      "Epoch 192, Train Loss: 895.3961, Test Loss: 401.8043\n",
      "Epoch 193, Train Loss: 937.7983, Test Loss: 397.8067\n",
      "Epoch 194, Train Loss: 846.0654, Test Loss: 394.6424\n",
      "Epoch 195, Train Loss: 1067.2106, Test Loss: 392.1346\n",
      "Epoch 196, Train Loss: 950.0364, Test Loss: 389.4816\n",
      "Epoch 197, Train Loss: 931.8727, Test Loss: 386.5110\n",
      "Epoch 198, Train Loss: 978.2421, Test Loss: 383.8698\n",
      "Epoch 199, Train Loss: 943.1070, Test Loss: 380.9636\n",
      "Epoch 200, Train Loss: 898.3212, Test Loss: 378.4980\n",
      "Epoch 201, Train Loss: 932.5530, Test Loss: 375.8000\n",
      "Epoch 202, Train Loss: 920.1915, Test Loss: 373.1696\n",
      "Epoch 203, Train Loss: 880.2266, Test Loss: 369.3672\n",
      "Epoch 204, Train Loss: 955.7410, Test Loss: 365.6862\n",
      "Epoch 205, Train Loss: 865.8478, Test Loss: 362.5773\n",
      "Epoch 206, Train Loss: 793.7930, Test Loss: 359.3772\n",
      "Epoch 207, Train Loss: 793.5189, Test Loss: 356.6765\n",
      "Epoch 208, Train Loss: 802.3834, Test Loss: 354.1570\n",
      "Epoch 209, Train Loss: 808.9993, Test Loss: 350.9135\n",
      "Epoch 210, Train Loss: 763.9037, Test Loss: 347.1712\n",
      "Epoch 211, Train Loss: 874.6485, Test Loss: 343.4114\n",
      "Epoch 212, Train Loss: 697.7661, Test Loss: 338.8520\n",
      "Epoch 213, Train Loss: 762.7413, Test Loss: 334.7321\n",
      "Epoch 214, Train Loss: 893.7911, Test Loss: 329.9507\n",
      "Epoch 215, Train Loss: 869.7343, Test Loss: 325.0312\n",
      "Epoch 216, Train Loss: 879.5159, Test Loss: 320.6446\n",
      "Epoch 217, Train Loss: 802.4013, Test Loss: 316.1493\n",
      "Epoch 218, Train Loss: 821.1945, Test Loss: 311.4768\n",
      "Epoch 219, Train Loss: 693.8216, Test Loss: 306.4935\n",
      "Epoch 220, Train Loss: 750.8344, Test Loss: 301.3021\n",
      "Epoch 221, Train Loss: 736.3876, Test Loss: 296.9483\n",
      "Epoch 222, Train Loss: 840.8375, Test Loss: 293.3075\n",
      "Epoch 223, Train Loss: 852.0499, Test Loss: 289.5400\n",
      "Epoch 224, Train Loss: 727.0825, Test Loss: 285.0596\n",
      "Epoch 225, Train Loss: 729.0197, Test Loss: 282.1597\n",
      "Epoch 226, Train Loss: 694.2867, Test Loss: 279.0735\n",
      "Epoch 227, Train Loss: 676.5693, Test Loss: 276.5062\n",
      "Epoch 228, Train Loss: 759.4966, Test Loss: 273.5660\n",
      "Epoch 229, Train Loss: 755.4404, Test Loss: 270.4859\n",
      "Epoch 230, Train Loss: 663.8347, Test Loss: 268.2699\n",
      "Epoch 231, Train Loss: 708.3265, Test Loss: 266.5396\n",
      "Epoch 232, Train Loss: 637.3958, Test Loss: 264.6630\n",
      "Epoch 233, Train Loss: 648.2201, Test Loss: 262.5218\n",
      "Epoch 234, Train Loss: 629.1282, Test Loss: 259.9843\n",
      "Epoch 235, Train Loss: 762.4092, Test Loss: 257.3881\n",
      "Epoch 236, Train Loss: 660.5807, Test Loss: 255.5324\n",
      "Epoch 237, Train Loss: 691.6587, Test Loss: 252.5225\n",
      "Epoch 238, Train Loss: 610.2944, Test Loss: 249.8905\n",
      "Epoch 239, Train Loss: 653.2852, Test Loss: 247.4278\n",
      "Epoch 240, Train Loss: 697.3769, Test Loss: 244.8832\n",
      "Epoch 241, Train Loss: 625.9005, Test Loss: 242.3499\n",
      "Epoch 242, Train Loss: 702.8152, Test Loss: 240.4027\n",
      "Epoch 243, Train Loss: 641.0015, Test Loss: 238.2967\n",
      "Epoch 244, Train Loss: 631.6494, Test Loss: 235.9178\n",
      "Epoch 245, Train Loss: 682.7505, Test Loss: 233.9525\n",
      "Epoch 246, Train Loss: 617.5529, Test Loss: 232.1110\n",
      "Epoch 247, Train Loss: 635.0571, Test Loss: 230.6149\n",
      "Epoch 248, Train Loss: 549.1349, Test Loss: 228.6545\n",
      "Epoch 249, Train Loss: 636.1653, Test Loss: 226.4635\n",
      "Epoch 250, Train Loss: 543.2286, Test Loss: 224.9469\n",
      "Epoch 251, Train Loss: 625.2924, Test Loss: 223.6579\n",
      "Epoch 252, Train Loss: 597.2773, Test Loss: 222.5013\n",
      "Epoch 253, Train Loss: 537.7444, Test Loss: 221.1999\n",
      "Epoch 254, Train Loss: 534.7074, Test Loss: 219.7457\n",
      "Epoch 255, Train Loss: 576.6688, Test Loss: 218.2546\n",
      "Epoch 256, Train Loss: 516.8392, Test Loss: 217.1949\n",
      "Epoch 257, Train Loss: 578.3777, Test Loss: 215.8672\n",
      "Epoch 258, Train Loss: 624.1705, Test Loss: 214.5400\n",
      "Epoch 259, Train Loss: 566.9271, Test Loss: 213.0404\n",
      "Epoch 260, Train Loss: 509.5434, Test Loss: 211.4755\n",
      "Epoch 261, Train Loss: 494.4088, Test Loss: 209.7186\n",
      "Epoch 262, Train Loss: 562.9047, Test Loss: 207.5458\n",
      "Epoch 263, Train Loss: 542.7070, Test Loss: 204.8521\n",
      "Epoch 264, Train Loss: 504.0597, Test Loss: 201.9502\n",
      "Epoch 265, Train Loss: 546.4821, Test Loss: 199.1298\n",
      "Epoch 266, Train Loss: 549.9253, Test Loss: 197.1470\n",
      "Epoch 267, Train Loss: 528.1841, Test Loss: 195.2549\n",
      "Epoch 268, Train Loss: 590.8196, Test Loss: 193.6009\n",
      "Epoch 269, Train Loss: 548.1635, Test Loss: 191.8537\n",
      "Epoch 270, Train Loss: 590.3495, Test Loss: 189.6601\n",
      "Epoch 271, Train Loss: 531.7886, Test Loss: 187.0549\n",
      "Epoch 272, Train Loss: 572.5755, Test Loss: 184.7990\n",
      "Epoch 273, Train Loss: 572.6517, Test Loss: 182.2811\n",
      "Epoch 274, Train Loss: 518.7662, Test Loss: 179.5702\n",
      "Epoch 275, Train Loss: 524.6906, Test Loss: 176.8680\n",
      "Epoch 276, Train Loss: 488.0136, Test Loss: 174.6443\n",
      "Epoch 277, Train Loss: 436.1790, Test Loss: 172.6962\n",
      "Epoch 278, Train Loss: 445.3992, Test Loss: 170.9272\n",
      "Epoch 279, Train Loss: 461.4236, Test Loss: 169.0841\n",
      "Epoch 280, Train Loss: 454.0508, Test Loss: 167.3444\n",
      "Epoch 281, Train Loss: 485.1503, Test Loss: 165.2085\n",
      "Epoch 282, Train Loss: 540.6033, Test Loss: 162.6884\n",
      "Epoch 283, Train Loss: 495.9177, Test Loss: 160.0887\n",
      "Epoch 284, Train Loss: 514.7110, Test Loss: 157.9232\n",
      "Epoch 285, Train Loss: 505.3961, Test Loss: 156.2132\n",
      "Epoch 286, Train Loss: 398.0238, Test Loss: 154.5704\n",
      "Epoch 287, Train Loss: 461.2339, Test Loss: 153.3043\n",
      "Epoch 288, Train Loss: 446.4183, Test Loss: 152.3730\n",
      "Epoch 289, Train Loss: 457.0166, Test Loss: 151.6702\n",
      "Epoch 290, Train Loss: 457.4052, Test Loss: 151.3037\n",
      "Epoch 291, Train Loss: 496.6580, Test Loss: 150.9672\n",
      "Epoch 292, Train Loss: 436.7956, Test Loss: 150.5003\n",
      "Epoch 293, Train Loss: 451.8661, Test Loss: 150.2616\n",
      "Epoch 294, Train Loss: 517.2689, Test Loss: 149.6938\n",
      "Epoch 295, Train Loss: 420.2245, Test Loss: 148.9773\n",
      "Epoch 296, Train Loss: 393.8984, Test Loss: 148.3027\n",
      "Epoch 297, Train Loss: 363.8426, Test Loss: 147.5414\n",
      "Epoch 298, Train Loss: 388.7384, Test Loss: 146.4516\n",
      "Epoch 299, Train Loss: 475.2990, Test Loss: 145.6725\n",
      "Epoch 300, Train Loss: 429.0713, Test Loss: 144.7112\n",
      "Epoch 301, Train Loss: 433.5130, Test Loss: 143.7641\n",
      "Epoch 302, Train Loss: 442.1296, Test Loss: 142.9673\n",
      "Epoch 303, Train Loss: 442.0025, Test Loss: 142.4424\n",
      "Epoch 304, Train Loss: 444.0450, Test Loss: 141.8776\n",
      "Epoch 305, Train Loss: 397.0323, Test Loss: 141.2798\n",
      "Epoch 306, Train Loss: 394.7368, Test Loss: 140.7474\n",
      "Epoch 307, Train Loss: 451.2437, Test Loss: 140.2058\n",
      "Epoch 308, Train Loss: 430.3051, Test Loss: 140.0581\n",
      "Epoch 309, Train Loss: 444.7137, Test Loss: 139.6157\n",
      "Epoch 310, Train Loss: 411.6301, Test Loss: 139.4160\n",
      "Epoch 311, Train Loss: 376.4083, Test Loss: 139.0875\n",
      "Epoch 312, Train Loss: 428.4126, Test Loss: 138.6649\n",
      "Epoch 313, Train Loss: 380.2607, Test Loss: 137.9182\n",
      "Epoch 314, Train Loss: 391.8992, Test Loss: 136.8677\n",
      "Epoch 315, Train Loss: 387.6147, Test Loss: 135.7581\n",
      "Epoch 316, Train Loss: 388.1047, Test Loss: 134.5778\n",
      "Epoch 317, Train Loss: 369.8935, Test Loss: 133.0812\n",
      "Epoch 318, Train Loss: 363.3728, Test Loss: 131.3538\n",
      "Epoch 319, Train Loss: 330.0777, Test Loss: 129.3781\n",
      "Epoch 320, Train Loss: 386.7617, Test Loss: 127.2753\n",
      "Epoch 321, Train Loss: 331.6743, Test Loss: 125.0776\n",
      "Epoch 322, Train Loss: 363.8290, Test Loss: 122.8725\n",
      "Epoch 323, Train Loss: 352.7284, Test Loss: 120.8708\n",
      "Epoch 324, Train Loss: 343.7788, Test Loss: 118.9831\n",
      "Epoch 325, Train Loss: 348.6664, Test Loss: 117.2558\n",
      "Epoch 326, Train Loss: 379.5036, Test Loss: 115.5588\n",
      "Epoch 327, Train Loss: 305.7792, Test Loss: 113.9560\n",
      "Epoch 328, Train Loss: 366.0390, Test Loss: 112.4638\n",
      "Epoch 329, Train Loss: 311.4834, Test Loss: 110.9784\n",
      "Epoch 330, Train Loss: 309.5500, Test Loss: 109.7278\n",
      "Epoch 331, Train Loss: 323.4461, Test Loss: 108.7138\n",
      "Epoch 332, Train Loss: 347.4859, Test Loss: 108.0817\n",
      "Epoch 333, Train Loss: 351.2861, Test Loss: 107.7483\n",
      "Epoch 334, Train Loss: 368.3477, Test Loss: 107.2905\n",
      "Epoch 335, Train Loss: 341.0149, Test Loss: 106.9548\n",
      "Epoch 336, Train Loss: 344.6739, Test Loss: 106.3727\n",
      "Epoch 337, Train Loss: 348.9211, Test Loss: 105.8186\n",
      "Epoch 338, Train Loss: 314.7324, Test Loss: 105.3101\n",
      "Epoch 339, Train Loss: 302.5228, Test Loss: 104.7371\n",
      "Epoch 340, Train Loss: 325.9551, Test Loss: 104.1822\n",
      "Epoch 341, Train Loss: 342.6038, Test Loss: 103.8427\n",
      "Epoch 342, Train Loss: 329.7346, Test Loss: 103.3394\n",
      "Epoch 343, Train Loss: 288.9390, Test Loss: 102.9357\n",
      "Epoch 344, Train Loss: 333.7045, Test Loss: 102.5499\n",
      "Epoch 345, Train Loss: 314.6256, Test Loss: 102.2372\n",
      "Epoch 346, Train Loss: 310.9002, Test Loss: 101.7786\n",
      "Epoch 347, Train Loss: 346.4078, Test Loss: 101.2712\n",
      "Epoch 348, Train Loss: 286.2722, Test Loss: 100.4862\n",
      "Epoch 349, Train Loss: 297.7160, Test Loss: 99.7082\n",
      "Epoch 350, Train Loss: 301.3974, Test Loss: 98.6614\n",
      "Epoch 351, Train Loss: 302.2190, Test Loss: 97.6793\n",
      "Epoch 352, Train Loss: 303.0200, Test Loss: 96.8595\n",
      "Epoch 353, Train Loss: 303.1989, Test Loss: 96.0331\n",
      "Epoch 354, Train Loss: 281.6788, Test Loss: 95.2464\n",
      "Epoch 355, Train Loss: 307.2722, Test Loss: 94.5100\n",
      "Epoch 356, Train Loss: 270.3284, Test Loss: 93.8079\n",
      "Epoch 357, Train Loss: 279.1521, Test Loss: 93.1483\n",
      "Epoch 358, Train Loss: 293.4448, Test Loss: 92.5687\n",
      "Epoch 359, Train Loss: 295.5591, Test Loss: 91.9009\n",
      "Epoch 360, Train Loss: 267.4100, Test Loss: 91.2966\n",
      "Epoch 361, Train Loss: 310.9836, Test Loss: 90.5770\n",
      "Epoch 362, Train Loss: 276.7522, Test Loss: 89.8375\n",
      "Epoch 363, Train Loss: 293.3511, Test Loss: 89.2968\n",
      "Epoch 364, Train Loss: 288.5550, Test Loss: 88.7243\n",
      "Epoch 365, Train Loss: 284.3195, Test Loss: 88.2046\n",
      "Epoch 366, Train Loss: 283.2777, Test Loss: 87.6718\n",
      "Epoch 367, Train Loss: 277.1425, Test Loss: 87.4000\n",
      "Epoch 368, Train Loss: 257.8378, Test Loss: 87.0479\n",
      "Epoch 369, Train Loss: 246.7143, Test Loss: 86.7275\n",
      "Epoch 370, Train Loss: 305.8757, Test Loss: 86.4860\n",
      "Epoch 371, Train Loss: 284.2499, Test Loss: 86.1347\n",
      "Epoch 372, Train Loss: 265.9662, Test Loss: 85.7860\n",
      "Epoch 373, Train Loss: 268.8322, Test Loss: 85.3632\n",
      "Epoch 374, Train Loss: 253.9697, Test Loss: 84.8534\n",
      "Epoch 375, Train Loss: 267.2198, Test Loss: 84.3194\n",
      "Epoch 376, Train Loss: 306.2998, Test Loss: 83.6820\n",
      "Epoch 377, Train Loss: 248.8985, Test Loss: 82.9506\n",
      "Epoch 378, Train Loss: 227.2151, Test Loss: 82.3051\n",
      "Epoch 379, Train Loss: 293.8132, Test Loss: 81.7362\n",
      "Epoch 380, Train Loss: 259.6413, Test Loss: 81.2398\n",
      "Epoch 381, Train Loss: 294.3553, Test Loss: 80.8532\n",
      "Epoch 382, Train Loss: 277.9977, Test Loss: 80.4767\n",
      "Epoch 383, Train Loss: 271.4695, Test Loss: 79.9327\n",
      "Epoch 384, Train Loss: 257.7817, Test Loss: 79.6195\n",
      "Epoch 385, Train Loss: 243.9276, Test Loss: 79.2522\n",
      "Epoch 386, Train Loss: 236.1877, Test Loss: 78.8932\n",
      "Epoch 387, Train Loss: 254.4729, Test Loss: 78.5029\n",
      "Epoch 388, Train Loss: 287.8619, Test Loss: 78.0571\n",
      "Epoch 389, Train Loss: 264.9324, Test Loss: 77.3926\n",
      "Epoch 390, Train Loss: 244.8259, Test Loss: 76.8632\n",
      "Epoch 391, Train Loss: 218.3294, Test Loss: 76.2388\n",
      "Epoch 392, Train Loss: 297.5484, Test Loss: 75.6042\n",
      "Epoch 393, Train Loss: 238.0791, Test Loss: 75.0597\n",
      "Epoch 394, Train Loss: 254.8867, Test Loss: 74.4627\n",
      "Epoch 395, Train Loss: 258.1110, Test Loss: 73.9821\n",
      "Epoch 396, Train Loss: 232.7957, Test Loss: 73.3155\n",
      "Epoch 397, Train Loss: 236.7141, Test Loss: 72.7268\n",
      "Epoch 398, Train Loss: 242.5750, Test Loss: 72.3247\n",
      "Epoch 399, Train Loss: 249.8427, Test Loss: 71.9540\n",
      "Epoch 400, Train Loss: 244.0706, Test Loss: 71.6325\n",
      "Epoch 401, Train Loss: 229.2940, Test Loss: 71.2335\n",
      "Epoch 402, Train Loss: 235.8288, Test Loss: 70.8261\n",
      "Epoch 403, Train Loss: 252.3605, Test Loss: 70.5150\n",
      "Epoch 404, Train Loss: 237.6725, Test Loss: 70.1802\n",
      "Epoch 405, Train Loss: 220.9055, Test Loss: 69.9386\n",
      "Epoch 406, Train Loss: 241.8817, Test Loss: 69.6413\n",
      "Epoch 407, Train Loss: 228.8721, Test Loss: 69.2653\n",
      "Epoch 408, Train Loss: 210.1369, Test Loss: 68.7477\n",
      "Epoch 409, Train Loss: 228.4987, Test Loss: 68.2132\n",
      "Epoch 410, Train Loss: 201.9184, Test Loss: 67.6609\n",
      "Epoch 411, Train Loss: 192.6003, Test Loss: 67.0159\n",
      "Epoch 412, Train Loss: 234.0241, Test Loss: 66.4927\n",
      "Epoch 413, Train Loss: 231.6841, Test Loss: 66.0545\n",
      "Epoch 414, Train Loss: 233.4096, Test Loss: 65.6140\n",
      "Epoch 415, Train Loss: 245.9829, Test Loss: 65.3228\n",
      "Epoch 416, Train Loss: 250.2511, Test Loss: 64.8266\n",
      "Epoch 417, Train Loss: 235.5142, Test Loss: 64.4459\n",
      "Epoch 418, Train Loss: 229.5056, Test Loss: 64.0474\n",
      "Epoch 419, Train Loss: 247.5889, Test Loss: 63.7250\n",
      "Epoch 420, Train Loss: 181.9558, Test Loss: 63.3855\n",
      "Epoch 421, Train Loss: 201.1699, Test Loss: 63.0051\n",
      "Epoch 422, Train Loss: 204.3436, Test Loss: 62.5104\n",
      "Epoch 423, Train Loss: 194.9538, Test Loss: 61.9108\n",
      "Epoch 424, Train Loss: 216.8236, Test Loss: 61.5182\n",
      "Epoch 425, Train Loss: 233.5415, Test Loss: 61.1380\n",
      "Epoch 426, Train Loss: 200.5104, Test Loss: 60.7930\n",
      "Epoch 427, Train Loss: 211.0246, Test Loss: 60.4836\n",
      "Epoch 428, Train Loss: 197.7920, Test Loss: 60.1765\n",
      "Epoch 429, Train Loss: 187.7122, Test Loss: 59.8594\n",
      "Epoch 430, Train Loss: 191.1565, Test Loss: 59.4836\n",
      "Epoch 431, Train Loss: 185.3898, Test Loss: 58.9770\n",
      "Epoch 432, Train Loss: 195.2441, Test Loss: 58.5775\n",
      "Epoch 433, Train Loss: 201.3394, Test Loss: 58.1867\n",
      "Epoch 434, Train Loss: 185.9334, Test Loss: 57.6445\n",
      "Epoch 435, Train Loss: 194.4725, Test Loss: 57.1261\n",
      "Epoch 436, Train Loss: 185.4021, Test Loss: 56.6199\n",
      "Epoch 437, Train Loss: 223.8423, Test Loss: 55.9718\n",
      "Epoch 438, Train Loss: 199.8590, Test Loss: 55.3416\n",
      "Epoch 439, Train Loss: 207.4902, Test Loss: 54.7813\n",
      "Epoch 440, Train Loss: 237.4105, Test Loss: 54.1934\n",
      "Epoch 441, Train Loss: 202.6178, Test Loss: 53.7600\n",
      "Epoch 442, Train Loss: 223.2559, Test Loss: 53.3372\n",
      "Epoch 443, Train Loss: 198.6051, Test Loss: 52.9373\n",
      "Epoch 444, Train Loss: 169.7296, Test Loss: 52.5590\n",
      "Epoch 445, Train Loss: 209.7317, Test Loss: 52.2471\n",
      "Epoch 446, Train Loss: 151.7983, Test Loss: 52.0109\n",
      "Epoch 447, Train Loss: 161.6185, Test Loss: 51.8457\n",
      "Epoch 448, Train Loss: 210.1678, Test Loss: 51.6679\n",
      "Epoch 449, Train Loss: 187.6554, Test Loss: 51.4840\n",
      "Epoch 450, Train Loss: 179.1381, Test Loss: 51.3203\n",
      "Epoch 451, Train Loss: 212.1282, Test Loss: 51.1158\n",
      "Epoch 452, Train Loss: 203.0343, Test Loss: 50.9040\n",
      "Epoch 453, Train Loss: 173.5444, Test Loss: 50.6638\n",
      "Epoch 454, Train Loss: 174.7487, Test Loss: 50.5437\n",
      "Epoch 455, Train Loss: 192.5611, Test Loss: 50.4246\n",
      "Epoch 456, Train Loss: 201.5590, Test Loss: 50.2051\n",
      "Epoch 457, Train Loss: 168.9922, Test Loss: 50.0380\n",
      "Epoch 458, Train Loss: 170.3971, Test Loss: 49.6848\n",
      "Epoch 459, Train Loss: 162.6368, Test Loss: 49.3459\n",
      "Epoch 460, Train Loss: 163.3849, Test Loss: 49.0001\n",
      "Epoch 461, Train Loss: 178.5981, Test Loss: 48.5522\n",
      "Epoch 462, Train Loss: 157.6561, Test Loss: 48.1988\n",
      "Epoch 463, Train Loss: 163.3590, Test Loss: 47.8492\n",
      "Epoch 464, Train Loss: 174.9790, Test Loss: 47.4921\n",
      "Epoch 465, Train Loss: 178.9023, Test Loss: 47.1862\n",
      "Epoch 466, Train Loss: 178.5167, Test Loss: 46.7642\n",
      "Epoch 467, Train Loss: 188.4099, Test Loss: 46.2812\n",
      "Epoch 468, Train Loss: 180.5609, Test Loss: 45.8602\n",
      "Epoch 469, Train Loss: 170.7030, Test Loss: 45.2628\n",
      "Epoch 470, Train Loss: 166.2693, Test Loss: 44.7306\n",
      "Epoch 471, Train Loss: 163.8628, Test Loss: 44.2216\n",
      "Epoch 472, Train Loss: 171.1640, Test Loss: 43.8380\n",
      "Epoch 473, Train Loss: 162.8291, Test Loss: 43.4686\n",
      "Epoch 474, Train Loss: 187.5148, Test Loss: 43.1727\n",
      "Epoch 475, Train Loss: 168.0023, Test Loss: 42.8755\n",
      "Epoch 476, Train Loss: 175.2696, Test Loss: 42.6608\n",
      "Epoch 477, Train Loss: 168.1789, Test Loss: 42.4352\n",
      "Epoch 478, Train Loss: 163.4259, Test Loss: 42.2212\n",
      "Epoch 479, Train Loss: 160.5023, Test Loss: 42.0618\n",
      "Epoch 480, Train Loss: 180.7253, Test Loss: 42.0082\n",
      "Epoch 481, Train Loss: 168.2561, Test Loss: 41.8351\n",
      "Epoch 482, Train Loss: 159.3457, Test Loss: 41.7560\n",
      "Epoch 483, Train Loss: 158.3435, Test Loss: 41.6478\n",
      "Epoch 484, Train Loss: 148.8433, Test Loss: 41.5117\n",
      "Epoch 485, Train Loss: 177.0954, Test Loss: 41.4860\n",
      "Epoch 486, Train Loss: 147.4654, Test Loss: 41.3701\n",
      "Epoch 487, Train Loss: 153.0990, Test Loss: 41.2310\n",
      "Epoch 488, Train Loss: 166.1248, Test Loss: 41.0321\n",
      "Epoch 489, Train Loss: 178.4534, Test Loss: 40.7350\n",
      "Epoch 490, Train Loss: 164.1402, Test Loss: 40.4791\n",
      "Epoch 491, Train Loss: 152.6808, Test Loss: 40.2822\n",
      "Epoch 492, Train Loss: 158.0928, Test Loss: 39.9697\n",
      "Epoch 493, Train Loss: 167.3859, Test Loss: 39.6491\n",
      "Epoch 494, Train Loss: 142.7529, Test Loss: 39.2400\n",
      "Epoch 495, Train Loss: 141.5010, Test Loss: 38.8926\n",
      "Epoch 496, Train Loss: 147.8656, Test Loss: 38.4661\n",
      "Epoch 497, Train Loss: 160.3978, Test Loss: 38.0512\n",
      "Epoch 498, Train Loss: 142.7859, Test Loss: 37.5833\n",
      "Epoch 499, Train Loss: 138.3161, Test Loss: 37.1176\n",
      "Epoch 500, Train Loss: 149.7039, Test Loss: 36.7468\n",
      "Epoch 501, Train Loss: 140.5941, Test Loss: 36.4217\n",
      "Epoch 502, Train Loss: 149.4650, Test Loss: 36.1291\n",
      "Epoch 503, Train Loss: 154.6922, Test Loss: 35.8398\n",
      "Epoch 504, Train Loss: 131.5400, Test Loss: 35.5788\n",
      "Epoch 505, Train Loss: 158.2966, Test Loss: 35.4326\n",
      "Epoch 506, Train Loss: 135.2933, Test Loss: 35.2775\n",
      "Epoch 507, Train Loss: 154.3911, Test Loss: 35.1554\n",
      "Epoch 508, Train Loss: 130.5987, Test Loss: 35.0805\n",
      "Epoch 509, Train Loss: 131.7608, Test Loss: 34.8860\n",
      "Epoch 510, Train Loss: 134.3950, Test Loss: 34.7429\n",
      "Epoch 511, Train Loss: 150.0463, Test Loss: 34.5985\n",
      "Epoch 512, Train Loss: 150.1704, Test Loss: 34.5324\n",
      "Epoch 513, Train Loss: 144.8316, Test Loss: 34.4657\n",
      "Epoch 514, Train Loss: 131.9536, Test Loss: 34.4567\n",
      "Epoch 515, Train Loss: 146.6924, Test Loss: 34.3859\n",
      "Epoch 516, Train Loss: 156.1448, Test Loss: 34.2847\n",
      "Epoch 517, Train Loss: 147.2327, Test Loss: 34.0433\n",
      "Epoch 518, Train Loss: 143.8425, Test Loss: 33.8625\n",
      "Epoch 519, Train Loss: 129.8037, Test Loss: 33.6689\n",
      "Epoch 520, Train Loss: 134.5605, Test Loss: 33.5602\n",
      "Epoch 521, Train Loss: 151.5161, Test Loss: 33.3831\n",
      "Epoch 522, Train Loss: 129.3647, Test Loss: 33.2693\n",
      "Epoch 523, Train Loss: 138.8454, Test Loss: 33.2156\n",
      "Epoch 524, Train Loss: 141.0868, Test Loss: 33.1466\n",
      "Epoch 525, Train Loss: 136.7447, Test Loss: 33.0320\n",
      "Epoch 526, Train Loss: 147.1374, Test Loss: 32.9782\n",
      "Epoch 527, Train Loss: 142.0160, Test Loss: 32.9089\n",
      "Epoch 528, Train Loss: 124.3192, Test Loss: 32.8150\n",
      "Epoch 529, Train Loss: 120.3422, Test Loss: 32.6933\n",
      "Epoch 530, Train Loss: 109.7299, Test Loss: 32.5698\n",
      "Epoch 531, Train Loss: 150.5450, Test Loss: 32.4525\n",
      "Epoch 532, Train Loss: 127.5856, Test Loss: 32.2867\n",
      "Epoch 533, Train Loss: 117.8758, Test Loss: 32.1475\n",
      "Epoch 534, Train Loss: 141.4044, Test Loss: 31.9443\n",
      "Epoch 535, Train Loss: 152.8863, Test Loss: 31.7581\n",
      "Epoch 536, Train Loss: 142.1658, Test Loss: 31.6371\n",
      "Epoch 537, Train Loss: 106.1439, Test Loss: 31.5438\n",
      "Epoch 538, Train Loss: 134.9220, Test Loss: 31.3933\n",
      "Epoch 539, Train Loss: 109.4757, Test Loss: 31.2442\n",
      "Epoch 540, Train Loss: 130.5503, Test Loss: 31.0897\n",
      "Epoch 541, Train Loss: 143.2886, Test Loss: 30.9337\n",
      "Epoch 542, Train Loss: 105.1949, Test Loss: 30.7772\n",
      "Epoch 543, Train Loss: 121.4775, Test Loss: 30.6398\n",
      "Epoch 544, Train Loss: 105.1952, Test Loss: 30.5220\n",
      "Epoch 545, Train Loss: 135.8275, Test Loss: 30.3931\n",
      "Epoch 546, Train Loss: 132.0540, Test Loss: 30.2472\n",
      "Epoch 547, Train Loss: 128.1603, Test Loss: 30.1168\n",
      "Epoch 548, Train Loss: 96.4380, Test Loss: 29.9742\n",
      "Epoch 549, Train Loss: 131.0923, Test Loss: 29.7827\n",
      "Epoch 550, Train Loss: 131.8116, Test Loss: 29.5688\n",
      "Epoch 551, Train Loss: 124.7107, Test Loss: 29.3491\n",
      "Epoch 552, Train Loss: 117.0412, Test Loss: 29.1359\n",
      "Epoch 553, Train Loss: 131.7221, Test Loss: 28.9381\n",
      "Epoch 554, Train Loss: 124.3241, Test Loss: 28.7621\n",
      "Epoch 555, Train Loss: 120.7718, Test Loss: 28.5558\n",
      "Epoch 556, Train Loss: 120.3138, Test Loss: 28.3606\n",
      "Epoch 557, Train Loss: 118.9558, Test Loss: 28.1946\n",
      "Epoch 558, Train Loss: 118.7032, Test Loss: 27.9767\n",
      "Epoch 559, Train Loss: 124.1863, Test Loss: 27.7858\n",
      "Epoch 560, Train Loss: 129.4493, Test Loss: 27.5707\n",
      "Epoch 561, Train Loss: 131.6089, Test Loss: 27.4020\n",
      "Epoch 562, Train Loss: 103.1398, Test Loss: 27.2575\n",
      "Epoch 563, Train Loss: 118.5548, Test Loss: 27.1174\n",
      "Epoch 564, Train Loss: 106.4173, Test Loss: 26.9804\n",
      "Epoch 565, Train Loss: 112.7187, Test Loss: 26.8514\n",
      "Epoch 566, Train Loss: 103.3213, Test Loss: 26.7115\n",
      "Epoch 567, Train Loss: 119.4599, Test Loss: 26.6395\n",
      "Epoch 568, Train Loss: 117.6735, Test Loss: 26.5449\n",
      "Epoch 569, Train Loss: 129.8850, Test Loss: 26.4549\n",
      "Epoch 570, Train Loss: 128.9659, Test Loss: 26.3763\n",
      "Epoch 571, Train Loss: 130.5542, Test Loss: 26.2571\n",
      "Epoch 572, Train Loss: 144.2608, Test Loss: 26.1941\n",
      "Epoch 573, Train Loss: 118.1869, Test Loss: 26.1678\n",
      "Epoch 574, Train Loss: 93.9608, Test Loss: 26.0856\n",
      "Epoch 575, Train Loss: 118.7570, Test Loss: 26.0120\n",
      "Epoch 576, Train Loss: 107.4070, Test Loss: 25.9400\n",
      "Epoch 577, Train Loss: 112.7236, Test Loss: 25.8973\n",
      "Epoch 578, Train Loss: 113.9546, Test Loss: 25.9055\n",
      "Epoch 579, Train Loss: 129.8370, Test Loss: 25.9063\n",
      "Epoch 580, Train Loss: 115.0334, Test Loss: 25.8573\n",
      "Epoch 581, Train Loss: 124.2442, Test Loss: 25.7770\n",
      "Epoch 582, Train Loss: 110.4016, Test Loss: 25.7000\n",
      "Epoch 583, Train Loss: 115.3404, Test Loss: 25.5627\n",
      "Epoch 584, Train Loss: 105.7497, Test Loss: 25.3976\n",
      "Epoch 585, Train Loss: 119.2752, Test Loss: 25.2370\n",
      "Epoch 586, Train Loss: 110.9905, Test Loss: 25.0733\n",
      "Epoch 587, Train Loss: 104.8709, Test Loss: 24.8999\n",
      "Epoch 588, Train Loss: 114.0592, Test Loss: 24.6921\n",
      "Epoch 589, Train Loss: 99.1539, Test Loss: 24.4844\n",
      "Epoch 590, Train Loss: 95.0924, Test Loss: 24.3100\n",
      "Epoch 591, Train Loss: 100.1597, Test Loss: 24.1006\n",
      "Epoch 592, Train Loss: 103.8519, Test Loss: 23.9801\n",
      "Epoch 593, Train Loss: 105.6886, Test Loss: 23.7847\n",
      "Epoch 594, Train Loss: 108.7957, Test Loss: 23.5817\n",
      "Epoch 595, Train Loss: 121.2066, Test Loss: 23.4228\n",
      "Epoch 596, Train Loss: 96.2659, Test Loss: 23.2659\n",
      "Epoch 597, Train Loss: 119.7542, Test Loss: 23.1276\n",
      "Epoch 598, Train Loss: 108.7353, Test Loss: 22.9486\n",
      "Epoch 599, Train Loss: 126.0391, Test Loss: 22.7849\n",
      "Epoch 600, Train Loss: 93.3643, Test Loss: 22.6477\n",
      "Epoch 601, Train Loss: 85.3183, Test Loss: 22.5083\n",
      "Epoch 602, Train Loss: 111.7473, Test Loss: 22.4068\n",
      "Epoch 603, Train Loss: 118.5735, Test Loss: 22.3293\n",
      "Epoch 604, Train Loss: 132.7894, Test Loss: 22.2610\n",
      "Epoch 605, Train Loss: 111.9915, Test Loss: 22.1929\n",
      "Epoch 606, Train Loss: 103.4979, Test Loss: 22.0829\n",
      "Epoch 607, Train Loss: 97.6609, Test Loss: 21.9989\n",
      "Epoch 608, Train Loss: 84.8578, Test Loss: 21.9006\n",
      "Epoch 609, Train Loss: 107.6254, Test Loss: 21.8230\n",
      "Epoch 610, Train Loss: 103.7630, Test Loss: 21.7891\n",
      "Epoch 611, Train Loss: 99.4048, Test Loss: 21.7797\n",
      "Epoch 612, Train Loss: 87.0054, Test Loss: 21.7491\n",
      "Epoch 613, Train Loss: 126.7829, Test Loss: 21.6958\n",
      "Epoch 614, Train Loss: 113.2203, Test Loss: 21.6923\n",
      "Epoch 615, Train Loss: 119.8873, Test Loss: 21.7042\n",
      "Epoch 616, Train Loss: 114.7890, Test Loss: 21.7039\n",
      "Epoch 617, Train Loss: 109.2379, Test Loss: 21.7524\n",
      "Epoch 618, Train Loss: 105.7399, Test Loss: 21.8400\n",
      "Epoch 619, Train Loss: 109.2817, Test Loss: 21.9103\n",
      "Epoch 620, Train Loss: 85.3948, Test Loss: 21.8995\n",
      "Epoch 621, Train Loss: 84.6201, Test Loss: 21.8374\n",
      "Epoch 622, Train Loss: 92.3836, Test Loss: 21.7612\n",
      "Epoch 623, Train Loss: 114.6892, Test Loss: 21.6569\n",
      "Epoch 624, Train Loss: 98.7321, Test Loss: 21.5689\n",
      "Epoch 625, Train Loss: 108.7902, Test Loss: 21.5071\n",
      "Epoch 626, Train Loss: 88.0331, Test Loss: 21.4425\n",
      "Epoch 627, Train Loss: 101.2817, Test Loss: 21.3303\n",
      "Epoch 628, Train Loss: 100.3140, Test Loss: 21.2755\n",
      "Epoch 629, Train Loss: 85.8985, Test Loss: 21.2072\n",
      "Epoch 630, Train Loss: 95.2893, Test Loss: 21.1824\n",
      "Epoch 631, Train Loss: 92.5659, Test Loss: 21.1808\n",
      "Epoch 632, Train Loss: 97.2037, Test Loss: 21.1966\n",
      "Epoch 633, Train Loss: 84.8451, Test Loss: 21.2121\n",
      "Epoch 634, Train Loss: 93.9430, Test Loss: 21.1827\n",
      "Epoch 635, Train Loss: 87.7034, Test Loss: 21.1439\n",
      "Epoch 636, Train Loss: 78.4567, Test Loss: 21.0980\n",
      "Epoch 637, Train Loss: 107.2878, Test Loss: 20.9693\n",
      "Epoch 638, Train Loss: 111.4454, Test Loss: 20.8306\n",
      "Epoch 639, Train Loss: 84.6160, Test Loss: 20.6407\n",
      "Epoch 640, Train Loss: 96.9202, Test Loss: 20.4734\n",
      "Epoch 641, Train Loss: 85.1291, Test Loss: 20.3106\n",
      "Epoch 642, Train Loss: 85.7350, Test Loss: 20.1630\n",
      "Epoch 643, Train Loss: 87.4063, Test Loss: 20.0126\n",
      "Epoch 644, Train Loss: 73.2997, Test Loss: 19.8521\n",
      "Epoch 645, Train Loss: 91.6586, Test Loss: 19.6842\n",
      "Epoch 646, Train Loss: 87.3188, Test Loss: 19.5585\n",
      "Epoch 647, Train Loss: 102.0115, Test Loss: 19.4377\n",
      "Epoch 648, Train Loss: 88.6849, Test Loss: 19.3268\n",
      "Epoch 649, Train Loss: 89.3603, Test Loss: 19.2178\n",
      "Epoch 650, Train Loss: 89.4678, Test Loss: 19.0930\n",
      "Epoch 651, Train Loss: 64.3591, Test Loss: 19.0041\n",
      "Epoch 652, Train Loss: 80.9023, Test Loss: 18.9049\n",
      "Epoch 653, Train Loss: 79.3943, Test Loss: 18.8371\n",
      "Epoch 654, Train Loss: 81.4646, Test Loss: 18.7588\n",
      "Epoch 655, Train Loss: 96.0920, Test Loss: 18.7101\n",
      "Epoch 656, Train Loss: 82.6350, Test Loss: 18.6081\n",
      "Epoch 657, Train Loss: 82.3551, Test Loss: 18.5512\n",
      "Epoch 658, Train Loss: 78.7267, Test Loss: 18.4885\n",
      "Epoch 659, Train Loss: 76.5248, Test Loss: 18.4470\n",
      "Epoch 660, Train Loss: 73.7961, Test Loss: 18.4213\n",
      "Epoch 661, Train Loss: 87.6702, Test Loss: 18.4086\n",
      "Epoch 662, Train Loss: 79.3528, Test Loss: 18.4124\n",
      "Epoch 663, Train Loss: 91.5037, Test Loss: 18.3971\n",
      "Epoch 664, Train Loss: 96.4283, Test Loss: 18.4161\n",
      "Epoch 665, Train Loss: 88.4552, Test Loss: 18.4797\n",
      "Epoch 666, Train Loss: 91.7306, Test Loss: 18.5053\n",
      "Epoch 667, Train Loss: 79.4104, Test Loss: 18.5305\n",
      "Epoch 668, Train Loss: 99.4708, Test Loss: 18.5006\n",
      "Epoch 669, Train Loss: 89.5313, Test Loss: 18.4884\n",
      "Epoch 670, Train Loss: 109.2442, Test Loss: 18.4507\n",
      "Epoch 671, Train Loss: 77.8417, Test Loss: 18.3903\n",
      "Epoch 672, Train Loss: 94.0345, Test Loss: 18.3617\n",
      "Epoch 673, Train Loss: 84.6135, Test Loss: 18.2950\n",
      "Epoch 674, Train Loss: 72.5625, Test Loss: 18.2511\n",
      "Epoch 675, Train Loss: 81.0089, Test Loss: 18.1589\n",
      "Epoch 676, Train Loss: 74.7072, Test Loss: 18.0678\n",
      "Epoch 677, Train Loss: 84.3866, Test Loss: 17.9267\n",
      "Epoch 678, Train Loss: 86.2992, Test Loss: 17.8035\n",
      "Epoch 679, Train Loss: 78.1538, Test Loss: 17.6908\n",
      "Epoch 680, Train Loss: 75.1846, Test Loss: 17.5636\n",
      "Epoch 681, Train Loss: 82.7979, Test Loss: 17.4411\n",
      "Epoch 682, Train Loss: 81.7349, Test Loss: 17.3346\n",
      "Epoch 683, Train Loss: 87.0899, Test Loss: 17.2355\n",
      "Epoch 684, Train Loss: 79.4746, Test Loss: 17.1415\n",
      "Epoch 685, Train Loss: 81.5958, Test Loss: 17.0698\n",
      "Epoch 686, Train Loss: 82.9211, Test Loss: 17.0221\n",
      "Epoch 687, Train Loss: 66.4255, Test Loss: 16.9655\n",
      "Epoch 688, Train Loss: 72.7975, Test Loss: 16.9350\n",
      "Epoch 689, Train Loss: 63.6970, Test Loss: 16.8982\n",
      "Epoch 690, Train Loss: 79.5255, Test Loss: 16.8457\n",
      "Epoch 691, Train Loss: 80.0843, Test Loss: 16.7918\n",
      "Epoch 692, Train Loss: 84.1514, Test Loss: 16.7473\n",
      "Epoch 693, Train Loss: 78.8561, Test Loss: 16.7133\n",
      "Epoch 694, Train Loss: 73.5474, Test Loss: 16.6560\n",
      "Epoch 695, Train Loss: 83.0972, Test Loss: 16.5833\n",
      "Epoch 696, Train Loss: 77.1025, Test Loss: 16.5153\n",
      "Epoch 697, Train Loss: 85.2671, Test Loss: 16.4612\n",
      "Epoch 698, Train Loss: 80.7502, Test Loss: 16.4299\n",
      "Epoch 699, Train Loss: 74.8444, Test Loss: 16.4022\n",
      "Epoch 700, Train Loss: 85.1175, Test Loss: 16.3840\n",
      "Epoch 701, Train Loss: 82.2487, Test Loss: 16.3573\n",
      "Epoch 702, Train Loss: 77.9044, Test Loss: 16.3197\n",
      "Epoch 703, Train Loss: 66.4776, Test Loss: 16.2539\n",
      "Epoch 704, Train Loss: 66.7700, Test Loss: 16.1817\n",
      "Epoch 705, Train Loss: 71.1715, Test Loss: 16.0854\n",
      "Epoch 706, Train Loss: 66.9771, Test Loss: 15.9721\n",
      "Epoch 707, Train Loss: 77.8683, Test Loss: 15.8652\n",
      "Epoch 708, Train Loss: 77.4803, Test Loss: 15.7590\n",
      "Epoch 709, Train Loss: 71.7957, Test Loss: 15.6573\n",
      "Epoch 710, Train Loss: 76.5171, Test Loss: 15.5521\n",
      "Epoch 711, Train Loss: 64.2939, Test Loss: 15.4417\n",
      "Epoch 712, Train Loss: 72.3345, Test Loss: 15.3162\n",
      "Epoch 713, Train Loss: 69.6606, Test Loss: 15.1981\n",
      "Epoch 714, Train Loss: 96.6295, Test Loss: 15.0955\n",
      "Epoch 715, Train Loss: 90.9767, Test Loss: 14.9919\n",
      "Epoch 716, Train Loss: 81.4250, Test Loss: 14.8982\n",
      "Epoch 717, Train Loss: 74.8128, Test Loss: 14.8112\n",
      "Epoch 718, Train Loss: 72.5864, Test Loss: 14.7493\n",
      "Epoch 719, Train Loss: 84.8788, Test Loss: 14.6962\n",
      "Epoch 720, Train Loss: 83.8149, Test Loss: 14.6647\n",
      "Epoch 721, Train Loss: 72.3295, Test Loss: 14.6306\n",
      "Epoch 722, Train Loss: 80.7229, Test Loss: 14.5958\n",
      "Epoch 723, Train Loss: 71.7464, Test Loss: 14.5792\n",
      "Epoch 724, Train Loss: 65.1840, Test Loss: 14.5570\n",
      "Epoch 725, Train Loss: 78.5248, Test Loss: 14.5497\n",
      "Epoch 726, Train Loss: 70.4735, Test Loss: 14.5483\n",
      "Epoch 727, Train Loss: 76.8309, Test Loss: 14.5398\n",
      "Epoch 728, Train Loss: 72.8266, Test Loss: 14.5334\n",
      "Epoch 729, Train Loss: 66.2060, Test Loss: 14.5174\n",
      "Epoch 730, Train Loss: 90.5399, Test Loss: 14.5271\n",
      "Epoch 731, Train Loss: 70.1807, Test Loss: 14.4991\n",
      "Epoch 732, Train Loss: 79.8954, Test Loss: 14.4721\n",
      "Epoch 733, Train Loss: 73.7197, Test Loss: 14.4153\n",
      "Epoch 734, Train Loss: 64.1426, Test Loss: 14.3596\n",
      "Epoch 735, Train Loss: 82.9320, Test Loss: 14.2909\n",
      "Epoch 736, Train Loss: 66.8502, Test Loss: 14.2134\n",
      "Epoch 737, Train Loss: 67.6845, Test Loss: 14.0814\n",
      "Epoch 738, Train Loss: 67.9323, Test Loss: 13.9671\n",
      "Epoch 739, Train Loss: 79.4333, Test Loss: 13.8588\n",
      "Epoch 740, Train Loss: 65.1056, Test Loss: 13.7750\n",
      "Epoch 741, Train Loss: 65.8427, Test Loss: 13.6879\n",
      "Epoch 742, Train Loss: 64.3147, Test Loss: 13.6035\n",
      "Epoch 743, Train Loss: 93.5064, Test Loss: 13.5343\n",
      "Epoch 744, Train Loss: 68.1293, Test Loss: 13.4647\n",
      "Epoch 745, Train Loss: 65.3619, Test Loss: 13.4091\n",
      "Epoch 746, Train Loss: 69.4964, Test Loss: 13.3704\n",
      "Epoch 747, Train Loss: 69.3298, Test Loss: 13.3166\n",
      "Epoch 748, Train Loss: 75.9994, Test Loss: 13.2892\n",
      "Epoch 749, Train Loss: 72.6250, Test Loss: 13.2496\n",
      "Epoch 750, Train Loss: 63.6704, Test Loss: 13.2033\n",
      "Epoch 751, Train Loss: 67.9671, Test Loss: 13.1858\n",
      "Epoch 752, Train Loss: 83.3776, Test Loss: 13.1837\n",
      "Epoch 753, Train Loss: 81.1682, Test Loss: 13.1933\n",
      "Epoch 754, Train Loss: 66.4458, Test Loss: 13.1844\n",
      "Epoch 755, Train Loss: 65.8107, Test Loss: 13.1809\n",
      "Epoch 756, Train Loss: 66.2821, Test Loss: 13.1718\n",
      "Epoch 757, Train Loss: 73.1175, Test Loss: 13.1423\n",
      "Epoch 758, Train Loss: 65.0663, Test Loss: 13.1027\n",
      "Epoch 759, Train Loss: 64.6657, Test Loss: 13.0732\n",
      "Epoch 760, Train Loss: 61.4285, Test Loss: 13.0394\n",
      "Epoch 761, Train Loss: 55.7068, Test Loss: 12.9917\n",
      "Epoch 762, Train Loss: 65.7170, Test Loss: 12.9314\n",
      "Epoch 763, Train Loss: 58.8653, Test Loss: 12.8628\n",
      "Epoch 764, Train Loss: 60.1323, Test Loss: 12.7996\n",
      "Epoch 765, Train Loss: 73.4768, Test Loss: 12.7286\n",
      "Epoch 766, Train Loss: 55.9030, Test Loss: 12.6441\n",
      "Epoch 767, Train Loss: 66.3097, Test Loss: 12.5730\n",
      "Epoch 768, Train Loss: 63.4163, Test Loss: 12.5198\n",
      "Epoch 769, Train Loss: 63.7330, Test Loss: 12.4552\n",
      "Epoch 770, Train Loss: 61.8345, Test Loss: 12.3944\n",
      "Epoch 771, Train Loss: 60.9142, Test Loss: 12.3242\n",
      "Epoch 772, Train Loss: 55.4835, Test Loss: 12.2595\n",
      "Epoch 773, Train Loss: 72.1940, Test Loss: 12.2109\n",
      "Epoch 774, Train Loss: 65.8515, Test Loss: 12.1571\n",
      "Epoch 775, Train Loss: 71.7640, Test Loss: 12.1100\n",
      "Epoch 776, Train Loss: 56.5001, Test Loss: 12.0473\n",
      "Epoch 777, Train Loss: 66.0074, Test Loss: 11.9921\n",
      "Epoch 778, Train Loss: 65.1118, Test Loss: 11.9390\n",
      "Epoch 779, Train Loss: 61.7646, Test Loss: 11.8944\n",
      "Epoch 780, Train Loss: 52.8341, Test Loss: 11.8537\n",
      "Epoch 781, Train Loss: 70.1387, Test Loss: 11.8047\n",
      "Epoch 782, Train Loss: 66.1489, Test Loss: 11.7488\n",
      "Epoch 783, Train Loss: 67.7995, Test Loss: 11.6981\n",
      "Epoch 784, Train Loss: 58.6167, Test Loss: 11.6548\n",
      "Epoch 785, Train Loss: 61.7340, Test Loss: 11.6150\n",
      "Epoch 786, Train Loss: 64.6820, Test Loss: 11.5824\n",
      "Epoch 787, Train Loss: 59.4031, Test Loss: 11.5620\n",
      "Epoch 788, Train Loss: 75.5106, Test Loss: 11.5397\n",
      "Epoch 789, Train Loss: 64.2565, Test Loss: 11.5225\n",
      "Epoch 790, Train Loss: 50.3451, Test Loss: 11.5020\n",
      "Epoch 791, Train Loss: 57.3053, Test Loss: 11.4827\n",
      "Epoch 792, Train Loss: 65.0585, Test Loss: 11.4719\n",
      "Epoch 793, Train Loss: 79.0926, Test Loss: 11.4780\n",
      "Epoch 794, Train Loss: 56.5822, Test Loss: 11.4858\n",
      "Epoch 795, Train Loss: 69.7090, Test Loss: 11.5116\n",
      "Epoch 796, Train Loss: 51.6696, Test Loss: 11.5260\n",
      "Epoch 797, Train Loss: 61.1569, Test Loss: 11.5392\n",
      "Epoch 798, Train Loss: 55.3118, Test Loss: 11.5523\n",
      "Epoch 799, Train Loss: 55.0283, Test Loss: 11.5555\n",
      "Epoch 800, Train Loss: 66.1942, Test Loss: 11.5832\n",
      "Epoch 801, Train Loss: 54.7532, Test Loss: 11.6053\n",
      "Epoch 802, Train Loss: 58.1521, Test Loss: 11.6279\n",
      "Epoch 803, Train Loss: 61.9360, Test Loss: 11.6489\n",
      "Epoch 804, Train Loss: 51.9132, Test Loss: 11.6418\n",
      "Epoch 805, Train Loss: 63.8672, Test Loss: 11.6030\n",
      "Epoch 806, Train Loss: 50.7126, Test Loss: 11.5418\n",
      "Epoch 807, Train Loss: 59.3497, Test Loss: 11.4749\n",
      "Epoch 808, Train Loss: 68.2967, Test Loss: 11.4299\n",
      "Epoch 809, Train Loss: 52.5959, Test Loss: 11.3869\n",
      "Epoch 810, Train Loss: 66.4522, Test Loss: 11.3608\n",
      "Epoch 811, Train Loss: 62.1537, Test Loss: 11.3445\n",
      "Epoch 812, Train Loss: 58.8926, Test Loss: 11.3307\n",
      "Epoch 813, Train Loss: 55.4417, Test Loss: 11.3135\n",
      "Epoch 814, Train Loss: 49.0274, Test Loss: 11.3130\n",
      "Epoch 815, Train Loss: 52.0526, Test Loss: 11.3142\n",
      "Epoch 816, Train Loss: 50.5643, Test Loss: 11.3108\n",
      "Epoch 817, Train Loss: 60.1922, Test Loss: 11.2859\n",
      "Epoch 818, Train Loss: 61.0269, Test Loss: 11.2737\n",
      "Epoch 819, Train Loss: 58.5205, Test Loss: 11.2613\n",
      "Epoch 820, Train Loss: 53.1452, Test Loss: 11.2428\n",
      "Epoch 821, Train Loss: 54.3341, Test Loss: 11.2276\n",
      "Epoch 822, Train Loss: 73.8582, Test Loss: 11.2331\n",
      "Epoch 823, Train Loss: 52.4936, Test Loss: 11.2205\n",
      "Epoch 824, Train Loss: 61.3691, Test Loss: 11.1515\n",
      "Epoch 825, Train Loss: 70.0892, Test Loss: 11.0929\n",
      "Epoch 826, Train Loss: 50.9801, Test Loss: 11.0277\n",
      "Epoch 827, Train Loss: 50.3859, Test Loss: 10.9357\n",
      "Epoch 828, Train Loss: 51.6512, Test Loss: 10.8518\n",
      "Epoch 829, Train Loss: 55.8854, Test Loss: 10.7931\n",
      "Epoch 830, Train Loss: 48.9350, Test Loss: 10.7389\n",
      "Epoch 831, Train Loss: 53.8231, Test Loss: 10.6971\n",
      "Epoch 832, Train Loss: 62.3916, Test Loss: 10.6424\n",
      "Epoch 833, Train Loss: 52.4832, Test Loss: 10.5922\n",
      "Epoch 834, Train Loss: 52.1093, Test Loss: 10.5337\n",
      "Epoch 835, Train Loss: 52.5597, Test Loss: 10.4728\n",
      "Epoch 836, Train Loss: 56.9323, Test Loss: 10.4195\n",
      "Epoch 837, Train Loss: 49.0664, Test Loss: 10.3822\n",
      "Epoch 838, Train Loss: 59.4172, Test Loss: 10.3461\n",
      "Epoch 839, Train Loss: 58.4452, Test Loss: 10.3109\n",
      "Epoch 840, Train Loss: 56.2662, Test Loss: 10.2766\n",
      "Epoch 841, Train Loss: 54.2772, Test Loss: 10.2368\n",
      "Epoch 842, Train Loss: 50.2540, Test Loss: 10.2050\n",
      "Epoch 843, Train Loss: 56.8906, Test Loss: 10.1754\n",
      "Epoch 844, Train Loss: 53.9793, Test Loss: 10.1574\n",
      "Epoch 845, Train Loss: 54.2346, Test Loss: 10.1358\n",
      "Epoch 846, Train Loss: 55.5706, Test Loss: 10.1247\n",
      "Epoch 847, Train Loss: 53.0238, Test Loss: 10.1134\n",
      "Epoch 848, Train Loss: 49.3825, Test Loss: 10.0923\n",
      "Epoch 849, Train Loss: 54.1147, Test Loss: 10.0879\n",
      "Epoch 850, Train Loss: 51.0791, Test Loss: 10.0891\n",
      "Epoch 851, Train Loss: 51.7157, Test Loss: 10.0838\n",
      "Epoch 852, Train Loss: 60.8818, Test Loss: 10.1004\n",
      "Epoch 853, Train Loss: 49.3039, Test Loss: 10.1095\n",
      "Epoch 854, Train Loss: 47.9951, Test Loss: 10.0932\n",
      "Epoch 855, Train Loss: 51.6656, Test Loss: 10.0718\n",
      "Epoch 856, Train Loss: 42.4548, Test Loss: 10.0624\n",
      "Epoch 857, Train Loss: 51.2126, Test Loss: 10.0615\n",
      "Epoch 858, Train Loss: 53.7851, Test Loss: 10.0668\n",
      "Epoch 859, Train Loss: 66.0459, Test Loss: 10.0517\n",
      "Epoch 860, Train Loss: 52.0325, Test Loss: 10.0247\n",
      "Epoch 861, Train Loss: 48.6665, Test Loss: 9.9872\n",
      "Epoch 862, Train Loss: 50.1766, Test Loss: 9.9699\n",
      "Epoch 863, Train Loss: 60.6882, Test Loss: 9.9441\n",
      "Epoch 864, Train Loss: 53.0215, Test Loss: 9.9265\n",
      "Epoch 865, Train Loss: 53.7464, Test Loss: 9.9223\n",
      "Epoch 866, Train Loss: 50.9760, Test Loss: 9.9029\n",
      "Epoch 867, Train Loss: 46.4778, Test Loss: 9.9044\n",
      "Epoch 868, Train Loss: 47.1174, Test Loss: 9.9059\n",
      "Epoch 869, Train Loss: 55.4622, Test Loss: 9.9068\n",
      "Epoch 870, Train Loss: 52.2405, Test Loss: 9.9233\n",
      "Epoch 871, Train Loss: 59.1516, Test Loss: 9.9130\n",
      "Epoch 872, Train Loss: 44.4176, Test Loss: 9.8669\n",
      "Epoch 873, Train Loss: 47.9830, Test Loss: 9.8098\n",
      "Epoch 874, Train Loss: 56.4094, Test Loss: 9.7422\n",
      "Epoch 875, Train Loss: 46.5464, Test Loss: 9.6727\n",
      "Epoch 876, Train Loss: 45.0747, Test Loss: 9.5857\n",
      "Epoch 877, Train Loss: 45.9745, Test Loss: 9.5151\n",
      "Epoch 878, Train Loss: 42.7223, Test Loss: 9.4508\n",
      "Epoch 879, Train Loss: 46.3983, Test Loss: 9.3948\n",
      "Epoch 880, Train Loss: 52.8878, Test Loss: 9.3476\n",
      "Epoch 881, Train Loss: 42.1941, Test Loss: 9.3058\n",
      "Epoch 882, Train Loss: 45.2082, Test Loss: 9.2770\n",
      "Epoch 883, Train Loss: 50.9910, Test Loss: 9.2467\n",
      "Epoch 884, Train Loss: 48.6336, Test Loss: 9.2149\n",
      "Epoch 885, Train Loss: 56.1031, Test Loss: 9.1850\n",
      "Epoch 886, Train Loss: 39.2794, Test Loss: 9.1625\n",
      "Epoch 887, Train Loss: 47.9124, Test Loss: 9.1524\n",
      "Epoch 888, Train Loss: 48.8190, Test Loss: 9.1461\n",
      "Epoch 889, Train Loss: 46.4575, Test Loss: 9.1438\n",
      "Epoch 890, Train Loss: 47.3614, Test Loss: 9.1412\n",
      "Epoch 891, Train Loss: 51.1580, Test Loss: 9.1336\n",
      "Epoch 892, Train Loss: 46.6674, Test Loss: 9.1233\n",
      "Epoch 893, Train Loss: 56.4225, Test Loss: 9.1062\n",
      "Epoch 894, Train Loss: 51.0001, Test Loss: 9.0926\n",
      "Epoch 895, Train Loss: 46.9538, Test Loss: 9.0786\n",
      "Epoch 896, Train Loss: 47.4491, Test Loss: 9.0557\n",
      "Epoch 897, Train Loss: 40.8790, Test Loss: 9.0335\n",
      "Epoch 898, Train Loss: 42.4031, Test Loss: 9.0068\n",
      "Epoch 899, Train Loss: 65.2158, Test Loss: 8.9632\n",
      "Epoch 900, Train Loss: 47.6628, Test Loss: 8.9209\n",
      "Epoch 901, Train Loss: 45.1937, Test Loss: 8.8818\n",
      "Epoch 902, Train Loss: 48.9452, Test Loss: 8.8559\n",
      "Epoch 903, Train Loss: 47.0269, Test Loss: 8.8316\n",
      "Epoch 904, Train Loss: 45.5153, Test Loss: 8.8243\n",
      "Epoch 905, Train Loss: 45.4925, Test Loss: 8.8198\n",
      "Epoch 906, Train Loss: 54.1763, Test Loss: 8.8338\n",
      "Epoch 907, Train Loss: 43.1876, Test Loss: 8.8538\n",
      "Epoch 908, Train Loss: 47.6011, Test Loss: 8.8614\n",
      "Epoch 909, Train Loss: 47.7548, Test Loss: 8.8568\n",
      "Epoch 910, Train Loss: 46.6205, Test Loss: 8.8453\n",
      "Epoch 911, Train Loss: 45.8798, Test Loss: 8.8300\n",
      "Epoch 912, Train Loss: 50.1137, Test Loss: 8.8109\n",
      "Epoch 913, Train Loss: 46.3703, Test Loss: 8.7992\n",
      "Epoch 914, Train Loss: 47.9359, Test Loss: 8.7800\n",
      "Epoch 915, Train Loss: 45.1567, Test Loss: 8.7780\n",
      "Epoch 916, Train Loss: 39.3689, Test Loss: 8.7950\n",
      "Epoch 917, Train Loss: 56.6011, Test Loss: 8.8117\n",
      "Epoch 918, Train Loss: 49.4624, Test Loss: 8.7983\n",
      "Epoch 919, Train Loss: 42.1995, Test Loss: 8.7813\n",
      "Epoch 920, Train Loss: 51.5184, Test Loss: 8.7569\n",
      "Epoch 921, Train Loss: 46.5876, Test Loss: 8.7284\n",
      "Epoch 922, Train Loss: 42.2807, Test Loss: 8.6909\n",
      "Epoch 923, Train Loss: 46.6463, Test Loss: 8.6674\n",
      "Epoch 924, Train Loss: 39.2379, Test Loss: 8.6332\n",
      "Epoch 925, Train Loss: 43.7511, Test Loss: 8.6003\n",
      "Epoch 926, Train Loss: 42.3504, Test Loss: 8.5742\n",
      "Epoch 927, Train Loss: 46.8037, Test Loss: 8.5315\n",
      "Epoch 928, Train Loss: 52.8012, Test Loss: 8.5076\n",
      "Epoch 929, Train Loss: 58.7955, Test Loss: 8.5067\n",
      "Epoch 930, Train Loss: 43.8033, Test Loss: 8.4862\n",
      "Epoch 931, Train Loss: 43.2849, Test Loss: 8.4487\n",
      "Epoch 932, Train Loss: 45.1999, Test Loss: 8.4198\n",
      "Epoch 933, Train Loss: 51.7147, Test Loss: 8.3990\n",
      "Epoch 934, Train Loss: 60.4751, Test Loss: 8.3718\n",
      "Epoch 935, Train Loss: 43.3671, Test Loss: 8.3692\n",
      "Epoch 936, Train Loss: 56.8087, Test Loss: 8.3928\n",
      "Epoch 937, Train Loss: 40.4608, Test Loss: 8.4046\n",
      "Epoch 938, Train Loss: 44.1248, Test Loss: 8.3937\n",
      "Epoch 939, Train Loss: 39.4863, Test Loss: 8.3867\n",
      "Epoch 940, Train Loss: 42.6121, Test Loss: 8.3573\n",
      "Epoch 941, Train Loss: 44.0302, Test Loss: 8.2929\n",
      "Epoch 942, Train Loss: 41.0244, Test Loss: 8.2294\n",
      "Epoch 943, Train Loss: 36.4329, Test Loss: 8.1827\n",
      "Epoch 944, Train Loss: 47.0266, Test Loss: 8.1458\n",
      "Epoch 945, Train Loss: 44.1129, Test Loss: 8.1187\n",
      "Epoch 946, Train Loss: 42.5288, Test Loss: 8.0839\n",
      "Epoch 947, Train Loss: 48.0224, Test Loss: 8.0543\n",
      "Epoch 948, Train Loss: 45.7580, Test Loss: 8.0257\n",
      "Epoch 949, Train Loss: 34.9271, Test Loss: 8.0013\n",
      "Epoch 950, Train Loss: 43.0589, Test Loss: 7.9774\n",
      "Epoch 951, Train Loss: 43.7980, Test Loss: 7.9558\n",
      "Epoch 952, Train Loss: 47.4147, Test Loss: 7.9353\n",
      "Epoch 953, Train Loss: 46.3085, Test Loss: 7.9051\n",
      "Epoch 954, Train Loss: 45.4595, Test Loss: 7.8765\n",
      "Epoch 955, Train Loss: 46.1255, Test Loss: 7.8574\n",
      "Epoch 956, Train Loss: 48.4599, Test Loss: 7.8341\n",
      "Epoch 957, Train Loss: 42.8708, Test Loss: 7.8083\n",
      "Epoch 958, Train Loss: 41.5896, Test Loss: 7.7808\n",
      "Epoch 959, Train Loss: 45.1370, Test Loss: 7.7580\n",
      "Epoch 960, Train Loss: 39.8020, Test Loss: 7.7354\n",
      "Epoch 961, Train Loss: 47.7462, Test Loss: 7.7173\n",
      "Epoch 962, Train Loss: 38.3009, Test Loss: 7.6978\n",
      "Epoch 963, Train Loss: 38.4142, Test Loss: 7.6921\n",
      "Epoch 964, Train Loss: 50.6478, Test Loss: 7.6760\n",
      "Epoch 965, Train Loss: 42.3904, Test Loss: 7.6551\n",
      "Epoch 966, Train Loss: 37.5605, Test Loss: 7.6292\n",
      "Epoch 967, Train Loss: 35.3949, Test Loss: 7.6109\n",
      "Epoch 968, Train Loss: 42.1050, Test Loss: 7.5987\n",
      "Epoch 969, Train Loss: 43.4196, Test Loss: 7.5933\n",
      "Epoch 970, Train Loss: 46.4573, Test Loss: 7.5884\n",
      "Epoch 971, Train Loss: 41.9631, Test Loss: 7.5805\n",
      "Epoch 972, Train Loss: 42.8231, Test Loss: 7.5754\n",
      "Epoch 973, Train Loss: 44.8953, Test Loss: 7.5608\n",
      "Epoch 974, Train Loss: 37.5320, Test Loss: 7.5401\n",
      "Epoch 975, Train Loss: 47.9426, Test Loss: 7.5123\n",
      "Epoch 976, Train Loss: 38.6589, Test Loss: 7.4700\n",
      "Epoch 977, Train Loss: 37.6710, Test Loss: 7.4339\n",
      "Epoch 978, Train Loss: 40.0325, Test Loss: 7.4023\n",
      "Epoch 979, Train Loss: 38.6611, Test Loss: 7.3747\n",
      "Epoch 980, Train Loss: 39.3782, Test Loss: 7.3435\n",
      "Epoch 981, Train Loss: 35.3387, Test Loss: 7.3116\n",
      "Epoch 982, Train Loss: 43.4921, Test Loss: 7.2880\n",
      "Epoch 983, Train Loss: 44.4920, Test Loss: 7.2773\n",
      "Epoch 984, Train Loss: 45.6704, Test Loss: 7.2724\n",
      "Epoch 985, Train Loss: 36.5119, Test Loss: 7.2853\n",
      "Epoch 986, Train Loss: 47.8031, Test Loss: 7.2966\n",
      "Epoch 987, Train Loss: 37.9367, Test Loss: 7.3050\n",
      "Epoch 988, Train Loss: 34.7981, Test Loss: 7.3151\n",
      "Epoch 989, Train Loss: 39.6451, Test Loss: 7.3277\n",
      "Epoch 990, Train Loss: 39.7513, Test Loss: 7.3129\n",
      "Epoch 991, Train Loss: 43.0276, Test Loss: 7.3055\n",
      "Epoch 992, Train Loss: 40.3769, Test Loss: 7.2921\n",
      "Epoch 993, Train Loss: 42.1985, Test Loss: 7.2883\n",
      "Epoch 994, Train Loss: 43.1875, Test Loss: 7.3065\n",
      "Epoch 995, Train Loss: 37.9838, Test Loss: 7.2963\n",
      "Epoch 996, Train Loss: 33.8257, Test Loss: 7.2718\n",
      "Epoch 997, Train Loss: 39.7505, Test Loss: 7.2440\n",
      "Epoch 998, Train Loss: 38.4784, Test Loss: 7.2160\n",
      "Epoch 999, Train Loss: 33.4649, Test Loss: 7.1837\n",
      "Epoch 1000, Train Loss: 39.8377, Test Loss: 7.1463\n",
      "Epoch 1001, Train Loss: 42.1696, Test Loss: 7.1131\n",
      "Epoch 1002, Train Loss: 36.0129, Test Loss: 7.0775\n",
      "Epoch 1003, Train Loss: 39.3302, Test Loss: 7.0474\n",
      "Epoch 1004, Train Loss: 40.8716, Test Loss: 7.0262\n",
      "Epoch 1005, Train Loss: 39.5151, Test Loss: 7.0096\n",
      "Epoch 1006, Train Loss: 29.1998, Test Loss: 7.0043\n",
      "Epoch 1007, Train Loss: 42.5233, Test Loss: 7.0109\n",
      "Epoch 1008, Train Loss: 37.5429, Test Loss: 7.0257\n",
      "Epoch 1009, Train Loss: 35.1016, Test Loss: 7.0528\n",
      "Epoch 1010, Train Loss: 34.8305, Test Loss: 7.0772\n",
      "Epoch 1011, Train Loss: 37.2318, Test Loss: 7.0977\n",
      "Epoch 1012, Train Loss: 36.1906, Test Loss: 7.1059\n",
      "Epoch 1013, Train Loss: 41.4481, Test Loss: 7.1256\n",
      "Epoch 1014, Train Loss: 38.3108, Test Loss: 7.1555\n",
      "Epoch 1015, Train Loss: 39.7255, Test Loss: 7.1741\n",
      "Epoch 1016, Train Loss: 38.5340, Test Loss: 7.1821\n",
      "Epoch 1017, Train Loss: 39.0353, Test Loss: 7.1807\n",
      "Epoch 1018, Train Loss: 39.1255, Test Loss: 7.1794\n",
      "Epoch 1019, Train Loss: 26.5625, Test Loss: 7.1554\n",
      "Epoch 1020, Train Loss: 38.0410, Test Loss: 7.1176\n",
      "Epoch 1021, Train Loss: 38.2231, Test Loss: 7.0745\n",
      "Epoch 1022, Train Loss: 33.1972, Test Loss: 7.0217\n",
      "Epoch 1023, Train Loss: 47.9033, Test Loss: 6.9760\n",
      "Epoch 1024, Train Loss: 37.5140, Test Loss: 6.9377\n",
      "Epoch 1025, Train Loss: 32.4985, Test Loss: 6.9003\n",
      "Epoch 1026, Train Loss: 51.5218, Test Loss: 6.8661\n",
      "Epoch 1027, Train Loss: 39.0065, Test Loss: 6.8346\n",
      "Epoch 1028, Train Loss: 32.7498, Test Loss: 6.8087\n",
      "Epoch 1029, Train Loss: 31.1473, Test Loss: 6.7757\n",
      "Epoch 1030, Train Loss: 34.9925, Test Loss: 6.7496\n",
      "Epoch 1031, Train Loss: 37.7386, Test Loss: 6.7246\n",
      "Epoch 1032, Train Loss: 34.3177, Test Loss: 6.7095\n",
      "Epoch 1033, Train Loss: 42.1717, Test Loss: 6.6985\n",
      "Epoch 1034, Train Loss: 35.5734, Test Loss: 6.6872\n",
      "Epoch 1035, Train Loss: 36.7293, Test Loss: 6.6772\n",
      "Epoch 1036, Train Loss: 35.8684, Test Loss: 6.6653\n",
      "Epoch 1037, Train Loss: 33.5062, Test Loss: 6.6453\n",
      "Epoch 1038, Train Loss: 37.2451, Test Loss: 6.6249\n",
      "Epoch 1039, Train Loss: 35.7860, Test Loss: 6.6066\n",
      "Epoch 1040, Train Loss: 34.5129, Test Loss: 6.5856\n",
      "Epoch 1041, Train Loss: 37.1599, Test Loss: 6.5671\n",
      "Epoch 1042, Train Loss: 38.9845, Test Loss: 6.5546\n",
      "Epoch 1043, Train Loss: 33.2559, Test Loss: 6.5428\n",
      "Epoch 1044, Train Loss: 34.8431, Test Loss: 6.5317\n",
      "Epoch 1045, Train Loss: 33.3072, Test Loss: 6.5173\n",
      "Epoch 1046, Train Loss: 38.0843, Test Loss: 6.4954\n",
      "Epoch 1047, Train Loss: 33.6513, Test Loss: 6.4762\n",
      "Epoch 1048, Train Loss: 39.1039, Test Loss: 6.4636\n",
      "Epoch 1049, Train Loss: 30.6911, Test Loss: 6.4476\n",
      "Epoch 1050, Train Loss: 34.8983, Test Loss: 6.4340\n",
      "Epoch 1051, Train Loss: 37.8253, Test Loss: 6.4139\n",
      "Epoch 1052, Train Loss: 31.3374, Test Loss: 6.3922\n",
      "Epoch 1053, Train Loss: 35.5912, Test Loss: 6.3767\n",
      "Epoch 1054, Train Loss: 39.7138, Test Loss: 6.3651\n",
      "Epoch 1055, Train Loss: 33.4778, Test Loss: 6.3575\n",
      "Epoch 1056, Train Loss: 28.2181, Test Loss: 6.3518\n",
      "Epoch 1057, Train Loss: 34.3307, Test Loss: 6.3453\n",
      "Epoch 1058, Train Loss: 39.5966, Test Loss: 6.3479\n",
      "Epoch 1059, Train Loss: 32.0939, Test Loss: 6.3506\n",
      "Epoch 1060, Train Loss: 29.7047, Test Loss: 6.3390\n",
      "Epoch 1061, Train Loss: 33.3492, Test Loss: 6.3129\n",
      "Epoch 1062, Train Loss: 30.1848, Test Loss: 6.2822\n",
      "Epoch 1063, Train Loss: 29.5707, Test Loss: 6.2535\n",
      "Epoch 1064, Train Loss: 38.0307, Test Loss: 6.2237\n",
      "Epoch 1065, Train Loss: 36.0055, Test Loss: 6.2024\n",
      "Epoch 1066, Train Loss: 33.6520, Test Loss: 6.1743\n",
      "Epoch 1067, Train Loss: 35.7345, Test Loss: 6.1424\n",
      "Epoch 1068, Train Loss: 40.5902, Test Loss: 6.1164\n",
      "Epoch 1069, Train Loss: 30.3677, Test Loss: 6.0974\n",
      "Epoch 1070, Train Loss: 29.9233, Test Loss: 6.0805\n",
      "Epoch 1071, Train Loss: 38.7685, Test Loss: 6.0615\n",
      "Epoch 1072, Train Loss: 40.8795, Test Loss: 6.0437\n",
      "Epoch 1073, Train Loss: 43.5041, Test Loss: 6.0277\n",
      "Epoch 1074, Train Loss: 36.6182, Test Loss: 6.0132\n",
      "Epoch 1075, Train Loss: 32.2409, Test Loss: 6.0060\n",
      "Epoch 1076, Train Loss: 31.6155, Test Loss: 6.0028\n",
      "Epoch 1077, Train Loss: 37.9047, Test Loss: 5.9981\n",
      "Epoch 1078, Train Loss: 33.6542, Test Loss: 5.9907\n",
      "Epoch 1079, Train Loss: 33.3848, Test Loss: 5.9855\n",
      "Epoch 1080, Train Loss: 38.7224, Test Loss: 5.9821\n",
      "Epoch 1081, Train Loss: 34.3440, Test Loss: 5.9787\n",
      "Epoch 1082, Train Loss: 36.5151, Test Loss: 5.9711\n",
      "Epoch 1083, Train Loss: 32.7653, Test Loss: 5.9654\n",
      "Epoch 1084, Train Loss: 37.7815, Test Loss: 5.9601\n",
      "Epoch 1085, Train Loss: 29.5696, Test Loss: 5.9518\n",
      "Epoch 1086, Train Loss: 31.2547, Test Loss: 5.9449\n",
      "Epoch 1087, Train Loss: 31.3526, Test Loss: 5.9402\n",
      "Epoch 1088, Train Loss: 35.3457, Test Loss: 5.9317\n",
      "Epoch 1089, Train Loss: 32.1988, Test Loss: 5.9229\n",
      "Epoch 1090, Train Loss: 32.5673, Test Loss: 5.9159\n",
      "Epoch 1091, Train Loss: 32.7852, Test Loss: 5.9146\n",
      "Epoch 1092, Train Loss: 33.2921, Test Loss: 5.9132\n",
      "Epoch 1093, Train Loss: 38.5460, Test Loss: 5.9141\n",
      "Epoch 1094, Train Loss: 34.7281, Test Loss: 5.9150\n",
      "Epoch 1095, Train Loss: 31.8874, Test Loss: 5.9128\n",
      "Epoch 1096, Train Loss: 32.0795, Test Loss: 5.9119\n",
      "Epoch 1097, Train Loss: 26.8802, Test Loss: 5.9173\n",
      "Epoch 1098, Train Loss: 34.2155, Test Loss: 5.9270\n",
      "Epoch 1099, Train Loss: 31.5763, Test Loss: 5.9348\n",
      "Epoch 1100, Train Loss: 33.5465, Test Loss: 5.9373\n",
      "Epoch 1101, Train Loss: 27.6601, Test Loss: 5.9408\n",
      "Epoch 1102, Train Loss: 27.9667, Test Loss: 5.9319\n",
      "Epoch 1103, Train Loss: 44.2143, Test Loss: 5.9147\n",
      "Epoch 1104, Train Loss: 29.3714, Test Loss: 5.9035\n",
      "Epoch 1105, Train Loss: 28.4983, Test Loss: 5.8991\n",
      "Epoch 1106, Train Loss: 29.9052, Test Loss: 5.9145\n",
      "Epoch 1107, Train Loss: 33.4013, Test Loss: 5.9427\n",
      "Epoch 1108, Train Loss: 26.3877, Test Loss: 5.9566\n",
      "Epoch 1109, Train Loss: 31.3689, Test Loss: 5.9606\n",
      "Epoch 1110, Train Loss: 34.6350, Test Loss: 5.9542\n",
      "Epoch 1111, Train Loss: 32.1650, Test Loss: 5.9468\n",
      "Epoch 1112, Train Loss: 35.3622, Test Loss: 5.9257\n",
      "Epoch 1113, Train Loss: 28.5380, Test Loss: 5.8984\n",
      "Epoch 1114, Train Loss: 27.6612, Test Loss: 5.8698\n",
      "Epoch 1115, Train Loss: 30.8973, Test Loss: 5.8412\n",
      "Epoch 1116, Train Loss: 33.2415, Test Loss: 5.8114\n",
      "Epoch 1117, Train Loss: 27.9677, Test Loss: 5.7878\n",
      "Epoch 1118, Train Loss: 24.9829, Test Loss: 5.7608\n",
      "Epoch 1119, Train Loss: 26.9832, Test Loss: 5.7329\n",
      "Epoch 1120, Train Loss: 32.1245, Test Loss: 5.7174\n",
      "Epoch 1121, Train Loss: 27.2760, Test Loss: 5.6974\n",
      "Epoch 1122, Train Loss: 31.0706, Test Loss: 5.6856\n",
      "Epoch 1123, Train Loss: 34.5601, Test Loss: 5.6796\n",
      "Epoch 1124, Train Loss: 37.1808, Test Loss: 5.6909\n",
      "Epoch 1125, Train Loss: 35.1143, Test Loss: 5.7046\n",
      "Epoch 1126, Train Loss: 30.9206, Test Loss: 5.7098\n",
      "Epoch 1127, Train Loss: 29.9831, Test Loss: 5.7124\n",
      "Epoch 1128, Train Loss: 32.0472, Test Loss: 5.7054\n",
      "Epoch 1129, Train Loss: 36.3277, Test Loss: 5.6940\n",
      "Epoch 1130, Train Loss: 27.6191, Test Loss: 5.6851\n",
      "Epoch 1131, Train Loss: 29.7046, Test Loss: 5.6784\n",
      "Epoch 1132, Train Loss: 33.6628, Test Loss: 5.6600\n",
      "Epoch 1133, Train Loss: 25.1414, Test Loss: 5.6270\n",
      "Epoch 1134, Train Loss: 24.6374, Test Loss: 5.5977\n",
      "Epoch 1135, Train Loss: 27.5190, Test Loss: 5.5652\n",
      "Epoch 1136, Train Loss: 34.0266, Test Loss: 5.5339\n",
      "Epoch 1137, Train Loss: 26.9962, Test Loss: 5.5052\n",
      "Epoch 1138, Train Loss: 26.3651, Test Loss: 5.4736\n",
      "Epoch 1139, Train Loss: 31.6603, Test Loss: 5.4546\n",
      "Epoch 1140, Train Loss: 32.9253, Test Loss: 5.4295\n",
      "Epoch 1141, Train Loss: 28.0225, Test Loss: 5.4012\n",
      "Epoch 1142, Train Loss: 37.9495, Test Loss: 5.3840\n",
      "Epoch 1143, Train Loss: 33.7770, Test Loss: 5.3744\n",
      "Epoch 1144, Train Loss: 34.0486, Test Loss: 5.3705\n",
      "Epoch 1145, Train Loss: 29.8455, Test Loss: 5.3679\n",
      "Epoch 1146, Train Loss: 28.2998, Test Loss: 5.3591\n",
      "Epoch 1147, Train Loss: 24.5800, Test Loss: 5.3473\n",
      "Epoch 1148, Train Loss: 26.2117, Test Loss: 5.3391\n",
      "Epoch 1149, Train Loss: 30.4672, Test Loss: 5.3228\n",
      "Epoch 1150, Train Loss: 24.6154, Test Loss: 5.3059\n",
      "Epoch 1151, Train Loss: 36.8827, Test Loss: 5.2836\n",
      "Epoch 1152, Train Loss: 29.6585, Test Loss: 5.2681\n",
      "Epoch 1153, Train Loss: 25.6295, Test Loss: 5.2544\n",
      "Epoch 1154, Train Loss: 30.9124, Test Loss: 5.2496\n",
      "Epoch 1155, Train Loss: 26.8599, Test Loss: 5.2476\n",
      "Epoch 1156, Train Loss: 27.6442, Test Loss: 5.2409\n",
      "Epoch 1157, Train Loss: 36.8081, Test Loss: 5.2362\n",
      "Epoch 1158, Train Loss: 25.0859, Test Loss: 5.2326\n",
      "Epoch 1159, Train Loss: 25.7269, Test Loss: 5.2310\n",
      "Epoch 1160, Train Loss: 36.7443, Test Loss: 5.2164\n",
      "Epoch 1161, Train Loss: 23.8914, Test Loss: 5.1981\n",
      "Epoch 1162, Train Loss: 31.0396, Test Loss: 5.1753\n",
      "Epoch 1163, Train Loss: 24.8124, Test Loss: 5.1519\n",
      "Epoch 1164, Train Loss: 28.1532, Test Loss: 5.1302\n",
      "Epoch 1165, Train Loss: 28.9460, Test Loss: 5.1220\n",
      "Epoch 1166, Train Loss: 33.1453, Test Loss: 5.1242\n",
      "Epoch 1167, Train Loss: 28.4203, Test Loss: 5.1250\n",
      "Epoch 1168, Train Loss: 28.3412, Test Loss: 5.1127\n",
      "Epoch 1169, Train Loss: 24.2916, Test Loss: 5.1021\n",
      "Epoch 1170, Train Loss: 28.0684, Test Loss: 5.0874\n",
      "Epoch 1171, Train Loss: 23.4627, Test Loss: 5.0751\n",
      "Epoch 1172, Train Loss: 29.2828, Test Loss: 5.0646\n",
      "Epoch 1173, Train Loss: 27.3074, Test Loss: 5.0469\n",
      "Epoch 1174, Train Loss: 24.1588, Test Loss: 5.0245\n",
      "Epoch 1175, Train Loss: 26.1410, Test Loss: 5.0091\n",
      "Epoch 1176, Train Loss: 35.5245, Test Loss: 5.0006\n",
      "Epoch 1177, Train Loss: 24.1222, Test Loss: 4.9899\n",
      "Epoch 1178, Train Loss: 28.9964, Test Loss: 4.9827\n",
      "Epoch 1179, Train Loss: 25.7909, Test Loss: 4.9799\n",
      "Epoch 1180, Train Loss: 30.5253, Test Loss: 4.9853\n",
      "Epoch 1181, Train Loss: 24.9405, Test Loss: 4.9868\n",
      "Epoch 1182, Train Loss: 23.4968, Test Loss: 4.9822\n",
      "Epoch 1183, Train Loss: 26.6864, Test Loss: 4.9716\n",
      "Epoch 1184, Train Loss: 27.1298, Test Loss: 4.9520\n",
      "Epoch 1185, Train Loss: 28.0201, Test Loss: 4.9329\n",
      "Epoch 1186, Train Loss: 25.8321, Test Loss: 4.9162\n",
      "Epoch 1187, Train Loss: 25.8389, Test Loss: 4.8987\n",
      "Epoch 1188, Train Loss: 25.5410, Test Loss: 4.8779\n",
      "Epoch 1189, Train Loss: 24.3972, Test Loss: 4.8566\n",
      "Epoch 1190, Train Loss: 25.5215, Test Loss: 4.8299\n",
      "Epoch 1191, Train Loss: 28.0393, Test Loss: 4.8142\n",
      "Epoch 1192, Train Loss: 25.5222, Test Loss: 4.7955\n",
      "Epoch 1193, Train Loss: 28.9303, Test Loss: 4.7899\n",
      "Epoch 1194, Train Loss: 25.9137, Test Loss: 4.7835\n",
      "Epoch 1195, Train Loss: 20.8750, Test Loss: 4.7780\n",
      "Epoch 1196, Train Loss: 24.5772, Test Loss: 4.7790\n",
      "Epoch 1197, Train Loss: 24.9425, Test Loss: 4.7935\n",
      "Epoch 1198, Train Loss: 22.6967, Test Loss: 4.8027\n",
      "Epoch 1199, Train Loss: 23.4818, Test Loss: 4.8062\n",
      "Epoch 1200, Train Loss: 25.2251, Test Loss: 4.8154\n",
      "Epoch 1201, Train Loss: 25.8493, Test Loss: 4.8155\n",
      "Epoch 1202, Train Loss: 24.9934, Test Loss: 4.8069\n",
      "Epoch 1203, Train Loss: 27.5898, Test Loss: 4.7922\n",
      "Epoch 1204, Train Loss: 26.1394, Test Loss: 4.7738\n",
      "Epoch 1205, Train Loss: 26.5480, Test Loss: 4.7572\n",
      "Epoch 1206, Train Loss: 24.3510, Test Loss: 4.7310\n",
      "Epoch 1207, Train Loss: 25.4957, Test Loss: 4.7088\n",
      "Epoch 1208, Train Loss: 26.4668, Test Loss: 4.6900\n",
      "Epoch 1209, Train Loss: 24.6882, Test Loss: 4.6782\n",
      "Epoch 1210, Train Loss: 25.4634, Test Loss: 4.6603\n",
      "Epoch 1211, Train Loss: 30.1521, Test Loss: 4.6416\n",
      "Epoch 1212, Train Loss: 21.9058, Test Loss: 4.6303\n",
      "Epoch 1213, Train Loss: 23.0209, Test Loss: 4.6163\n",
      "Epoch 1214, Train Loss: 31.5897, Test Loss: 4.6110\n",
      "Epoch 1215, Train Loss: 31.0512, Test Loss: 4.6033\n",
      "Epoch 1216, Train Loss: 26.4720, Test Loss: 4.5943\n",
      "Epoch 1217, Train Loss: 30.6736, Test Loss: 4.5848\n",
      "Epoch 1218, Train Loss: 35.5957, Test Loss: 4.5783\n",
      "Epoch 1219, Train Loss: 25.7987, Test Loss: 4.5693\n",
      "Epoch 1220, Train Loss: 25.3373, Test Loss: 4.5659\n",
      "Epoch 1221, Train Loss: 30.5040, Test Loss: 4.5576\n",
      "Epoch 1222, Train Loss: 27.3984, Test Loss: 4.5492\n",
      "Epoch 1223, Train Loss: 28.7447, Test Loss: 4.5394\n",
      "Epoch 1224, Train Loss: 23.6957, Test Loss: 4.5279\n",
      "Epoch 1225, Train Loss: 25.7558, Test Loss: 4.5108\n",
      "Epoch 1226, Train Loss: 27.3765, Test Loss: 4.4984\n",
      "Epoch 1227, Train Loss: 23.6954, Test Loss: 4.4908\n",
      "Epoch 1228, Train Loss: 19.5694, Test Loss: 4.4856\n",
      "Epoch 1229, Train Loss: 27.6267, Test Loss: 4.4854\n",
      "Epoch 1230, Train Loss: 28.3801, Test Loss: 4.4875\n",
      "Epoch 1231, Train Loss: 24.1917, Test Loss: 4.4861\n",
      "Epoch 1232, Train Loss: 21.5940, Test Loss: 4.4834\n",
      "Epoch 1233, Train Loss: 26.9347, Test Loss: 4.4747\n",
      "Epoch 1234, Train Loss: 29.3928, Test Loss: 4.4630\n",
      "Epoch 1235, Train Loss: 23.3279, Test Loss: 4.4505\n",
      "Epoch 1236, Train Loss: 24.1907, Test Loss: 4.4406\n",
      "Epoch 1237, Train Loss: 24.0596, Test Loss: 4.4269\n",
      "Epoch 1238, Train Loss: 25.5871, Test Loss: 4.4180\n",
      "Epoch 1239, Train Loss: 30.6325, Test Loss: 4.4109\n",
      "Epoch 1240, Train Loss: 23.5774, Test Loss: 4.4018\n",
      "Epoch 1241, Train Loss: 23.0687, Test Loss: 4.3883\n",
      "Epoch 1242, Train Loss: 24.0766, Test Loss: 4.3794\n",
      "Epoch 1243, Train Loss: 21.1224, Test Loss: 4.3682\n",
      "Epoch 1244, Train Loss: 27.2709, Test Loss: 4.3535\n",
      "Epoch 1245, Train Loss: 27.3433, Test Loss: 4.3396\n",
      "Epoch 1246, Train Loss: 30.6961, Test Loss: 4.3266\n",
      "Epoch 1247, Train Loss: 26.0214, Test Loss: 4.3180\n",
      "Epoch 1248, Train Loss: 29.7024, Test Loss: 4.3105\n",
      "Epoch 1249, Train Loss: 25.0976, Test Loss: 4.2963\n",
      "Epoch 1250, Train Loss: 22.9236, Test Loss: 4.2842\n",
      "Epoch 1251, Train Loss: 20.7414, Test Loss: 4.2704\n",
      "Epoch 1252, Train Loss: 20.6610, Test Loss: 4.2543\n",
      "Epoch 1253, Train Loss: 26.8745, Test Loss: 4.2443\n",
      "Epoch 1254, Train Loss: 23.0284, Test Loss: 4.2361\n",
      "Epoch 1255, Train Loss: 26.1109, Test Loss: 4.2294\n",
      "Epoch 1256, Train Loss: 26.1557, Test Loss: 4.2177\n",
      "Epoch 1257, Train Loss: 27.5907, Test Loss: 4.2058\n",
      "Epoch 1258, Train Loss: 26.4167, Test Loss: 4.1894\n",
      "Epoch 1259, Train Loss: 22.4404, Test Loss: 4.1729\n",
      "Epoch 1260, Train Loss: 30.5970, Test Loss: 4.1600\n",
      "Epoch 1261, Train Loss: 28.8836, Test Loss: 4.1493\n",
      "Epoch 1262, Train Loss: 20.3986, Test Loss: 4.1376\n",
      "Epoch 1263, Train Loss: 24.8777, Test Loss: 4.1346\n",
      "Epoch 1264, Train Loss: 28.9776, Test Loss: 4.1373\n",
      "Epoch 1265, Train Loss: 21.7975, Test Loss: 4.1389\n",
      "Epoch 1266, Train Loss: 22.2663, Test Loss: 4.1379\n",
      "Epoch 1267, Train Loss: 21.0670, Test Loss: 4.1348\n",
      "Epoch 1268, Train Loss: 24.8419, Test Loss: 4.1292\n",
      "Epoch 1269, Train Loss: 23.5468, Test Loss: 4.1270\n",
      "Epoch 1270, Train Loss: 24.4664, Test Loss: 4.1170\n",
      "Epoch 1271, Train Loss: 30.1349, Test Loss: 4.1098\n",
      "Epoch 1272, Train Loss: 19.7649, Test Loss: 4.1055\n",
      "Epoch 1273, Train Loss: 24.1721, Test Loss: 4.1078\n",
      "Epoch 1274, Train Loss: 19.9990, Test Loss: 4.1136\n",
      "Epoch 1275, Train Loss: 21.3288, Test Loss: 4.1244\n",
      "Epoch 1276, Train Loss: 23.5096, Test Loss: 4.1350\n",
      "Epoch 1277, Train Loss: 24.4174, Test Loss: 4.1302\n",
      "Epoch 1278, Train Loss: 24.7083, Test Loss: 4.1258\n",
      "Epoch 1279, Train Loss: 22.6884, Test Loss: 4.1235\n",
      "Epoch 1280, Train Loss: 26.8625, Test Loss: 4.1259\n",
      "Epoch 1281, Train Loss: 23.5592, Test Loss: 4.1288\n",
      "Epoch 1282, Train Loss: 24.6137, Test Loss: 4.1251\n",
      "Epoch 1283, Train Loss: 21.0694, Test Loss: 4.1203\n",
      "Epoch 1284, Train Loss: 23.8488, Test Loss: 4.1115\n",
      "Epoch 1285, Train Loss: 22.0701, Test Loss: 4.0903\n",
      "Epoch 1286, Train Loss: 19.3457, Test Loss: 4.0680\n",
      "Epoch 1287, Train Loss: 23.6335, Test Loss: 4.0407\n",
      "Epoch 1288, Train Loss: 22.0216, Test Loss: 4.0216\n",
      "Epoch 1289, Train Loss: 24.2305, Test Loss: 4.0069\n",
      "Epoch 1290, Train Loss: 18.0945, Test Loss: 3.9919\n",
      "Epoch 1291, Train Loss: 26.7359, Test Loss: 3.9777\n",
      "Epoch 1292, Train Loss: 22.0859, Test Loss: 3.9659\n",
      "Epoch 1293, Train Loss: 18.8536, Test Loss: 3.9576\n",
      "Epoch 1294, Train Loss: 20.5980, Test Loss: 3.9494\n",
      "Epoch 1295, Train Loss: 26.1357, Test Loss: 3.9394\n",
      "Epoch 1296, Train Loss: 24.5262, Test Loss: 3.9301\n",
      "Epoch 1297, Train Loss: 19.0786, Test Loss: 3.9204\n",
      "Epoch 1298, Train Loss: 23.7195, Test Loss: 3.9078\n",
      "Epoch 1299, Train Loss: 22.6856, Test Loss: 3.8955\n",
      "Epoch 1300, Train Loss: 25.3132, Test Loss: 3.8826\n",
      "Epoch 1301, Train Loss: 18.1040, Test Loss: 3.8717\n",
      "Epoch 1302, Train Loss: 23.3391, Test Loss: 3.8609\n",
      "Epoch 1303, Train Loss: 20.2088, Test Loss: 3.8520\n",
      "Epoch 1304, Train Loss: 21.8423, Test Loss: 3.8418\n",
      "Epoch 1305, Train Loss: 22.6815, Test Loss: 3.8277\n",
      "Epoch 1306, Train Loss: 22.3138, Test Loss: 3.8168\n",
      "Epoch 1307, Train Loss: 22.1168, Test Loss: 3.8097\n",
      "Epoch 1308, Train Loss: 23.4786, Test Loss: 3.8018\n",
      "Epoch 1309, Train Loss: 22.0489, Test Loss: 3.7919\n",
      "Epoch 1310, Train Loss: 26.3495, Test Loss: 3.7834\n",
      "Epoch 1311, Train Loss: 15.8207, Test Loss: 3.7771\n",
      "Epoch 1312, Train Loss: 19.0351, Test Loss: 3.7733\n",
      "Epoch 1313, Train Loss: 23.6091, Test Loss: 3.7710\n",
      "Epoch 1314, Train Loss: 25.1424, Test Loss: 3.7728\n",
      "Epoch 1315, Train Loss: 18.8837, Test Loss: 3.7750\n",
      "Epoch 1316, Train Loss: 23.2081, Test Loss: 3.7749\n",
      "Epoch 1317, Train Loss: 19.4209, Test Loss: 3.7727\n",
      "Epoch 1318, Train Loss: 21.8387, Test Loss: 3.7706\n",
      "Epoch 1319, Train Loss: 20.0518, Test Loss: 3.7686\n",
      "Epoch 1320, Train Loss: 17.2724, Test Loss: 3.7646\n",
      "Epoch 1321, Train Loss: 20.1195, Test Loss: 3.7576\n",
      "Epoch 1322, Train Loss: 19.3976, Test Loss: 3.7547\n",
      "Epoch 1323, Train Loss: 22.2342, Test Loss: 3.7494\n",
      "Epoch 1324, Train Loss: 20.2129, Test Loss: 3.7454\n",
      "Epoch 1325, Train Loss: 24.2512, Test Loss: 3.7406\n",
      "Epoch 1326, Train Loss: 18.8744, Test Loss: 3.7377\n",
      "Epoch 1327, Train Loss: 22.9501, Test Loss: 3.7358\n",
      "Epoch 1328, Train Loss: 24.3662, Test Loss: 3.7431\n",
      "Epoch 1329, Train Loss: 20.1688, Test Loss: 3.7443\n",
      "Epoch 1330, Train Loss: 18.2421, Test Loss: 3.7451\n",
      "Epoch 1331, Train Loss: 21.0989, Test Loss: 3.7437\n",
      "Epoch 1332, Train Loss: 19.1354, Test Loss: 3.7394\n",
      "Epoch 1333, Train Loss: 23.2465, Test Loss: 3.7305\n",
      "Epoch 1334, Train Loss: 18.4352, Test Loss: 3.7212\n",
      "Epoch 1335, Train Loss: 22.1991, Test Loss: 3.7082\n",
      "Epoch 1336, Train Loss: 18.3134, Test Loss: 3.6980\n",
      "Epoch 1337, Train Loss: 21.4244, Test Loss: 3.6899\n",
      "Epoch 1338, Train Loss: 20.3784, Test Loss: 3.6871\n",
      "Epoch 1339, Train Loss: 20.2786, Test Loss: 3.6835\n",
      "Epoch 1340, Train Loss: 21.9632, Test Loss: 3.6716\n",
      "Epoch 1341, Train Loss: 19.2980, Test Loss: 3.6598\n",
      "Epoch 1342, Train Loss: 21.1231, Test Loss: 3.6519\n",
      "Epoch 1343, Train Loss: 19.5478, Test Loss: 3.6482\n",
      "Epoch 1344, Train Loss: 19.8658, Test Loss: 3.6442\n",
      "Epoch 1345, Train Loss: 25.9825, Test Loss: 3.6435\n",
      "Epoch 1346, Train Loss: 17.3298, Test Loss: 3.6479\n",
      "Epoch 1347, Train Loss: 21.8083, Test Loss: 3.6520\n",
      "Epoch 1348, Train Loss: 19.3444, Test Loss: 3.6558\n",
      "Epoch 1349, Train Loss: 19.7144, Test Loss: 3.6542\n",
      "Epoch 1350, Train Loss: 19.7615, Test Loss: 3.6544\n",
      "Epoch 1351, Train Loss: 23.0746, Test Loss: 3.6456\n",
      "Epoch 1352, Train Loss: 19.5862, Test Loss: 3.6428\n",
      "Epoch 1353, Train Loss: 21.3280, Test Loss: 3.6495\n",
      "Epoch 1354, Train Loss: 21.6742, Test Loss: 3.6480\n",
      "Epoch 1355, Train Loss: 19.6211, Test Loss: 3.6463\n",
      "Epoch 1356, Train Loss: 23.3178, Test Loss: 3.6419\n",
      "Epoch 1357, Train Loss: 17.5793, Test Loss: 3.6369\n",
      "Epoch 1358, Train Loss: 17.1230, Test Loss: 3.6255\n",
      "Epoch 1359, Train Loss: 21.3089, Test Loss: 3.6034\n",
      "Epoch 1360, Train Loss: 25.5204, Test Loss: 3.5832\n",
      "Epoch 1361, Train Loss: 22.0102, Test Loss: 3.5663\n",
      "Epoch 1362, Train Loss: 18.0962, Test Loss: 3.5557\n",
      "Epoch 1363, Train Loss: 19.2491, Test Loss: 3.5398\n",
      "Epoch 1364, Train Loss: 19.6162, Test Loss: 3.5226\n",
      "Epoch 1365, Train Loss: 17.8666, Test Loss: 3.5071\n",
      "Epoch 1366, Train Loss: 21.9540, Test Loss: 3.4933\n",
      "Epoch 1367, Train Loss: 20.7124, Test Loss: 3.4836\n",
      "Epoch 1368, Train Loss: 18.9687, Test Loss: 3.4736\n",
      "Epoch 1369, Train Loss: 17.4782, Test Loss: 3.4579\n",
      "Epoch 1370, Train Loss: 18.9627, Test Loss: 3.4525\n",
      "Epoch 1371, Train Loss: 20.1968, Test Loss: 3.4516\n",
      "Epoch 1372, Train Loss: 23.2529, Test Loss: 3.4587\n",
      "Epoch 1373, Train Loss: 23.7822, Test Loss: 3.4577\n",
      "Epoch 1374, Train Loss: 18.7860, Test Loss: 3.4581\n",
      "Epoch 1375, Train Loss: 21.7648, Test Loss: 3.4604\n",
      "Epoch 1376, Train Loss: 18.4053, Test Loss: 3.4630\n",
      "Epoch 1377, Train Loss: 20.7705, Test Loss: 3.4647\n",
      "Epoch 1378, Train Loss: 19.1262, Test Loss: 3.4680\n",
      "Epoch 1379, Train Loss: 19.8902, Test Loss: 3.4687\n",
      "Epoch 1380, Train Loss: 21.5717, Test Loss: 3.4695\n",
      "Epoch 1381, Train Loss: 21.0196, Test Loss: 3.4682\n",
      "Epoch 1382, Train Loss: 19.4548, Test Loss: 3.4677\n",
      "Epoch 1383, Train Loss: 19.8187, Test Loss: 3.4640\n",
      "Epoch 1384, Train Loss: 24.1320, Test Loss: 3.4635\n",
      "Epoch 1385, Train Loss: 21.2046, Test Loss: 3.4526\n",
      "Epoch 1386, Train Loss: 21.9538, Test Loss: 3.4354\n",
      "Epoch 1387, Train Loss: 18.5028, Test Loss: 3.4118\n",
      "Epoch 1388, Train Loss: 17.8655, Test Loss: 3.3951\n",
      "Epoch 1389, Train Loss: 22.9791, Test Loss: 3.3843\n",
      "Epoch 1390, Train Loss: 17.9185, Test Loss: 3.3709\n",
      "Epoch 1391, Train Loss: 17.8393, Test Loss: 3.3515\n",
      "Epoch 1392, Train Loss: 16.4073, Test Loss: 3.3313\n",
      "Epoch 1393, Train Loss: 17.8222, Test Loss: 3.3130\n",
      "Epoch 1394, Train Loss: 16.8730, Test Loss: 3.2989\n",
      "Epoch 1395, Train Loss: 17.0972, Test Loss: 3.2891\n",
      "Epoch 1396, Train Loss: 16.7710, Test Loss: 3.2731\n",
      "Epoch 1397, Train Loss: 19.5135, Test Loss: 3.2596\n",
      "Epoch 1398, Train Loss: 16.0903, Test Loss: 3.2540\n",
      "Epoch 1399, Train Loss: 20.0780, Test Loss: 3.2511\n",
      "Epoch 1400, Train Loss: 18.7975, Test Loss: 3.2550\n",
      "Epoch 1401, Train Loss: 18.0002, Test Loss: 3.2651\n",
      "Epoch 1402, Train Loss: 25.0352, Test Loss: 3.2769\n",
      "Epoch 1403, Train Loss: 18.4733, Test Loss: 3.2800\n",
      "Epoch 1404, Train Loss: 20.0307, Test Loss: 3.2782\n",
      "Epoch 1405, Train Loss: 17.4117, Test Loss: 3.2756\n",
      "Epoch 1406, Train Loss: 18.4840, Test Loss: 3.2738\n",
      "Epoch 1407, Train Loss: 18.2958, Test Loss: 3.2845\n",
      "Epoch 1408, Train Loss: 16.6324, Test Loss: 3.2899\n",
      "Epoch 1409, Train Loss: 18.9550, Test Loss: 3.2981\n",
      "Epoch 1410, Train Loss: 21.6140, Test Loss: 3.3135\n",
      "Epoch 1411, Train Loss: 16.2032, Test Loss: 3.3271\n",
      "Epoch 1412, Train Loss: 16.8429, Test Loss: 3.3455\n",
      "Epoch 1413, Train Loss: 16.8417, Test Loss: 3.3582\n",
      "Epoch 1414, Train Loss: 18.1341, Test Loss: 3.3764\n",
      "Epoch 1415, Train Loss: 16.7393, Test Loss: 3.3877\n",
      "Epoch 1416, Train Loss: 19.4094, Test Loss: 3.3912\n",
      "Epoch 1417, Train Loss: 18.2442, Test Loss: 3.3976\n",
      "Epoch 1418, Train Loss: 19.3393, Test Loss: 3.3886\n",
      "Epoch 1419, Train Loss: 18.1610, Test Loss: 3.3575\n",
      "Epoch 1420, Train Loss: 17.1984, Test Loss: 3.3254\n",
      "Epoch 1421, Train Loss: 16.2097, Test Loss: 3.3006\n",
      "Epoch 1422, Train Loss: 16.1344, Test Loss: 3.2846\n",
      "Epoch 1423, Train Loss: 15.9214, Test Loss: 3.2632\n",
      "Epoch 1424, Train Loss: 14.8813, Test Loss: 3.2505\n",
      "Epoch 1425, Train Loss: 16.9778, Test Loss: 3.2369\n",
      "Epoch 1426, Train Loss: 17.8864, Test Loss: 3.2237\n",
      "Epoch 1427, Train Loss: 16.1119, Test Loss: 3.2151\n",
      "Epoch 1428, Train Loss: 17.2830, Test Loss: 3.2085\n",
      "Epoch 1429, Train Loss: 17.6117, Test Loss: 3.1987\n",
      "Epoch 1430, Train Loss: 20.2419, Test Loss: 3.1879\n",
      "Epoch 1431, Train Loss: 16.6223, Test Loss: 3.1720\n",
      "Epoch 1432, Train Loss: 18.2432, Test Loss: 3.1590\n",
      "Epoch 1433, Train Loss: 16.1729, Test Loss: 3.1464\n",
      "Epoch 1434, Train Loss: 18.2307, Test Loss: 3.1360\n",
      "Epoch 1435, Train Loss: 17.2709, Test Loss: 3.1265\n",
      "Epoch 1436, Train Loss: 18.4412, Test Loss: 3.1173\n",
      "Epoch 1437, Train Loss: 18.3859, Test Loss: 3.1221\n",
      "Epoch 1438, Train Loss: 16.5938, Test Loss: 3.1374\n",
      "Epoch 1439, Train Loss: 15.4455, Test Loss: 3.1584\n",
      "Epoch 1440, Train Loss: 18.8824, Test Loss: 3.1671\n",
      "Epoch 1441, Train Loss: 15.6175, Test Loss: 3.1755\n",
      "Epoch 1442, Train Loss: 16.0371, Test Loss: 3.1913\n",
      "Epoch 1443, Train Loss: 14.8776, Test Loss: 3.2095\n",
      "Epoch 1444, Train Loss: 16.2583, Test Loss: 3.2125\n",
      "Epoch 1445, Train Loss: 19.4592, Test Loss: 3.2046\n",
      "Epoch 1446, Train Loss: 19.5712, Test Loss: 3.2039\n",
      "Epoch 1447, Train Loss: 18.4351, Test Loss: 3.1935\n",
      "Epoch 1448, Train Loss: 17.5798, Test Loss: 3.1878\n",
      "Epoch 1449, Train Loss: 17.2528, Test Loss: 3.1841\n",
      "Epoch 1450, Train Loss: 18.0693, Test Loss: 3.1761\n",
      "Epoch 1451, Train Loss: 17.0076, Test Loss: 3.1566\n",
      "Epoch 1452, Train Loss: 14.7874, Test Loss: 3.1258\n",
      "Epoch 1453, Train Loss: 16.9091, Test Loss: 3.0889\n",
      "Epoch 1454, Train Loss: 15.3367, Test Loss: 3.0524\n",
      "Epoch 1455, Train Loss: 15.8592, Test Loss: 3.0188\n",
      "Epoch 1456, Train Loss: 18.0877, Test Loss: 3.0008\n",
      "Epoch 1457, Train Loss: 16.6584, Test Loss: 2.9901\n",
      "Epoch 1458, Train Loss: 19.6699, Test Loss: 2.9940\n",
      "Epoch 1459, Train Loss: 16.0921, Test Loss: 2.9979\n",
      "Epoch 1460, Train Loss: 15.0698, Test Loss: 3.0012\n",
      "Epoch 1461, Train Loss: 16.3276, Test Loss: 3.0032\n",
      "Epoch 1462, Train Loss: 16.5892, Test Loss: 3.0070\n",
      "Epoch 1463, Train Loss: 18.6761, Test Loss: 3.0041\n",
      "Epoch 1464, Train Loss: 14.2813, Test Loss: 3.0044\n",
      "Epoch 1465, Train Loss: 16.5761, Test Loss: 3.0026\n",
      "Epoch 1466, Train Loss: 14.3688, Test Loss: 3.0005\n",
      "Epoch 1467, Train Loss: 17.6434, Test Loss: 2.9913\n",
      "Epoch 1468, Train Loss: 15.4517, Test Loss: 2.9778\n",
      "Epoch 1469, Train Loss: 16.5295, Test Loss: 2.9534\n",
      "Epoch 1470, Train Loss: 16.4536, Test Loss: 2.9258\n",
      "Epoch 1471, Train Loss: 16.3950, Test Loss: 2.9042\n",
      "Epoch 1472, Train Loss: 17.5802, Test Loss: 2.8913\n",
      "Epoch 1473, Train Loss: 16.3477, Test Loss: 2.8764\n",
      "Epoch 1474, Train Loss: 19.7556, Test Loss: 2.8578\n",
      "Epoch 1475, Train Loss: 15.6120, Test Loss: 2.8404\n",
      "Epoch 1476, Train Loss: 16.4778, Test Loss: 2.8239\n",
      "Epoch 1477, Train Loss: 16.8386, Test Loss: 2.8088\n",
      "Epoch 1478, Train Loss: 15.3254, Test Loss: 2.7985\n",
      "Epoch 1479, Train Loss: 16.9409, Test Loss: 2.7925\n",
      "Epoch 1480, Train Loss: 22.2896, Test Loss: 2.7919\n",
      "Epoch 1481, Train Loss: 18.4511, Test Loss: 2.7955\n",
      "Epoch 1482, Train Loss: 14.2723, Test Loss: 2.7994\n",
      "Epoch 1483, Train Loss: 17.4207, Test Loss: 2.8029\n",
      "Epoch 1484, Train Loss: 17.9000, Test Loss: 2.8065\n",
      "Epoch 1485, Train Loss: 16.1021, Test Loss: 2.8071\n",
      "Epoch 1486, Train Loss: 15.7381, Test Loss: 2.8104\n",
      "Epoch 1487, Train Loss: 15.4756, Test Loss: 2.8099\n",
      "Epoch 1488, Train Loss: 15.3339, Test Loss: 2.8101\n",
      "Epoch 1489, Train Loss: 17.2771, Test Loss: 2.8037\n",
      "Epoch 1490, Train Loss: 16.9447, Test Loss: 2.8002\n",
      "Epoch 1491, Train Loss: 13.9951, Test Loss: 2.7970\n",
      "Epoch 1492, Train Loss: 15.0834, Test Loss: 2.7973\n",
      "Epoch 1493, Train Loss: 17.8669, Test Loss: 2.7898\n",
      "Epoch 1494, Train Loss: 15.5750, Test Loss: 2.7837\n",
      "Epoch 1495, Train Loss: 14.9738, Test Loss: 2.7749\n",
      "Epoch 1496, Train Loss: 20.8291, Test Loss: 2.7730\n",
      "Epoch 1497, Train Loss: 16.7920, Test Loss: 2.7657\n",
      "Epoch 1498, Train Loss: 16.1183, Test Loss: 2.7608\n",
      "Epoch 1499, Train Loss: 17.7279, Test Loss: 2.7598\n",
      "Epoch 1500, Train Loss: 15.6767, Test Loss: 2.7557\n",
      "Epoch 1501, Train Loss: 15.8774, Test Loss: 2.7493\n",
      "Epoch 1502, Train Loss: 15.9247, Test Loss: 2.7445\n",
      "Epoch 1503, Train Loss: 16.5331, Test Loss: 2.7360\n",
      "Epoch 1504, Train Loss: 16.5148, Test Loss: 2.7328\n",
      "Epoch 1505, Train Loss: 18.3103, Test Loss: 2.7311\n",
      "Epoch 1506, Train Loss: 17.7696, Test Loss: 2.7302\n",
      "Epoch 1507, Train Loss: 16.5639, Test Loss: 2.7339\n",
      "Epoch 1508, Train Loss: 14.4599, Test Loss: 2.7374\n",
      "Epoch 1509, Train Loss: 17.4067, Test Loss: 2.7399\n",
      "Epoch 1510, Train Loss: 13.7979, Test Loss: 2.7452\n",
      "Epoch 1511, Train Loss: 15.5326, Test Loss: 2.7454\n",
      "Epoch 1512, Train Loss: 15.7909, Test Loss: 2.7500\n",
      "Epoch 1513, Train Loss: 15.9629, Test Loss: 2.7592\n",
      "Epoch 1514, Train Loss: 14.0441, Test Loss: 2.7763\n",
      "Epoch 1515, Train Loss: 20.8304, Test Loss: 2.7911\n",
      "Epoch 1516, Train Loss: 14.7537, Test Loss: 2.8019\n",
      "Epoch 1517, Train Loss: 17.3705, Test Loss: 2.7965\n",
      "Epoch 1518, Train Loss: 15.3145, Test Loss: 2.7957\n",
      "Epoch 1519, Train Loss: 13.7558, Test Loss: 2.7880\n",
      "Epoch 1520, Train Loss: 14.5250, Test Loss: 2.7808\n",
      "Epoch 1521, Train Loss: 15.1845, Test Loss: 2.7738\n",
      "Epoch 1522, Train Loss: 16.5734, Test Loss: 2.7686\n",
      "Epoch 1523, Train Loss: 16.1389, Test Loss: 2.7521\n",
      "Epoch 1524, Train Loss: 14.5521, Test Loss: 2.7328\n",
      "Epoch 1525, Train Loss: 16.4731, Test Loss: 2.7145\n",
      "Epoch 1526, Train Loss: 14.5456, Test Loss: 2.7041\n",
      "Epoch 1527, Train Loss: 14.3709, Test Loss: 2.6908\n",
      "Epoch 1528, Train Loss: 14.6607, Test Loss: 2.6750\n",
      "Epoch 1529, Train Loss: 15.9702, Test Loss: 2.6618\n",
      "Epoch 1530, Train Loss: 15.7297, Test Loss: 2.6522\n",
      "Epoch 1531, Train Loss: 15.4647, Test Loss: 2.6486\n",
      "Epoch 1532, Train Loss: 13.9799, Test Loss: 2.6470\n",
      "Epoch 1533, Train Loss: 14.0318, Test Loss: 2.6445\n",
      "Epoch 1534, Train Loss: 14.1517, Test Loss: 2.6407\n",
      "Epoch 1535, Train Loss: 14.3385, Test Loss: 2.6387\n",
      "Epoch 1536, Train Loss: 14.7014, Test Loss: 2.6303\n",
      "Epoch 1537, Train Loss: 15.1076, Test Loss: 2.6159\n",
      "Epoch 1538, Train Loss: 15.0647, Test Loss: 2.6040\n",
      "Epoch 1539, Train Loss: 14.9738, Test Loss: 2.5978\n",
      "Epoch 1540, Train Loss: 13.0160, Test Loss: 2.5930\n",
      "Epoch 1541, Train Loss: 17.8091, Test Loss: 2.5883\n",
      "Epoch 1542, Train Loss: 16.0169, Test Loss: 2.5827\n",
      "Epoch 1543, Train Loss: 16.1940, Test Loss: 2.5816\n",
      "Epoch 1544, Train Loss: 15.8694, Test Loss: 2.5818\n",
      "Epoch 1545, Train Loss: 14.5349, Test Loss: 2.5827\n",
      "Epoch 1546, Train Loss: 12.9993, Test Loss: 2.5856\n",
      "Epoch 1547, Train Loss: 17.2297, Test Loss: 2.5909\n",
      "Epoch 1548, Train Loss: 15.3320, Test Loss: 2.5988\n",
      "Epoch 1549, Train Loss: 15.2338, Test Loss: 2.6036\n",
      "Epoch 1550, Train Loss: 14.2308, Test Loss: 2.6076\n",
      "Epoch 1551, Train Loss: 14.6790, Test Loss: 2.6060\n",
      "Epoch 1552, Train Loss: 14.6080, Test Loss: 2.6103\n",
      "Epoch 1553, Train Loss: 15.1765, Test Loss: 2.6165\n",
      "Epoch 1554, Train Loss: 11.7549, Test Loss: 2.6276\n",
      "Epoch 1555, Train Loss: 13.7844, Test Loss: 2.6376\n",
      "Epoch 1556, Train Loss: 14.6211, Test Loss: 2.6463\n",
      "Epoch 1557, Train Loss: 13.7094, Test Loss: 2.6525\n",
      "Epoch 1558, Train Loss: 12.2243, Test Loss: 2.6541\n",
      "Epoch 1559, Train Loss: 15.7511, Test Loss: 2.6561\n",
      "Epoch 1560, Train Loss: 13.0698, Test Loss: 2.6626\n",
      "Epoch 1561, Train Loss: 11.8399, Test Loss: 2.6618\n",
      "Epoch 1562, Train Loss: 13.1790, Test Loss: 2.6651\n",
      "Epoch 1563, Train Loss: 12.8640, Test Loss: 2.6702\n",
      "Epoch 1564, Train Loss: 12.3541, Test Loss: 2.6789\n",
      "Epoch 1565, Train Loss: 14.1189, Test Loss: 2.6878\n",
      "Epoch 1566, Train Loss: 16.2448, Test Loss: 2.7006\n",
      "Epoch 1567, Train Loss: 14.0473, Test Loss: 2.7149\n",
      "Epoch 1568, Train Loss: 11.7061, Test Loss: 2.7300\n",
      "Epoch 1569, Train Loss: 15.3190, Test Loss: 2.7325\n",
      "Epoch 1570, Train Loss: 14.4539, Test Loss: 2.7193\n",
      "Epoch 1571, Train Loss: 15.1201, Test Loss: 2.7013\n",
      "Epoch 1572, Train Loss: 14.8495, Test Loss: 2.6948\n",
      "Epoch 1573, Train Loss: 13.8334, Test Loss: 2.6779\n",
      "Epoch 1574, Train Loss: 12.6530, Test Loss: 2.6473\n",
      "Epoch 1575, Train Loss: 14.5215, Test Loss: 2.6206\n",
      "Epoch 1576, Train Loss: 13.1439, Test Loss: 2.5927\n",
      "Epoch 1577, Train Loss: 15.6245, Test Loss: 2.5791\n",
      "Epoch 1578, Train Loss: 14.3252, Test Loss: 2.5676\n",
      "Epoch 1579, Train Loss: 16.4936, Test Loss: 2.5525\n",
      "Epoch 1580, Train Loss: 12.5486, Test Loss: 2.5387\n",
      "Epoch 1581, Train Loss: 14.6863, Test Loss: 2.5251\n",
      "Epoch 1582, Train Loss: 13.9288, Test Loss: 2.5155\n",
      "Epoch 1583, Train Loss: 15.0678, Test Loss: 2.5022\n",
      "Epoch 1584, Train Loss: 13.5739, Test Loss: 2.4906\n",
      "Epoch 1585, Train Loss: 13.5093, Test Loss: 2.4786\n",
      "Epoch 1586, Train Loss: 14.1911, Test Loss: 2.4660\n",
      "Epoch 1587, Train Loss: 14.8245, Test Loss: 2.4615\n",
      "Epoch 1588, Train Loss: 14.5100, Test Loss: 2.4556\n",
      "Epoch 1589, Train Loss: 11.8959, Test Loss: 2.4518\n",
      "Epoch 1590, Train Loss: 12.0621, Test Loss: 2.4463\n",
      "Epoch 1591, Train Loss: 15.1600, Test Loss: 2.4417\n",
      "Epoch 1592, Train Loss: 11.3679, Test Loss: 2.4372\n",
      "Epoch 1593, Train Loss: 13.9454, Test Loss: 2.4337\n",
      "Epoch 1594, Train Loss: 13.5541, Test Loss: 2.4337\n",
      "Epoch 1595, Train Loss: 15.1748, Test Loss: 2.4343\n",
      "Epoch 1596, Train Loss: 13.4102, Test Loss: 2.4346\n",
      "Epoch 1597, Train Loss: 12.5209, Test Loss: 2.4376\n",
      "Epoch 1598, Train Loss: 13.3140, Test Loss: 2.4413\n",
      "Epoch 1599, Train Loss: 14.6776, Test Loss: 2.4426\n",
      "Epoch 1600, Train Loss: 13.3951, Test Loss: 2.4439\n",
      "Epoch 1601, Train Loss: 14.3710, Test Loss: 2.4470\n",
      "Epoch 1602, Train Loss: 13.6711, Test Loss: 2.4489\n",
      "Epoch 1603, Train Loss: 12.5885, Test Loss: 2.4544\n",
      "Epoch 1604, Train Loss: 11.4599, Test Loss: 2.4597\n",
      "Epoch 1605, Train Loss: 12.4782, Test Loss: 2.4593\n",
      "Epoch 1606, Train Loss: 12.4716, Test Loss: 2.4602\n",
      "Epoch 1607, Train Loss: 13.8559, Test Loss: 2.4580\n",
      "Epoch 1608, Train Loss: 13.5867, Test Loss: 2.4567\n",
      "Epoch 1609, Train Loss: 13.6826, Test Loss: 2.4535\n",
      "Epoch 1610, Train Loss: 11.8940, Test Loss: 2.4521\n",
      "Epoch 1611, Train Loss: 13.3168, Test Loss: 2.4485\n",
      "Epoch 1612, Train Loss: 13.8950, Test Loss: 2.4429\n",
      "Epoch 1613, Train Loss: 13.5771, Test Loss: 2.4368\n",
      "Epoch 1614, Train Loss: 12.5800, Test Loss: 2.4308\n",
      "Epoch 1615, Train Loss: 12.9074, Test Loss: 2.4254\n",
      "Epoch 1616, Train Loss: 12.8498, Test Loss: 2.4159\n",
      "Epoch 1617, Train Loss: 13.6576, Test Loss: 2.4069\n",
      "Epoch 1618, Train Loss: 14.9459, Test Loss: 2.3976\n",
      "Epoch 1619, Train Loss: 13.2959, Test Loss: 2.3843\n",
      "Epoch 1620, Train Loss: 15.8258, Test Loss: 2.3766\n",
      "Epoch 1621, Train Loss: 14.7835, Test Loss: 2.3644\n",
      "Epoch 1622, Train Loss: 13.6546, Test Loss: 2.3554\n",
      "Epoch 1623, Train Loss: 15.1475, Test Loss: 2.3470\n",
      "Epoch 1624, Train Loss: 13.4622, Test Loss: 2.3388\n",
      "Epoch 1625, Train Loss: 14.8666, Test Loss: 2.3335\n",
      "Epoch 1626, Train Loss: 14.3442, Test Loss: 2.3286\n",
      "Epoch 1627, Train Loss: 9.9873, Test Loss: 2.3302\n",
      "Epoch 1628, Train Loss: 12.1074, Test Loss: 2.3317\n",
      "Epoch 1629, Train Loss: 13.7291, Test Loss: 2.3379\n",
      "Epoch 1630, Train Loss: 14.8481, Test Loss: 2.3422\n",
      "Epoch 1631, Train Loss: 13.3086, Test Loss: 2.3421\n",
      "Epoch 1632, Train Loss: 11.3002, Test Loss: 2.3441\n",
      "Epoch 1633, Train Loss: 12.4016, Test Loss: 2.3456\n",
      "Epoch 1634, Train Loss: 14.3183, Test Loss: 2.3486\n",
      "Epoch 1635, Train Loss: 12.3212, Test Loss: 2.3577\n",
      "Epoch 1636, Train Loss: 12.9929, Test Loss: 2.3631\n",
      "Epoch 1637, Train Loss: 13.4611, Test Loss: 2.3647\n",
      "Epoch 1638, Train Loss: 11.7872, Test Loss: 2.3581\n",
      "Epoch 1639, Train Loss: 14.2896, Test Loss: 2.3539\n",
      "Epoch 1640, Train Loss: 12.2361, Test Loss: 2.3467\n",
      "Epoch 1641, Train Loss: 15.8955, Test Loss: 2.3416\n",
      "Epoch 1642, Train Loss: 15.0749, Test Loss: 2.3296\n",
      "Epoch 1643, Train Loss: 13.1870, Test Loss: 2.3219\n",
      "Epoch 1644, Train Loss: 11.7798, Test Loss: 2.3216\n",
      "Epoch 1645, Train Loss: 10.7608, Test Loss: 2.3226\n",
      "Epoch 1646, Train Loss: 14.4268, Test Loss: 2.3285\n",
      "Epoch 1647, Train Loss: 13.6554, Test Loss: 2.3314\n",
      "Epoch 1648, Train Loss: 12.2304, Test Loss: 2.3319\n",
      "Epoch 1649, Train Loss: 12.6368, Test Loss: 2.3366\n",
      "Epoch 1650, Train Loss: 11.0394, Test Loss: 2.3459\n",
      "Epoch 1651, Train Loss: 10.7407, Test Loss: 2.3487\n",
      "Epoch 1652, Train Loss: 12.2990, Test Loss: 2.3505\n",
      "Epoch 1653, Train Loss: 13.5651, Test Loss: 2.3531\n",
      "Epoch 1654, Train Loss: 14.7219, Test Loss: 2.3571\n",
      "Epoch 1655, Train Loss: 13.3154, Test Loss: 2.3652\n",
      "Epoch 1656, Train Loss: 14.7073, Test Loss: 2.3780\n",
      "Epoch 1657, Train Loss: 12.8842, Test Loss: 2.3843\n",
      "Epoch 1658, Train Loss: 12.5358, Test Loss: 2.3723\n",
      "Epoch 1659, Train Loss: 10.8361, Test Loss: 2.3717\n",
      "Epoch 1660, Train Loss: 11.2873, Test Loss: 2.3701\n",
      "Epoch 1661, Train Loss: 13.7473, Test Loss: 2.3659\n",
      "Epoch 1662, Train Loss: 13.7955, Test Loss: 2.3713\n",
      "Epoch 1663, Train Loss: 11.8040, Test Loss: 2.3719\n",
      "Epoch 1664, Train Loss: 12.5857, Test Loss: 2.3753\n",
      "Epoch 1665, Train Loss: 14.2180, Test Loss: 2.3697\n",
      "Epoch 1666, Train Loss: 13.2500, Test Loss: 2.3617\n",
      "Epoch 1667, Train Loss: 12.1399, Test Loss: 2.3518\n",
      "Epoch 1668, Train Loss: 11.6104, Test Loss: 2.3350\n",
      "Epoch 1669, Train Loss: 10.9125, Test Loss: 2.3211\n",
      "Epoch 1670, Train Loss: 9.4088, Test Loss: 2.3113\n",
      "Epoch 1671, Train Loss: 13.0644, Test Loss: 2.2919\n",
      "Epoch 1672, Train Loss: 14.9414, Test Loss: 2.2777\n",
      "Epoch 1673, Train Loss: 11.3536, Test Loss: 2.2633\n",
      "Epoch 1674, Train Loss: 11.5996, Test Loss: 2.2540\n",
      "Epoch 1675, Train Loss: 11.0308, Test Loss: 2.2462\n",
      "Epoch 1676, Train Loss: 12.3239, Test Loss: 2.2385\n",
      "Epoch 1677, Train Loss: 11.3705, Test Loss: 2.2266\n",
      "Epoch 1678, Train Loss: 13.3604, Test Loss: 2.2145\n",
      "Epoch 1679, Train Loss: 11.8219, Test Loss: 2.2112\n",
      "Epoch 1680, Train Loss: 11.1531, Test Loss: 2.2063\n",
      "Epoch 1681, Train Loss: 13.0528, Test Loss: 2.2053\n",
      "Epoch 1682, Train Loss: 11.1802, Test Loss: 2.2035\n",
      "Epoch 1683, Train Loss: 12.1134, Test Loss: 2.2030\n",
      "Epoch 1684, Train Loss: 10.5260, Test Loss: 2.2052\n",
      "Epoch 1685, Train Loss: 13.0826, Test Loss: 2.2104\n",
      "Epoch 1686, Train Loss: 12.7430, Test Loss: 2.2159\n",
      "Epoch 1687, Train Loss: 11.9012, Test Loss: 2.2166\n",
      "Epoch 1688, Train Loss: 10.9752, Test Loss: 2.2116\n",
      "Epoch 1689, Train Loss: 12.1446, Test Loss: 2.2019\n",
      "Epoch 1690, Train Loss: 13.2435, Test Loss: 2.1957\n",
      "Epoch 1691, Train Loss: 11.3707, Test Loss: 2.1916\n",
      "Epoch 1692, Train Loss: 13.8466, Test Loss: 2.1869\n",
      "Epoch 1693, Train Loss: 10.3909, Test Loss: 2.1784\n",
      "Epoch 1694, Train Loss: 11.4586, Test Loss: 2.1763\n",
      "Epoch 1695, Train Loss: 11.5236, Test Loss: 2.1773\n",
      "Epoch 1696, Train Loss: 10.6138, Test Loss: 2.1768\n",
      "Epoch 1697, Train Loss: 14.0027, Test Loss: 2.1750\n",
      "Epoch 1698, Train Loss: 11.1402, Test Loss: 2.1751\n",
      "Epoch 1699, Train Loss: 10.3641, Test Loss: 2.1764\n",
      "Epoch 1700, Train Loss: 12.1747, Test Loss: 2.1823\n",
      "Epoch 1701, Train Loss: 11.5323, Test Loss: 2.1888\n",
      "Epoch 1702, Train Loss: 9.9588, Test Loss: 2.1979\n",
      "Epoch 1703, Train Loss: 9.7505, Test Loss: 2.2059\n",
      "Epoch 1704, Train Loss: 11.4136, Test Loss: 2.2169\n",
      "Epoch 1705, Train Loss: 13.9711, Test Loss: 2.2231\n",
      "Epoch 1706, Train Loss: 12.4840, Test Loss: 2.2288\n",
      "Epoch 1707, Train Loss: 11.2154, Test Loss: 2.2290\n",
      "Epoch 1708, Train Loss: 12.7848, Test Loss: 2.2338\n",
      "Epoch 1709, Train Loss: 9.4069, Test Loss: 2.2351\n",
      "Epoch 1710, Train Loss: 10.6986, Test Loss: 2.2324\n",
      "Epoch 1711, Train Loss: 11.3231, Test Loss: 2.2321\n",
      "Epoch 1712, Train Loss: 9.5774, Test Loss: 2.2330\n",
      "Epoch 1713, Train Loss: 10.1379, Test Loss: 2.2345\n",
      "Epoch 1714, Train Loss: 11.4565, Test Loss: 2.2391\n",
      "Epoch 1715, Train Loss: 13.1170, Test Loss: 2.2477\n",
      "Epoch 1716, Train Loss: 12.1108, Test Loss: 2.2551\n",
      "Epoch 1717, Train Loss: 10.6376, Test Loss: 2.2560\n",
      "Epoch 1718, Train Loss: 13.1503, Test Loss: 2.2540\n",
      "Epoch 1719, Train Loss: 10.0711, Test Loss: 2.2518\n",
      "Epoch 1720, Train Loss: 11.2801, Test Loss: 2.2465\n",
      "Epoch 1721, Train Loss: 9.6093, Test Loss: 2.2385\n",
      "Epoch 1722, Train Loss: 11.5690, Test Loss: 2.2349\n",
      "Epoch 1723, Train Loss: 11.1101, Test Loss: 2.2288\n",
      "Epoch 1724, Train Loss: 10.6224, Test Loss: 2.2192\n",
      "Epoch 1725, Train Loss: 9.7847, Test Loss: 2.2024\n",
      "Epoch 1726, Train Loss: 11.2756, Test Loss: 2.1830\n",
      "Epoch 1727, Train Loss: 11.5139, Test Loss: 2.1613\n",
      "Epoch 1728, Train Loss: 11.2014, Test Loss: 2.1369\n",
      "Epoch 1729, Train Loss: 9.8045, Test Loss: 2.1189\n",
      "Epoch 1730, Train Loss: 8.6643, Test Loss: 2.1060\n",
      "Epoch 1731, Train Loss: 11.4674, Test Loss: 2.0944\n",
      "Epoch 1732, Train Loss: 10.4536, Test Loss: 2.0856\n",
      "Epoch 1733, Train Loss: 10.3438, Test Loss: 2.0827\n",
      "Epoch 1734, Train Loss: 10.6962, Test Loss: 2.0812\n",
      "Epoch 1735, Train Loss: 10.4603, Test Loss: 2.0783\n",
      "Epoch 1736, Train Loss: 11.2191, Test Loss: 2.0791\n",
      "Epoch 1737, Train Loss: 11.9627, Test Loss: 2.0763\n",
      "Epoch 1738, Train Loss: 10.8449, Test Loss: 2.0806\n",
      "Epoch 1739, Train Loss: 10.4704, Test Loss: 2.0907\n",
      "Epoch 1740, Train Loss: 12.9638, Test Loss: 2.1014\n",
      "Epoch 1741, Train Loss: 9.3850, Test Loss: 2.1120\n",
      "Epoch 1742, Train Loss: 11.5472, Test Loss: 2.1201\n",
      "Epoch 1743, Train Loss: 8.7816, Test Loss: 2.1273\n",
      "Epoch 1744, Train Loss: 9.2515, Test Loss: 2.1320\n",
      "Epoch 1745, Train Loss: 10.1106, Test Loss: 2.1342\n",
      "Epoch 1746, Train Loss: 9.7939, Test Loss: 2.1354\n",
      "Epoch 1747, Train Loss: 9.8671, Test Loss: 2.1354\n",
      "Epoch 1748, Train Loss: 11.6538, Test Loss: 2.1360\n",
      "Epoch 1749, Train Loss: 10.9140, Test Loss: 2.1335\n",
      "Epoch 1750, Train Loss: 9.3122, Test Loss: 2.1279\n",
      "Epoch 1751, Train Loss: 9.8881, Test Loss: 2.1193\n",
      "Epoch 1752, Train Loss: 12.9599, Test Loss: 2.1100\n",
      "Epoch 1753, Train Loss: 9.4897, Test Loss: 2.0994\n",
      "Epoch 1754, Train Loss: 11.4076, Test Loss: 2.0929\n",
      "Epoch 1755, Train Loss: 12.2855, Test Loss: 2.0947\n",
      "Epoch 1756, Train Loss: 11.0330, Test Loss: 2.0979\n",
      "Epoch 1757, Train Loss: 10.9383, Test Loss: 2.1044\n",
      "Epoch 1758, Train Loss: 11.1932, Test Loss: 2.1124\n",
      "Epoch 1759, Train Loss: 9.6290, Test Loss: 2.1198\n",
      "Epoch 1760, Train Loss: 11.5830, Test Loss: 2.1300\n",
      "Epoch 1761, Train Loss: 10.6104, Test Loss: 2.1436\n",
      "Epoch 1762, Train Loss: 10.3641, Test Loss: 2.1501\n",
      "Epoch 1763, Train Loss: 10.1158, Test Loss: 2.1608\n",
      "Epoch 1764, Train Loss: 9.5654, Test Loss: 2.1739\n",
      "Epoch 1765, Train Loss: 10.2414, Test Loss: 2.1848\n",
      "Epoch 1766, Train Loss: 8.9591, Test Loss: 2.1902\n",
      "Epoch 1767, Train Loss: 11.5829, Test Loss: 2.1936\n",
      "Epoch 1768, Train Loss: 11.6878, Test Loss: 2.1931\n",
      "Epoch 1769, Train Loss: 9.5296, Test Loss: 2.1884\n",
      "Epoch 1770, Train Loss: 8.8044, Test Loss: 2.1775\n",
      "Epoch 1771, Train Loss: 10.2882, Test Loss: 2.1666\n",
      "Epoch 1772, Train Loss: 8.6851, Test Loss: 2.1548\n",
      "Epoch 1773, Train Loss: 11.7167, Test Loss: 2.1400\n",
      "Epoch 1774, Train Loss: 10.7364, Test Loss: 2.1261\n",
      "Epoch 1775, Train Loss: 9.6452, Test Loss: 2.1126\n",
      "Epoch 1776, Train Loss: 9.6547, Test Loss: 2.1049\n",
      "Epoch 1777, Train Loss: 12.2025, Test Loss: 2.1036\n",
      "Epoch 1778, Train Loss: 11.1267, Test Loss: 2.0989\n",
      "Epoch 1779, Train Loss: 10.6769, Test Loss: 2.0955\n",
      "Epoch 1780, Train Loss: 10.4021, Test Loss: 2.0913\n",
      "Epoch 1781, Train Loss: 9.0256, Test Loss: 2.0906\n",
      "Epoch 1782, Train Loss: 8.6040, Test Loss: 2.0880\n",
      "Epoch 1783, Train Loss: 11.8990, Test Loss: 2.0875\n",
      "Epoch 1784, Train Loss: 9.9980, Test Loss: 2.0909\n",
      "Epoch 1785, Train Loss: 12.3445, Test Loss: 2.0937\n",
      "Epoch 1786, Train Loss: 8.9360, Test Loss: 2.0953\n",
      "Epoch 1787, Train Loss: 10.3523, Test Loss: 2.0964\n",
      "Epoch 1788, Train Loss: 9.4749, Test Loss: 2.0995\n",
      "Epoch 1789, Train Loss: 11.3949, Test Loss: 2.1089\n",
      "Epoch 1790, Train Loss: 10.6626, Test Loss: 2.1168\n",
      "Epoch 1791, Train Loss: 10.9027, Test Loss: 2.1252\n",
      "Epoch 1792, Train Loss: 8.8537, Test Loss: 2.1313\n",
      "Epoch 1793, Train Loss: 9.5525, Test Loss: 2.1346\n",
      "Epoch 1794, Train Loss: 9.8649, Test Loss: 2.1323\n",
      "Epoch 1795, Train Loss: 9.2174, Test Loss: 2.1276\n",
      "Epoch 1796, Train Loss: 12.3764, Test Loss: 2.1251\n",
      "Epoch 1797, Train Loss: 11.1121, Test Loss: 2.1239\n",
      "Epoch 1798, Train Loss: 10.4274, Test Loss: 2.1211\n",
      "Epoch 1799, Train Loss: 10.3840, Test Loss: 2.1128\n",
      "Epoch 1800, Train Loss: 10.9238, Test Loss: 2.1011\n",
      "Epoch 1801, Train Loss: 11.1796, Test Loss: 2.0849\n",
      "Epoch 1802, Train Loss: 10.3020, Test Loss: 2.0692\n",
      "Epoch 1803, Train Loss: 8.0771, Test Loss: 2.0595\n",
      "Epoch 1804, Train Loss: 9.8968, Test Loss: 2.0463\n",
      "Epoch 1805, Train Loss: 9.2952, Test Loss: 2.0355\n",
      "Epoch 1806, Train Loss: 10.4383, Test Loss: 2.0265\n",
      "Epoch 1807, Train Loss: 10.7200, Test Loss: 2.0094\n",
      "Epoch 1808, Train Loss: 10.9906, Test Loss: 1.9967\n",
      "Epoch 1809, Train Loss: 9.4481, Test Loss: 1.9843\n",
      "Epoch 1810, Train Loss: 9.7452, Test Loss: 1.9722\n",
      "Epoch 1811, Train Loss: 9.0396, Test Loss: 1.9622\n",
      "Epoch 1812, Train Loss: 11.3892, Test Loss: 1.9561\n",
      "Epoch 1813, Train Loss: 10.3642, Test Loss: 1.9489\n",
      "Epoch 1814, Train Loss: 9.9201, Test Loss: 1.9423\n",
      "Epoch 1815, Train Loss: 13.2647, Test Loss: 1.9434\n",
      "Epoch 1816, Train Loss: 9.6447, Test Loss: 1.9473\n",
      "Epoch 1817, Train Loss: 10.1538, Test Loss: 1.9537\n",
      "Epoch 1818, Train Loss: 9.1825, Test Loss: 1.9572\n",
      "Epoch 1819, Train Loss: 9.9337, Test Loss: 1.9583\n",
      "Epoch 1820, Train Loss: 8.2970, Test Loss: 1.9594\n",
      "Epoch 1821, Train Loss: 9.5160, Test Loss: 1.9635\n",
      "Epoch 1822, Train Loss: 10.7784, Test Loss: 1.9668\n",
      "Epoch 1823, Train Loss: 10.1452, Test Loss: 1.9720\n",
      "Epoch 1824, Train Loss: 10.4388, Test Loss: 1.9761\n",
      "Epoch 1825, Train Loss: 9.9508, Test Loss: 1.9734\n",
      "Epoch 1826, Train Loss: 9.3834, Test Loss: 1.9682\n",
      "Epoch 1827, Train Loss: 8.1508, Test Loss: 1.9620\n",
      "Epoch 1828, Train Loss: 11.1586, Test Loss: 1.9584\n",
      "Epoch 1829, Train Loss: 10.9067, Test Loss: 1.9565\n",
      "Epoch 1830, Train Loss: 10.9766, Test Loss: 1.9556\n",
      "Epoch 1831, Train Loss: 10.4119, Test Loss: 1.9534\n",
      "Epoch 1832, Train Loss: 9.1917, Test Loss: 1.9540\n",
      "Epoch 1833, Train Loss: 9.5426, Test Loss: 1.9509\n",
      "Epoch 1834, Train Loss: 9.4812, Test Loss: 1.9453\n",
      "Epoch 1835, Train Loss: 11.9125, Test Loss: 1.9421\n",
      "Epoch 1836, Train Loss: 11.2124, Test Loss: 1.9438\n",
      "Epoch 1837, Train Loss: 7.5119, Test Loss: 1.9437\n",
      "Epoch 1838, Train Loss: 10.0601, Test Loss: 1.9411\n",
      "Epoch 1839, Train Loss: 10.0399, Test Loss: 1.9383\n",
      "Epoch 1840, Train Loss: 9.9608, Test Loss: 1.9382\n",
      "Epoch 1841, Train Loss: 10.1190, Test Loss: 1.9403\n",
      "Epoch 1842, Train Loss: 9.2186, Test Loss: 1.9400\n",
      "Epoch 1843, Train Loss: 10.0460, Test Loss: 1.9367\n",
      "Epoch 1844, Train Loss: 9.3738, Test Loss: 1.9356\n",
      "Epoch 1845, Train Loss: 8.6708, Test Loss: 1.9347\n",
      "Epoch 1846, Train Loss: 9.7977, Test Loss: 1.9346\n",
      "Epoch 1847, Train Loss: 7.9179, Test Loss: 1.9381\n",
      "Epoch 1848, Train Loss: 8.4714, Test Loss: 1.9347\n",
      "Epoch 1849, Train Loss: 9.0682, Test Loss: 1.9343\n",
      "Epoch 1850, Train Loss: 8.1575, Test Loss: 1.9337\n",
      "Epoch 1851, Train Loss: 10.6007, Test Loss: 1.9367\n",
      "Epoch 1852, Train Loss: 10.3085, Test Loss: 1.9402\n",
      "Epoch 1853, Train Loss: 8.0086, Test Loss: 1.9399\n",
      "Epoch 1854, Train Loss: 10.5179, Test Loss: 1.9398\n",
      "Epoch 1855, Train Loss: 8.4744, Test Loss: 1.9400\n",
      "Epoch 1856, Train Loss: 9.6675, Test Loss: 1.9318\n",
      "Epoch 1857, Train Loss: 7.9631, Test Loss: 1.9194\n",
      "Epoch 1858, Train Loss: 9.7939, Test Loss: 1.9098\n",
      "Epoch 1859, Train Loss: 10.0930, Test Loss: 1.9059\n",
      "Epoch 1860, Train Loss: 10.5924, Test Loss: 1.9057\n",
      "Epoch 1861, Train Loss: 9.8884, Test Loss: 1.9096\n",
      "Epoch 1862, Train Loss: 9.6380, Test Loss: 1.9111\n",
      "Epoch 1863, Train Loss: 9.9181, Test Loss: 1.9104\n",
      "Epoch 1864, Train Loss: 8.3041, Test Loss: 1.9078\n",
      "Epoch 1865, Train Loss: 8.5940, Test Loss: 1.8987\n",
      "Epoch 1866, Train Loss: 7.7426, Test Loss: 1.8899\n",
      "Epoch 1867, Train Loss: 9.8806, Test Loss: 1.8800\n",
      "Epoch 1868, Train Loss: 9.4881, Test Loss: 1.8688\n",
      "Epoch 1869, Train Loss: 8.0447, Test Loss: 1.8638\n",
      "Epoch 1870, Train Loss: 6.9510, Test Loss: 1.8592\n",
      "Epoch 1871, Train Loss: 8.8795, Test Loss: 1.8580\n",
      "Epoch 1872, Train Loss: 8.4080, Test Loss: 1.8597\n",
      "Epoch 1873, Train Loss: 8.6730, Test Loss: 1.8641\n",
      "Epoch 1874, Train Loss: 8.6954, Test Loss: 1.8680\n",
      "Epoch 1875, Train Loss: 9.0730, Test Loss: 1.8715\n",
      "Epoch 1876, Train Loss: 8.4433, Test Loss: 1.8783\n",
      "Epoch 1877, Train Loss: 8.0869, Test Loss: 1.8866\n",
      "Epoch 1878, Train Loss: 8.0381, Test Loss: 1.8905\n",
      "Epoch 1879, Train Loss: 8.2788, Test Loss: 1.8916\n",
      "Epoch 1880, Train Loss: 8.8801, Test Loss: 1.8918\n",
      "Epoch 1881, Train Loss: 7.7351, Test Loss: 1.8914\n",
      "Epoch 1882, Train Loss: 8.2829, Test Loss: 1.8934\n",
      "Epoch 1883, Train Loss: 10.1234, Test Loss: 1.8934\n",
      "Epoch 1884, Train Loss: 9.0990, Test Loss: 1.8913\n",
      "Epoch 1885, Train Loss: 8.9798, Test Loss: 1.8900\n",
      "Epoch 1886, Train Loss: 7.9705, Test Loss: 1.8914\n",
      "Epoch 1887, Train Loss: 7.8050, Test Loss: 1.8915\n",
      "Epoch 1888, Train Loss: 8.7046, Test Loss: 1.8925\n",
      "Epoch 1889, Train Loss: 10.1197, Test Loss: 1.8967\n",
      "Epoch 1890, Train Loss: 9.3672, Test Loss: 1.9075\n",
      "Epoch 1891, Train Loss: 9.8128, Test Loss: 1.9177\n",
      "Epoch 1892, Train Loss: 8.3192, Test Loss: 1.9201\n",
      "Epoch 1893, Train Loss: 7.9712, Test Loss: 1.9208\n",
      "Epoch 1894, Train Loss: 9.2164, Test Loss: 1.9207\n",
      "Epoch 1895, Train Loss: 9.0308, Test Loss: 1.9235\n",
      "Epoch 1896, Train Loss: 8.6855, Test Loss: 1.9168\n",
      "Epoch 1897, Train Loss: 7.9917, Test Loss: 1.9121\n",
      "Epoch 1898, Train Loss: 8.9336, Test Loss: 1.9027\n",
      "Epoch 1899, Train Loss: 8.5254, Test Loss: 1.8866\n",
      "Epoch 1900, Train Loss: 7.7417, Test Loss: 1.8700\n",
      "Epoch 1901, Train Loss: 9.1581, Test Loss: 1.8561\n",
      "Epoch 1902, Train Loss: 8.1061, Test Loss: 1.8473\n",
      "Epoch 1903, Train Loss: 8.3460, Test Loss: 1.8407\n",
      "Epoch 1904, Train Loss: 7.3028, Test Loss: 1.8345\n",
      "Epoch 1905, Train Loss: 8.4288, Test Loss: 1.8340\n",
      "Epoch 1906, Train Loss: 7.9301, Test Loss: 1.8347\n",
      "Epoch 1907, Train Loss: 8.2535, Test Loss: 1.8402\n",
      "Epoch 1908, Train Loss: 8.7080, Test Loss: 1.8444\n",
      "Epoch 1909, Train Loss: 7.5843, Test Loss: 1.8515\n",
      "Epoch 1910, Train Loss: 7.8307, Test Loss: 1.8551\n",
      "Epoch 1911, Train Loss: 8.0763, Test Loss: 1.8605\n",
      "Epoch 1912, Train Loss: 7.9192, Test Loss: 1.8689\n",
      "Epoch 1913, Train Loss: 8.9738, Test Loss: 1.8834\n",
      "Epoch 1914, Train Loss: 8.9368, Test Loss: 1.8868\n",
      "Epoch 1915, Train Loss: 8.9041, Test Loss: 1.8909\n",
      "Epoch 1916, Train Loss: 8.5538, Test Loss: 1.8993\n",
      "Epoch 1917, Train Loss: 7.7738, Test Loss: 1.9034\n",
      "Epoch 1918, Train Loss: 7.9375, Test Loss: 1.8981\n",
      "Epoch 1919, Train Loss: 8.2841, Test Loss: 1.8909\n",
      "Epoch 1920, Train Loss: 7.9520, Test Loss: 1.8816\n",
      "Epoch 1921, Train Loss: 9.1539, Test Loss: 1.8684\n",
      "Epoch 1922, Train Loss: 8.1139, Test Loss: 1.8503\n",
      "Epoch 1923, Train Loss: 8.0486, Test Loss: 1.8321\n",
      "Epoch 1924, Train Loss: 7.9456, Test Loss: 1.8136\n",
      "Epoch 1925, Train Loss: 8.2491, Test Loss: 1.7920\n",
      "Epoch 1926, Train Loss: 7.1608, Test Loss: 1.7736\n",
      "Epoch 1927, Train Loss: 7.8251, Test Loss: 1.7584\n",
      "Epoch 1928, Train Loss: 9.4581, Test Loss: 1.7446\n",
      "Epoch 1929, Train Loss: 7.9708, Test Loss: 1.7352\n",
      "Epoch 1930, Train Loss: 7.9191, Test Loss: 1.7281\n",
      "Epoch 1931, Train Loss: 7.2722, Test Loss: 1.7244\n",
      "Epoch 1932, Train Loss: 8.7985, Test Loss: 1.7236\n",
      "Epoch 1933, Train Loss: 7.1374, Test Loss: 1.7268\n",
      "Epoch 1934, Train Loss: 7.7260, Test Loss: 1.7317\n",
      "Epoch 1935, Train Loss: 9.5152, Test Loss: 1.7364\n",
      "Epoch 1936, Train Loss: 7.0551, Test Loss: 1.7390\n",
      "Epoch 1937, Train Loss: 9.0077, Test Loss: 1.7429\n",
      "Epoch 1938, Train Loss: 7.4580, Test Loss: 1.7475\n",
      "Epoch 1939, Train Loss: 7.8229, Test Loss: 1.7567\n",
      "Epoch 1940, Train Loss: 8.3894, Test Loss: 1.7657\n",
      "Epoch 1941, Train Loss: 8.3125, Test Loss: 1.7792\n",
      "Epoch 1942, Train Loss: 7.5132, Test Loss: 1.7974\n",
      "Epoch 1943, Train Loss: 8.1835, Test Loss: 1.8131\n",
      "Epoch 1944, Train Loss: 8.7119, Test Loss: 1.8283\n",
      "Epoch 1945, Train Loss: 7.5235, Test Loss: 1.8391\n",
      "Epoch 1946, Train Loss: 8.9210, Test Loss: 1.8421\n",
      "Epoch 1947, Train Loss: 7.4056, Test Loss: 1.8463\n",
      "Epoch 1948, Train Loss: 8.2272, Test Loss: 1.8445\n",
      "Epoch 1949, Train Loss: 7.9083, Test Loss: 1.8429\n",
      "Epoch 1950, Train Loss: 9.6390, Test Loss: 1.8408\n",
      "Epoch 1951, Train Loss: 7.2662, Test Loss: 1.8386\n",
      "Epoch 1952, Train Loss: 7.3652, Test Loss: 1.8408\n",
      "Epoch 1953, Train Loss: 8.3456, Test Loss: 1.8363\n",
      "Epoch 1954, Train Loss: 8.5550, Test Loss: 1.8277\n",
      "Epoch 1955, Train Loss: 7.8788, Test Loss: 1.8212\n",
      "Epoch 1956, Train Loss: 7.7698, Test Loss: 1.8090\n",
      "Epoch 1957, Train Loss: 6.5837, Test Loss: 1.8044\n",
      "Epoch 1958, Train Loss: 7.5890, Test Loss: 1.7947\n",
      "Epoch 1959, Train Loss: 7.7519, Test Loss: 1.7864\n",
      "Epoch 1960, Train Loss: 9.0096, Test Loss: 1.7780\n",
      "Epoch 1961, Train Loss: 7.7472, Test Loss: 1.7709\n",
      "Epoch 1962, Train Loss: 9.1475, Test Loss: 1.7674\n",
      "Epoch 1963, Train Loss: 6.6120, Test Loss: 1.7656\n",
      "Epoch 1964, Train Loss: 7.2173, Test Loss: 1.7647\n",
      "Epoch 1965, Train Loss: 7.0610, Test Loss: 1.7654\n",
      "Epoch 1966, Train Loss: 7.2372, Test Loss: 1.7693\n",
      "Epoch 1967, Train Loss: 6.5666, Test Loss: 1.7758\n",
      "Epoch 1968, Train Loss: 9.1491, Test Loss: 1.7879\n",
      "Epoch 1969, Train Loss: 8.5950, Test Loss: 1.7975\n",
      "Epoch 1970, Train Loss: 7.5424, Test Loss: 1.8061\n",
      "Epoch 1971, Train Loss: 6.8068, Test Loss: 1.8183\n",
      "Epoch 1972, Train Loss: 7.8460, Test Loss: 1.8300\n",
      "Epoch 1973, Train Loss: 7.2021, Test Loss: 1.8375\n",
      "Epoch 1974, Train Loss: 8.5827, Test Loss: 1.8351\n",
      "Epoch 1975, Train Loss: 6.7891, Test Loss: 1.8223\n",
      "Epoch 1976, Train Loss: 9.2342, Test Loss: 1.8055\n",
      "Epoch 1977, Train Loss: 6.7923, Test Loss: 1.7920\n",
      "Epoch 1978, Train Loss: 10.0965, Test Loss: 1.7804\n",
      "Epoch 1979, Train Loss: 8.6152, Test Loss: 1.7693\n",
      "Epoch 1980, Train Loss: 7.1416, Test Loss: 1.7615\n",
      "Epoch 1981, Train Loss: 6.9217, Test Loss: 1.7579\n",
      "Epoch 1982, Train Loss: 7.5174, Test Loss: 1.7530\n",
      "Epoch 1983, Train Loss: 7.7736, Test Loss: 1.7488\n",
      "Epoch 1984, Train Loss: 7.7495, Test Loss: 1.7398\n",
      "Epoch 1985, Train Loss: 8.4055, Test Loss: 1.7379\n",
      "Epoch 1986, Train Loss: 8.9588, Test Loss: 1.7383\n",
      "Epoch 1987, Train Loss: 8.1998, Test Loss: 1.7426\n",
      "Epoch 1988, Train Loss: 8.6550, Test Loss: 1.7488\n",
      "Epoch 1989, Train Loss: 6.8166, Test Loss: 1.7482\n",
      "Epoch 1990, Train Loss: 6.8269, Test Loss: 1.7448\n",
      "Epoch 1991, Train Loss: 7.3972, Test Loss: 1.7435\n",
      "Epoch 1992, Train Loss: 8.7859, Test Loss: 1.7384\n",
      "Epoch 1993, Train Loss: 7.3070, Test Loss: 1.7272\n",
      "Epoch 1994, Train Loss: 7.6176, Test Loss: 1.7183\n",
      "Epoch 1995, Train Loss: 6.6616, Test Loss: 1.7094\n",
      "Epoch 1996, Train Loss: 8.8602, Test Loss: 1.7041\n",
      "Epoch 1997, Train Loss: 7.2303, Test Loss: 1.6976\n",
      "Epoch 1998, Train Loss: 7.3277, Test Loss: 1.6874\n",
      "Epoch 1999, Train Loss: 8.1857, Test Loss: 1.6765\n",
      "Epoch 2000, Train Loss: 6.6915, Test Loss: 1.6664\n",
      "Epoch 2001, Train Loss: 8.1191, Test Loss: 1.6623\n",
      "Epoch 2002, Train Loss: 6.5521, Test Loss: 1.6613\n",
      "Epoch 2003, Train Loss: 7.1327, Test Loss: 1.6598\n",
      "Epoch 2004, Train Loss: 7.5831, Test Loss: 1.6598\n",
      "Epoch 2005, Train Loss: 7.2157, Test Loss: 1.6630\n",
      "Epoch 2006, Train Loss: 7.7379, Test Loss: 1.6672\n",
      "Epoch 2007, Train Loss: 7.2139, Test Loss: 1.6727\n",
      "Epoch 2008, Train Loss: 6.8549, Test Loss: 1.6765\n",
      "Epoch 2009, Train Loss: 7.6906, Test Loss: 1.6779\n",
      "Epoch 2010, Train Loss: 8.7546, Test Loss: 1.6836\n",
      "Epoch 2011, Train Loss: 7.7829, Test Loss: 1.6872\n",
      "Epoch 2012, Train Loss: 7.4942, Test Loss: 1.6910\n",
      "Epoch 2013, Train Loss: 7.2532, Test Loss: 1.6944\n",
      "Epoch 2014, Train Loss: 7.3460, Test Loss: 1.6953\n",
      "Epoch 2015, Train Loss: 6.8455, Test Loss: 1.6922\n",
      "Epoch 2016, Train Loss: 8.1204, Test Loss: 1.6844\n",
      "Epoch 2017, Train Loss: 7.4132, Test Loss: 1.6772\n",
      "Epoch 2018, Train Loss: 7.0044, Test Loss: 1.6795\n",
      "Epoch 2019, Train Loss: 6.9593, Test Loss: 1.6763\n",
      "Epoch 2020, Train Loss: 6.1307, Test Loss: 1.6734\n",
      "Epoch 2021, Train Loss: 8.4966, Test Loss: 1.6699\n",
      "Epoch 2022, Train Loss: 6.8791, Test Loss: 1.6598\n",
      "Epoch 2023, Train Loss: 8.7106, Test Loss: 1.6532\n",
      "Epoch 2024, Train Loss: 7.5671, Test Loss: 1.6480\n",
      "Epoch 2025, Train Loss: 7.0702, Test Loss: 1.6441\n",
      "Epoch 2026, Train Loss: 6.1538, Test Loss: 1.6402\n",
      "Epoch 2027, Train Loss: 6.7148, Test Loss: 1.6366\n",
      "Epoch 2028, Train Loss: 9.3916, Test Loss: 1.6360\n",
      "Epoch 2029, Train Loss: 5.9304, Test Loss: 1.6377\n",
      "Epoch 2030, Train Loss: 6.0156, Test Loss: 1.6437\n",
      "Epoch 2031, Train Loss: 7.2765, Test Loss: 1.6531\n",
      "Epoch 2032, Train Loss: 7.3694, Test Loss: 1.6645\n",
      "Epoch 2033, Train Loss: 5.6852, Test Loss: 1.6766\n",
      "Epoch 2034, Train Loss: 6.8153, Test Loss: 1.6948\n",
      "Epoch 2035, Train Loss: 7.3769, Test Loss: 1.7177\n",
      "Epoch 2036, Train Loss: 6.8550, Test Loss: 1.7430\n",
      "Epoch 2037, Train Loss: 7.8867, Test Loss: 1.7647\n",
      "Epoch 2038, Train Loss: 7.4071, Test Loss: 1.7809\n",
      "Epoch 2039, Train Loss: 7.6655, Test Loss: 1.7919\n",
      "Epoch 2040, Train Loss: 9.0601, Test Loss: 1.8023\n",
      "Epoch 2041, Train Loss: 6.6664, Test Loss: 1.8077\n",
      "Epoch 2042, Train Loss: 7.2057, Test Loss: 1.8114\n",
      "Epoch 2043, Train Loss: 8.3356, Test Loss: 1.8057\n",
      "Epoch 2044, Train Loss: 6.9162, Test Loss: 1.8055\n",
      "Epoch 2045, Train Loss: 8.0988, Test Loss: 1.8032\n",
      "Epoch 2046, Train Loss: 6.6623, Test Loss: 1.7984\n",
      "Epoch 2047, Train Loss: 6.7261, Test Loss: 1.7955\n",
      "Epoch 2048, Train Loss: 7.2456, Test Loss: 1.7910\n",
      "Epoch 2049, Train Loss: 6.6184, Test Loss: 1.7886\n",
      "Epoch 2050, Train Loss: 9.0170, Test Loss: 1.7799\n",
      "Epoch 2051, Train Loss: 6.7150, Test Loss: 1.7708\n",
      "Epoch 2052, Train Loss: 6.1198, Test Loss: 1.7612\n",
      "Epoch 2053, Train Loss: 7.4454, Test Loss: 1.7493\n",
      "Epoch 2054, Train Loss: 7.1525, Test Loss: 1.7417\n",
      "Epoch 2055, Train Loss: 8.4593, Test Loss: 1.7258\n",
      "Epoch 2056, Train Loss: 7.4280, Test Loss: 1.7120\n",
      "Epoch 2057, Train Loss: 7.2817, Test Loss: 1.7082\n",
      "Epoch 2058, Train Loss: 7.8352, Test Loss: 1.7004\n",
      "Epoch 2059, Train Loss: 7.0797, Test Loss: 1.7025\n",
      "Epoch 2060, Train Loss: 7.4225, Test Loss: 1.7096\n",
      "Epoch 2061, Train Loss: 6.4390, Test Loss: 1.7140\n",
      "Epoch 2062, Train Loss: 7.0181, Test Loss: 1.7100\n",
      "Epoch 2063, Train Loss: 6.5801, Test Loss: 1.7008\n",
      "Epoch 2064, Train Loss: 6.7699, Test Loss: 1.6912\n",
      "Epoch 2065, Train Loss: 7.6550, Test Loss: 1.6949\n",
      "Epoch 2066, Train Loss: 5.7222, Test Loss: 1.6979\n",
      "Epoch 2067, Train Loss: 6.6047, Test Loss: 1.6926\n",
      "Epoch 2068, Train Loss: 6.8260, Test Loss: 1.6831\n",
      "Epoch 2069, Train Loss: 7.0930, Test Loss: 1.6778\n",
      "Epoch 2070, Train Loss: 7.1101, Test Loss: 1.6807\n",
      "Epoch 2071, Train Loss: 6.7493, Test Loss: 1.6777\n",
      "Epoch 2072, Train Loss: 5.7968, Test Loss: 1.6736\n",
      "Epoch 2073, Train Loss: 6.3885, Test Loss: 1.6761\n",
      "Epoch 2074, Train Loss: 5.5007, Test Loss: 1.6733\n",
      "Epoch 2075, Train Loss: 7.3069, Test Loss: 1.6708\n",
      "Epoch 2076, Train Loss: 6.8933, Test Loss: 1.6685\n",
      "Epoch 2077, Train Loss: 7.2344, Test Loss: 1.6592\n",
      "Epoch 2078, Train Loss: 7.1913, Test Loss: 1.6554\n",
      "Epoch 2079, Train Loss: 7.0924, Test Loss: 1.6511\n",
      "Epoch 2080, Train Loss: 7.8420, Test Loss: 1.6555\n",
      "Epoch 2081, Train Loss: 6.4861, Test Loss: 1.6677\n",
      "Epoch 2082, Train Loss: 6.2994, Test Loss: 1.6743\n",
      "Epoch 2083, Train Loss: 6.1409, Test Loss: 1.6781\n",
      "Epoch 2084, Train Loss: 7.1601, Test Loss: 1.6847\n",
      "Epoch 2085, Train Loss: 5.8668, Test Loss: 1.6902\n",
      "Epoch 2086, Train Loss: 6.5563, Test Loss: 1.6923\n",
      "Epoch 2087, Train Loss: 6.0454, Test Loss: 1.6959\n",
      "Epoch 2088, Train Loss: 6.8347, Test Loss: 1.7047\n",
      "Epoch 2089, Train Loss: 6.9553, Test Loss: 1.7209\n",
      "Epoch 2090, Train Loss: 5.6867, Test Loss: 1.7366\n",
      "Epoch 2091, Train Loss: 6.8751, Test Loss: 1.7404\n",
      "Epoch 2092, Train Loss: 7.5329, Test Loss: 1.7385\n",
      "Epoch 2093, Train Loss: 6.1135, Test Loss: 1.7329\n",
      "Epoch 2094, Train Loss: 6.5983, Test Loss: 1.7154\n",
      "Epoch 2095, Train Loss: 6.8281, Test Loss: 1.7033\n",
      "Epoch 2096, Train Loss: 5.4383, Test Loss: 1.6908\n",
      "Epoch 2097, Train Loss: 5.8315, Test Loss: 1.6755\n",
      "Epoch 2098, Train Loss: 6.4143, Test Loss: 1.6582\n",
      "Epoch 2099, Train Loss: 6.4278, Test Loss: 1.6392\n",
      "Epoch 2100, Train Loss: 6.2860, Test Loss: 1.6326\n",
      "Epoch 2101, Train Loss: 6.3773, Test Loss: 1.6214\n",
      "Epoch 2102, Train Loss: 6.8914, Test Loss: 1.6157\n",
      "Epoch 2103, Train Loss: 7.0973, Test Loss: 1.6108\n",
      "Epoch 2104, Train Loss: 6.2760, Test Loss: 1.6026\n",
      "Epoch 2105, Train Loss: 5.6747, Test Loss: 1.5957\n",
      "Epoch 2106, Train Loss: 6.8060, Test Loss: 1.5868\n",
      "Epoch 2107, Train Loss: 5.6119, Test Loss: 1.5779\n",
      "Epoch 2108, Train Loss: 6.8220, Test Loss: 1.5737\n",
      "Epoch 2109, Train Loss: 6.5634, Test Loss: 1.5690\n",
      "Epoch 2110, Train Loss: 6.2713, Test Loss: 1.5658\n",
      "Epoch 2111, Train Loss: 7.3944, Test Loss: 1.5629\n",
      "Epoch 2112, Train Loss: 5.9954, Test Loss: 1.5624\n",
      "Epoch 2113, Train Loss: 5.9254, Test Loss: 1.5591\n",
      "Epoch 2114, Train Loss: 6.4886, Test Loss: 1.5558\n",
      "Epoch 2115, Train Loss: 5.4574, Test Loss: 1.5549\n",
      "Epoch 2116, Train Loss: 5.8781, Test Loss: 1.5568\n",
      "Epoch 2117, Train Loss: 6.4594, Test Loss: 1.5615\n",
      "Epoch 2118, Train Loss: 5.4365, Test Loss: 1.5696\n",
      "Epoch 2119, Train Loss: 6.0734, Test Loss: 1.5801\n",
      "Epoch 2120, Train Loss: 5.5101, Test Loss: 1.5919\n",
      "Epoch 2121, Train Loss: 6.1443, Test Loss: 1.6000\n",
      "Epoch 2122, Train Loss: 6.3084, Test Loss: 1.6070\n",
      "Epoch 2123, Train Loss: 6.8707, Test Loss: 1.6109\n",
      "Epoch 2124, Train Loss: 7.3374, Test Loss: 1.6133\n",
      "Epoch 2125, Train Loss: 6.8194, Test Loss: 1.6170\n",
      "Epoch 2126, Train Loss: 5.9422, Test Loss: 1.6168\n",
      "Epoch 2127, Train Loss: 5.5235, Test Loss: 1.6204\n",
      "Epoch 2128, Train Loss: 5.8667, Test Loss: 1.6289\n",
      "Epoch 2129, Train Loss: 6.0490, Test Loss: 1.6315\n",
      "Epoch 2130, Train Loss: 4.9912, Test Loss: 1.6242\n",
      "Epoch 2131, Train Loss: 6.2148, Test Loss: 1.6075\n",
      "Epoch 2132, Train Loss: 6.3426, Test Loss: 1.5884\n",
      "Epoch 2133, Train Loss: 7.2974, Test Loss: 1.5743\n",
      "Epoch 2134, Train Loss: 6.0170, Test Loss: 1.5665\n",
      "Epoch 2135, Train Loss: 5.6142, Test Loss: 1.5630\n",
      "Epoch 2136, Train Loss: 6.1820, Test Loss: 1.5652\n",
      "Epoch 2137, Train Loss: 7.1519, Test Loss: 1.5717\n",
      "Epoch 2138, Train Loss: 5.9165, Test Loss: 1.5764\n",
      "Epoch 2139, Train Loss: 6.1326, Test Loss: 1.5747\n",
      "Epoch 2140, Train Loss: 6.8990, Test Loss: 1.5685\n",
      "Epoch 2141, Train Loss: 6.0477, Test Loss: 1.5670\n",
      "Epoch 2142, Train Loss: 7.3258, Test Loss: 1.5625\n",
      "Epoch 2143, Train Loss: 6.9539, Test Loss: 1.5621\n",
      "Epoch 2144, Train Loss: 6.4774, Test Loss: 1.5590\n",
      "Epoch 2145, Train Loss: 6.0980, Test Loss: 1.5559\n",
      "Epoch 2146, Train Loss: 5.9097, Test Loss: 1.5537\n",
      "Epoch 2147, Train Loss: 6.8371, Test Loss: 1.5554\n",
      "Epoch 2148, Train Loss: 6.8432, Test Loss: 1.5629\n",
      "Epoch 2149, Train Loss: 6.3954, Test Loss: 1.5761\n",
      "Epoch 2150, Train Loss: 6.3952, Test Loss: 1.5913\n",
      "Epoch 2151, Train Loss: 6.3215, Test Loss: 1.6008\n",
      "Epoch 2152, Train Loss: 6.5215, Test Loss: 1.6028\n",
      "Epoch 2153, Train Loss: 6.2980, Test Loss: 1.6084\n",
      "Epoch 2154, Train Loss: 6.1308, Test Loss: 1.6125\n",
      "Epoch 2155, Train Loss: 4.9675, Test Loss: 1.6164\n",
      "Epoch 2156, Train Loss: 6.3784, Test Loss: 1.6214\n",
      "Epoch 2157, Train Loss: 4.9104, Test Loss: 1.6318\n",
      "Epoch 2158, Train Loss: 6.2925, Test Loss: 1.6473\n",
      "Epoch 2159, Train Loss: 6.5625, Test Loss: 1.6669\n",
      "Epoch 2160, Train Loss: 6.7602, Test Loss: 1.6785\n",
      "Epoch 2161, Train Loss: 5.9824, Test Loss: 1.6921\n",
      "Epoch 2162, Train Loss: 4.5670, Test Loss: 1.7035\n",
      "Epoch 2163, Train Loss: 6.0211, Test Loss: 1.7101\n",
      "Epoch 2164, Train Loss: 7.6420, Test Loss: 1.7088\n",
      "Epoch 2165, Train Loss: 5.1899, Test Loss: 1.7089\n",
      "Epoch 2166, Train Loss: 7.2868, Test Loss: 1.7098\n",
      "Epoch 2167, Train Loss: 6.3052, Test Loss: 1.7075\n",
      "Epoch 2168, Train Loss: 5.4061, Test Loss: 1.6962\n",
      "Epoch 2169, Train Loss: 6.6930, Test Loss: 1.6810\n",
      "Epoch 2170, Train Loss: 5.1454, Test Loss: 1.6639\n",
      "Epoch 2171, Train Loss: 6.4105, Test Loss: 1.6406\n",
      "Epoch 2172, Train Loss: 6.2159, Test Loss: 1.6255\n",
      "Epoch 2173, Train Loss: 5.7502, Test Loss: 1.6096\n",
      "Epoch 2174, Train Loss: 6.1924, Test Loss: 1.5949\n",
      "Epoch 2175, Train Loss: 5.9085, Test Loss: 1.5837\n",
      "Epoch 2176, Train Loss: 5.8652, Test Loss: 1.5805\n",
      "Epoch 2177, Train Loss: 5.4257, Test Loss: 1.5823\n",
      "Epoch 2178, Train Loss: 6.3701, Test Loss: 1.5801\n",
      "Epoch 2179, Train Loss: 5.2634, Test Loss: 1.5803\n",
      "Epoch 2180, Train Loss: 4.4310, Test Loss: 1.5823\n",
      "Epoch 2181, Train Loss: 5.3931, Test Loss: 1.5827\n",
      "Epoch 2182, Train Loss: 5.9986, Test Loss: 1.5821\n",
      "Epoch 2183, Train Loss: 5.7422, Test Loss: 1.5859\n",
      "Epoch 2184, Train Loss: 5.5929, Test Loss: 1.5858\n",
      "Epoch 2185, Train Loss: 6.2336, Test Loss: 1.5791\n",
      "Epoch 2186, Train Loss: 5.9690, Test Loss: 1.5780\n",
      "Epoch 2187, Train Loss: 6.2739, Test Loss: 1.5778\n",
      "Epoch 2188, Train Loss: 5.8909, Test Loss: 1.5769\n",
      "Epoch 2189, Train Loss: 5.2267, Test Loss: 1.5753\n",
      "Epoch 2190, Train Loss: 5.6883, Test Loss: 1.5771\n",
      "Epoch 2191, Train Loss: 7.3829, Test Loss: 1.5803\n",
      "Epoch 2192, Train Loss: 6.1829, Test Loss: 1.5810\n",
      "Epoch 2193, Train Loss: 5.3743, Test Loss: 1.5819\n",
      "Epoch 2194, Train Loss: 5.2722, Test Loss: 1.5886\n",
      "Epoch 2195, Train Loss: 5.6904, Test Loss: 1.5969\n",
      "Epoch 2196, Train Loss: 6.1751, Test Loss: 1.5982\n",
      "Epoch 2197, Train Loss: 5.7392, Test Loss: 1.5986\n",
      "Epoch 2198, Train Loss: 6.7014, Test Loss: 1.5959\n",
      "Epoch 2199, Train Loss: 6.0706, Test Loss: 1.5867\n",
      "Epoch 2200, Train Loss: 6.1808, Test Loss: 1.5785\n",
      "Epoch 2201, Train Loss: 5.7909, Test Loss: 1.5736\n",
      "Epoch 2202, Train Loss: 6.3763, Test Loss: 1.5617\n",
      "Epoch 2203, Train Loss: 6.8033, Test Loss: 1.5538\n",
      "Epoch 2204, Train Loss: 7.2005, Test Loss: 1.5489\n",
      "Epoch 2205, Train Loss: 5.5845, Test Loss: 1.5467\n",
      "Epoch 2206, Train Loss: 6.0481, Test Loss: 1.5449\n",
      "Epoch 2207, Train Loss: 6.5945, Test Loss: 1.5467\n",
      "Epoch 2208, Train Loss: 5.3633, Test Loss: 1.5452\n",
      "Epoch 2209, Train Loss: 5.9951, Test Loss: 1.5480\n",
      "Epoch 2210, Train Loss: 6.1558, Test Loss: 1.5524\n",
      "Epoch 2211, Train Loss: 5.6202, Test Loss: 1.5575\n",
      "Epoch 2212, Train Loss: 6.0787, Test Loss: 1.5583\n",
      "Epoch 2213, Train Loss: 6.5074, Test Loss: 1.5606\n",
      "Epoch 2214, Train Loss: 5.1673, Test Loss: 1.5654\n",
      "Epoch 2215, Train Loss: 6.0526, Test Loss: 1.5709\n",
      "Epoch 2216, Train Loss: 5.8843, Test Loss: 1.5727\n",
      "Epoch 2217, Train Loss: 4.9180, Test Loss: 1.5725\n",
      "Epoch 2218, Train Loss: 5.0333, Test Loss: 1.5760\n",
      "Epoch 2219, Train Loss: 5.1363, Test Loss: 1.5810\n",
      "Epoch 2220, Train Loss: 5.3203, Test Loss: 1.5898\n",
      "Epoch 2221, Train Loss: 5.0302, Test Loss: 1.6020\n",
      "Epoch 2222, Train Loss: 6.4936, Test Loss: 1.6090\n",
      "Epoch 2223, Train Loss: 6.4272, Test Loss: 1.6056\n",
      "Epoch 2224, Train Loss: 6.0469, Test Loss: 1.6040\n",
      "Epoch 2225, Train Loss: 6.0113, Test Loss: 1.6023\n",
      "Epoch 2226, Train Loss: 6.4202, Test Loss: 1.6008\n",
      "Epoch 2227, Train Loss: 5.0302, Test Loss: 1.6071\n",
      "Epoch 2228, Train Loss: 5.3261, Test Loss: 1.6126\n",
      "Epoch 2229, Train Loss: 5.4902, Test Loss: 1.6158\n",
      "Epoch 2230, Train Loss: 6.4429, Test Loss: 1.6285\n",
      "Epoch 2231, Train Loss: 5.1453, Test Loss: 1.6427\n",
      "Epoch 2232, Train Loss: 5.4575, Test Loss: 1.6559\n",
      "Epoch 2233, Train Loss: 5.4583, Test Loss: 1.6781\n",
      "Epoch 2234, Train Loss: 5.9482, Test Loss: 1.6885\n",
      "Epoch 2235, Train Loss: 5.8430, Test Loss: 1.6923\n",
      "Epoch 2236, Train Loss: 4.5831, Test Loss: 1.6887\n",
      "Epoch 2237, Train Loss: 7.0724, Test Loss: 1.6866\n",
      "Epoch 2238, Train Loss: 5.0212, Test Loss: 1.6758\n",
      "Epoch 2239, Train Loss: 5.7199, Test Loss: 1.6652\n",
      "Epoch 2240, Train Loss: 5.4662, Test Loss: 1.6515\n",
      "Epoch 2241, Train Loss: 5.2439, Test Loss: 1.6416\n",
      "Epoch 2242, Train Loss: 4.7008, Test Loss: 1.6346\n",
      "Epoch 2243, Train Loss: 5.0189, Test Loss: 1.6244\n",
      "Epoch 2244, Train Loss: 5.4572, Test Loss: 1.6191\n",
      "Epoch 2245, Train Loss: 5.6247, Test Loss: 1.6095\n",
      "Epoch 2246, Train Loss: 6.1621, Test Loss: 1.6022\n",
      "Epoch 2247, Train Loss: 6.1821, Test Loss: 1.5915\n",
      "Epoch 2248, Train Loss: 5.8202, Test Loss: 1.5855\n",
      "Epoch 2249, Train Loss: 6.0233, Test Loss: 1.5829\n",
      "Epoch 2250, Train Loss: 5.5027, Test Loss: 1.5804\n",
      "Epoch 2251, Train Loss: 5.1817, Test Loss: 1.5832\n",
      "Epoch 2252, Train Loss: 5.3240, Test Loss: 1.5859\n",
      "Epoch 2253, Train Loss: 5.9238, Test Loss: 1.5916\n",
      "Epoch 2254, Train Loss: 5.7995, Test Loss: 1.5963\n",
      "Epoch 2255, Train Loss: 4.7135, Test Loss: 1.5989\n",
      "Epoch 2256, Train Loss: 6.7448, Test Loss: 1.5980\n",
      "Epoch 2257, Train Loss: 4.6679, Test Loss: 1.5990\n",
      "Epoch 2258, Train Loss: 5.9838, Test Loss: 1.6128\n",
      "Epoch 2259, Train Loss: 5.0850, Test Loss: 1.6287\n",
      "Epoch 2260, Train Loss: 6.6700, Test Loss: 1.6487\n",
      "Epoch 2261, Train Loss: 5.0509, Test Loss: 1.6680\n",
      "Epoch 2262, Train Loss: 6.2264, Test Loss: 1.6822\n",
      "Epoch 2263, Train Loss: 5.1231, Test Loss: 1.6988\n",
      "Epoch 2264, Train Loss: 7.1003, Test Loss: 1.7179\n",
      "Epoch 2265, Train Loss: 5.5976, Test Loss: 1.7218\n",
      "Epoch 2266, Train Loss: 5.6214, Test Loss: 1.7294\n",
      "Epoch 2267, Train Loss: 4.9247, Test Loss: 1.7351\n",
      "Epoch 2268, Train Loss: 5.2085, Test Loss: 1.7326\n",
      "Epoch 2269, Train Loss: 6.3850, Test Loss: 1.7144\n",
      "Epoch 2270, Train Loss: 5.7699, Test Loss: 1.6964\n",
      "Epoch 2271, Train Loss: 5.4223, Test Loss: 1.6809\n",
      "Epoch 2272, Train Loss: 5.0395, Test Loss: 1.6660\n",
      "Epoch 2273, Train Loss: 6.2279, Test Loss: 1.6564\n",
      "Epoch 2274, Train Loss: 5.5453, Test Loss: 1.6473\n",
      "Epoch 2275, Train Loss: 5.3421, Test Loss: 1.6404\n",
      "Epoch 2276, Train Loss: 5.5567, Test Loss: 1.6351\n",
      "Epoch 2277, Train Loss: 4.7661, Test Loss: 1.6354\n",
      "Epoch 2278, Train Loss: 5.5091, Test Loss: 1.6300\n",
      "Epoch 2279, Train Loss: 5.8096, Test Loss: 1.6188\n",
      "Epoch 2280, Train Loss: 5.6582, Test Loss: 1.6044\n",
      "Epoch 2281, Train Loss: 5.4928, Test Loss: 1.5918\n",
      "Epoch 2282, Train Loss: 5.8507, Test Loss: 1.5878\n",
      "Epoch 2283, Train Loss: 4.5289, Test Loss: 1.5816\n",
      "Epoch 2284, Train Loss: 5.6041, Test Loss: 1.5731\n",
      "Epoch 2285, Train Loss: 5.2891, Test Loss: 1.5684\n",
      "Epoch 2286, Train Loss: 5.7696, Test Loss: 1.5637\n",
      "Epoch 2287, Train Loss: 4.7157, Test Loss: 1.5658\n",
      "Epoch 2288, Train Loss: 5.9494, Test Loss: 1.5664\n",
      "Epoch 2289, Train Loss: 5.3126, Test Loss: 1.5683\n",
      "Epoch 2290, Train Loss: 4.9455, Test Loss: 1.5686\n",
      "Epoch 2291, Train Loss: 5.3974, Test Loss: 1.5663\n",
      "Epoch 2292, Train Loss: 4.6873, Test Loss: 1.5625\n",
      "Epoch 2293, Train Loss: 4.2156, Test Loss: 1.5565\n",
      "Epoch 2294, Train Loss: 5.5720, Test Loss: 1.5506\n",
      "Epoch 2295, Train Loss: 4.5633, Test Loss: 1.5452\n",
      "Epoch 2296, Train Loss: 5.1034, Test Loss: 1.5402\n",
      "Epoch 2297, Train Loss: 4.8987, Test Loss: 1.5373\n",
      "Epoch 2298, Train Loss: 5.4768, Test Loss: 1.5451\n",
      "Epoch 2299, Train Loss: 4.6839, Test Loss: 1.5525\n",
      "Epoch 2300, Train Loss: 4.6707, Test Loss: 1.5565\n",
      "Epoch 2301, Train Loss: 4.9192, Test Loss: 1.5566\n",
      "Epoch 2302, Train Loss: 5.4721, Test Loss: 1.5594\n",
      "Epoch 2303, Train Loss: 5.5692, Test Loss: 1.5620\n",
      "Epoch 2304, Train Loss: 6.0778, Test Loss: 1.5698\n",
      "Epoch 2305, Train Loss: 5.1572, Test Loss: 1.5730\n",
      "Epoch 2306, Train Loss: 5.0952, Test Loss: 1.5818\n",
      "Epoch 2307, Train Loss: 5.1771, Test Loss: 1.5901\n",
      "Epoch 2308, Train Loss: 5.5946, Test Loss: 1.6054\n",
      "Epoch 2309, Train Loss: 5.0004, Test Loss: 1.6105\n",
      "Epoch 2310, Train Loss: 5.4674, Test Loss: 1.6120\n",
      "Epoch 2311, Train Loss: 5.5780, Test Loss: 1.6222\n",
      "Epoch 2312, Train Loss: 5.0807, Test Loss: 1.6291\n",
      "Epoch 2313, Train Loss: 4.9079, Test Loss: 1.6367\n",
      "Epoch 2314, Train Loss: 4.6230, Test Loss: 1.6328\n",
      "Epoch 2315, Train Loss: 4.8590, Test Loss: 1.6271\n",
      "Epoch 2316, Train Loss: 5.5024, Test Loss: 1.6180\n",
      "Epoch 2317, Train Loss: 5.1191, Test Loss: 1.6068\n",
      "Epoch 2318, Train Loss: 5.2929, Test Loss: 1.5939\n",
      "Epoch 2319, Train Loss: 5.0621, Test Loss: 1.5788\n",
      "Epoch 2320, Train Loss: 5.4173, Test Loss: 1.5685\n",
      "Epoch 2321, Train Loss: 4.3886, Test Loss: 1.5598\n",
      "Epoch 2322, Train Loss: 4.8313, Test Loss: 1.5466\n",
      "Epoch 2323, Train Loss: 5.3204, Test Loss: 1.5341\n",
      "Epoch 2324, Train Loss: 4.4254, Test Loss: 1.5186\n",
      "Epoch 2325, Train Loss: 4.6328, Test Loss: 1.5038\n",
      "Epoch 2326, Train Loss: 5.1790, Test Loss: 1.4956\n",
      "Epoch 2327, Train Loss: 5.2668, Test Loss: 1.4973\n",
      "Epoch 2328, Train Loss: 5.0050, Test Loss: 1.5010\n",
      "Epoch 2329, Train Loss: 4.9837, Test Loss: 1.5048\n",
      "Epoch 2330, Train Loss: 4.2724, Test Loss: 1.5075\n",
      "Epoch 2331, Train Loss: 5.7133, Test Loss: 1.5148\n",
      "Epoch 2332, Train Loss: 5.1305, Test Loss: 1.5238\n",
      "Epoch 2333, Train Loss: 4.8846, Test Loss: 1.5354\n",
      "Epoch 2334, Train Loss: 4.7253, Test Loss: 1.5413\n",
      "Epoch 2335, Train Loss: 5.1414, Test Loss: 1.5468\n",
      "Epoch 2336, Train Loss: 4.3460, Test Loss: 1.5494\n",
      "Epoch 2337, Train Loss: 4.6027, Test Loss: 1.5470\n",
      "Epoch 2338, Train Loss: 4.8413, Test Loss: 1.5459\n",
      "Epoch 2339, Train Loss: 4.9301, Test Loss: 1.5417\n",
      "Epoch 2340, Train Loss: 5.5684, Test Loss: 1.5453\n",
      "Epoch 2341, Train Loss: 5.4323, Test Loss: 1.5471\n",
      "Epoch 2342, Train Loss: 5.4325, Test Loss: 1.5432\n",
      "Epoch 2343, Train Loss: 4.3300, Test Loss: 1.5465\n",
      "Epoch 2344, Train Loss: 4.3669, Test Loss: 1.5509\n",
      "Epoch 2345, Train Loss: 5.4446, Test Loss: 1.5577\n",
      "Epoch 2346, Train Loss: 4.7739, Test Loss: 1.5711\n",
      "Epoch 2347, Train Loss: 4.9064, Test Loss: 1.5834\n",
      "Epoch 2348, Train Loss: 4.8674, Test Loss: 1.5857\n",
      "Epoch 2349, Train Loss: 5.4585, Test Loss: 1.5905\n",
      "Epoch 2350, Train Loss: 5.2739, Test Loss: 1.6032\n",
      "Epoch 2351, Train Loss: 5.1944, Test Loss: 1.6076\n",
      "Epoch 2352, Train Loss: 4.5252, Test Loss: 1.6095\n",
      "Epoch 2353, Train Loss: 3.6535, Test Loss: 1.6106\n",
      "Epoch 2354, Train Loss: 4.9631, Test Loss: 1.6155\n",
      "Epoch 2355, Train Loss: 5.4928, Test Loss: 1.6166\n",
      "Epoch 2356, Train Loss: 5.3643, Test Loss: 1.6166\n",
      "Epoch 2357, Train Loss: 6.1033, Test Loss: 1.6127\n",
      "Epoch 2358, Train Loss: 4.4712, Test Loss: 1.6131\n",
      "Epoch 2359, Train Loss: 4.9241, Test Loss: 1.6175\n",
      "Epoch 2360, Train Loss: 4.3724, Test Loss: 1.6136\n",
      "Epoch 2361, Train Loss: 5.0752, Test Loss: 1.6118\n",
      "Epoch 2362, Train Loss: 4.8364, Test Loss: 1.6059\n",
      "Epoch 2363, Train Loss: 5.0709, Test Loss: 1.6019\n",
      "Epoch 2364, Train Loss: 4.0129, Test Loss: 1.6002\n",
      "Epoch 2365, Train Loss: 4.0693, Test Loss: 1.6003\n",
      "Epoch 2366, Train Loss: 4.5787, Test Loss: 1.6026\n",
      "Epoch 2367, Train Loss: 4.8806, Test Loss: 1.6055\n",
      "Epoch 2368, Train Loss: 3.5510, Test Loss: 1.6124\n",
      "Epoch 2369, Train Loss: 4.3960, Test Loss: 1.6170\n",
      "Epoch 2370, Train Loss: 4.8789, Test Loss: 1.6177\n",
      "Epoch 2371, Train Loss: 5.0879, Test Loss: 1.6134\n",
      "Epoch 2372, Train Loss: 4.3015, Test Loss: 1.6110\n",
      "Epoch 2373, Train Loss: 4.3562, Test Loss: 1.6067\n",
      "Epoch 2374, Train Loss: 5.1627, Test Loss: 1.6040\n",
      "Epoch 2375, Train Loss: 4.3498, Test Loss: 1.6081\n",
      "Epoch 2376, Train Loss: 5.1082, Test Loss: 1.6026\n",
      "Epoch 2377, Train Loss: 4.3765, Test Loss: 1.6011\n",
      "Epoch 2378, Train Loss: 4.4957, Test Loss: 1.5949\n",
      "Epoch 2379, Train Loss: 5.1097, Test Loss: 1.6026\n",
      "Epoch 2380, Train Loss: 4.9575, Test Loss: 1.6079\n",
      "Epoch 2381, Train Loss: 4.0996, Test Loss: 1.6126\n",
      "Epoch 2382, Train Loss: 4.2094, Test Loss: 1.6156\n",
      "Epoch 2383, Train Loss: 4.9571, Test Loss: 1.6112\n",
      "Epoch 2384, Train Loss: 4.3081, Test Loss: 1.5970\n",
      "Epoch 2385, Train Loss: 3.7199, Test Loss: 1.5800\n",
      "Epoch 2386, Train Loss: 4.4088, Test Loss: 1.5661\n",
      "Epoch 2387, Train Loss: 4.9437, Test Loss: 1.5533\n",
      "Epoch 2388, Train Loss: 4.8775, Test Loss: 1.5392\n",
      "Epoch 2389, Train Loss: 4.6056, Test Loss: 1.5229\n",
      "Epoch 2390, Train Loss: 4.9111, Test Loss: 1.5090\n",
      "Epoch 2391, Train Loss: 4.9843, Test Loss: 1.5000\n",
      "Epoch 2392, Train Loss: 4.3665, Test Loss: 1.4972\n",
      "Epoch 2393, Train Loss: 4.3032, Test Loss: 1.4951\n",
      "Epoch 2394, Train Loss: 4.3689, Test Loss: 1.4965\n",
      "Epoch 2395, Train Loss: 4.7945, Test Loss: 1.5006\n",
      "Epoch 2396, Train Loss: 4.2047, Test Loss: 1.5066\n",
      "Epoch 2397, Train Loss: 4.1020, Test Loss: 1.5183\n",
      "Epoch 2398, Train Loss: 5.5626, Test Loss: 1.5265\n",
      "Epoch 2399, Train Loss: 4.6623, Test Loss: 1.5304\n",
      "Epoch 2400, Train Loss: 3.9503, Test Loss: 1.5298\n",
      "Epoch 2401, Train Loss: 4.5406, Test Loss: 1.5246\n",
      "Epoch 2402, Train Loss: 4.3992, Test Loss: 1.5232\n",
      "Epoch 2403, Train Loss: 4.6816, Test Loss: 1.5219\n",
      "Epoch 2404, Train Loss: 4.2670, Test Loss: 1.5151\n",
      "Epoch 2405, Train Loss: 4.8829, Test Loss: 1.5050\n",
      "Epoch 2406, Train Loss: 4.4168, Test Loss: 1.4996\n",
      "Epoch 2407, Train Loss: 4.8721, Test Loss: 1.4908\n",
      "Epoch 2408, Train Loss: 5.1568, Test Loss: 1.4890\n",
      "Epoch 2409, Train Loss: 4.0184, Test Loss: 1.4830\n",
      "Epoch 2410, Train Loss: 4.2318, Test Loss: 1.4796\n",
      "Epoch 2411, Train Loss: 4.9786, Test Loss: 1.4746\n",
      "Epoch 2412, Train Loss: 4.3119, Test Loss: 1.4717\n",
      "Epoch 2413, Train Loss: 4.2997, Test Loss: 1.4775\n",
      "Epoch 2414, Train Loss: 5.4664, Test Loss: 1.4783\n",
      "Epoch 2415, Train Loss: 4.1012, Test Loss: 1.4833\n",
      "Epoch 2416, Train Loss: 3.8245, Test Loss: 1.4932\n",
      "Epoch 2417, Train Loss: 4.9703, Test Loss: 1.5084\n",
      "Epoch 2418, Train Loss: 4.4692, Test Loss: 1.5235\n",
      "Epoch 2419, Train Loss: 4.9487, Test Loss: 1.5327\n",
      "Epoch 2420, Train Loss: 4.1169, Test Loss: 1.5319\n",
      "Epoch 2421, Train Loss: 4.4792, Test Loss: 1.5231\n",
      "Epoch 2422, Train Loss: 4.8866, Test Loss: 1.5147\n",
      "Epoch 2423, Train Loss: 4.1529, Test Loss: 1.5103\n",
      "Epoch 2424, Train Loss: 4.8395, Test Loss: 1.5026\n",
      "Epoch 2425, Train Loss: 5.2034, Test Loss: 1.5013\n",
      "Epoch 2426, Train Loss: 3.9312, Test Loss: 1.5054\n",
      "Epoch 2427, Train Loss: 4.7336, Test Loss: 1.5123\n",
      "Epoch 2428, Train Loss: 5.1493, Test Loss: 1.5257\n",
      "Epoch 2429, Train Loss: 4.6558, Test Loss: 1.5318\n",
      "Epoch 2430, Train Loss: 4.2258, Test Loss: 1.5412\n",
      "Epoch 2431, Train Loss: 4.7882, Test Loss: 1.5426\n",
      "Epoch 2432, Train Loss: 5.0171, Test Loss: 1.5428\n",
      "Epoch 2433, Train Loss: 4.4546, Test Loss: 1.5441\n",
      "Epoch 2434, Train Loss: 4.0084, Test Loss: 1.5450\n",
      "Epoch 2435, Train Loss: 4.1544, Test Loss: 1.5414\n",
      "Epoch 2436, Train Loss: 3.5618, Test Loss: 1.5472\n",
      "Epoch 2437, Train Loss: 4.1348, Test Loss: 1.5583\n",
      "Epoch 2438, Train Loss: 3.8958, Test Loss: 1.5666\n",
      "Epoch 2439, Train Loss: 4.0084, Test Loss: 1.5669\n",
      "Epoch 2440, Train Loss: 4.5499, Test Loss: 1.5634\n",
      "Epoch 2441, Train Loss: 4.4520, Test Loss: 1.5546\n",
      "Epoch 2442, Train Loss: 5.0012, Test Loss: 1.5454\n",
      "Epoch 2443, Train Loss: 4.2083, Test Loss: 1.5358\n",
      "Epoch 2444, Train Loss: 5.0381, Test Loss: 1.5211\n",
      "Epoch 2445, Train Loss: 4.2888, Test Loss: 1.5083\n",
      "Epoch 2446, Train Loss: 3.7395, Test Loss: 1.4868\n",
      "Epoch 2447, Train Loss: 4.2714, Test Loss: 1.4735\n",
      "Epoch 2448, Train Loss: 4.4071, Test Loss: 1.4674\n",
      "Epoch 2449, Train Loss: 5.0461, Test Loss: 1.4655\n",
      "Epoch 2450, Train Loss: 4.2596, Test Loss: 1.4696\n",
      "Epoch 2451, Train Loss: 4.3183, Test Loss: 1.4772\n",
      "Epoch 2452, Train Loss: 3.9762, Test Loss: 1.4880\n",
      "Epoch 2453, Train Loss: 4.2795, Test Loss: 1.4955\n",
      "Epoch 2454, Train Loss: 4.8780, Test Loss: 1.5069\n",
      "Epoch 2455, Train Loss: 3.2873, Test Loss: 1.5218\n",
      "Epoch 2456, Train Loss: 4.2699, Test Loss: 1.5345\n",
      "Epoch 2457, Train Loss: 3.4267, Test Loss: 1.5471\n",
      "Epoch 2458, Train Loss: 4.2620, Test Loss: 1.5597\n",
      "Epoch 2459, Train Loss: 4.4947, Test Loss: 1.5607\n",
      "Epoch 2460, Train Loss: 4.1664, Test Loss: 1.5596\n",
      "Epoch 2461, Train Loss: 4.4462, Test Loss: 1.5627\n",
      "Epoch 2462, Train Loss: 4.3655, Test Loss: 1.5625\n",
      "Epoch 2463, Train Loss: 3.8851, Test Loss: 1.5612\n",
      "Epoch 2464, Train Loss: 5.0060, Test Loss: 1.5574\n",
      "Epoch 2465, Train Loss: 4.1428, Test Loss: 1.5514\n",
      "Epoch 2466, Train Loss: 4.6561, Test Loss: 1.5501\n",
      "Epoch 2467, Train Loss: 3.9470, Test Loss: 1.5454\n",
      "Epoch 2468, Train Loss: 4.6017, Test Loss: 1.5410\n",
      "Epoch 2469, Train Loss: 5.0998, Test Loss: 1.5399\n",
      "Epoch 2470, Train Loss: 4.3987, Test Loss: 1.5348\n",
      "Epoch 2471, Train Loss: 4.2027, Test Loss: 1.5298\n",
      "Epoch 2472, Train Loss: 4.3512, Test Loss: 1.5211\n",
      "Epoch 2473, Train Loss: 4.4012, Test Loss: 1.5122\n",
      "Epoch 2474, Train Loss: 4.6326, Test Loss: 1.5123\n",
      "Epoch 2475, Train Loss: 4.1098, Test Loss: 1.5092\n",
      "Epoch 2476, Train Loss: 3.5054, Test Loss: 1.5020\n",
      "Epoch 2477, Train Loss: 3.9204, Test Loss: 1.5045\n",
      "Epoch 2478, Train Loss: 5.0166, Test Loss: 1.5065\n",
      "Epoch 2479, Train Loss: 4.2590, Test Loss: 1.5098\n",
      "Epoch 2480, Train Loss: 4.1851, Test Loss: 1.5113\n",
      "Epoch 2481, Train Loss: 4.5314, Test Loss: 1.5171\n",
      "Epoch 2482, Train Loss: 4.7456, Test Loss: 1.5221\n",
      "Epoch 2483, Train Loss: 4.6837, Test Loss: 1.5248\n",
      "Epoch 2484, Train Loss: 4.5227, Test Loss: 1.5295\n",
      "Epoch 2485, Train Loss: 4.1023, Test Loss: 1.5349\n",
      "Epoch 2486, Train Loss: 3.8648, Test Loss: 1.5344\n",
      "Epoch 2487, Train Loss: 4.0970, Test Loss: 1.5436\n",
      "Epoch 2488, Train Loss: 4.3204, Test Loss: 1.5629\n",
      "Epoch 2489, Train Loss: 4.1991, Test Loss: 1.5827\n",
      "Epoch 2490, Train Loss: 4.3975, Test Loss: 1.6006\n",
      "Epoch 2491, Train Loss: 4.3765, Test Loss: 1.6138\n",
      "Epoch 2492, Train Loss: 3.9250, Test Loss: 1.6176\n",
      "Epoch 2493, Train Loss: 3.1888, Test Loss: 1.6210\n",
      "Epoch 2494, Train Loss: 4.1193, Test Loss: 1.6202\n",
      "Epoch 2495, Train Loss: 3.8262, Test Loss: 1.6068\n",
      "Epoch 2496, Train Loss: 3.2905, Test Loss: 1.5879\n",
      "Epoch 2497, Train Loss: 4.0937, Test Loss: 1.5742\n",
      "Epoch 2498, Train Loss: 3.3743, Test Loss: 1.5651\n",
      "Epoch 2499, Train Loss: 4.4365, Test Loss: 1.5654\n",
      "Epoch 2500, Train Loss: 3.0964, Test Loss: 1.5620\n",
      "Epoch 2501, Train Loss: 4.5125, Test Loss: 1.5566\n",
      "Epoch 2502, Train Loss: 4.1784, Test Loss: 1.5517\n",
      "Epoch 2503, Train Loss: 4.1383, Test Loss: 1.5465\n",
      "Epoch 2504, Train Loss: 3.4057, Test Loss: 1.5348\n",
      "Epoch 2505, Train Loss: 4.0339, Test Loss: 1.5265\n",
      "Epoch 2506, Train Loss: 3.5812, Test Loss: 1.5248\n",
      "Epoch 2507, Train Loss: 4.1816, Test Loss: 1.5241\n",
      "Epoch 2508, Train Loss: 4.5880, Test Loss: 1.5211\n",
      "Epoch 2509, Train Loss: 5.1733, Test Loss: 1.5197\n",
      "Epoch 2510, Train Loss: 4.2640, Test Loss: 1.5161\n",
      "Epoch 2511, Train Loss: 3.5770, Test Loss: 1.5174\n",
      "Epoch 2512, Train Loss: 3.7576, Test Loss: 1.5175\n",
      "Epoch 2513, Train Loss: 4.2122, Test Loss: 1.5148\n",
      "Epoch 2514, Train Loss: 3.7338, Test Loss: 1.5072\n",
      "Epoch 2515, Train Loss: 3.9309, Test Loss: 1.5025\n",
      "Epoch 2516, Train Loss: 4.2746, Test Loss: 1.5005\n",
      "Epoch 2517, Train Loss: 4.0502, Test Loss: 1.4966\n",
      "Epoch 2518, Train Loss: 4.3756, Test Loss: 1.4883\n",
      "Epoch 2519, Train Loss: 3.6488, Test Loss: 1.4843\n",
      "Epoch 2520, Train Loss: 4.3601, Test Loss: 1.4768\n",
      "Epoch 2521, Train Loss: 3.6016, Test Loss: 1.4736\n",
      "Epoch 2522, Train Loss: 3.9606, Test Loss: 1.4709\n",
      "Epoch 2523, Train Loss: 3.6370, Test Loss: 1.4788\n",
      "Epoch 2524, Train Loss: 3.4235, Test Loss: 1.4829\n",
      "Epoch 2525, Train Loss: 3.9409, Test Loss: 1.4751\n",
      "Epoch 2526, Train Loss: 3.6717, Test Loss: 1.4619\n",
      "Epoch 2527, Train Loss: 4.2938, Test Loss: 1.4613\n",
      "Epoch 2528, Train Loss: 4.2950, Test Loss: 1.4589\n",
      "Epoch 2529, Train Loss: 4.1483, Test Loss: 1.4677\n",
      "Epoch 2530, Train Loss: 3.8976, Test Loss: 1.4760\n",
      "Epoch 2531, Train Loss: 3.6456, Test Loss: 1.4832\n",
      "Epoch 2532, Train Loss: 4.3160, Test Loss: 1.4975\n",
      "Epoch 2533, Train Loss: 3.6875, Test Loss: 1.5135\n",
      "Epoch 2534, Train Loss: 3.9723, Test Loss: 1.5181\n",
      "Epoch 2535, Train Loss: 4.7014, Test Loss: 1.5242\n",
      "Epoch 2536, Train Loss: 3.8393, Test Loss: 1.5326\n",
      "Epoch 2537, Train Loss: 3.4683, Test Loss: 1.5449\n",
      "Epoch 2538, Train Loss: 4.7038, Test Loss: 1.5509\n",
      "Epoch 2539, Train Loss: 4.1589, Test Loss: 1.5478\n",
      "Epoch 2540, Train Loss: 3.8829, Test Loss: 1.5319\n",
      "Epoch 2541, Train Loss: 3.9373, Test Loss: 1.5104\n",
      "Epoch 2542, Train Loss: 3.7441, Test Loss: 1.4887\n",
      "Epoch 2543, Train Loss: 3.3905, Test Loss: 1.4610\n",
      "Epoch 2544, Train Loss: 3.8260, Test Loss: 1.4356\n",
      "Epoch 2545, Train Loss: 3.6485, Test Loss: 1.4118\n",
      "Epoch 2546, Train Loss: 4.0365, Test Loss: 1.3906\n",
      "Epoch 2547, Train Loss: 3.8578, Test Loss: 1.3749\n",
      "Epoch 2548, Train Loss: 3.4214, Test Loss: 1.3611\n",
      "Epoch 2549, Train Loss: 4.5533, Test Loss: 1.3612\n",
      "Epoch 2550, Train Loss: 4.2610, Test Loss: 1.3668\n",
      "Epoch 2551, Train Loss: 4.0538, Test Loss: 1.3721\n",
      "Epoch 2552, Train Loss: 3.8172, Test Loss: 1.3807\n",
      "Epoch 2553, Train Loss: 3.8147, Test Loss: 1.3912\n",
      "Epoch 2554, Train Loss: 3.9844, Test Loss: 1.3983\n",
      "Epoch 2555, Train Loss: 3.6940, Test Loss: 1.4069\n",
      "Epoch 2556, Train Loss: 4.6355, Test Loss: 1.4113\n",
      "Epoch 2557, Train Loss: 3.9316, Test Loss: 1.4155\n",
      "Epoch 2558, Train Loss: 3.8114, Test Loss: 1.4243\n",
      "Epoch 2559, Train Loss: 3.8725, Test Loss: 1.4303\n",
      "Epoch 2560, Train Loss: 3.5430, Test Loss: 1.4370\n",
      "Epoch 2561, Train Loss: 3.5933, Test Loss: 1.4433\n",
      "Epoch 2562, Train Loss: 4.5688, Test Loss: 1.4374\n",
      "Epoch 2563, Train Loss: 3.9432, Test Loss: 1.4397\n",
      "Epoch 2564, Train Loss: 3.6854, Test Loss: 1.4413\n",
      "Epoch 2565, Train Loss: 3.7472, Test Loss: 1.4439\n",
      "Epoch 2566, Train Loss: 3.2976, Test Loss: 1.4453\n",
      "Epoch 2567, Train Loss: 3.6551, Test Loss: 1.4435\n",
      "Epoch 2568, Train Loss: 3.8604, Test Loss: 1.4390\n",
      "Epoch 2569, Train Loss: 3.9703, Test Loss: 1.4391\n",
      "Epoch 2570, Train Loss: 3.7081, Test Loss: 1.4382\n",
      "Epoch 2571, Train Loss: 3.7361, Test Loss: 1.4345\n",
      "Epoch 2572, Train Loss: 3.9164, Test Loss: 1.4254\n",
      "Epoch 2573, Train Loss: 3.3323, Test Loss: 1.4192\n",
      "Epoch 2574, Train Loss: 3.9341, Test Loss: 1.4204\n",
      "Epoch 2575, Train Loss: 3.9161, Test Loss: 1.4179\n",
      "Epoch 2576, Train Loss: 3.8196, Test Loss: 1.4190\n",
      "Epoch 2577, Train Loss: 3.6629, Test Loss: 1.4183\n",
      "Epoch 2578, Train Loss: 3.8890, Test Loss: 1.4164\n",
      "Epoch 2579, Train Loss: 3.5529, Test Loss: 1.4132\n",
      "Epoch 2580, Train Loss: 3.9524, Test Loss: 1.4173\n",
      "Epoch 2581, Train Loss: 3.4799, Test Loss: 1.4165\n",
      "Epoch 2582, Train Loss: 3.7245, Test Loss: 1.4238\n",
      "Epoch 2583, Train Loss: 3.7955, Test Loss: 1.4352\n",
      "Epoch 2584, Train Loss: 3.4710, Test Loss: 1.4492\n",
      "Epoch 2585, Train Loss: 3.4319, Test Loss: 1.4603\n",
      "Epoch 2586, Train Loss: 3.7550, Test Loss: 1.4763\n",
      "Epoch 2587, Train Loss: 3.5749, Test Loss: 1.4848\n",
      "Epoch 2588, Train Loss: 3.1786, Test Loss: 1.4910\n",
      "Epoch 2589, Train Loss: 4.0341, Test Loss: 1.4993\n",
      "Epoch 2590, Train Loss: 3.6338, Test Loss: 1.5022\n",
      "Epoch 2591, Train Loss: 3.3960, Test Loss: 1.5074\n",
      "Epoch 2592, Train Loss: 3.8950, Test Loss: 1.5081\n",
      "Epoch 2593, Train Loss: 3.8037, Test Loss: 1.5068\n",
      "Epoch 2594, Train Loss: 3.4300, Test Loss: 1.5059\n",
      "Epoch 2595, Train Loss: 3.6423, Test Loss: 1.5069\n",
      "Epoch 2596, Train Loss: 3.9467, Test Loss: 1.5171\n",
      "Epoch 2597, Train Loss: 3.1715, Test Loss: 1.5331\n",
      "Epoch 2598, Train Loss: 3.6606, Test Loss: 1.5389\n",
      "Epoch 2599, Train Loss: 3.5219, Test Loss: 1.5389\n",
      "Epoch 2600, Train Loss: 2.7734, Test Loss: 1.5316\n",
      "Epoch 2601, Train Loss: 3.2652, Test Loss: 1.5221\n",
      "Epoch 2602, Train Loss: 3.5908, Test Loss: 1.5090\n",
      "Epoch 2603, Train Loss: 3.7013, Test Loss: 1.4918\n",
      "Epoch 2604, Train Loss: 3.7982, Test Loss: 1.4785\n",
      "Epoch 2605, Train Loss: 4.2835, Test Loss: 1.4768\n",
      "Epoch 2606, Train Loss: 3.8604, Test Loss: 1.4749\n",
      "Epoch 2607, Train Loss: 3.3390, Test Loss: 1.4714\n",
      "Epoch 2608, Train Loss: 4.4300, Test Loss: 1.4708\n",
      "Epoch 2609, Train Loss: 3.1894, Test Loss: 1.4734\n",
      "Epoch 2610, Train Loss: 3.9133, Test Loss: 1.4803\n",
      "Epoch 2611, Train Loss: 3.4070, Test Loss: 1.4895\n",
      "Epoch 2612, Train Loss: 3.3123, Test Loss: 1.4925\n",
      "Epoch 2613, Train Loss: 3.1556, Test Loss: 1.4977\n",
      "Epoch 2614, Train Loss: 3.8322, Test Loss: 1.5065\n",
      "Epoch 2615, Train Loss: 3.6733, Test Loss: 1.5085\n",
      "Epoch 2616, Train Loss: 3.6007, Test Loss: 1.5002\n",
      "Epoch 2617, Train Loss: 2.8662, Test Loss: 1.4996\n",
      "Epoch 2618, Train Loss: 3.8618, Test Loss: 1.4906\n",
      "Epoch 2619, Train Loss: 4.4735, Test Loss: 1.4812\n",
      "Epoch 2620, Train Loss: 3.3234, Test Loss: 1.4638\n",
      "Epoch 2621, Train Loss: 2.7956, Test Loss: 1.4510\n",
      "Epoch 2622, Train Loss: 4.1132, Test Loss: 1.4357\n",
      "Epoch 2623, Train Loss: 3.8594, Test Loss: 1.4137\n",
      "Epoch 2624, Train Loss: 3.3449, Test Loss: 1.3859\n",
      "Epoch 2625, Train Loss: 3.6416, Test Loss: 1.3610\n",
      "Epoch 2626, Train Loss: 3.6627, Test Loss: 1.3345\n",
      "Epoch 2627, Train Loss: 3.3973, Test Loss: 1.3159\n",
      "Epoch 2628, Train Loss: 3.4812, Test Loss: 1.3032\n",
      "Epoch 2629, Train Loss: 3.8081, Test Loss: 1.2943\n",
      "Epoch 2630, Train Loss: 3.4128, Test Loss: 1.2888\n",
      "Epoch 2631, Train Loss: 3.1809, Test Loss: 1.2848\n",
      "Epoch 2632, Train Loss: 3.3996, Test Loss: 1.2861\n",
      "Epoch 2633, Train Loss: 3.3249, Test Loss: 1.2931\n",
      "Epoch 2634, Train Loss: 3.2800, Test Loss: 1.3035\n",
      "Epoch 2635, Train Loss: 3.1869, Test Loss: 1.3075\n",
      "Epoch 2636, Train Loss: 3.5126, Test Loss: 1.3135\n",
      "Epoch 2637, Train Loss: 3.6498, Test Loss: 1.3235\n",
      "Epoch 2638, Train Loss: 2.9276, Test Loss: 1.3361\n",
      "Epoch 2639, Train Loss: 4.0720, Test Loss: 1.3431\n",
      "Epoch 2640, Train Loss: 3.5734, Test Loss: 1.3523\n",
      "Epoch 2641, Train Loss: 4.0071, Test Loss: 1.3612\n",
      "Epoch 2642, Train Loss: 3.5332, Test Loss: 1.3696\n",
      "Epoch 2643, Train Loss: 2.9718, Test Loss: 1.3791\n",
      "Epoch 2644, Train Loss: 2.9783, Test Loss: 1.3895\n",
      "Epoch 2645, Train Loss: 3.0674, Test Loss: 1.3975\n",
      "Epoch 2646, Train Loss: 2.9365, Test Loss: 1.3975\n",
      "Epoch 2647, Train Loss: 3.2808, Test Loss: 1.4005\n",
      "Epoch 2648, Train Loss: 3.2194, Test Loss: 1.4091\n",
      "Epoch 2649, Train Loss: 2.7803, Test Loss: 1.4170\n",
      "Epoch 2650, Train Loss: 3.8379, Test Loss: 1.4220\n",
      "Epoch 2651, Train Loss: 3.9072, Test Loss: 1.4300\n",
      "Epoch 2652, Train Loss: 4.5913, Test Loss: 1.4442\n",
      "Epoch 2653, Train Loss: 3.5286, Test Loss: 1.4571\n",
      "Epoch 2654, Train Loss: 3.8247, Test Loss: 1.4671\n",
      "Epoch 2655, Train Loss: 3.1514, Test Loss: 1.4695\n",
      "Epoch 2656, Train Loss: 3.8708, Test Loss: 1.4705\n",
      "Epoch 2657, Train Loss: 3.0536, Test Loss: 1.4684\n",
      "Epoch 2658, Train Loss: 3.3264, Test Loss: 1.4676\n",
      "Epoch 2659, Train Loss: 3.5087, Test Loss: 1.4618\n",
      "Epoch 2660, Train Loss: 3.5485, Test Loss: 1.4499\n",
      "Epoch 2661, Train Loss: 3.5574, Test Loss: 1.4357\n",
      "Epoch 2662, Train Loss: 3.0849, Test Loss: 1.4229\n",
      "Epoch 2663, Train Loss: 3.1536, Test Loss: 1.4151\n",
      "Epoch 2664, Train Loss: 3.5822, Test Loss: 1.4045\n",
      "Epoch 2665, Train Loss: 2.8109, Test Loss: 1.3828\n",
      "Epoch 2666, Train Loss: 3.3518, Test Loss: 1.3590\n",
      "Epoch 2667, Train Loss: 3.7772, Test Loss: 1.3439\n",
      "Epoch 2668, Train Loss: 3.3585, Test Loss: 1.3350\n",
      "Epoch 2669, Train Loss: 3.0031, Test Loss: 1.3290\n",
      "Epoch 2670, Train Loss: 3.6268, Test Loss: 1.3276\n",
      "Epoch 2671, Train Loss: 3.7032, Test Loss: 1.3331\n",
      "Epoch 2672, Train Loss: 3.5016, Test Loss: 1.3377\n",
      "Epoch 2673, Train Loss: 3.1766, Test Loss: 1.3516\n",
      "Epoch 2674, Train Loss: 3.2167, Test Loss: 1.3682\n",
      "Epoch 2675, Train Loss: 3.4615, Test Loss: 1.3858\n",
      "Epoch 2676, Train Loss: 3.4590, Test Loss: 1.4107\n",
      "Epoch 2677, Train Loss: 2.9998, Test Loss: 1.4422\n",
      "Epoch 2678, Train Loss: 3.2994, Test Loss: 1.4609\n",
      "Epoch 2679, Train Loss: 3.5576, Test Loss: 1.4722\n",
      "Epoch 2680, Train Loss: 3.0874, Test Loss: 1.4766\n",
      "Epoch 2681, Train Loss: 2.8461, Test Loss: 1.4757\n",
      "Epoch 2682, Train Loss: 3.0684, Test Loss: 1.4713\n",
      "Epoch 2683, Train Loss: 3.4715, Test Loss: 1.4724\n",
      "Epoch 2684, Train Loss: 3.4980, Test Loss: 1.4723\n",
      "Epoch 2685, Train Loss: 3.6271, Test Loss: 1.4771\n",
      "Epoch 2686, Train Loss: 3.6388, Test Loss: 1.4748\n",
      "Epoch 2687, Train Loss: 2.9279, Test Loss: 1.4661\n",
      "Epoch 2688, Train Loss: 3.1805, Test Loss: 1.4687\n",
      "Epoch 2689, Train Loss: 3.4502, Test Loss: 1.4638\n",
      "Epoch 2690, Train Loss: 2.5267, Test Loss: 1.4546\n",
      "Epoch 2691, Train Loss: 3.3170, Test Loss: 1.4489\n",
      "Epoch 2692, Train Loss: 3.1490, Test Loss: 1.4417\n",
      "Epoch 2693, Train Loss: 3.0279, Test Loss: 1.4318\n",
      "Epoch 2694, Train Loss: 3.0242, Test Loss: 1.4272\n",
      "Epoch 2695, Train Loss: 2.6648, Test Loss: 1.4293\n",
      "Epoch 2696, Train Loss: 3.0046, Test Loss: 1.4282\n",
      "Epoch 2697, Train Loss: 3.1410, Test Loss: 1.4308\n",
      "Epoch 2698, Train Loss: 3.8691, Test Loss: 1.4403\n",
      "Epoch 2699, Train Loss: 3.0729, Test Loss: 1.4351\n",
      "Epoch 2700, Train Loss: 3.0266, Test Loss: 1.4264\n",
      "Epoch 2701, Train Loss: 3.1919, Test Loss: 1.4165\n",
      "Epoch 2702, Train Loss: 3.5373, Test Loss: 1.4103\n",
      "Epoch 2703, Train Loss: 2.9913, Test Loss: 1.4106\n",
      "Epoch 2704, Train Loss: 2.7565, Test Loss: 1.4158\n",
      "Epoch 2705, Train Loss: 2.9054, Test Loss: 1.4114\n",
      "Epoch 2706, Train Loss: 2.9382, Test Loss: 1.4093\n",
      "Epoch 2707, Train Loss: 2.8971, Test Loss: 1.4127\n",
      "Epoch 2708, Train Loss: 3.1565, Test Loss: 1.4183\n",
      "Epoch 2709, Train Loss: 2.9831, Test Loss: 1.4272\n",
      "Epoch 2710, Train Loss: 3.6015, Test Loss: 1.4379\n",
      "Epoch 2711, Train Loss: 3.2810, Test Loss: 1.4481\n",
      "Epoch 2712, Train Loss: 3.2304, Test Loss: 1.4475\n",
      "Epoch 2713, Train Loss: 3.3057, Test Loss: 1.4349\n",
      "Epoch 2714, Train Loss: 3.3181, Test Loss: 1.4112\n",
      "Epoch 2715, Train Loss: 3.1804, Test Loss: 1.3978\n",
      "Epoch 2716, Train Loss: 2.7310, Test Loss: 1.3879\n",
      "Epoch 2717, Train Loss: 3.4635, Test Loss: 1.3888\n",
      "Epoch 2718, Train Loss: 3.1764, Test Loss: 1.3915\n",
      "Epoch 2719, Train Loss: 3.2668, Test Loss: 1.3937\n",
      "Epoch 2720, Train Loss: 3.4625, Test Loss: 1.3899\n",
      "Epoch 2721, Train Loss: 2.7761, Test Loss: 1.3904\n",
      "Epoch 2722, Train Loss: 3.0820, Test Loss: 1.3838\n",
      "Epoch 2723, Train Loss: 3.5479, Test Loss: 1.3740\n",
      "Epoch 2724, Train Loss: 3.0600, Test Loss: 1.3637\n",
      "Epoch 2725, Train Loss: 2.7532, Test Loss: 1.3485\n",
      "Epoch 2726, Train Loss: 3.1618, Test Loss: 1.3333\n",
      "Epoch 2727, Train Loss: 3.3726, Test Loss: 1.3183\n",
      "Epoch 2728, Train Loss: 3.5448, Test Loss: 1.3068\n",
      "Epoch 2729, Train Loss: 3.2476, Test Loss: 1.3000\n",
      "Epoch 2730, Train Loss: 3.0268, Test Loss: 1.3044\n",
      "Epoch 2731, Train Loss: 3.0993, Test Loss: 1.3156\n",
      "Epoch 2732, Train Loss: 3.0569, Test Loss: 1.3329\n",
      "Epoch 2733, Train Loss: 3.0857, Test Loss: 1.3538\n",
      "Epoch 2734, Train Loss: 3.4096, Test Loss: 1.3669\n",
      "Epoch 2735, Train Loss: 2.8212, Test Loss: 1.3792\n",
      "Epoch 2736, Train Loss: 3.0742, Test Loss: 1.3985\n",
      "Epoch 2737, Train Loss: 3.1562, Test Loss: 1.4106\n",
      "Epoch 2738, Train Loss: 3.0253, Test Loss: 1.4198\n",
      "Epoch 2739, Train Loss: 2.8700, Test Loss: 1.4255\n",
      "Epoch 2740, Train Loss: 2.9502, Test Loss: 1.4297\n",
      "Epoch 2741, Train Loss: 3.1709, Test Loss: 1.4294\n",
      "Epoch 2742, Train Loss: 3.1228, Test Loss: 1.4334\n",
      "Epoch 2743, Train Loss: 3.0399, Test Loss: 1.4290\n",
      "Epoch 2744, Train Loss: 2.6764, Test Loss: 1.4204\n",
      "Epoch 2745, Train Loss: 2.5726, Test Loss: 1.4133\n",
      "Epoch 2746, Train Loss: 2.6318, Test Loss: 1.4121\n",
      "Epoch 2747, Train Loss: 3.0700, Test Loss: 1.4146\n",
      "Epoch 2748, Train Loss: 2.8641, Test Loss: 1.4240\n",
      "Epoch 2749, Train Loss: 3.3121, Test Loss: 1.4334\n",
      "Epoch 2750, Train Loss: 2.6792, Test Loss: 1.4404\n",
      "Epoch 2751, Train Loss: 2.5636, Test Loss: 1.4418\n",
      "Epoch 2752, Train Loss: 3.6172, Test Loss: 1.4390\n",
      "Epoch 2753, Train Loss: 3.0495, Test Loss: 1.4344\n",
      "Epoch 2754, Train Loss: 3.2007, Test Loss: 1.4305\n",
      "Epoch 2755, Train Loss: 2.6704, Test Loss: 1.4200\n",
      "Epoch 2756, Train Loss: 2.8391, Test Loss: 1.4177\n",
      "Epoch 2757, Train Loss: 3.1154, Test Loss: 1.4148\n",
      "Epoch 2758, Train Loss: 2.8769, Test Loss: 1.4109\n",
      "Epoch 2759, Train Loss: 2.9804, Test Loss: 1.4074\n",
      "Epoch 2760, Train Loss: 2.8817, Test Loss: 1.3967\n",
      "Epoch 2761, Train Loss: 2.8751, Test Loss: 1.3899\n",
      "Epoch 2762, Train Loss: 2.6951, Test Loss: 1.3878\n",
      "Epoch 2763, Train Loss: 2.8153, Test Loss: 1.3863\n",
      "Epoch 2764, Train Loss: 3.1933, Test Loss: 1.3804\n",
      "Epoch 2765, Train Loss: 2.8222, Test Loss: 1.3724\n",
      "Epoch 2766, Train Loss: 2.9685, Test Loss: 1.3682\n",
      "Epoch 2767, Train Loss: 3.5693, Test Loss: 1.3702\n",
      "Epoch 2768, Train Loss: 3.1653, Test Loss: 1.3653\n",
      "Epoch 2769, Train Loss: 3.2587, Test Loss: 1.3626\n",
      "Epoch 2770, Train Loss: 2.7882, Test Loss: 1.3624\n",
      "Epoch 2771, Train Loss: 2.7434, Test Loss: 1.3656\n",
      "Epoch 2772, Train Loss: 2.7967, Test Loss: 1.3654\n",
      "Epoch 2773, Train Loss: 3.3711, Test Loss: 1.3700\n",
      "Epoch 2774, Train Loss: 2.9949, Test Loss: 1.3757\n",
      "Epoch 2775, Train Loss: 3.5143, Test Loss: 1.3779\n",
      "Epoch 2776, Train Loss: 2.8234, Test Loss: 1.3757\n",
      "Epoch 2777, Train Loss: 3.2065, Test Loss: 1.3713\n",
      "Epoch 2778, Train Loss: 2.6311, Test Loss: 1.3630\n",
      "Epoch 2779, Train Loss: 3.4073, Test Loss: 1.3577\n",
      "Epoch 2780, Train Loss: 2.8344, Test Loss: 1.3564\n",
      "Epoch 2781, Train Loss: 2.6957, Test Loss: 1.3563\n",
      "Epoch 2782, Train Loss: 2.6017, Test Loss: 1.3574\n",
      "Epoch 2783, Train Loss: 2.9183, Test Loss: 1.3575\n",
      "Epoch 2784, Train Loss: 2.6192, Test Loss: 1.3651\n",
      "Epoch 2785, Train Loss: 2.7270, Test Loss: 1.3661\n",
      "Epoch 2786, Train Loss: 3.2220, Test Loss: 1.3623\n",
      "Epoch 2787, Train Loss: 2.8470, Test Loss: 1.3635\n",
      "Epoch 2788, Train Loss: 3.4139, Test Loss: 1.3604\n",
      "Epoch 2789, Train Loss: 2.9369, Test Loss: 1.3557\n",
      "Epoch 2790, Train Loss: 2.8912, Test Loss: 1.3517\n",
      "Epoch 2791, Train Loss: 2.6270, Test Loss: 1.3449\n",
      "Epoch 2792, Train Loss: 2.2554, Test Loss: 1.3344\n",
      "Epoch 2793, Train Loss: 3.4827, Test Loss: 1.3265\n",
      "Epoch 2794, Train Loss: 3.2308, Test Loss: 1.3229\n",
      "Epoch 2795, Train Loss: 2.8247, Test Loss: 1.3234\n",
      "Epoch 2796, Train Loss: 3.2845, Test Loss: 1.3277\n",
      "Epoch 2797, Train Loss: 3.1193, Test Loss: 1.3385\n",
      "Epoch 2798, Train Loss: 2.9862, Test Loss: 1.3488\n",
      "Epoch 2799, Train Loss: 2.8789, Test Loss: 1.3634\n",
      "Epoch 2800, Train Loss: 3.1409, Test Loss: 1.3775\n",
      "Epoch 2801, Train Loss: 2.9392, Test Loss: 1.3827\n",
      "Epoch 2802, Train Loss: 2.5768, Test Loss: 1.3864\n",
      "Epoch 2803, Train Loss: 3.3160, Test Loss: 1.3862\n",
      "Epoch 2804, Train Loss: 2.6523, Test Loss: 1.3834\n",
      "Epoch 2805, Train Loss: 3.4456, Test Loss: 1.3837\n",
      "Epoch 2806, Train Loss: 2.9978, Test Loss: 1.3791\n",
      "Epoch 2807, Train Loss: 2.6184, Test Loss: 1.3754\n",
      "Epoch 2808, Train Loss: 3.3492, Test Loss: 1.3690\n",
      "Epoch 2809, Train Loss: 3.1467, Test Loss: 1.3693\n",
      "Epoch 2810, Train Loss: 2.6846, Test Loss: 1.3671\n",
      "Epoch 2811, Train Loss: 2.5816, Test Loss: 1.3639\n",
      "Epoch 2812, Train Loss: 2.4589, Test Loss: 1.3615\n",
      "Epoch 2813, Train Loss: 2.9917, Test Loss: 1.3612\n",
      "Epoch 2814, Train Loss: 3.1631, Test Loss: 1.3569\n",
      "Epoch 2815, Train Loss: 2.8694, Test Loss: 1.3591\n",
      "Epoch 2816, Train Loss: 2.8280, Test Loss: 1.3551\n",
      "Epoch 2817, Train Loss: 2.6367, Test Loss: 1.3498\n",
      "Epoch 2818, Train Loss: 2.6880, Test Loss: 1.3482\n",
      "Epoch 2819, Train Loss: 2.9924, Test Loss: 1.3441\n",
      "Epoch 2820, Train Loss: 2.9435, Test Loss: 1.3412\n",
      "Epoch 2821, Train Loss: 2.4879, Test Loss: 1.3362\n",
      "Epoch 2822, Train Loss: 2.6222, Test Loss: 1.3280\n",
      "Epoch 2823, Train Loss: 2.5746, Test Loss: 1.3212\n",
      "Epoch 2824, Train Loss: 2.7825, Test Loss: 1.3195\n",
      "Epoch 2825, Train Loss: 2.7990, Test Loss: 1.3257\n",
      "Epoch 2826, Train Loss: 2.8512, Test Loss: 1.3319\n",
      "Epoch 2827, Train Loss: 2.9598, Test Loss: 1.3394\n",
      "Epoch 2828, Train Loss: 2.7597, Test Loss: 1.3503\n",
      "Epoch 2829, Train Loss: 2.5302, Test Loss: 1.3594\n",
      "Epoch 2830, Train Loss: 3.1213, Test Loss: 1.3692\n",
      "Epoch 2831, Train Loss: 3.0494, Test Loss: 1.3782\n",
      "Epoch 2832, Train Loss: 2.8855, Test Loss: 1.3831\n",
      "Epoch 2833, Train Loss: 2.9912, Test Loss: 1.3829\n",
      "Epoch 2834, Train Loss: 2.6866, Test Loss: 1.3861\n",
      "Epoch 2835, Train Loss: 2.4348, Test Loss: 1.3864\n",
      "Epoch 2836, Train Loss: 2.7160, Test Loss: 1.3825\n",
      "Epoch 2837, Train Loss: 2.8368, Test Loss: 1.3828\n",
      "Epoch 2838, Train Loss: 2.8003, Test Loss: 1.3859\n",
      "Epoch 2839, Train Loss: 2.9232, Test Loss: 1.3870\n",
      "Epoch 2840, Train Loss: 2.7579, Test Loss: 1.3913\n",
      "Epoch 2841, Train Loss: 2.4986, Test Loss: 1.3947\n",
      "Epoch 2842, Train Loss: 2.8726, Test Loss: 1.3923\n",
      "Epoch 2843, Train Loss: 2.1937, Test Loss: 1.3853\n",
      "Epoch 2844, Train Loss: 2.6508, Test Loss: 1.3774\n",
      "Epoch 2845, Train Loss: 2.2223, Test Loss: 1.3695\n",
      "Epoch 2846, Train Loss: 2.8165, Test Loss: 1.3645\n",
      "Epoch 2847, Train Loss: 2.6018, Test Loss: 1.3642\n",
      "Epoch 2848, Train Loss: 2.4704, Test Loss: 1.3579\n",
      "Epoch 2849, Train Loss: 2.8223, Test Loss: 1.3440\n",
      "Epoch 2850, Train Loss: 2.4477, Test Loss: 1.3232\n",
      "Epoch 2851, Train Loss: 2.6861, Test Loss: 1.3093\n",
      "Epoch 2852, Train Loss: 3.4518, Test Loss: 1.2934\n",
      "Epoch 2853, Train Loss: 2.8455, Test Loss: 1.2859\n",
      "Epoch 2854, Train Loss: 2.6349, Test Loss: 1.2780\n",
      "Epoch 2855, Train Loss: 3.4453, Test Loss: 1.2745\n",
      "Epoch 2856, Train Loss: 2.6659, Test Loss: 1.2709\n",
      "Epoch 2857, Train Loss: 2.4211, Test Loss: 1.2700\n",
      "Epoch 2858, Train Loss: 2.9637, Test Loss: 1.2739\n",
      "Epoch 2859, Train Loss: 2.6781, Test Loss: 1.2815\n",
      "Epoch 2860, Train Loss: 2.7418, Test Loss: 1.2984\n",
      "Epoch 2861, Train Loss: 2.8747, Test Loss: 1.3072\n",
      "Epoch 2862, Train Loss: 2.6363, Test Loss: 1.3198\n",
      "Epoch 2863, Train Loss: 2.5776, Test Loss: 1.3253\n",
      "Epoch 2864, Train Loss: 2.4824, Test Loss: 1.3266\n",
      "Epoch 2865, Train Loss: 2.7219, Test Loss: 1.3357\n",
      "Epoch 2866, Train Loss: 2.5858, Test Loss: 1.3439\n",
      "Epoch 2867, Train Loss: 2.4069, Test Loss: 1.3555\n",
      "Epoch 2868, Train Loss: 2.9323, Test Loss: 1.3715\n",
      "Epoch 2869, Train Loss: 2.7154, Test Loss: 1.3734\n",
      "Epoch 2870, Train Loss: 2.7590, Test Loss: 1.3813\n",
      "Epoch 2871, Train Loss: 2.8745, Test Loss: 1.3896\n",
      "Epoch 2872, Train Loss: 2.6981, Test Loss: 1.3872\n",
      "Epoch 2873, Train Loss: 2.3695, Test Loss: 1.3803\n",
      "Epoch 2874, Train Loss: 2.6886, Test Loss: 1.3628\n",
      "Epoch 2875, Train Loss: 2.7849, Test Loss: 1.3378\n",
      "Epoch 2876, Train Loss: 2.6118, Test Loss: 1.3160\n",
      "Epoch 2877, Train Loss: 2.2599, Test Loss: 1.3020\n",
      "Epoch 2878, Train Loss: 2.4822, Test Loss: 1.2779\n",
      "Epoch 2879, Train Loss: 2.6777, Test Loss: 1.2625\n",
      "Epoch 2880, Train Loss: 2.8282, Test Loss: 1.2505\n",
      "Epoch 2881, Train Loss: 2.7955, Test Loss: 1.2355\n",
      "Epoch 2882, Train Loss: 2.7176, Test Loss: 1.2277\n",
      "Epoch 2883, Train Loss: 2.8233, Test Loss: 1.2241\n",
      "Epoch 2884, Train Loss: 2.6210, Test Loss: 1.2149\n",
      "Epoch 2885, Train Loss: 2.2318, Test Loss: 1.2087\n",
      "Epoch 2886, Train Loss: 2.1953, Test Loss: 1.2057\n",
      "Epoch 2887, Train Loss: 2.3452, Test Loss: 1.2025\n",
      "Epoch 2888, Train Loss: 2.3229, Test Loss: 1.2003\n",
      "Epoch 2889, Train Loss: 2.3704, Test Loss: 1.2007\n",
      "Epoch 2890, Train Loss: 2.5148, Test Loss: 1.2020\n",
      "Epoch 2891, Train Loss: 2.6179, Test Loss: 1.2017\n",
      "Epoch 2892, Train Loss: 3.0107, Test Loss: 1.2152\n",
      "Epoch 2893, Train Loss: 2.3083, Test Loss: 1.2288\n",
      "Epoch 2894, Train Loss: 2.5581, Test Loss: 1.2452\n",
      "Epoch 2895, Train Loss: 2.8835, Test Loss: 1.2602\n",
      "Epoch 2896, Train Loss: 2.7871, Test Loss: 1.2776\n",
      "Epoch 2897, Train Loss: 2.8518, Test Loss: 1.2934\n",
      "Epoch 2898, Train Loss: 2.5863, Test Loss: 1.3159\n",
      "Epoch 2899, Train Loss: 2.5209, Test Loss: 1.3254\n",
      "Epoch 2900, Train Loss: 2.5270, Test Loss: 1.3319\n",
      "Epoch 2901, Train Loss: 2.8433, Test Loss: 1.3396\n",
      "Epoch 2902, Train Loss: 2.1439, Test Loss: 1.3515\n",
      "Epoch 2903, Train Loss: 2.5423, Test Loss: 1.3609\n",
      "Epoch 2904, Train Loss: 2.4481, Test Loss: 1.3656\n",
      "Epoch 2905, Train Loss: 2.5026, Test Loss: 1.3696\n",
      "Epoch 2906, Train Loss: 2.6418, Test Loss: 1.3720\n",
      "Epoch 2907, Train Loss: 2.5485, Test Loss: 1.3826\n",
      "Epoch 2908, Train Loss: 2.5046, Test Loss: 1.3817\n",
      "Epoch 2909, Train Loss: 2.2864, Test Loss: 1.3805\n",
      "Epoch 2910, Train Loss: 2.5687, Test Loss: 1.3832\n",
      "Epoch 2911, Train Loss: 2.0756, Test Loss: 1.3832\n",
      "Epoch 2912, Train Loss: 2.3727, Test Loss: 1.3774\n",
      "Epoch 2913, Train Loss: 2.4484, Test Loss: 1.3724\n",
      "Epoch 2914, Train Loss: 2.3997, Test Loss: 1.3674\n",
      "Epoch 2915, Train Loss: 2.5888, Test Loss: 1.3598\n",
      "Epoch 2916, Train Loss: 2.8067, Test Loss: 1.3507\n",
      "Epoch 2917, Train Loss: 2.7686, Test Loss: 1.3406\n",
      "Epoch 2918, Train Loss: 2.4429, Test Loss: 1.3313\n",
      "Epoch 2919, Train Loss: 3.1997, Test Loss: 1.3205\n",
      "Epoch 2920, Train Loss: 2.9867, Test Loss: 1.3139\n",
      "Epoch 2921, Train Loss: 2.2936, Test Loss: 1.3060\n",
      "Epoch 2922, Train Loss: 3.0533, Test Loss: 1.3012\n",
      "Epoch 2923, Train Loss: 2.2601, Test Loss: 1.2972\n",
      "Epoch 2924, Train Loss: 2.5341, Test Loss: 1.2920\n",
      "Epoch 2925, Train Loss: 2.0722, Test Loss: 1.2849\n",
      "Epoch 2926, Train Loss: 2.3365, Test Loss: 1.2715\n",
      "Epoch 2927, Train Loss: 2.7642, Test Loss: 1.2587\n",
      "Epoch 2928, Train Loss: 2.4869, Test Loss: 1.2446\n",
      "Epoch 2929, Train Loss: 2.6496, Test Loss: 1.2329\n",
      "Epoch 2930, Train Loss: 2.5351, Test Loss: 1.2237\n",
      "Epoch 2931, Train Loss: 2.3076, Test Loss: 1.2189\n",
      "Epoch 2932, Train Loss: 2.2556, Test Loss: 1.2101\n",
      "Epoch 2933, Train Loss: 2.2337, Test Loss: 1.1994\n",
      "Epoch 2934, Train Loss: 2.8860, Test Loss: 1.1915\n",
      "Epoch 2935, Train Loss: 2.7010, Test Loss: 1.1866\n",
      "Epoch 2936, Train Loss: 2.6268, Test Loss: 1.1855\n",
      "Epoch 2937, Train Loss: 2.4346, Test Loss: 1.1791\n",
      "Epoch 2938, Train Loss: 2.7479, Test Loss: 1.1723\n",
      "Epoch 2939, Train Loss: 2.4362, Test Loss: 1.1750\n",
      "Epoch 2940, Train Loss: 2.2008, Test Loss: 1.1802\n",
      "Epoch 2941, Train Loss: 2.1293, Test Loss: 1.1830\n",
      "Epoch 2942, Train Loss: 2.7005, Test Loss: 1.1882\n",
      "Epoch 2943, Train Loss: 2.1370, Test Loss: 1.2019\n",
      "Epoch 2944, Train Loss: 2.4532, Test Loss: 1.2144\n",
      "Epoch 2945, Train Loss: 2.2880, Test Loss: 1.2253\n",
      "Epoch 2946, Train Loss: 2.1424, Test Loss: 1.2451\n",
      "Epoch 2947, Train Loss: 2.4705, Test Loss: 1.2648\n",
      "Epoch 2948, Train Loss: 2.4793, Test Loss: 1.2816\n",
      "Epoch 2949, Train Loss: 2.1318, Test Loss: 1.2970\n",
      "Epoch 2950, Train Loss: 2.4206, Test Loss: 1.3175\n",
      "Epoch 2951, Train Loss: 2.6071, Test Loss: 1.3432\n",
      "Epoch 2952, Train Loss: 2.4073, Test Loss: 1.3537\n",
      "Epoch 2953, Train Loss: 2.2266, Test Loss: 1.3662\n",
      "Epoch 2954, Train Loss: 2.5953, Test Loss: 1.3703\n",
      "Epoch 2955, Train Loss: 2.0260, Test Loss: 1.3665\n",
      "Epoch 2956, Train Loss: 2.5672, Test Loss: 1.3623\n",
      "Epoch 2957, Train Loss: 1.9757, Test Loss: 1.3492\n",
      "Epoch 2958, Train Loss: 2.4653, Test Loss: 1.3322\n",
      "Epoch 2959, Train Loss: 2.0180, Test Loss: 1.3108\n",
      "Epoch 2960, Train Loss: 2.1106, Test Loss: 1.2872\n",
      "Epoch 2961, Train Loss: 2.7777, Test Loss: 1.2659\n",
      "Epoch 2962, Train Loss: 2.4668, Test Loss: 1.2554\n",
      "Epoch 2963, Train Loss: 2.2991, Test Loss: 1.2416\n",
      "Epoch 2964, Train Loss: 2.3717, Test Loss: 1.2368\n",
      "Epoch 2965, Train Loss: 2.1942, Test Loss: 1.2303\n",
      "Epoch 2966, Train Loss: 2.0609, Test Loss: 1.2297\n",
      "Epoch 2967, Train Loss: 1.9214, Test Loss: 1.2281\n",
      "Epoch 2968, Train Loss: 2.3286, Test Loss: 1.2237\n",
      "Epoch 2969, Train Loss: 2.3661, Test Loss: 1.2262\n",
      "Epoch 2970, Train Loss: 2.2534, Test Loss: 1.2311\n",
      "Epoch 2971, Train Loss: 2.5773, Test Loss: 1.2338\n",
      "Epoch 2972, Train Loss: 2.2921, Test Loss: 1.2401\n",
      "Epoch 2973, Train Loss: 2.2403, Test Loss: 1.2483\n",
      "Epoch 2974, Train Loss: 2.3968, Test Loss: 1.2497\n",
      "Epoch 2975, Train Loss: 2.4534, Test Loss: 1.2588\n",
      "Epoch 2976, Train Loss: 2.8700, Test Loss: 1.2613\n",
      "Epoch 2977, Train Loss: 2.1827, Test Loss: 1.2652\n",
      "Epoch 2978, Train Loss: 1.9617, Test Loss: 1.2733\n",
      "Epoch 2979, Train Loss: 2.4684, Test Loss: 1.2811\n",
      "Epoch 2980, Train Loss: 2.5513, Test Loss: 1.2822\n",
      "Epoch 2981, Train Loss: 2.7276, Test Loss: 1.2811\n",
      "Epoch 2982, Train Loss: 2.0532, Test Loss: 1.2806\n",
      "Epoch 2983, Train Loss: 2.3655, Test Loss: 1.2760\n",
      "Epoch 2984, Train Loss: 2.3065, Test Loss: 1.2671\n",
      "Epoch 2985, Train Loss: 2.2583, Test Loss: 1.2566\n",
      "Epoch 2986, Train Loss: 2.4848, Test Loss: 1.2392\n",
      "Epoch 2987, Train Loss: 2.2058, Test Loss: 1.2154\n",
      "Epoch 2988, Train Loss: 3.0215, Test Loss: 1.1980\n",
      "Epoch 2989, Train Loss: 1.9845, Test Loss: 1.1879\n",
      "Epoch 2990, Train Loss: 2.7496, Test Loss: 1.1896\n",
      "Epoch 2991, Train Loss: 2.2803, Test Loss: 1.1897\n",
      "Epoch 2992, Train Loss: 2.1891, Test Loss: 1.1903\n",
      "Epoch 2993, Train Loss: 2.0624, Test Loss: 1.1947\n",
      "Epoch 2994, Train Loss: 2.5138, Test Loss: 1.1974\n",
      "Epoch 2995, Train Loss: 2.3117, Test Loss: 1.2053\n",
      "Epoch 2996, Train Loss: 2.5247, Test Loss: 1.2139\n",
      "Epoch 2997, Train Loss: 2.0047, Test Loss: 1.2255\n",
      "Epoch 2998, Train Loss: 2.2552, Test Loss: 1.2333\n",
      "Epoch 2999, Train Loss: 2.1676, Test Loss: 1.2418\n",
      "Epoch 3000, Train Loss: 2.2565, Test Loss: 1.2496\n",
      "Epoch 3001, Train Loss: 2.1565, Test Loss: 1.2529\n",
      "Epoch 3002, Train Loss: 2.1263, Test Loss: 1.2515\n",
      "Epoch 3003, Train Loss: 2.5401, Test Loss: 1.2471\n",
      "Epoch 3004, Train Loss: 2.4817, Test Loss: 1.2486\n",
      "Epoch 3005, Train Loss: 2.2859, Test Loss: 1.2497\n",
      "Epoch 3006, Train Loss: 2.3473, Test Loss: 1.2545\n",
      "Epoch 3007, Train Loss: 2.2395, Test Loss: 1.2632\n",
      "Epoch 3008, Train Loss: 2.2330, Test Loss: 1.2748\n",
      "Epoch 3009, Train Loss: 2.3283, Test Loss: 1.2924\n",
      "Epoch 3010, Train Loss: 2.3291, Test Loss: 1.3031\n",
      "Epoch 3011, Train Loss: 2.1761, Test Loss: 1.3074\n",
      "Epoch 3012, Train Loss: 2.1892, Test Loss: 1.3107\n",
      "Epoch 3013, Train Loss: 2.3733, Test Loss: 1.3135\n",
      "Epoch 3014, Train Loss: 2.5500, Test Loss: 1.3149\n",
      "Epoch 3015, Train Loss: 2.0930, Test Loss: 1.3118\n",
      "Epoch 3016, Train Loss: 2.1956, Test Loss: 1.3092\n",
      "Epoch 3017, Train Loss: 2.2714, Test Loss: 1.3056\n",
      "Epoch 3018, Train Loss: 2.1433, Test Loss: 1.2961\n",
      "Epoch 3019, Train Loss: 2.2606, Test Loss: 1.2954\n",
      "Epoch 3020, Train Loss: 2.1637, Test Loss: 1.2845\n",
      "Epoch 3021, Train Loss: 2.3431, Test Loss: 1.2816\n",
      "Epoch 3022, Train Loss: 1.9985, Test Loss: 1.2738\n",
      "Epoch 3023, Train Loss: 1.8920, Test Loss: 1.2705\n",
      "Epoch 3024, Train Loss: 1.9295, Test Loss: 1.2728\n",
      "Epoch 3025, Train Loss: 2.2734, Test Loss: 1.2756\n",
      "Epoch 3026, Train Loss: 2.3485, Test Loss: 1.2818\n",
      "Epoch 3027, Train Loss: 2.5236, Test Loss: 1.2828\n",
      "Epoch 3028, Train Loss: 2.2800, Test Loss: 1.2854\n",
      "Epoch 3029, Train Loss: 2.1060, Test Loss: 1.2888\n",
      "Epoch 3030, Train Loss: 1.8399, Test Loss: 1.2895\n",
      "Epoch 3031, Train Loss: 1.8328, Test Loss: 1.2966\n",
      "Epoch 3032, Train Loss: 2.3331, Test Loss: 1.2970\n",
      "Epoch 3033, Train Loss: 1.9818, Test Loss: 1.2951\n",
      "Epoch 3034, Train Loss: 2.5673, Test Loss: 1.2903\n",
      "Epoch 3035, Train Loss: 1.6295, Test Loss: 1.2868\n",
      "Epoch 3036, Train Loss: 1.8576, Test Loss: 1.2805\n",
      "Epoch 3037, Train Loss: 2.0670, Test Loss: 1.2665\n",
      "Epoch 3038, Train Loss: 1.8170, Test Loss: 1.2475\n",
      "Epoch 3039, Train Loss: 2.2740, Test Loss: 1.2357\n",
      "Epoch 3040, Train Loss: 2.1226, Test Loss: 1.2291\n",
      "Epoch 3041, Train Loss: 2.0053, Test Loss: 1.2237\n",
      "Epoch 3042, Train Loss: 1.7774, Test Loss: 1.2214\n",
      "Epoch 3043, Train Loss: 2.2990, Test Loss: 1.2169\n",
      "Epoch 3044, Train Loss: 2.1608, Test Loss: 1.2157\n",
      "Epoch 3045, Train Loss: 1.9234, Test Loss: 1.2095\n",
      "Epoch 3046, Train Loss: 1.9835, Test Loss: 1.2047\n",
      "Epoch 3047, Train Loss: 2.4214, Test Loss: 1.2007\n",
      "Epoch 3048, Train Loss: 2.3050, Test Loss: 1.1985\n",
      "Epoch 3049, Train Loss: 2.4991, Test Loss: 1.1965\n",
      "Epoch 3050, Train Loss: 2.0366, Test Loss: 1.1957\n",
      "Epoch 3051, Train Loss: 1.9106, Test Loss: 1.1989\n",
      "Epoch 3052, Train Loss: 2.0465, Test Loss: 1.2070\n",
      "Epoch 3053, Train Loss: 2.0851, Test Loss: 1.2150\n",
      "Epoch 3054, Train Loss: 2.0247, Test Loss: 1.2263\n",
      "Epoch 3055, Train Loss: 2.1061, Test Loss: 1.2329\n",
      "Epoch 3056, Train Loss: 2.1268, Test Loss: 1.2422\n",
      "Epoch 3057, Train Loss: 1.7358, Test Loss: 1.2443\n",
      "Epoch 3058, Train Loss: 2.0248, Test Loss: 1.2559\n",
      "Epoch 3059, Train Loss: 2.3288, Test Loss: 1.2669\n",
      "Epoch 3060, Train Loss: 1.7385, Test Loss: 1.2780\n",
      "Epoch 3061, Train Loss: 2.1202, Test Loss: 1.2840\n",
      "Epoch 3062, Train Loss: 2.0908, Test Loss: 1.2908\n",
      "Epoch 3063, Train Loss: 1.9080, Test Loss: 1.3012\n",
      "Epoch 3064, Train Loss: 2.2957, Test Loss: 1.3099\n",
      "Epoch 3065, Train Loss: 2.0995, Test Loss: 1.3211\n",
      "Epoch 3066, Train Loss: 2.2698, Test Loss: 1.3282\n",
      "Epoch 3067, Train Loss: 2.0537, Test Loss: 1.3208\n",
      "Epoch 3068, Train Loss: 2.4459, Test Loss: 1.3074\n",
      "Epoch 3069, Train Loss: 2.1420, Test Loss: 1.2837\n",
      "Epoch 3070, Train Loss: 2.1166, Test Loss: 1.2692\n",
      "Epoch 3071, Train Loss: 2.1028, Test Loss: 1.2552\n",
      "Epoch 3072, Train Loss: 2.3393, Test Loss: 1.2358\n",
      "Epoch 3073, Train Loss: 1.8498, Test Loss: 1.2211\n",
      "Epoch 3074, Train Loss: 2.1866, Test Loss: 1.2190\n",
      "Epoch 3075, Train Loss: 2.1812, Test Loss: 1.2262\n",
      "Epoch 3076, Train Loss: 1.8602, Test Loss: 1.2325\n",
      "Epoch 3077, Train Loss: 2.0244, Test Loss: 1.2392\n",
      "Epoch 3078, Train Loss: 2.2084, Test Loss: 1.2391\n",
      "Epoch 3079, Train Loss: 1.8849, Test Loss: 1.2424\n",
      "Epoch 3080, Train Loss: 1.9245, Test Loss: 1.2442\n",
      "Epoch 3081, Train Loss: 1.9900, Test Loss: 1.2458\n",
      "Epoch 3082, Train Loss: 2.1702, Test Loss: 1.2422\n",
      "Epoch 3083, Train Loss: 2.2273, Test Loss: 1.2440\n",
      "Epoch 3084, Train Loss: 1.8020, Test Loss: 1.2455\n",
      "Epoch 3085, Train Loss: 1.8963, Test Loss: 1.2509\n",
      "Epoch 3086, Train Loss: 2.4431, Test Loss: 1.2552\n",
      "Epoch 3087, Train Loss: 2.1907, Test Loss: 1.2608\n",
      "Epoch 3088, Train Loss: 2.0840, Test Loss: 1.2639\n",
      "Epoch 3089, Train Loss: 1.9430, Test Loss: 1.2755\n",
      "Epoch 3090, Train Loss: 1.6262, Test Loss: 1.2864\n",
      "Epoch 3091, Train Loss: 1.9505, Test Loss: 1.2760\n",
      "Epoch 3092, Train Loss: 1.8737, Test Loss: 1.2583\n",
      "Epoch 3093, Train Loss: 2.1211, Test Loss: 1.2498\n",
      "Epoch 3094, Train Loss: 1.9669, Test Loss: 1.2426\n",
      "Epoch 3095, Train Loss: 1.7893, Test Loss: 1.2300\n",
      "Epoch 3096, Train Loss: 1.8982, Test Loss: 1.2259\n",
      "Epoch 3097, Train Loss: 2.1901, Test Loss: 1.2295\n",
      "Epoch 3098, Train Loss: 1.6956, Test Loss: 1.2324\n",
      "Epoch 3099, Train Loss: 2.1474, Test Loss: 1.2307\n",
      "Epoch 3100, Train Loss: 1.7032, Test Loss: 1.2341\n",
      "Epoch 3101, Train Loss: 2.0171, Test Loss: 1.2378\n",
      "Epoch 3102, Train Loss: 1.9866, Test Loss: 1.2388\n",
      "Epoch 3103, Train Loss: 1.7602, Test Loss: 1.2394\n",
      "Epoch 3104, Train Loss: 2.5204, Test Loss: 1.2431\n",
      "Epoch 3105, Train Loss: 1.8335, Test Loss: 1.2498\n",
      "Epoch 3106, Train Loss: 1.9528, Test Loss: 1.2555\n",
      "Epoch 3107, Train Loss: 2.1461, Test Loss: 1.2567\n",
      "Epoch 3108, Train Loss: 2.0667, Test Loss: 1.2573\n",
      "Epoch 3109, Train Loss: 2.3845, Test Loss: 1.2516\n",
      "Epoch 3110, Train Loss: 2.1324, Test Loss: 1.2436\n",
      "Epoch 3111, Train Loss: 1.9168, Test Loss: 1.2262\n",
      "Epoch 3112, Train Loss: 2.2256, Test Loss: 1.2193\n",
      "Epoch 3113, Train Loss: 2.3891, Test Loss: 1.2113\n",
      "Epoch 3114, Train Loss: 1.9280, Test Loss: 1.2107\n",
      "Epoch 3115, Train Loss: 1.6092, Test Loss: 1.2117\n",
      "Epoch 3116, Train Loss: 1.9741, Test Loss: 1.2114\n",
      "Epoch 3117, Train Loss: 2.3372, Test Loss: 1.2079\n",
      "Epoch 3118, Train Loss: 2.0478, Test Loss: 1.2003\n",
      "Epoch 3119, Train Loss: 1.8297, Test Loss: 1.1989\n",
      "Epoch 3120, Train Loss: 1.9950, Test Loss: 1.2069\n",
      "Epoch 3121, Train Loss: 1.8990, Test Loss: 1.2122\n",
      "Epoch 3122, Train Loss: 1.8812, Test Loss: 1.2195\n",
      "Epoch 3123, Train Loss: 1.9571, Test Loss: 1.2265\n",
      "Epoch 3124, Train Loss: 2.0547, Test Loss: 1.2280\n",
      "Epoch 3125, Train Loss: 2.2333, Test Loss: 1.2315\n",
      "Epoch 3126, Train Loss: 1.6543, Test Loss: 1.2351\n",
      "Epoch 3127, Train Loss: 1.8052, Test Loss: 1.2364\n",
      "Epoch 3128, Train Loss: 1.8011, Test Loss: 1.2397\n",
      "Epoch 3129, Train Loss: 2.0686, Test Loss: 1.2387\n",
      "Epoch 3130, Train Loss: 1.9980, Test Loss: 1.2421\n",
      "Epoch 3131, Train Loss: 2.2524, Test Loss: 1.2395\n",
      "Epoch 3132, Train Loss: 1.7172, Test Loss: 1.2376\n",
      "Epoch 3133, Train Loss: 1.9103, Test Loss: 1.2371\n",
      "Epoch 3134, Train Loss: 2.3265, Test Loss: 1.2302\n",
      "Epoch 3135, Train Loss: 1.9730, Test Loss: 1.2160\n",
      "Epoch 3136, Train Loss: 1.7776, Test Loss: 1.2018\n",
      "Epoch 3137, Train Loss: 1.5694, Test Loss: 1.1881\n",
      "Epoch 3138, Train Loss: 1.8879, Test Loss: 1.1751\n",
      "Epoch 3139, Train Loss: 2.0763, Test Loss: 1.1635\n",
      "Epoch 3140, Train Loss: 2.1661, Test Loss: 1.1492\n",
      "Epoch 3141, Train Loss: 1.8047, Test Loss: 1.1418\n",
      "Epoch 3142, Train Loss: 1.8138, Test Loss: 1.1310\n",
      "Epoch 3143, Train Loss: 1.7852, Test Loss: 1.1234\n",
      "Epoch 3144, Train Loss: 1.8075, Test Loss: 1.1217\n",
      "Epoch 3145, Train Loss: 1.6481, Test Loss: 1.1152\n",
      "Epoch 3146, Train Loss: 1.7928, Test Loss: 1.1133\n",
      "Epoch 3147, Train Loss: 1.7770, Test Loss: 1.1119\n",
      "Epoch 3148, Train Loss: 2.1693, Test Loss: 1.1190\n",
      "Epoch 3149, Train Loss: 1.5431, Test Loss: 1.1225\n",
      "Epoch 3150, Train Loss: 2.0679, Test Loss: 1.1296\n",
      "Epoch 3151, Train Loss: 1.8835, Test Loss: 1.1384\n",
      "Epoch 3152, Train Loss: 2.1867, Test Loss: 1.1475\n",
      "Epoch 3153, Train Loss: 1.8705, Test Loss: 1.1512\n",
      "Epoch 3154, Train Loss: 2.2446, Test Loss: 1.1556\n",
      "Epoch 3155, Train Loss: 2.0743, Test Loss: 1.1614\n",
      "Epoch 3156, Train Loss: 1.6584, Test Loss: 1.1627\n",
      "Epoch 3157, Train Loss: 1.9505, Test Loss: 1.1654\n",
      "Epoch 3158, Train Loss: 2.0689, Test Loss: 1.1622\n",
      "Epoch 3159, Train Loss: 1.7676, Test Loss: 1.1633\n",
      "Epoch 3160, Train Loss: 1.9706, Test Loss: 1.1579\n",
      "Epoch 3161, Train Loss: 2.2324, Test Loss: 1.1537\n",
      "Epoch 3162, Train Loss: 2.3561, Test Loss: 1.1508\n",
      "Epoch 3163, Train Loss: 1.8391, Test Loss: 1.1511\n",
      "Epoch 3164, Train Loss: 1.9437, Test Loss: 1.1494\n",
      "Epoch 3165, Train Loss: 1.8758, Test Loss: 1.1511\n",
      "Epoch 3166, Train Loss: 1.7120, Test Loss: 1.1547\n",
      "Epoch 3167, Train Loss: 1.9834, Test Loss: 1.1577\n",
      "Epoch 3168, Train Loss: 2.0775, Test Loss: 1.1590\n",
      "Epoch 3169, Train Loss: 1.4497, Test Loss: 1.1627\n",
      "Epoch 3170, Train Loss: 1.8019, Test Loss: 1.1713\n",
      "Epoch 3171, Train Loss: 1.9936, Test Loss: 1.1861\n",
      "Epoch 3172, Train Loss: 2.0279, Test Loss: 1.1957\n",
      "Epoch 3173, Train Loss: 1.9132, Test Loss: 1.1989\n",
      "Epoch 3174, Train Loss: 2.0307, Test Loss: 1.1898\n",
      "Epoch 3175, Train Loss: 2.0654, Test Loss: 1.1816\n",
      "Epoch 3176, Train Loss: 1.8843, Test Loss: 1.1687\n",
      "Epoch 3177, Train Loss: 2.0765, Test Loss: 1.1468\n",
      "Epoch 3178, Train Loss: 1.7138, Test Loss: 1.1261\n",
      "Epoch 3179, Train Loss: 1.7452, Test Loss: 1.1159\n",
      "Epoch 3180, Train Loss: 1.8270, Test Loss: 1.1109\n",
      "Epoch 3181, Train Loss: 2.1869, Test Loss: 1.1189\n",
      "Epoch 3182, Train Loss: 1.7209, Test Loss: 1.1225\n",
      "Epoch 3183, Train Loss: 1.7642, Test Loss: 1.1281\n",
      "Epoch 3184, Train Loss: 2.0947, Test Loss: 1.1341\n",
      "Epoch 3185, Train Loss: 2.0093, Test Loss: 1.1432\n",
      "Epoch 3186, Train Loss: 1.8864, Test Loss: 1.1488\n",
      "Epoch 3187, Train Loss: 1.9309, Test Loss: 1.1486\n",
      "Epoch 3188, Train Loss: 1.9704, Test Loss: 1.1444\n",
      "Epoch 3189, Train Loss: 1.7550, Test Loss: 1.1427\n",
      "Epoch 3190, Train Loss: 1.6386, Test Loss: 1.1426\n",
      "Epoch 3191, Train Loss: 1.9363, Test Loss: 1.1385\n",
      "Epoch 3192, Train Loss: 2.0925, Test Loss: 1.1411\n",
      "Epoch 3193, Train Loss: 1.6337, Test Loss: 1.1481\n",
      "Epoch 3194, Train Loss: 1.7057, Test Loss: 1.1638\n",
      "Epoch 3195, Train Loss: 1.7824, Test Loss: 1.1752\n",
      "Epoch 3196, Train Loss: 1.7287, Test Loss: 1.1889\n",
      "Epoch 3197, Train Loss: 1.7753, Test Loss: 1.1991\n",
      "Epoch 3198, Train Loss: 1.5585, Test Loss: 1.2066\n",
      "Epoch 3199, Train Loss: 1.6971, Test Loss: 1.2119\n",
      "Epoch 3200, Train Loss: 1.7610, Test Loss: 1.2109\n",
      "Epoch 3201, Train Loss: 2.0266, Test Loss: 1.2088\n",
      "Epoch 3202, Train Loss: 1.9169, Test Loss: 1.2121\n",
      "Epoch 3203, Train Loss: 1.5266, Test Loss: 1.2087\n",
      "Epoch 3204, Train Loss: 1.7437, Test Loss: 1.1995\n",
      "Epoch 3205, Train Loss: 1.6984, Test Loss: 1.1946\n",
      "Epoch 3206, Train Loss: 1.8577, Test Loss: 1.1845\n",
      "Epoch 3207, Train Loss: 1.9118, Test Loss: 1.1759\n",
      "Epoch 3208, Train Loss: 1.8201, Test Loss: 1.1643\n",
      "Epoch 3209, Train Loss: 1.8609, Test Loss: 1.1511\n",
      "Epoch 3210, Train Loss: 1.8713, Test Loss: 1.1354\n",
      "Epoch 3211, Train Loss: 1.7656, Test Loss: 1.1208\n",
      "Epoch 3212, Train Loss: 1.7548, Test Loss: 1.1059\n",
      "Epoch 3213, Train Loss: 1.8038, Test Loss: 1.0995\n",
      "Epoch 3214, Train Loss: 1.7387, Test Loss: 1.0885\n",
      "Epoch 3215, Train Loss: 1.7980, Test Loss: 1.0821\n",
      "Epoch 3216, Train Loss: 1.7215, Test Loss: 1.0710\n",
      "Epoch 3217, Train Loss: 1.9255, Test Loss: 1.0599\n",
      "Epoch 3218, Train Loss: 1.6912, Test Loss: 1.0503\n",
      "Epoch 3219, Train Loss: 1.7941, Test Loss: 1.0499\n",
      "Epoch 3220, Train Loss: 1.7346, Test Loss: 1.0493\n",
      "Epoch 3221, Train Loss: 1.4921, Test Loss: 1.0486\n",
      "Epoch 3222, Train Loss: 1.7263, Test Loss: 1.0532\n",
      "Epoch 3223, Train Loss: 1.9101, Test Loss: 1.0574\n",
      "Epoch 3224, Train Loss: 1.7318, Test Loss: 1.0613\n",
      "Epoch 3225, Train Loss: 1.5490, Test Loss: 1.0606\n",
      "Epoch 3226, Train Loss: 1.7555, Test Loss: 1.0568\n",
      "Epoch 3227, Train Loss: 1.7544, Test Loss: 1.0559\n",
      "Epoch 3228, Train Loss: 1.8122, Test Loss: 1.0607\n",
      "Epoch 3229, Train Loss: 1.7160, Test Loss: 1.0705\n",
      "Epoch 3230, Train Loss: 1.7837, Test Loss: 1.0793\n",
      "Epoch 3231, Train Loss: 1.7457, Test Loss: 1.0930\n",
      "Epoch 3232, Train Loss: 1.5741, Test Loss: 1.1075\n",
      "Epoch 3233, Train Loss: 1.5671, Test Loss: 1.1137\n",
      "Epoch 3234, Train Loss: 1.7902, Test Loss: 1.1202\n",
      "Epoch 3235, Train Loss: 1.7400, Test Loss: 1.1305\n",
      "Epoch 3236, Train Loss: 1.5680, Test Loss: 1.1459\n",
      "Epoch 3237, Train Loss: 1.6971, Test Loss: 1.1579\n",
      "Epoch 3238, Train Loss: 1.8174, Test Loss: 1.1606\n",
      "Epoch 3239, Train Loss: 1.7277, Test Loss: 1.1605\n",
      "Epoch 3240, Train Loss: 1.4831, Test Loss: 1.1574\n",
      "Epoch 3241, Train Loss: 2.0195, Test Loss: 1.1507\n",
      "Epoch 3242, Train Loss: 1.7733, Test Loss: 1.1395\n",
      "Epoch 3243, Train Loss: 1.6944, Test Loss: 1.1327\n",
      "Epoch 3244, Train Loss: 1.6678, Test Loss: 1.1278\n",
      "Epoch 3245, Train Loss: 1.6367, Test Loss: 1.1243\n",
      "Epoch 3246, Train Loss: 1.5391, Test Loss: 1.1263\n",
      "Epoch 3247, Train Loss: 1.6078, Test Loss: 1.1275\n",
      "Epoch 3248, Train Loss: 1.6539, Test Loss: 1.1302\n",
      "Epoch 3249, Train Loss: 1.5866, Test Loss: 1.1263\n",
      "Epoch 3250, Train Loss: 1.9581, Test Loss: 1.1212\n",
      "Epoch 3251, Train Loss: 1.6766, Test Loss: 1.1199\n",
      "Epoch 3252, Train Loss: 1.6834, Test Loss: 1.1133\n",
      "Epoch 3253, Train Loss: 1.5526, Test Loss: 1.1042\n",
      "Epoch 3254, Train Loss: 1.6231, Test Loss: 1.1015\n",
      "Epoch 3255, Train Loss: 1.8312, Test Loss: 1.0991\n",
      "Epoch 3256, Train Loss: 1.6211, Test Loss: 1.0973\n",
      "Epoch 3257, Train Loss: 1.3114, Test Loss: 1.1007\n",
      "Epoch 3258, Train Loss: 1.7313, Test Loss: 1.1051\n",
      "Epoch 3259, Train Loss: 1.5922, Test Loss: 1.1047\n",
      "Epoch 3260, Train Loss: 1.6670, Test Loss: 1.1027\n",
      "Epoch 3261, Train Loss: 1.7430, Test Loss: 1.1114\n",
      "Epoch 3262, Train Loss: 1.4508, Test Loss: 1.1239\n",
      "Epoch 3263, Train Loss: 1.6336, Test Loss: 1.1316\n",
      "Epoch 3264, Train Loss: 1.6128, Test Loss: 1.1417\n",
      "Epoch 3265, Train Loss: 1.4724, Test Loss: 1.1457\n",
      "Epoch 3266, Train Loss: 1.4810, Test Loss: 1.1489\n",
      "Epoch 3267, Train Loss: 1.7277, Test Loss: 1.1520\n",
      "Epoch 3268, Train Loss: 1.7975, Test Loss: 1.1472\n",
      "Epoch 3269, Train Loss: 1.5498, Test Loss: 1.1411\n",
      "Epoch 3270, Train Loss: 1.9087, Test Loss: 1.1312\n",
      "Epoch 3271, Train Loss: 1.3762, Test Loss: 1.1200\n",
      "Epoch 3272, Train Loss: 1.5239, Test Loss: 1.1043\n",
      "Epoch 3273, Train Loss: 1.6213, Test Loss: 1.0893\n",
      "Epoch 3274, Train Loss: 1.4476, Test Loss: 1.0764\n",
      "Epoch 3275, Train Loss: 1.4797, Test Loss: 1.0691\n",
      "Epoch 3276, Train Loss: 1.8570, Test Loss: 1.0673\n",
      "Epoch 3277, Train Loss: 1.4485, Test Loss: 1.0678\n",
      "Epoch 3278, Train Loss: 1.6437, Test Loss: 1.0640\n",
      "Epoch 3279, Train Loss: 1.5275, Test Loss: 1.0556\n",
      "Epoch 3280, Train Loss: 1.4692, Test Loss: 1.0491\n",
      "Epoch 3281, Train Loss: 1.4063, Test Loss: 1.0424\n",
      "Epoch 3282, Train Loss: 1.4563, Test Loss: 1.0355\n",
      "Epoch 3283, Train Loss: 1.6503, Test Loss: 1.0287\n",
      "Epoch 3284, Train Loss: 1.6350, Test Loss: 1.0215\n",
      "Epoch 3285, Train Loss: 1.9962, Test Loss: 1.0175\n",
      "Epoch 3286, Train Loss: 1.6924, Test Loss: 1.0147\n",
      "Epoch 3287, Train Loss: 1.6451, Test Loss: 1.0145\n",
      "Epoch 3288, Train Loss: 1.4779, Test Loss: 1.0165\n",
      "Epoch 3289, Train Loss: 1.6857, Test Loss: 1.0198\n",
      "Epoch 3290, Train Loss: 1.6527, Test Loss: 1.0248\n",
      "Epoch 3291, Train Loss: 1.7730, Test Loss: 1.0326\n",
      "Epoch 3292, Train Loss: 1.4762, Test Loss: 1.0407\n",
      "Epoch 3293, Train Loss: 1.8060, Test Loss: 1.0480\n",
      "Epoch 3294, Train Loss: 1.5413, Test Loss: 1.0503\n",
      "Epoch 3295, Train Loss: 1.5892, Test Loss: 1.0512\n",
      "Epoch 3296, Train Loss: 1.5787, Test Loss: 1.0584\n",
      "Epoch 3297, Train Loss: 1.4130, Test Loss: 1.0594\n",
      "Epoch 3298, Train Loss: 1.8234, Test Loss: 1.0604\n",
      "Epoch 3299, Train Loss: 1.3427, Test Loss: 1.0593\n",
      "Epoch 3300, Train Loss: 1.5307, Test Loss: 1.0574\n",
      "Epoch 3301, Train Loss: 1.7892, Test Loss: 1.0574\n",
      "Epoch 3302, Train Loss: 1.6759, Test Loss: 1.0567\n",
      "Epoch 3303, Train Loss: 1.8544, Test Loss: 1.0579\n",
      "Epoch 3304, Train Loss: 1.6995, Test Loss: 1.0577\n",
      "Epoch 3305, Train Loss: 1.7079, Test Loss: 1.0591\n",
      "Epoch 3306, Train Loss: 1.6045, Test Loss: 1.0542\n",
      "Epoch 3307, Train Loss: 1.6384, Test Loss: 1.0502\n",
      "Epoch 3308, Train Loss: 1.6128, Test Loss: 1.0471\n",
      "Epoch 3309, Train Loss: 1.4957, Test Loss: 1.0477\n",
      "Epoch 3310, Train Loss: 1.3492, Test Loss: 1.0535\n",
      "Epoch 3311, Train Loss: 1.3846, Test Loss: 1.0590\n",
      "Epoch 3312, Train Loss: 1.5505, Test Loss: 1.0563\n",
      "Epoch 3313, Train Loss: 1.8182, Test Loss: 1.0507\n",
      "Epoch 3314, Train Loss: 1.3747, Test Loss: 1.0521\n",
      "Epoch 3315, Train Loss: 1.6265, Test Loss: 1.0493\n",
      "Epoch 3316, Train Loss: 1.5617, Test Loss: 1.0458\n",
      "Epoch 3317, Train Loss: 1.6464, Test Loss: 1.0378\n",
      "Epoch 3318, Train Loss: 1.7383, Test Loss: 1.0314\n",
      "Epoch 3319, Train Loss: 1.4365, Test Loss: 1.0332\n",
      "Epoch 3320, Train Loss: 1.3245, Test Loss: 1.0349\n",
      "Epoch 3321, Train Loss: 1.5693, Test Loss: 1.0361\n",
      "Epoch 3322, Train Loss: 1.3371, Test Loss: 1.0360\n",
      "Epoch 3323, Train Loss: 1.5868, Test Loss: 1.0357\n",
      "Epoch 3324, Train Loss: 1.5217, Test Loss: 1.0353\n",
      "Epoch 3325, Train Loss: 1.5109, Test Loss: 1.0373\n",
      "Epoch 3326, Train Loss: 1.7515, Test Loss: 1.0443\n",
      "Epoch 3327, Train Loss: 1.9400, Test Loss: 1.0472\n",
      "Epoch 3328, Train Loss: 1.4045, Test Loss: 1.0467\n",
      "Epoch 3329, Train Loss: 1.3926, Test Loss: 1.0481\n",
      "Epoch 3330, Train Loss: 1.2614, Test Loss: 1.0498\n",
      "Epoch 3331, Train Loss: 1.4347, Test Loss: 1.0501\n",
      "Epoch 3332, Train Loss: 1.2350, Test Loss: 1.0488\n",
      "Epoch 3333, Train Loss: 1.4703, Test Loss: 1.0484\n",
      "Epoch 3334, Train Loss: 1.3724, Test Loss: 1.0439\n",
      "Epoch 3335, Train Loss: 1.2742, Test Loss: 1.0405\n",
      "Epoch 3336, Train Loss: 1.5527, Test Loss: 1.0378\n",
      "Epoch 3337, Train Loss: 1.5192, Test Loss: 1.0385\n",
      "Epoch 3338, Train Loss: 1.5748, Test Loss: 1.0302\n",
      "Epoch 3339, Train Loss: 1.3723, Test Loss: 1.0243\n",
      "Epoch 3340, Train Loss: 1.4602, Test Loss: 1.0156\n",
      "Epoch 3341, Train Loss: 1.4296, Test Loss: 1.0078\n",
      "Epoch 3342, Train Loss: 1.7532, Test Loss: 1.0065\n",
      "Epoch 3343, Train Loss: 1.3515, Test Loss: 1.0074\n",
      "Epoch 3344, Train Loss: 1.5453, Test Loss: 1.0056\n",
      "Epoch 3345, Train Loss: 2.1696, Test Loss: 1.0066\n",
      "Epoch 3346, Train Loss: 1.5020, Test Loss: 1.0160\n",
      "Epoch 3347, Train Loss: 1.5329, Test Loss: 1.0301\n",
      "Epoch 3348, Train Loss: 1.5440, Test Loss: 1.0458\n",
      "Epoch 3349, Train Loss: 1.4454, Test Loss: 1.0622\n",
      "Epoch 3350, Train Loss: 1.6935, Test Loss: 1.0742\n",
      "Epoch 3351, Train Loss: 1.6066, Test Loss: 1.0801\n",
      "Epoch 3352, Train Loss: 1.8948, Test Loss: 1.0888\n",
      "Epoch 3353, Train Loss: 1.4352, Test Loss: 1.0952\n",
      "Epoch 3354, Train Loss: 1.5337, Test Loss: 1.0900\n",
      "Epoch 3355, Train Loss: 1.7907, Test Loss: 1.0821\n",
      "Epoch 3356, Train Loss: 1.4874, Test Loss: 1.0717\n",
      "Epoch 3357, Train Loss: 1.3159, Test Loss: 1.0620\n",
      "Epoch 3358, Train Loss: 1.4724, Test Loss: 1.0504\n",
      "Epoch 3359, Train Loss: 1.4798, Test Loss: 1.0399\n",
      "Epoch 3360, Train Loss: 1.4761, Test Loss: 1.0360\n",
      "Epoch 3361, Train Loss: 1.5580, Test Loss: 1.0291\n",
      "Epoch 3362, Train Loss: 1.3615, Test Loss: 1.0219\n",
      "Epoch 3363, Train Loss: 1.5862, Test Loss: 1.0108\n",
      "Epoch 3364, Train Loss: 1.6064, Test Loss: 1.0025\n",
      "Epoch 3365, Train Loss: 1.2781, Test Loss: 0.9912\n",
      "Epoch 3366, Train Loss: 1.3457, Test Loss: 0.9808\n",
      "Epoch 3367, Train Loss: 1.5053, Test Loss: 0.9732\n",
      "Epoch 3368, Train Loss: 1.6441, Test Loss: 0.9683\n",
      "Epoch 3369, Train Loss: 1.5530, Test Loss: 0.9648\n",
      "Epoch 3370, Train Loss: 1.4018, Test Loss: 0.9652\n",
      "Epoch 3371, Train Loss: 1.6823, Test Loss: 0.9708\n",
      "Epoch 3372, Train Loss: 1.5596, Test Loss: 0.9746\n",
      "Epoch 3373, Train Loss: 1.1843, Test Loss: 0.9761\n",
      "Epoch 3374, Train Loss: 1.5526, Test Loss: 0.9844\n",
      "Epoch 3375, Train Loss: 1.6418, Test Loss: 0.9902\n",
      "Epoch 3376, Train Loss: 1.2932, Test Loss: 0.9944\n",
      "Epoch 3377, Train Loss: 1.6183, Test Loss: 1.0055\n",
      "Epoch 3378, Train Loss: 1.2067, Test Loss: 1.0145\n",
      "Epoch 3379, Train Loss: 1.6010, Test Loss: 1.0256\n",
      "Epoch 3380, Train Loss: 1.3426, Test Loss: 1.0303\n",
      "Epoch 3381, Train Loss: 1.4650, Test Loss: 1.0312\n",
      "Epoch 3382, Train Loss: 1.5224, Test Loss: 1.0302\n",
      "Epoch 3383, Train Loss: 1.3053, Test Loss: 1.0239\n",
      "Epoch 3384, Train Loss: 1.8473, Test Loss: 1.0138\n",
      "Epoch 3385, Train Loss: 1.5108, Test Loss: 1.0028\n",
      "Epoch 3386, Train Loss: 1.2520, Test Loss: 0.9892\n",
      "Epoch 3387, Train Loss: 1.4918, Test Loss: 0.9812\n",
      "Epoch 3388, Train Loss: 1.4994, Test Loss: 0.9731\n",
      "Epoch 3389, Train Loss: 1.2817, Test Loss: 0.9704\n",
      "Epoch 3390, Train Loss: 1.2957, Test Loss: 0.9664\n",
      "Epoch 3391, Train Loss: 1.3971, Test Loss: 0.9612\n",
      "Epoch 3392, Train Loss: 1.3619, Test Loss: 0.9600\n",
      "Epoch 3393, Train Loss: 1.3766, Test Loss: 0.9592\n",
      "Epoch 3394, Train Loss: 1.4517, Test Loss: 0.9571\n",
      "Epoch 3395, Train Loss: 1.2965, Test Loss: 0.9492\n",
      "Epoch 3396, Train Loss: 1.3460, Test Loss: 0.9428\n",
      "Epoch 3397, Train Loss: 1.4600, Test Loss: 0.9412\n",
      "Epoch 3398, Train Loss: 1.5123, Test Loss: 0.9441\n",
      "Epoch 3399, Train Loss: 1.2117, Test Loss: 0.9491\n",
      "Epoch 3400, Train Loss: 1.3707, Test Loss: 0.9518\n",
      "Epoch 3401, Train Loss: 1.1102, Test Loss: 0.9561\n",
      "Epoch 3402, Train Loss: 1.3901, Test Loss: 0.9622\n",
      "Epoch 3403, Train Loss: 1.3592, Test Loss: 0.9614\n",
      "Epoch 3404, Train Loss: 1.3149, Test Loss: 0.9627\n",
      "Epoch 3405, Train Loss: 1.2741, Test Loss: 0.9681\n",
      "Epoch 3406, Train Loss: 1.4133, Test Loss: 0.9745\n",
      "Epoch 3407, Train Loss: 1.4015, Test Loss: 0.9750\n",
      "Epoch 3408, Train Loss: 1.3500, Test Loss: 0.9746\n",
      "Epoch 3409, Train Loss: 1.3443, Test Loss: 0.9737\n",
      "Epoch 3410, Train Loss: 1.3502, Test Loss: 0.9751\n",
      "Epoch 3411, Train Loss: 1.3663, Test Loss: 0.9746\n",
      "Epoch 3412, Train Loss: 1.3501, Test Loss: 0.9725\n",
      "Epoch 3413, Train Loss: 1.4568, Test Loss: 0.9750\n",
      "Epoch 3414, Train Loss: 1.3715, Test Loss: 0.9799\n",
      "Epoch 3415, Train Loss: 1.4609, Test Loss: 0.9805\n",
      "Epoch 3416, Train Loss: 1.5121, Test Loss: 0.9826\n",
      "Epoch 3417, Train Loss: 1.3433, Test Loss: 0.9857\n",
      "Epoch 3418, Train Loss: 1.3460, Test Loss: 0.9897\n",
      "Epoch 3419, Train Loss: 1.4729, Test Loss: 0.9893\n",
      "Epoch 3420, Train Loss: 1.3874, Test Loss: 0.9832\n",
      "Epoch 3421, Train Loss: 1.4096, Test Loss: 0.9805\n",
      "Epoch 3422, Train Loss: 1.5106, Test Loss: 0.9811\n",
      "Epoch 3423, Train Loss: 1.3280, Test Loss: 0.9780\n",
      "Epoch 3424, Train Loss: 1.3106, Test Loss: 0.9712\n",
      "Epoch 3425, Train Loss: 1.4586, Test Loss: 0.9650\n",
      "Epoch 3426, Train Loss: 1.1949, Test Loss: 0.9627\n",
      "Epoch 3427, Train Loss: 1.0655, Test Loss: 0.9587\n",
      "Epoch 3428, Train Loss: 1.4252, Test Loss: 0.9571\n",
      "Epoch 3429, Train Loss: 1.3763, Test Loss: 0.9570\n",
      "Epoch 3430, Train Loss: 1.4326, Test Loss: 0.9567\n",
      "Epoch 3431, Train Loss: 1.3029, Test Loss: 0.9557\n",
      "Epoch 3432, Train Loss: 1.5822, Test Loss: 0.9554\n",
      "Epoch 3433, Train Loss: 1.2361, Test Loss: 0.9548\n",
      "Epoch 3434, Train Loss: 1.5224, Test Loss: 0.9541\n",
      "Epoch 3435, Train Loss: 1.2773, Test Loss: 0.9475\n",
      "Epoch 3436, Train Loss: 1.2598, Test Loss: 0.9400\n",
      "Epoch 3437, Train Loss: 1.4148, Test Loss: 0.9306\n",
      "Epoch 3438, Train Loss: 1.5646, Test Loss: 0.9254\n",
      "Epoch 3439, Train Loss: 1.2899, Test Loss: 0.9255\n",
      "Epoch 3440, Train Loss: 1.3300, Test Loss: 0.9274\n",
      "Epoch 3441, Train Loss: 1.4903, Test Loss: 0.9355\n",
      "Epoch 3442, Train Loss: 1.3803, Test Loss: 0.9412\n",
      "Epoch 3443, Train Loss: 1.3549, Test Loss: 0.9429\n",
      "Epoch 3444, Train Loss: 1.2730, Test Loss: 0.9463\n",
      "Epoch 3445, Train Loss: 1.1435, Test Loss: 0.9468\n",
      "Epoch 3446, Train Loss: 1.3625, Test Loss: 0.9414\n",
      "Epoch 3447, Train Loss: 1.3456, Test Loss: 0.9339\n",
      "Epoch 3448, Train Loss: 1.4875, Test Loss: 0.9271\n",
      "Epoch 3449, Train Loss: 1.4931, Test Loss: 0.9223\n",
      "Epoch 3450, Train Loss: 1.3691, Test Loss: 0.9205\n",
      "Epoch 3451, Train Loss: 1.3604, Test Loss: 0.9198\n",
      "Epoch 3452, Train Loss: 1.2608, Test Loss: 0.9169\n",
      "Epoch 3453, Train Loss: 1.2445, Test Loss: 0.9135\n",
      "Epoch 3454, Train Loss: 1.2860, Test Loss: 0.9132\n",
      "Epoch 3455, Train Loss: 1.3787, Test Loss: 0.9169\n",
      "Epoch 3456, Train Loss: 1.5390, Test Loss: 0.9224\n",
      "Epoch 3457, Train Loss: 1.1927, Test Loss: 0.9223\n",
      "Epoch 3458, Train Loss: 1.2842, Test Loss: 0.9220\n",
      "Epoch 3459, Train Loss: 1.5570, Test Loss: 0.9163\n",
      "Epoch 3460, Train Loss: 1.2139, Test Loss: 0.9079\n",
      "Epoch 3461, Train Loss: 1.1925, Test Loss: 0.9007\n",
      "Epoch 3462, Train Loss: 1.4002, Test Loss: 0.8999\n",
      "Epoch 3463, Train Loss: 1.1268, Test Loss: 0.9015\n",
      "Epoch 3464, Train Loss: 1.2746, Test Loss: 0.9083\n",
      "Epoch 3465, Train Loss: 1.5389, Test Loss: 0.9115\n",
      "Epoch 3466, Train Loss: 1.2721, Test Loss: 0.9149\n",
      "Epoch 3467, Train Loss: 1.3219, Test Loss: 0.9156\n",
      "Epoch 3468, Train Loss: 1.1168, Test Loss: 0.9160\n",
      "Epoch 3469, Train Loss: 1.3070, Test Loss: 0.9163\n",
      "Epoch 3470, Train Loss: 1.1521, Test Loss: 0.9175\n",
      "Epoch 3471, Train Loss: 1.2711, Test Loss: 0.9183\n",
      "Epoch 3472, Train Loss: 1.1177, Test Loss: 0.9237\n",
      "Epoch 3473, Train Loss: 1.2781, Test Loss: 0.9302\n",
      "Epoch 3474, Train Loss: 1.2518, Test Loss: 0.9308\n",
      "Epoch 3475, Train Loss: 1.4582, Test Loss: 0.9277\n",
      "Epoch 3476, Train Loss: 1.2180, Test Loss: 0.9227\n",
      "Epoch 3477, Train Loss: 1.2596, Test Loss: 0.9193\n",
      "Epoch 3478, Train Loss: 1.2103, Test Loss: 0.9175\n",
      "Epoch 3479, Train Loss: 1.1731, Test Loss: 0.9166\n",
      "Epoch 3480, Train Loss: 1.2463, Test Loss: 0.9160\n",
      "Epoch 3481, Train Loss: 1.3718, Test Loss: 0.9142\n",
      "Epoch 3482, Train Loss: 1.2784, Test Loss: 0.9148\n",
      "Epoch 3483, Train Loss: 1.1964, Test Loss: 0.9180\n",
      "Epoch 3484, Train Loss: 1.1028, Test Loss: 0.9203\n",
      "Epoch 3485, Train Loss: 1.3372, Test Loss: 0.9228\n",
      "Epoch 3486, Train Loss: 1.2167, Test Loss: 0.9237\n",
      "Epoch 3487, Train Loss: 1.2523, Test Loss: 0.9254\n",
      "Epoch 3488, Train Loss: 1.1908, Test Loss: 0.9285\n",
      "Epoch 3489, Train Loss: 1.3430, Test Loss: 0.9278\n",
      "Epoch 3490, Train Loss: 1.2585, Test Loss: 0.9282\n",
      "Epoch 3491, Train Loss: 1.3191, Test Loss: 0.9253\n",
      "Epoch 3492, Train Loss: 1.4100, Test Loss: 0.9194\n",
      "Epoch 3493, Train Loss: 1.2620, Test Loss: 0.9167\n",
      "Epoch 3494, Train Loss: 1.3984, Test Loss: 0.9106\n",
      "Epoch 3495, Train Loss: 1.2151, Test Loss: 0.9041\n",
      "Epoch 3496, Train Loss: 1.1929, Test Loss: 0.8959\n",
      "Epoch 3497, Train Loss: 1.3380, Test Loss: 0.8895\n",
      "Epoch 3498, Train Loss: 1.1561, Test Loss: 0.8853\n",
      "Epoch 3499, Train Loss: 1.2421, Test Loss: 0.8842\n",
      "Epoch 3500, Train Loss: 1.3174, Test Loss: 0.8873\n",
      "Epoch 3501, Train Loss: 1.0892, Test Loss: 0.8903\n",
      "Epoch 3502, Train Loss: 1.1975, Test Loss: 0.8954\n",
      "Epoch 3503, Train Loss: 1.0297, Test Loss: 0.8998\n",
      "Epoch 3504, Train Loss: 1.3084, Test Loss: 0.9069\n",
      "Epoch 3505, Train Loss: 1.3998, Test Loss: 0.9128\n",
      "Epoch 3506, Train Loss: 1.1740, Test Loss: 0.9179\n",
      "Epoch 3507, Train Loss: 1.4102, Test Loss: 0.9258\n",
      "Epoch 3508, Train Loss: 1.1901, Test Loss: 0.9296\n",
      "Epoch 3509, Train Loss: 1.3042, Test Loss: 0.9310\n",
      "Epoch 3510, Train Loss: 1.0749, Test Loss: 0.9336\n",
      "Epoch 3511, Train Loss: 1.1542, Test Loss: 0.9330\n",
      "Epoch 3512, Train Loss: 1.2765, Test Loss: 0.9320\n",
      "Epoch 3513, Train Loss: 1.2767, Test Loss: 0.9287\n",
      "Epoch 3514, Train Loss: 1.1252, Test Loss: 0.9217\n",
      "Epoch 3515, Train Loss: 1.1053, Test Loss: 0.9093\n",
      "Epoch 3516, Train Loss: 1.2318, Test Loss: 0.8962\n",
      "Epoch 3517, Train Loss: 1.2911, Test Loss: 0.8830\n",
      "Epoch 3518, Train Loss: 1.3286, Test Loss: 0.8726\n",
      "Epoch 3519, Train Loss: 1.1909, Test Loss: 0.8602\n",
      "Epoch 3520, Train Loss: 1.0335, Test Loss: 0.8520\n",
      "Epoch 3521, Train Loss: 1.2310, Test Loss: 0.8416\n",
      "Epoch 3522, Train Loss: 1.1459, Test Loss: 0.8339\n",
      "Epoch 3523, Train Loss: 1.2986, Test Loss: 0.8285\n",
      "Epoch 3524, Train Loss: 1.1453, Test Loss: 0.8277\n",
      "Epoch 3525, Train Loss: 1.2639, Test Loss: 0.8293\n",
      "Epoch 3526, Train Loss: 1.2412, Test Loss: 0.8297\n",
      "Epoch 3527, Train Loss: 1.0791, Test Loss: 0.8312\n",
      "Epoch 3528, Train Loss: 1.2664, Test Loss: 0.8337\n",
      "Epoch 3529, Train Loss: 1.3538, Test Loss: 0.8389\n",
      "Epoch 3530, Train Loss: 1.4036, Test Loss: 0.8450\n",
      "Epoch 3531, Train Loss: 1.1347, Test Loss: 0.8507\n",
      "Epoch 3532, Train Loss: 1.4034, Test Loss: 0.8535\n",
      "Epoch 3533, Train Loss: 1.3276, Test Loss: 0.8541\n",
      "Epoch 3534, Train Loss: 1.2144, Test Loss: 0.8531\n",
      "Epoch 3535, Train Loss: 1.3300, Test Loss: 0.8529\n",
      "Epoch 3536, Train Loss: 1.2739, Test Loss: 0.8526\n",
      "Epoch 3537, Train Loss: 1.1745, Test Loss: 0.8534\n",
      "Epoch 3538, Train Loss: 1.4104, Test Loss: 0.8504\n",
      "Epoch 3539, Train Loss: 1.1337, Test Loss: 0.8450\n",
      "Epoch 3540, Train Loss: 1.1389, Test Loss: 0.8409\n",
      "Epoch 3541, Train Loss: 1.1519, Test Loss: 0.8371\n",
      "Epoch 3542, Train Loss: 1.0842, Test Loss: 0.8353\n",
      "Epoch 3543, Train Loss: 1.2712, Test Loss: 0.8377\n",
      "Epoch 3544, Train Loss: 1.3236, Test Loss: 0.8371\n",
      "Epoch 3545, Train Loss: 1.2702, Test Loss: 0.8363\n",
      "Epoch 3546, Train Loss: 1.1035, Test Loss: 0.8354\n",
      "Epoch 3547, Train Loss: 1.1064, Test Loss: 0.8338\n",
      "Epoch 3548, Train Loss: 1.1678, Test Loss: 0.8307\n",
      "Epoch 3549, Train Loss: 1.1161, Test Loss: 0.8267\n",
      "Epoch 3550, Train Loss: 1.3154, Test Loss: 0.8229\n",
      "Epoch 3551, Train Loss: 1.0413, Test Loss: 0.8209\n",
      "Epoch 3552, Train Loss: 0.9187, Test Loss: 0.8215\n",
      "Epoch 3553, Train Loss: 1.3729, Test Loss: 0.8239\n",
      "Epoch 3554, Train Loss: 0.9970, Test Loss: 0.8253\n",
      "Epoch 3555, Train Loss: 1.2497, Test Loss: 0.8287\n",
      "Epoch 3556, Train Loss: 1.1568, Test Loss: 0.8317\n",
      "Epoch 3557, Train Loss: 1.0711, Test Loss: 0.8369\n",
      "Epoch 3558, Train Loss: 1.0217, Test Loss: 0.8404\n",
      "Epoch 3559, Train Loss: 0.8451, Test Loss: 0.8424\n",
      "Epoch 3560, Train Loss: 1.1907, Test Loss: 0.8455\n",
      "Epoch 3561, Train Loss: 1.2857, Test Loss: 0.8511\n",
      "Epoch 3562, Train Loss: 1.0846, Test Loss: 0.8547\n",
      "Epoch 3563, Train Loss: 1.3281, Test Loss: 0.8569\n",
      "Epoch 3564, Train Loss: 1.0477, Test Loss: 0.8581\n",
      "Epoch 3565, Train Loss: 1.0133, Test Loss: 0.8572\n",
      "Epoch 3566, Train Loss: 1.0786, Test Loss: 0.8563\n",
      "Epoch 3567, Train Loss: 1.2747, Test Loss: 0.8561\n",
      "Epoch 3568, Train Loss: 1.0715, Test Loss: 0.8532\n",
      "Epoch 3569, Train Loss: 0.8923, Test Loss: 0.8506\n",
      "Epoch 3570, Train Loss: 1.0206, Test Loss: 0.8488\n",
      "Epoch 3571, Train Loss: 1.1453, Test Loss: 0.8466\n",
      "Epoch 3572, Train Loss: 1.2727, Test Loss: 0.8470\n",
      "Epoch 3573, Train Loss: 1.2228, Test Loss: 0.8455\n",
      "Epoch 3574, Train Loss: 1.1728, Test Loss: 0.8440\n",
      "Epoch 3575, Train Loss: 1.1417, Test Loss: 0.8429\n",
      "Epoch 3576, Train Loss: 1.3161, Test Loss: 0.8399\n",
      "Epoch 3577, Train Loss: 0.9778, Test Loss: 0.8371\n",
      "Epoch 3578, Train Loss: 0.9735, Test Loss: 0.8327\n",
      "Epoch 3579, Train Loss: 1.0038, Test Loss: 0.8306\n",
      "Epoch 3580, Train Loss: 1.1642, Test Loss: 0.8278\n",
      "Epoch 3581, Train Loss: 1.1899, Test Loss: 0.8254\n",
      "Epoch 3582, Train Loss: 1.2055, Test Loss: 0.8238\n",
      "Epoch 3583, Train Loss: 1.1871, Test Loss: 0.8238\n",
      "Epoch 3584, Train Loss: 0.9607, Test Loss: 0.8273\n",
      "Epoch 3585, Train Loss: 1.0315, Test Loss: 0.8308\n",
      "Epoch 3586, Train Loss: 1.2099, Test Loss: 0.8357\n",
      "Epoch 3587, Train Loss: 1.0439, Test Loss: 0.8399\n",
      "Epoch 3588, Train Loss: 0.9176, Test Loss: 0.8430\n",
      "Epoch 3589, Train Loss: 1.0616, Test Loss: 0.8491\n",
      "Epoch 3590, Train Loss: 1.1104, Test Loss: 0.8554\n",
      "Epoch 3591, Train Loss: 1.0727, Test Loss: 0.8626\n",
      "Epoch 3592, Train Loss: 1.0511, Test Loss: 0.8680\n",
      "Epoch 3593, Train Loss: 1.1759, Test Loss: 0.8728\n",
      "Epoch 3594, Train Loss: 1.2351, Test Loss: 0.8726\n",
      "Epoch 3595, Train Loss: 0.9917, Test Loss: 0.8694\n",
      "Epoch 3596, Train Loss: 0.9515, Test Loss: 0.8646\n",
      "Epoch 3597, Train Loss: 1.2113, Test Loss: 0.8569\n",
      "Epoch 3598, Train Loss: 1.2642, Test Loss: 0.8522\n",
      "Epoch 3599, Train Loss: 1.0603, Test Loss: 0.8440\n",
      "Epoch 3600, Train Loss: 1.2199, Test Loss: 0.8311\n",
      "Epoch 3601, Train Loss: 1.2294, Test Loss: 0.8200\n",
      "Epoch 3602, Train Loss: 0.9728, Test Loss: 0.8057\n",
      "Epoch 3603, Train Loss: 1.0270, Test Loss: 0.7939\n",
      "Epoch 3604, Train Loss: 1.1682, Test Loss: 0.7850\n",
      "Epoch 3605, Train Loss: 1.2673, Test Loss: 0.7806\n",
      "Epoch 3606, Train Loss: 1.0870, Test Loss: 0.7775\n",
      "Epoch 3607, Train Loss: 0.9115, Test Loss: 0.7760\n",
      "Epoch 3608, Train Loss: 1.1676, Test Loss: 0.7775\n",
      "Epoch 3609, Train Loss: 0.9069, Test Loss: 0.7771\n",
      "Epoch 3610, Train Loss: 0.9668, Test Loss: 0.7756\n",
      "Epoch 3611, Train Loss: 1.0524, Test Loss: 0.7755\n",
      "Epoch 3612, Train Loss: 1.0645, Test Loss: 0.7760\n",
      "Epoch 3613, Train Loss: 1.1397, Test Loss: 0.7796\n",
      "Epoch 3614, Train Loss: 0.9821, Test Loss: 0.7821\n",
      "Epoch 3615, Train Loss: 1.1066, Test Loss: 0.7858\n",
      "Epoch 3616, Train Loss: 0.9906, Test Loss: 0.7888\n",
      "Epoch 3617, Train Loss: 1.0424, Test Loss: 0.7922\n",
      "Epoch 3618, Train Loss: 1.1452, Test Loss: 0.7954\n",
      "Epoch 3619, Train Loss: 0.9470, Test Loss: 0.7996\n",
      "Epoch 3620, Train Loss: 1.0587, Test Loss: 0.8065\n",
      "Epoch 3621, Train Loss: 1.3169, Test Loss: 0.8110\n",
      "Epoch 3622, Train Loss: 1.1315, Test Loss: 0.8124\n",
      "Epoch 3623, Train Loss: 0.8622, Test Loss: 0.8119\n",
      "Epoch 3624, Train Loss: 0.9534, Test Loss: 0.8064\n",
      "Epoch 3625, Train Loss: 1.0758, Test Loss: 0.8016\n",
      "Epoch 3626, Train Loss: 1.0510, Test Loss: 0.7962\n",
      "Epoch 3627, Train Loss: 0.9708, Test Loss: 0.7916\n",
      "Epoch 3628, Train Loss: 1.0763, Test Loss: 0.7903\n",
      "Epoch 3629, Train Loss: 1.0478, Test Loss: 0.7894\n",
      "Epoch 3630, Train Loss: 0.9645, Test Loss: 0.7910\n",
      "Epoch 3631, Train Loss: 1.2519, Test Loss: 0.7926\n",
      "Epoch 3632, Train Loss: 1.1752, Test Loss: 0.7932\n",
      "Epoch 3633, Train Loss: 1.0210, Test Loss: 0.7910\n",
      "Epoch 3634, Train Loss: 1.1680, Test Loss: 0.7887\n",
      "Epoch 3635, Train Loss: 1.1203, Test Loss: 0.7855\n",
      "Epoch 3636, Train Loss: 1.1158, Test Loss: 0.7825\n",
      "Epoch 3637, Train Loss: 0.9063, Test Loss: 0.7822\n",
      "Epoch 3638, Train Loss: 1.1405, Test Loss: 0.7827\n",
      "Epoch 3639, Train Loss: 0.9708, Test Loss: 0.7871\n",
      "Epoch 3640, Train Loss: 1.0454, Test Loss: 0.7936\n",
      "Epoch 3641, Train Loss: 1.0304, Test Loss: 0.7984\n",
      "Epoch 3642, Train Loss: 1.1729, Test Loss: 0.7959\n",
      "Epoch 3643, Train Loss: 1.1790, Test Loss: 0.7919\n",
      "Epoch 3644, Train Loss: 0.8473, Test Loss: 0.7881\n",
      "Epoch 3645, Train Loss: 0.9122, Test Loss: 0.7860\n",
      "Epoch 3646, Train Loss: 0.9429, Test Loss: 0.7842\n",
      "Epoch 3647, Train Loss: 1.0023, Test Loss: 0.7855\n",
      "Epoch 3648, Train Loss: 0.9714, Test Loss: 0.7885\n",
      "Epoch 3649, Train Loss: 0.9181, Test Loss: 0.7905\n",
      "Epoch 3650, Train Loss: 1.1257, Test Loss: 0.7958\n",
      "Epoch 3651, Train Loss: 0.9938, Test Loss: 0.7966\n",
      "Epoch 3652, Train Loss: 0.8833, Test Loss: 0.7957\n",
      "Epoch 3653, Train Loss: 0.9994, Test Loss: 0.7930\n",
      "Epoch 3654, Train Loss: 1.0741, Test Loss: 0.7871\n",
      "Epoch 3655, Train Loss: 1.0021, Test Loss: 0.7794\n",
      "Epoch 3656, Train Loss: 1.0251, Test Loss: 0.7750\n",
      "Epoch 3657, Train Loss: 1.1576, Test Loss: 0.7721\n",
      "Epoch 3658, Train Loss: 1.0123, Test Loss: 0.7712\n",
      "Epoch 3659, Train Loss: 1.0766, Test Loss: 0.7732\n",
      "Epoch 3660, Train Loss: 1.0294, Test Loss: 0.7746\n",
      "Epoch 3661, Train Loss: 0.9530, Test Loss: 0.7751\n",
      "Epoch 3662, Train Loss: 1.1721, Test Loss: 0.7773\n",
      "Epoch 3663, Train Loss: 1.0880, Test Loss: 0.7791\n",
      "Epoch 3664, Train Loss: 0.8716, Test Loss: 0.7794\n",
      "Epoch 3665, Train Loss: 1.0716, Test Loss: 0.7772\n",
      "Epoch 3666, Train Loss: 0.9958, Test Loss: 0.7769\n",
      "Epoch 3667, Train Loss: 0.9875, Test Loss: 0.7790\n",
      "Epoch 3668, Train Loss: 0.9518, Test Loss: 0.7807\n",
      "Epoch 3669, Train Loss: 1.1070, Test Loss: 0.7842\n",
      "Epoch 3670, Train Loss: 1.0060, Test Loss: 0.7857\n",
      "Epoch 3671, Train Loss: 1.0339, Test Loss: 0.7882\n",
      "Epoch 3672, Train Loss: 0.9609, Test Loss: 0.7909\n",
      "Epoch 3673, Train Loss: 1.1710, Test Loss: 0.7948\n",
      "Epoch 3674, Train Loss: 0.9873, Test Loss: 0.7967\n",
      "Epoch 3675, Train Loss: 1.0209, Test Loss: 0.7988\n",
      "Epoch 3676, Train Loss: 1.1561, Test Loss: 0.7958\n",
      "Epoch 3677, Train Loss: 0.9376, Test Loss: 0.7949\n",
      "Epoch 3678, Train Loss: 0.9238, Test Loss: 0.7938\n",
      "Epoch 3679, Train Loss: 1.0701, Test Loss: 0.7952\n",
      "Epoch 3680, Train Loss: 1.0060, Test Loss: 0.7981\n",
      "Epoch 3681, Train Loss: 1.0613, Test Loss: 0.7980\n",
      "Epoch 3682, Train Loss: 1.3454, Test Loss: 0.8000\n",
      "Epoch 3683, Train Loss: 0.9603, Test Loss: 0.8007\n",
      "Epoch 3684, Train Loss: 1.0234, Test Loss: 0.7990\n",
      "Epoch 3685, Train Loss: 0.9151, Test Loss: 0.7944\n",
      "Epoch 3686, Train Loss: 0.9878, Test Loss: 0.7881\n",
      "Epoch 3687, Train Loss: 0.9425, Test Loss: 0.7828\n",
      "Epoch 3688, Train Loss: 0.9188, Test Loss: 0.7764\n",
      "Epoch 3689, Train Loss: 1.0356, Test Loss: 0.7712\n",
      "Epoch 3690, Train Loss: 0.9724, Test Loss: 0.7667\n",
      "Epoch 3691, Train Loss: 0.9367, Test Loss: 0.7643\n",
      "Epoch 3692, Train Loss: 0.9883, Test Loss: 0.7622\n",
      "Epoch 3693, Train Loss: 0.9740, Test Loss: 0.7578\n",
      "Epoch 3694, Train Loss: 0.8516, Test Loss: 0.7567\n",
      "Epoch 3695, Train Loss: 0.8360, Test Loss: 0.7550\n",
      "Epoch 3696, Train Loss: 0.9125, Test Loss: 0.7530\n",
      "Epoch 3697, Train Loss: 0.9002, Test Loss: 0.7517\n",
      "Epoch 3698, Train Loss: 1.0008, Test Loss: 0.7479\n",
      "Epoch 3699, Train Loss: 0.9849, Test Loss: 0.7454\n",
      "Epoch 3700, Train Loss: 0.9399, Test Loss: 0.7421\n",
      "Epoch 3701, Train Loss: 1.0465, Test Loss: 0.7386\n",
      "Epoch 3702, Train Loss: 1.1436, Test Loss: 0.7362\n",
      "Epoch 3703, Train Loss: 0.8937, Test Loss: 0.7341\n",
      "Epoch 3704, Train Loss: 0.9985, Test Loss: 0.7353\n",
      "Epoch 3705, Train Loss: 1.0104, Test Loss: 0.7363\n",
      "Epoch 3706, Train Loss: 1.1517, Test Loss: 0.7382\n",
      "Epoch 3707, Train Loss: 1.0169, Test Loss: 0.7420\n",
      "Epoch 3708, Train Loss: 1.1005, Test Loss: 0.7464\n",
      "Epoch 3709, Train Loss: 0.9555, Test Loss: 0.7474\n",
      "Epoch 3710, Train Loss: 0.8989, Test Loss: 0.7491\n",
      "Epoch 3711, Train Loss: 0.9836, Test Loss: 0.7521\n",
      "Epoch 3712, Train Loss: 0.9785, Test Loss: 0.7538\n",
      "Epoch 3713, Train Loss: 1.0027, Test Loss: 0.7561\n",
      "Epoch 3714, Train Loss: 1.0343, Test Loss: 0.7563\n",
      "Epoch 3715, Train Loss: 0.8981, Test Loss: 0.7540\n",
      "Epoch 3716, Train Loss: 0.9992, Test Loss: 0.7520\n",
      "Epoch 3717, Train Loss: 1.0642, Test Loss: 0.7500\n",
      "Epoch 3718, Train Loss: 0.9312, Test Loss: 0.7478\n",
      "Epoch 3719, Train Loss: 0.8791, Test Loss: 0.7471\n",
      "Epoch 3720, Train Loss: 0.8856, Test Loss: 0.7464\n",
      "Epoch 3721, Train Loss: 0.9009, Test Loss: 0.7454\n",
      "Epoch 3722, Train Loss: 0.9522, Test Loss: 0.7456\n",
      "Epoch 3723, Train Loss: 0.9235, Test Loss: 0.7436\n",
      "Epoch 3724, Train Loss: 0.9589, Test Loss: 0.7427\n",
      "Epoch 3725, Train Loss: 0.9185, Test Loss: 0.7388\n",
      "Epoch 3726, Train Loss: 1.0397, Test Loss: 0.7360\n",
      "Epoch 3727, Train Loss: 0.9703, Test Loss: 0.7319\n",
      "Epoch 3728, Train Loss: 0.9905, Test Loss: 0.7281\n",
      "Epoch 3729, Train Loss: 1.0798, Test Loss: 0.7264\n",
      "Epoch 3730, Train Loss: 0.8070, Test Loss: 0.7259\n",
      "Epoch 3731, Train Loss: 1.0139, Test Loss: 0.7270\n",
      "Epoch 3732, Train Loss: 0.9271, Test Loss: 0.7282\n",
      "Epoch 3733, Train Loss: 0.9623, Test Loss: 0.7297\n",
      "Epoch 3734, Train Loss: 0.9028, Test Loss: 0.7336\n",
      "Epoch 3735, Train Loss: 0.9698, Test Loss: 0.7369\n",
      "Epoch 3736, Train Loss: 0.7946, Test Loss: 0.7407\n",
      "Epoch 3737, Train Loss: 0.9770, Test Loss: 0.7469\n",
      "Epoch 3738, Train Loss: 1.0202, Test Loss: 0.7522\n",
      "Epoch 3739, Train Loss: 0.8163, Test Loss: 0.7561\n",
      "Epoch 3740, Train Loss: 0.9232, Test Loss: 0.7583\n",
      "Epoch 3741, Train Loss: 0.8294, Test Loss: 0.7628\n",
      "Epoch 3742, Train Loss: 0.8346, Test Loss: 0.7648\n",
      "Epoch 3743, Train Loss: 0.8726, Test Loss: 0.7634\n",
      "Epoch 3744, Train Loss: 0.9507, Test Loss: 0.7583\n",
      "Epoch 3745, Train Loss: 0.8876, Test Loss: 0.7526\n",
      "Epoch 3746, Train Loss: 0.7437, Test Loss: 0.7484\n",
      "Epoch 3747, Train Loss: 0.7980, Test Loss: 0.7449\n",
      "Epoch 3748, Train Loss: 1.0272, Test Loss: 0.7414\n",
      "Epoch 3749, Train Loss: 0.9608, Test Loss: 0.7374\n",
      "Epoch 3750, Train Loss: 0.8765, Test Loss: 0.7327\n",
      "Epoch 3751, Train Loss: 0.9838, Test Loss: 0.7292\n",
      "Epoch 3752, Train Loss: 1.0212, Test Loss: 0.7268\n",
      "Epoch 3753, Train Loss: 1.0879, Test Loss: 0.7225\n",
      "Epoch 3754, Train Loss: 0.9856, Test Loss: 0.7187\n",
      "Epoch 3755, Train Loss: 0.9839, Test Loss: 0.7155\n",
      "Epoch 3756, Train Loss: 0.8886, Test Loss: 0.7137\n",
      "Epoch 3757, Train Loss: 0.8921, Test Loss: 0.7125\n",
      "Epoch 3758, Train Loss: 0.9333, Test Loss: 0.7094\n",
      "Epoch 3759, Train Loss: 0.8518, Test Loss: 0.7102\n",
      "Epoch 3760, Train Loss: 0.9528, Test Loss: 0.7127\n",
      "Epoch 3761, Train Loss: 1.1402, Test Loss: 0.7120\n",
      "Epoch 3762, Train Loss: 0.8951, Test Loss: 0.7113\n",
      "Epoch 3763, Train Loss: 0.9986, Test Loss: 0.7094\n",
      "Epoch 3764, Train Loss: 0.9663, Test Loss: 0.7099\n",
      "Epoch 3765, Train Loss: 1.0260, Test Loss: 0.7112\n",
      "Epoch 3766, Train Loss: 0.9980, Test Loss: 0.7131\n",
      "Epoch 3767, Train Loss: 1.0908, Test Loss: 0.7167\n",
      "Epoch 3768, Train Loss: 1.1329, Test Loss: 0.7228\n",
      "Epoch 3769, Train Loss: 0.9151, Test Loss: 0.7270\n",
      "Epoch 3770, Train Loss: 0.9463, Test Loss: 0.7310\n",
      "Epoch 3771, Train Loss: 0.9055, Test Loss: 0.7336\n",
      "Epoch 3772, Train Loss: 0.9293, Test Loss: 0.7358\n",
      "Epoch 3773, Train Loss: 0.9184, Test Loss: 0.7375\n",
      "Epoch 3774, Train Loss: 0.8459, Test Loss: 0.7405\n",
      "Epoch 3775, Train Loss: 0.9052, Test Loss: 0.7400\n",
      "Epoch 3776, Train Loss: 0.9036, Test Loss: 0.7379\n",
      "Epoch 3777, Train Loss: 0.7899, Test Loss: 0.7349\n",
      "Epoch 3778, Train Loss: 0.9210, Test Loss: 0.7312\n",
      "Epoch 3779, Train Loss: 1.0144, Test Loss: 0.7262\n",
      "Epoch 3780, Train Loss: 0.8891, Test Loss: 0.7227\n",
      "Epoch 3781, Train Loss: 1.1547, Test Loss: 0.7220\n",
      "Epoch 3782, Train Loss: 0.9954, Test Loss: 0.7209\n",
      "Epoch 3783, Train Loss: 0.7482, Test Loss: 0.7181\n",
      "Epoch 3784, Train Loss: 1.0557, Test Loss: 0.7152\n",
      "Epoch 3785, Train Loss: 0.8911, Test Loss: 0.7119\n",
      "Epoch 3786, Train Loss: 0.8889, Test Loss: 0.7095\n",
      "Epoch 3787, Train Loss: 0.8790, Test Loss: 0.7076\n",
      "Epoch 3788, Train Loss: 0.9103, Test Loss: 0.7043\n",
      "Epoch 3789, Train Loss: 0.8370, Test Loss: 0.7024\n",
      "Epoch 3790, Train Loss: 0.8267, Test Loss: 0.7020\n",
      "Epoch 3791, Train Loss: 0.7506, Test Loss: 0.7023\n",
      "Epoch 3792, Train Loss: 0.8236, Test Loss: 0.7056\n",
      "Epoch 3793, Train Loss: 0.9240, Test Loss: 0.7067\n",
      "Epoch 3794, Train Loss: 0.7778, Test Loss: 0.7066\n",
      "Epoch 3795, Train Loss: 0.9155, Test Loss: 0.7076\n",
      "Epoch 3796, Train Loss: 0.8445, Test Loss: 0.7042\n",
      "Epoch 3797, Train Loss: 0.8393, Test Loss: 0.7019\n",
      "Epoch 3798, Train Loss: 0.7927, Test Loss: 0.7013\n",
      "Epoch 3799, Train Loss: 0.8861, Test Loss: 0.7034\n",
      "Epoch 3800, Train Loss: 0.9458, Test Loss: 0.7052\n",
      "Epoch 3801, Train Loss: 0.8732, Test Loss: 0.7060\n",
      "Epoch 3802, Train Loss: 0.9304, Test Loss: 0.7092\n",
      "Epoch 3803, Train Loss: 0.8041, Test Loss: 0.7110\n",
      "Epoch 3804, Train Loss: 0.9634, Test Loss: 0.7122\n",
      "Epoch 3805, Train Loss: 0.9022, Test Loss: 0.7127\n",
      "Epoch 3806, Train Loss: 1.0158, Test Loss: 0.7098\n",
      "Epoch 3807, Train Loss: 0.9318, Test Loss: 0.7062\n",
      "Epoch 3808, Train Loss: 0.8325, Test Loss: 0.7034\n",
      "Epoch 3809, Train Loss: 0.8462, Test Loss: 0.7027\n",
      "Epoch 3810, Train Loss: 0.9659, Test Loss: 0.7052\n",
      "Epoch 3811, Train Loss: 0.9062, Test Loss: 0.7071\n",
      "Epoch 3812, Train Loss: 0.8583, Test Loss: 0.7090\n",
      "Epoch 3813, Train Loss: 0.9285, Test Loss: 0.7113\n",
      "Epoch 3814, Train Loss: 0.8653, Test Loss: 0.7142\n",
      "Epoch 3815, Train Loss: 0.8787, Test Loss: 0.7176\n",
      "Epoch 3816, Train Loss: 0.9195, Test Loss: 0.7213\n",
      "Epoch 3817, Train Loss: 0.9365, Test Loss: 0.7238\n",
      "Epoch 3818, Train Loss: 0.7083, Test Loss: 0.7248\n",
      "Epoch 3819, Train Loss: 0.7854, Test Loss: 0.7242\n",
      "Epoch 3820, Train Loss: 0.9398, Test Loss: 0.7206\n",
      "Epoch 3821, Train Loss: 0.9783, Test Loss: 0.7160\n",
      "Epoch 3822, Train Loss: 0.8027, Test Loss: 0.7127\n",
      "Epoch 3823, Train Loss: 0.9898, Test Loss: 0.7073\n",
      "Epoch 3824, Train Loss: 0.9430, Test Loss: 0.7042\n",
      "Epoch 3825, Train Loss: 0.7598, Test Loss: 0.7022\n",
      "Epoch 3826, Train Loss: 0.9406, Test Loss: 0.6982\n",
      "Epoch 3827, Train Loss: 0.8708, Test Loss: 0.6970\n",
      "Epoch 3828, Train Loss: 0.7909, Test Loss: 0.6969\n",
      "Epoch 3829, Train Loss: 0.7621, Test Loss: 0.6982\n",
      "Epoch 3830, Train Loss: 0.8808, Test Loss: 0.7011\n",
      "Epoch 3831, Train Loss: 1.0034, Test Loss: 0.7032\n",
      "Epoch 3832, Train Loss: 0.9251, Test Loss: 0.7030\n",
      "Epoch 3833, Train Loss: 0.8378, Test Loss: 0.7005\n",
      "Epoch 3834, Train Loss: 0.7686, Test Loss: 0.6977\n",
      "Epoch 3835, Train Loss: 0.8073, Test Loss: 0.6937\n",
      "Epoch 3836, Train Loss: 0.8122, Test Loss: 0.6899\n",
      "Epoch 3837, Train Loss: 0.8651, Test Loss: 0.6876\n",
      "Epoch 3838, Train Loss: 0.8597, Test Loss: 0.6855\n",
      "Epoch 3839, Train Loss: 0.8332, Test Loss: 0.6844\n",
      "Epoch 3840, Train Loss: 0.8456, Test Loss: 0.6839\n",
      "Epoch 3841, Train Loss: 1.0428, Test Loss: 0.6853\n",
      "Epoch 3842, Train Loss: 0.8982, Test Loss: 0.6895\n",
      "Epoch 3843, Train Loss: 0.7832, Test Loss: 0.6937\n",
      "Epoch 3844, Train Loss: 0.8621, Test Loss: 0.6960\n",
      "Epoch 3845, Train Loss: 0.9867, Test Loss: 0.6972\n",
      "Epoch 3846, Train Loss: 0.8415, Test Loss: 0.6977\n",
      "Epoch 3847, Train Loss: 0.8108, Test Loss: 0.7008\n",
      "Epoch 3848, Train Loss: 0.8032, Test Loss: 0.7033\n",
      "Epoch 3849, Train Loss: 0.7315, Test Loss: 0.7070\n",
      "Epoch 3850, Train Loss: 0.7651, Test Loss: 0.7083\n",
      "Epoch 3851, Train Loss: 0.8683, Test Loss: 0.7097\n",
      "Epoch 3852, Train Loss: 0.8010, Test Loss: 0.7071\n",
      "Epoch 3853, Train Loss: 0.8201, Test Loss: 0.7045\n",
      "Epoch 3854, Train Loss: 0.9377, Test Loss: 0.7006\n",
      "Epoch 3855, Train Loss: 0.9651, Test Loss: 0.6963\n",
      "Epoch 3856, Train Loss: 0.7554, Test Loss: 0.6941\n",
      "Epoch 3857, Train Loss: 0.8395, Test Loss: 0.6887\n",
      "Epoch 3858, Train Loss: 0.8047, Test Loss: 0.6856\n",
      "Epoch 3859, Train Loss: 0.9243, Test Loss: 0.6824\n",
      "Epoch 3860, Train Loss: 0.7908, Test Loss: 0.6802\n",
      "Epoch 3861, Train Loss: 0.8556, Test Loss: 0.6782\n",
      "Epoch 3862, Train Loss: 0.8880, Test Loss: 0.6762\n",
      "Epoch 3863, Train Loss: 0.7893, Test Loss: 0.6751\n",
      "Epoch 3864, Train Loss: 0.6968, Test Loss: 0.6746\n",
      "Epoch 3865, Train Loss: 0.7826, Test Loss: 0.6747\n",
      "Epoch 3866, Train Loss: 0.8756, Test Loss: 0.6760\n",
      "Epoch 3867, Train Loss: 0.6882, Test Loss: 0.6784\n",
      "Epoch 3868, Train Loss: 0.9168, Test Loss: 0.6824\n",
      "Epoch 3869, Train Loss: 0.8236, Test Loss: 0.6842\n",
      "Epoch 3870, Train Loss: 0.8300, Test Loss: 0.6849\n",
      "Epoch 3871, Train Loss: 0.8079, Test Loss: 0.6861\n",
      "Epoch 3872, Train Loss: 1.0380, Test Loss: 0.6888\n",
      "Epoch 3873, Train Loss: 0.9424, Test Loss: 0.6906\n",
      "Epoch 3874, Train Loss: 0.8334, Test Loss: 0.6918\n",
      "Epoch 3875, Train Loss: 0.7533, Test Loss: 0.6908\n",
      "Epoch 3876, Train Loss: 0.7154, Test Loss: 0.6913\n",
      "Epoch 3877, Train Loss: 0.8585, Test Loss: 0.6933\n",
      "Epoch 3878, Train Loss: 0.8233, Test Loss: 0.6949\n",
      "Epoch 3879, Train Loss: 0.8224, Test Loss: 0.6936\n",
      "Epoch 3880, Train Loss: 0.8799, Test Loss: 0.6907\n",
      "Epoch 3881, Train Loss: 0.7668, Test Loss: 0.6889\n",
      "Epoch 3882, Train Loss: 0.9523, Test Loss: 0.6864\n",
      "Epoch 3883, Train Loss: 0.7439, Test Loss: 0.6848\n",
      "Epoch 3884, Train Loss: 0.8069, Test Loss: 0.6861\n",
      "Epoch 3885, Train Loss: 0.8463, Test Loss: 0.6876\n",
      "Epoch 3886, Train Loss: 0.6451, Test Loss: 0.6895\n",
      "Epoch 3887, Train Loss: 0.8112, Test Loss: 0.6885\n",
      "Epoch 3888, Train Loss: 0.7629, Test Loss: 0.6878\n",
      "Epoch 3889, Train Loss: 0.8414, Test Loss: 0.6861\n",
      "Epoch 3890, Train Loss: 0.6819, Test Loss: 0.6852\n",
      "Epoch 3891, Train Loss: 0.7761, Test Loss: 0.6836\n",
      "Epoch 3892, Train Loss: 0.7721, Test Loss: 0.6834\n",
      "Epoch 3893, Train Loss: 0.7843, Test Loss: 0.6840\n",
      "Epoch 3894, Train Loss: 0.8127, Test Loss: 0.6834\n",
      "Epoch 3895, Train Loss: 0.6972, Test Loss: 0.6828\n",
      "Epoch 3896, Train Loss: 0.7156, Test Loss: 0.6805\n",
      "Epoch 3897, Train Loss: 0.7309, Test Loss: 0.6790\n",
      "Epoch 3898, Train Loss: 0.7114, Test Loss: 0.6735\n",
      "Epoch 3899, Train Loss: 0.6737, Test Loss: 0.6696\n",
      "Epoch 3900, Train Loss: 0.8194, Test Loss: 0.6651\n",
      "Epoch 3901, Train Loss: 0.8696, Test Loss: 0.6625\n",
      "Epoch 3902, Train Loss: 0.7529, Test Loss: 0.6603\n",
      "Epoch 3903, Train Loss: 0.7849, Test Loss: 0.6606\n",
      "Epoch 3904, Train Loss: 0.6717, Test Loss: 0.6625\n",
      "Epoch 3905, Train Loss: 0.8740, Test Loss: 0.6666\n",
      "Epoch 3906, Train Loss: 0.9335, Test Loss: 0.6678\n",
      "Epoch 3907, Train Loss: 0.8415, Test Loss: 0.6684\n",
      "Epoch 3908, Train Loss: 0.8144, Test Loss: 0.6695\n",
      "Epoch 3909, Train Loss: 0.7027, Test Loss: 0.6678\n",
      "Epoch 3910, Train Loss: 0.8382, Test Loss: 0.6666\n",
      "Epoch 3911, Train Loss: 0.7783, Test Loss: 0.6661\n",
      "Epoch 3912, Train Loss: 0.7795, Test Loss: 0.6665\n",
      "Epoch 3913, Train Loss: 0.7188, Test Loss: 0.6676\n",
      "Epoch 3914, Train Loss: 0.7067, Test Loss: 0.6679\n",
      "Epoch 3915, Train Loss: 0.6970, Test Loss: 0.6644\n",
      "Epoch 3916, Train Loss: 0.8374, Test Loss: 0.6617\n",
      "Epoch 3917, Train Loss: 0.9181, Test Loss: 0.6570\n",
      "Epoch 3918, Train Loss: 0.9353, Test Loss: 0.6526\n",
      "Epoch 3919, Train Loss: 0.8491, Test Loss: 0.6496\n",
      "Epoch 3920, Train Loss: 0.7013, Test Loss: 0.6461\n",
      "Epoch 3921, Train Loss: 0.8969, Test Loss: 0.6439\n",
      "Epoch 3922, Train Loss: 0.9020, Test Loss: 0.6429\n",
      "Epoch 3923, Train Loss: 0.7844, Test Loss: 0.6422\n",
      "Epoch 3924, Train Loss: 0.7186, Test Loss: 0.6410\n",
      "Epoch 3925, Train Loss: 0.6685, Test Loss: 0.6416\n",
      "Epoch 3926, Train Loss: 0.7262, Test Loss: 0.6420\n",
      "Epoch 3927, Train Loss: 0.8245, Test Loss: 0.6427\n",
      "Epoch 3928, Train Loss: 0.9092, Test Loss: 0.6438\n",
      "Epoch 3929, Train Loss: 0.8702, Test Loss: 0.6468\n",
      "Epoch 3930, Train Loss: 0.6807, Test Loss: 0.6490\n",
      "Epoch 3931, Train Loss: 0.7146, Test Loss: 0.6524\n",
      "Epoch 3932, Train Loss: 0.8576, Test Loss: 0.6558\n",
      "Epoch 3933, Train Loss: 0.6843, Test Loss: 0.6566\n",
      "Epoch 3934, Train Loss: 0.9601, Test Loss: 0.6578\n",
      "Epoch 3935, Train Loss: 0.6006, Test Loss: 0.6584\n",
      "Epoch 3936, Train Loss: 0.8346, Test Loss: 0.6570\n",
      "Epoch 3937, Train Loss: 0.7188, Test Loss: 0.6536\n",
      "Epoch 3938, Train Loss: 0.8586, Test Loss: 0.6504\n",
      "Epoch 3939, Train Loss: 0.7599, Test Loss: 0.6471\n",
      "Epoch 3940, Train Loss: 0.8092, Test Loss: 0.6433\n",
      "Epoch 3941, Train Loss: 0.7808, Test Loss: 0.6409\n",
      "Epoch 3942, Train Loss: 0.6990, Test Loss: 0.6385\n",
      "Epoch 3943, Train Loss: 0.8256, Test Loss: 0.6362\n",
      "Epoch 3944, Train Loss: 0.6463, Test Loss: 0.6337\n",
      "Epoch 3945, Train Loss: 0.8206, Test Loss: 0.6314\n",
      "Epoch 3946, Train Loss: 0.8099, Test Loss: 0.6295\n",
      "Epoch 3947, Train Loss: 0.7602, Test Loss: 0.6284\n",
      "Epoch 3948, Train Loss: 0.7737, Test Loss: 0.6280\n",
      "Epoch 3949, Train Loss: 0.7731, Test Loss: 0.6259\n",
      "Epoch 3950, Train Loss: 0.7157, Test Loss: 0.6241\n",
      "Epoch 3951, Train Loss: 0.7109, Test Loss: 0.6232\n",
      "Epoch 3952, Train Loss: 0.6734, Test Loss: 0.6236\n",
      "Epoch 3953, Train Loss: 0.8097, Test Loss: 0.6232\n",
      "Epoch 3954, Train Loss: 0.6751, Test Loss: 0.6232\n",
      "Epoch 3955, Train Loss: 0.7165, Test Loss: 0.6231\n",
      "Epoch 3956, Train Loss: 0.6654, Test Loss: 0.6244\n",
      "Epoch 3957, Train Loss: 0.7694, Test Loss: 0.6266\n",
      "Epoch 3958, Train Loss: 0.6712, Test Loss: 0.6294\n",
      "Epoch 3959, Train Loss: 0.6676, Test Loss: 0.6314\n",
      "Epoch 3960, Train Loss: 0.7164, Test Loss: 0.6331\n",
      "Epoch 3961, Train Loss: 0.6286, Test Loss: 0.6348\n",
      "Epoch 3962, Train Loss: 0.8316, Test Loss: 0.6359\n",
      "Epoch 3963, Train Loss: 0.7265, Test Loss: 0.6374\n",
      "Epoch 3964, Train Loss: 0.6042, Test Loss: 0.6381\n",
      "Epoch 3965, Train Loss: 0.8148, Test Loss: 0.6388\n",
      "Epoch 3966, Train Loss: 0.6694, Test Loss: 0.6387\n",
      "Epoch 3967, Train Loss: 0.7873, Test Loss: 0.6393\n",
      "Epoch 3968, Train Loss: 0.7064, Test Loss: 0.6409\n",
      "Epoch 3969, Train Loss: 0.8244, Test Loss: 0.6408\n",
      "Epoch 3970, Train Loss: 0.6045, Test Loss: 0.6420\n",
      "Epoch 3971, Train Loss: 0.7483, Test Loss: 0.6433\n",
      "Epoch 3972, Train Loss: 0.6645, Test Loss: 0.6461\n",
      "Epoch 3973, Train Loss: 0.7745, Test Loss: 0.6466\n",
      "Epoch 3974, Train Loss: 0.7304, Test Loss: 0.6455\n",
      "Epoch 3975, Train Loss: 0.7444, Test Loss: 0.6438\n",
      "Epoch 3976, Train Loss: 0.7528, Test Loss: 0.6414\n",
      "Epoch 3977, Train Loss: 0.8340, Test Loss: 0.6402\n",
      "Epoch 3978, Train Loss: 0.6647, Test Loss: 0.6378\n",
      "Epoch 3979, Train Loss: 0.6180, Test Loss: 0.6338\n",
      "Epoch 3980, Train Loss: 0.6327, Test Loss: 0.6299\n",
      "Epoch 3981, Train Loss: 0.7401, Test Loss: 0.6274\n",
      "Epoch 3982, Train Loss: 0.7327, Test Loss: 0.6250\n",
      "Epoch 3983, Train Loss: 0.7072, Test Loss: 0.6241\n",
      "Epoch 3984, Train Loss: 0.7237, Test Loss: 0.6227\n",
      "Epoch 3985, Train Loss: 0.7952, Test Loss: 0.6222\n",
      "Epoch 3986, Train Loss: 0.8873, Test Loss: 0.6236\n",
      "Epoch 3987, Train Loss: 0.7043, Test Loss: 0.6248\n",
      "Epoch 3988, Train Loss: 0.6458, Test Loss: 0.6268\n",
      "Epoch 3989, Train Loss: 0.8405, Test Loss: 0.6275\n",
      "Epoch 3990, Train Loss: 0.7383, Test Loss: 0.6271\n",
      "Epoch 3991, Train Loss: 0.7394, Test Loss: 0.6272\n",
      "Epoch 3992, Train Loss: 0.7976, Test Loss: 0.6281\n",
      "Epoch 3993, Train Loss: 0.7170, Test Loss: 0.6298\n",
      "Epoch 3994, Train Loss: 0.8245, Test Loss: 0.6310\n",
      "Epoch 3995, Train Loss: 0.6699, Test Loss: 0.6325\n",
      "Epoch 3996, Train Loss: 0.7250, Test Loss: 0.6326\n",
      "Epoch 3997, Train Loss: 0.9490, Test Loss: 0.6319\n",
      "Epoch 3998, Train Loss: 0.7722, Test Loss: 0.6326\n",
      "Epoch 3999, Train Loss: 0.7259, Test Loss: 0.6326\n",
      "Epoch 4000, Train Loss: 0.6482, Test Loss: 0.6312\n",
      "Epoch 4001, Train Loss: 0.7188, Test Loss: 0.6309\n",
      "Epoch 4002, Train Loss: 0.6643, Test Loss: 0.6306\n",
      "Epoch 4003, Train Loss: 0.7535, Test Loss: 0.6278\n",
      "Epoch 4004, Train Loss: 0.6421, Test Loss: 0.6242\n",
      "Epoch 4005, Train Loss: 0.7123, Test Loss: 0.6208\n",
      "Epoch 4006, Train Loss: 0.7827, Test Loss: 0.6194\n",
      "Epoch 4007, Train Loss: 0.6830, Test Loss: 0.6185\n",
      "Epoch 4008, Train Loss: 0.7459, Test Loss: 0.6184\n",
      "Epoch 4009, Train Loss: 0.7079, Test Loss: 0.6191\n",
      "Epoch 4010, Train Loss: 0.7013, Test Loss: 0.6206\n",
      "Epoch 4011, Train Loss: 0.6404, Test Loss: 0.6205\n",
      "Epoch 4012, Train Loss: 0.7004, Test Loss: 0.6200\n",
      "Epoch 4013, Train Loss: 0.6881, Test Loss: 0.6188\n",
      "Epoch 4014, Train Loss: 0.6924, Test Loss: 0.6182\n",
      "Epoch 4015, Train Loss: 0.7517, Test Loss: 0.6190\n",
      "Epoch 4016, Train Loss: 0.6250, Test Loss: 0.6195\n",
      "Epoch 4017, Train Loss: 0.6064, Test Loss: 0.6198\n",
      "Epoch 4018, Train Loss: 0.6999, Test Loss: 0.6194\n",
      "Epoch 4019, Train Loss: 0.5675, Test Loss: 0.6183\n",
      "Epoch 4020, Train Loss: 0.8472, Test Loss: 0.6175\n",
      "Epoch 4021, Train Loss: 0.7188, Test Loss: 0.6175\n",
      "Epoch 4022, Train Loss: 0.7067, Test Loss: 0.6164\n",
      "Epoch 4023, Train Loss: 0.6875, Test Loss: 0.6157\n",
      "Epoch 4024, Train Loss: 0.6228, Test Loss: 0.6152\n",
      "Epoch 4025, Train Loss: 0.7323, Test Loss: 0.6136\n",
      "Epoch 4026, Train Loss: 0.6048, Test Loss: 0.6118\n",
      "Epoch 4027, Train Loss: 0.7131, Test Loss: 0.6105\n",
      "Epoch 4028, Train Loss: 0.6715, Test Loss: 0.6091\n",
      "Epoch 4029, Train Loss: 0.5429, Test Loss: 0.6070\n",
      "Epoch 4030, Train Loss: 0.6329, Test Loss: 0.6053\n",
      "Epoch 4031, Train Loss: 0.7103, Test Loss: 0.6033\n",
      "Epoch 4032, Train Loss: 0.6967, Test Loss: 0.6029\n",
      "Epoch 4033, Train Loss: 0.5983, Test Loss: 0.6031\n",
      "Epoch 4034, Train Loss: 0.7477, Test Loss: 0.6052\n",
      "Epoch 4035, Train Loss: 0.6352, Test Loss: 0.6074\n",
      "Epoch 4036, Train Loss: 0.8223, Test Loss: 0.6088\n",
      "Epoch 4037, Train Loss: 0.8129, Test Loss: 0.6103\n",
      "Epoch 4038, Train Loss: 0.7604, Test Loss: 0.6111\n",
      "Epoch 4039, Train Loss: 0.6115, Test Loss: 0.6110\n",
      "Epoch 4040, Train Loss: 0.6156, Test Loss: 0.6119\n",
      "Epoch 4041, Train Loss: 0.7003, Test Loss: 0.6127\n",
      "Epoch 4042, Train Loss: 0.6992, Test Loss: 0.6119\n",
      "Epoch 4043, Train Loss: 0.6293, Test Loss: 0.6093\n",
      "Epoch 4044, Train Loss: 0.6352, Test Loss: 0.6062\n",
      "Epoch 4045, Train Loss: 0.7740, Test Loss: 0.6040\n",
      "Epoch 4046, Train Loss: 0.6671, Test Loss: 0.6032\n",
      "Epoch 4047, Train Loss: 0.7979, Test Loss: 0.6034\n",
      "Epoch 4048, Train Loss: 0.6636, Test Loss: 0.6039\n",
      "Epoch 4049, Train Loss: 0.6510, Test Loss: 0.6056\n",
      "Epoch 4050, Train Loss: 0.6146, Test Loss: 0.6065\n",
      "Epoch 4051, Train Loss: 0.6360, Test Loss: 0.6074\n",
      "Epoch 4052, Train Loss: 0.7530, Test Loss: 0.6069\n",
      "Epoch 4053, Train Loss: 0.5607, Test Loss: 0.6057\n",
      "Epoch 4054, Train Loss: 0.6417, Test Loss: 0.6044\n",
      "Epoch 4055, Train Loss: 0.7452, Test Loss: 0.6044\n",
      "Epoch 4056, Train Loss: 0.7299, Test Loss: 0.6032\n",
      "Epoch 4057, Train Loss: 0.7039, Test Loss: 0.6022\n",
      "Epoch 4058, Train Loss: 0.6209, Test Loss: 0.6008\n",
      "Epoch 4059, Train Loss: 0.6899, Test Loss: 0.5992\n",
      "Epoch 4060, Train Loss: 0.6208, Test Loss: 0.5972\n",
      "Epoch 4061, Train Loss: 0.7588, Test Loss: 0.5951\n",
      "Epoch 4062, Train Loss: 0.5996, Test Loss: 0.5933\n",
      "Epoch 4063, Train Loss: 0.6391, Test Loss: 0.5923\n",
      "Epoch 4064, Train Loss: 0.6587, Test Loss: 0.5914\n",
      "Epoch 4065, Train Loss: 0.6729, Test Loss: 0.5913\n",
      "Epoch 4066, Train Loss: 0.5734, Test Loss: 0.5910\n",
      "Epoch 4067, Train Loss: 0.6985, Test Loss: 0.5913\n",
      "Epoch 4068, Train Loss: 0.6993, Test Loss: 0.5924\n",
      "Epoch 4069, Train Loss: 0.5792, Test Loss: 0.5939\n",
      "Epoch 4070, Train Loss: 0.5416, Test Loss: 0.5942\n",
      "Epoch 4071, Train Loss: 0.6194, Test Loss: 0.5957\n",
      "Epoch 4072, Train Loss: 0.6210, Test Loss: 0.5973\n",
      "Epoch 4073, Train Loss: 0.6474, Test Loss: 0.6001\n",
      "Epoch 4074, Train Loss: 0.6898, Test Loss: 0.6012\n",
      "Epoch 4075, Train Loss: 0.7605, Test Loss: 0.6011\n",
      "Epoch 4076, Train Loss: 0.6873, Test Loss: 0.6021\n",
      "Epoch 4077, Train Loss: 0.6687, Test Loss: 0.6042\n",
      "Epoch 4078, Train Loss: 0.6510, Test Loss: 0.6053\n",
      "Epoch 4079, Train Loss: 0.5965, Test Loss: 0.6035\n",
      "Epoch 4080, Train Loss: 0.6448, Test Loss: 0.5991\n",
      "Epoch 4081, Train Loss: 0.6865, Test Loss: 0.5948\n",
      "Epoch 4082, Train Loss: 0.6787, Test Loss: 0.5913\n",
      "Epoch 4083, Train Loss: 0.5981, Test Loss: 0.5887\n",
      "Epoch 4084, Train Loss: 0.5411, Test Loss: 0.5864\n",
      "Epoch 4085, Train Loss: 0.6080, Test Loss: 0.5852\n",
      "Epoch 4086, Train Loss: 0.6690, Test Loss: 0.5848\n",
      "Epoch 4087, Train Loss: 0.6692, Test Loss: 0.5845\n",
      "Epoch 4088, Train Loss: 0.6401, Test Loss: 0.5840\n",
      "Epoch 4089, Train Loss: 0.5781, Test Loss: 0.5838\n",
      "Epoch 4090, Train Loss: 0.6927, Test Loss: 0.5830\n",
      "Epoch 4091, Train Loss: 0.6514, Test Loss: 0.5824\n",
      "Epoch 4092, Train Loss: 0.5535, Test Loss: 0.5818\n",
      "Epoch 4093, Train Loss: 0.6331, Test Loss: 0.5814\n",
      "Epoch 4094, Train Loss: 0.7261, Test Loss: 0.5816\n",
      "Epoch 4095, Train Loss: 0.6872, Test Loss: 0.5825\n",
      "Epoch 4096, Train Loss: 0.5805, Test Loss: 0.5830\n",
      "Epoch 4097, Train Loss: 0.6003, Test Loss: 0.5821\n",
      "Epoch 4098, Train Loss: 0.5665, Test Loss: 0.5827\n",
      "Epoch 4099, Train Loss: 0.5455, Test Loss: 0.5843\n",
      "Epoch 4100, Train Loss: 0.6336, Test Loss: 0.5856\n",
      "Epoch 4101, Train Loss: 0.5896, Test Loss: 0.5858\n",
      "Epoch 4102, Train Loss: 0.7018, Test Loss: 0.5850\n",
      "Epoch 4103, Train Loss: 0.5744, Test Loss: 0.5848\n",
      "Epoch 4104, Train Loss: 0.6007, Test Loss: 0.5844\n",
      "Epoch 4105, Train Loss: 0.7854, Test Loss: 0.5844\n",
      "Epoch 4106, Train Loss: 0.6004, Test Loss: 0.5846\n",
      "Epoch 4107, Train Loss: 0.5903, Test Loss: 0.5856\n",
      "Epoch 4108, Train Loss: 0.6343, Test Loss: 0.5853\n",
      "Epoch 4109, Train Loss: 0.6345, Test Loss: 0.5837\n",
      "Epoch 4110, Train Loss: 0.6850, Test Loss: 0.5809\n",
      "Epoch 4111, Train Loss: 0.7741, Test Loss: 0.5807\n",
      "Epoch 4112, Train Loss: 0.6625, Test Loss: 0.5811\n",
      "Epoch 4113, Train Loss: 0.6517, Test Loss: 0.5815\n",
      "Epoch 4114, Train Loss: 0.7290, Test Loss: 0.5814\n",
      "Epoch 4115, Train Loss: 0.6889, Test Loss: 0.5823\n",
      "Epoch 4116, Train Loss: 0.7080, Test Loss: 0.5832\n",
      "Epoch 4117, Train Loss: 0.6648, Test Loss: 0.5844\n",
      "Epoch 4118, Train Loss: 0.6931, Test Loss: 0.5852\n",
      "Epoch 4119, Train Loss: 0.6539, Test Loss: 0.5860\n",
      "Epoch 4120, Train Loss: 0.6891, Test Loss: 0.5875\n",
      "Epoch 4121, Train Loss: 0.6275, Test Loss: 0.5879\n",
      "Epoch 4122, Train Loss: 0.5978, Test Loss: 0.5891\n",
      "Epoch 4123, Train Loss: 0.6487, Test Loss: 0.5900\n",
      "Epoch 4124, Train Loss: 0.7153, Test Loss: 0.5900\n",
      "Epoch 4125, Train Loss: 0.7476, Test Loss: 0.5880\n",
      "Epoch 4126, Train Loss: 0.6060, Test Loss: 0.5872\n",
      "Epoch 4127, Train Loss: 0.6010, Test Loss: 0.5861\n",
      "Epoch 4128, Train Loss: 0.5964, Test Loss: 0.5854\n",
      "Epoch 4129, Train Loss: 0.5951, Test Loss: 0.5854\n",
      "Epoch 4130, Train Loss: 0.7810, Test Loss: 0.5868\n",
      "Epoch 4131, Train Loss: 0.6253, Test Loss: 0.5888\n",
      "Epoch 4132, Train Loss: 0.7972, Test Loss: 0.5895\n",
      "Epoch 4133, Train Loss: 0.5374, Test Loss: 0.5911\n",
      "Epoch 4134, Train Loss: 0.6242, Test Loss: 0.5920\n",
      "Epoch 4135, Train Loss: 0.6208, Test Loss: 0.5909\n",
      "Epoch 4136, Train Loss: 0.5797, Test Loss: 0.5890\n",
      "Epoch 4137, Train Loss: 0.6550, Test Loss: 0.5879\n",
      "Epoch 4138, Train Loss: 0.6930, Test Loss: 0.5865\n",
      "Epoch 4139, Train Loss: 0.6409, Test Loss: 0.5853\n",
      "Epoch 4140, Train Loss: 0.6353, Test Loss: 0.5842\n",
      "Epoch 4141, Train Loss: 0.5375, Test Loss: 0.5828\n",
      "Epoch 4142, Train Loss: 0.6790, Test Loss: 0.5817\n",
      "Epoch 4143, Train Loss: 0.6148, Test Loss: 0.5810\n",
      "Epoch 4144, Train Loss: 0.6892, Test Loss: 0.5808\n",
      "Epoch 4145, Train Loss: 0.5616, Test Loss: 0.5811\n",
      "Epoch 4146, Train Loss: 0.6181, Test Loss: 0.5812\n",
      "Epoch 4147, Train Loss: 0.5314, Test Loss: 0.5815\n",
      "Epoch 4148, Train Loss: 0.4672, Test Loss: 0.5796\n",
      "Epoch 4149, Train Loss: 0.7429, Test Loss: 0.5771\n",
      "Epoch 4150, Train Loss: 0.5898, Test Loss: 0.5755\n",
      "Epoch 4151, Train Loss: 0.6107, Test Loss: 0.5741\n",
      "Epoch 4152, Train Loss: 0.6117, Test Loss: 0.5731\n",
      "Epoch 4153, Train Loss: 0.6384, Test Loss: 0.5722\n",
      "Epoch 4154, Train Loss: 0.5446, Test Loss: 0.5710\n",
      "Epoch 4155, Train Loss: 0.6686, Test Loss: 0.5685\n",
      "Epoch 4156, Train Loss: 0.5763, Test Loss: 0.5670\n",
      "Epoch 4157, Train Loss: 0.5784, Test Loss: 0.5660\n",
      "Epoch 4158, Train Loss: 0.5684, Test Loss: 0.5659\n",
      "Epoch 4159, Train Loss: 0.6257, Test Loss: 0.5657\n",
      "Epoch 4160, Train Loss: 0.5753, Test Loss: 0.5670\n",
      "Epoch 4161, Train Loss: 0.6721, Test Loss: 0.5685\n",
      "Epoch 4162, Train Loss: 0.5876, Test Loss: 0.5710\n",
      "Epoch 4163, Train Loss: 0.5907, Test Loss: 0.5724\n",
      "Epoch 4164, Train Loss: 0.5446, Test Loss: 0.5733\n",
      "Epoch 4165, Train Loss: 0.6443, Test Loss: 0.5732\n",
      "Epoch 4166, Train Loss: 0.5610, Test Loss: 0.5728\n",
      "Epoch 4167, Train Loss: 0.5811, Test Loss: 0.5717\n",
      "Epoch 4168, Train Loss: 0.6122, Test Loss: 0.5712\n",
      "Epoch 4169, Train Loss: 0.5233, Test Loss: 0.5720\n",
      "Epoch 4170, Train Loss: 0.6382, Test Loss: 0.5732\n",
      "Epoch 4171, Train Loss: 0.5943, Test Loss: 0.5751\n",
      "Epoch 4172, Train Loss: 0.5722, Test Loss: 0.5742\n",
      "Epoch 4173, Train Loss: 0.4848, Test Loss: 0.5733\n",
      "Epoch 4174, Train Loss: 0.6399, Test Loss: 0.5713\n",
      "Epoch 4175, Train Loss: 0.5348, Test Loss: 0.5686\n",
      "Epoch 4176, Train Loss: 0.5856, Test Loss: 0.5655\n",
      "Epoch 4177, Train Loss: 0.5110, Test Loss: 0.5630\n",
      "Epoch 4178, Train Loss: 0.6172, Test Loss: 0.5611\n",
      "Epoch 4179, Train Loss: 0.6648, Test Loss: 0.5600\n",
      "Epoch 4180, Train Loss: 0.5328, Test Loss: 0.5590\n",
      "Epoch 4181, Train Loss: 0.5206, Test Loss: 0.5587\n",
      "Epoch 4182, Train Loss: 0.6055, Test Loss: 0.5592\n",
      "Epoch 4183, Train Loss: 0.6185, Test Loss: 0.5596\n",
      "Epoch 4184, Train Loss: 0.5959, Test Loss: 0.5607\n",
      "Epoch 4185, Train Loss: 0.5621, Test Loss: 0.5615\n",
      "Epoch 4186, Train Loss: 0.6007, Test Loss: 0.5628\n",
      "Epoch 4187, Train Loss: 0.5582, Test Loss: 0.5648\n",
      "Epoch 4188, Train Loss: 0.6617, Test Loss: 0.5660\n",
      "Epoch 4189, Train Loss: 0.5302, Test Loss: 0.5666\n",
      "Epoch 4190, Train Loss: 0.4687, Test Loss: 0.5670\n",
      "Epoch 4191, Train Loss: 0.6407, Test Loss: 0.5679\n",
      "Epoch 4192, Train Loss: 0.6465, Test Loss: 0.5689\n",
      "Epoch 4193, Train Loss: 0.5296, Test Loss: 0.5686\n",
      "Epoch 4194, Train Loss: 0.5864, Test Loss: 0.5682\n",
      "Epoch 4195, Train Loss: 0.5585, Test Loss: 0.5685\n",
      "Epoch 4196, Train Loss: 0.6262, Test Loss: 0.5680\n",
      "Epoch 4197, Train Loss: 0.5135, Test Loss: 0.5671\n",
      "Epoch 4198, Train Loss: 0.5439, Test Loss: 0.5679\n",
      "Epoch 4199, Train Loss: 0.5192, Test Loss: 0.5681\n",
      "Epoch 4200, Train Loss: 0.5622, Test Loss: 0.5687\n",
      "Epoch 4201, Train Loss: 0.6243, Test Loss: 0.5669\n",
      "Epoch 4202, Train Loss: 0.5600, Test Loss: 0.5647\n",
      "Epoch 4203, Train Loss: 0.5524, Test Loss: 0.5637\n",
      "Epoch 4204, Train Loss: 0.6476, Test Loss: 0.5628\n",
      "Epoch 4205, Train Loss: 0.5360, Test Loss: 0.5614\n",
      "Epoch 4206, Train Loss: 0.5264, Test Loss: 0.5602\n",
      "Epoch 4207, Train Loss: 0.6088, Test Loss: 0.5596\n",
      "Epoch 4208, Train Loss: 0.5717, Test Loss: 0.5593\n",
      "Epoch 4209, Train Loss: 0.5446, Test Loss: 0.5598\n",
      "Epoch 4210, Train Loss: 0.4823, Test Loss: 0.5604\n",
      "Epoch 4211, Train Loss: 0.6615, Test Loss: 0.5613\n",
      "Epoch 4212, Train Loss: 0.5127, Test Loss: 0.5641\n",
      "Epoch 4213, Train Loss: 0.5492, Test Loss: 0.5659\n",
      "Epoch 4214, Train Loss: 0.5076, Test Loss: 0.5676\n",
      "Epoch 4215, Train Loss: 0.5626, Test Loss: 0.5683\n",
      "Epoch 4216, Train Loss: 0.5951, Test Loss: 0.5673\n",
      "Epoch 4217, Train Loss: 0.7175, Test Loss: 0.5669\n",
      "Epoch 4218, Train Loss: 0.5045, Test Loss: 0.5678\n",
      "Epoch 4219, Train Loss: 0.5782, Test Loss: 0.5686\n",
      "Epoch 4220, Train Loss: 0.5208, Test Loss: 0.5700\n",
      "Epoch 4221, Train Loss: 0.5249, Test Loss: 0.5714\n",
      "Epoch 4222, Train Loss: 0.5727, Test Loss: 0.5700\n",
      "Epoch 4223, Train Loss: 0.6563, Test Loss: 0.5684\n",
      "Epoch 4224, Train Loss: 0.6202, Test Loss: 0.5653\n",
      "Epoch 4225, Train Loss: 0.7339, Test Loss: 0.5634\n",
      "Epoch 4226, Train Loss: 0.6512, Test Loss: 0.5611\n",
      "Epoch 4227, Train Loss: 0.6422, Test Loss: 0.5578\n",
      "Epoch 4228, Train Loss: 0.6059, Test Loss: 0.5556\n",
      "Epoch 4229, Train Loss: 0.5115, Test Loss: 0.5540\n",
      "Epoch 4230, Train Loss: 0.6292, Test Loss: 0.5534\n",
      "Epoch 4231, Train Loss: 0.5134, Test Loss: 0.5526\n",
      "Epoch 4232, Train Loss: 0.5912, Test Loss: 0.5535\n",
      "Epoch 4233, Train Loss: 0.4607, Test Loss: 0.5551\n",
      "Epoch 4234, Train Loss: 0.5621, Test Loss: 0.5551\n",
      "Epoch 4235, Train Loss: 0.6308, Test Loss: 0.5559\n",
      "Epoch 4236, Train Loss: 0.5542, Test Loss: 0.5566\n",
      "Epoch 4237, Train Loss: 0.5395, Test Loss: 0.5559\n",
      "Epoch 4238, Train Loss: 0.5720, Test Loss: 0.5555\n",
      "Epoch 4239, Train Loss: 0.5585, Test Loss: 0.5551\n",
      "Epoch 4240, Train Loss: 0.6308, Test Loss: 0.5527\n",
      "Epoch 4241, Train Loss: 0.5576, Test Loss: 0.5506\n",
      "Epoch 4242, Train Loss: 0.6594, Test Loss: 0.5482\n",
      "Epoch 4243, Train Loss: 0.4870, Test Loss: 0.5469\n",
      "Epoch 4244, Train Loss: 0.5651, Test Loss: 0.5468\n",
      "Epoch 4245, Train Loss: 0.5506, Test Loss: 0.5469\n",
      "Epoch 4246, Train Loss: 0.5880, Test Loss: 0.5460\n",
      "Epoch 4247, Train Loss: 0.6037, Test Loss: 0.5465\n",
      "Epoch 4248, Train Loss: 0.5647, Test Loss: 0.5480\n",
      "Epoch 4249, Train Loss: 0.6638, Test Loss: 0.5500\n",
      "Epoch 4250, Train Loss: 0.5049, Test Loss: 0.5524\n",
      "Epoch 4251, Train Loss: 0.5152, Test Loss: 0.5542\n",
      "Epoch 4252, Train Loss: 0.4817, Test Loss: 0.5555\n",
      "Epoch 4253, Train Loss: 0.4952, Test Loss: 0.5565\n",
      "Epoch 4254, Train Loss: 0.5816, Test Loss: 0.5558\n",
      "Epoch 4255, Train Loss: 0.4713, Test Loss: 0.5556\n",
      "Epoch 4256, Train Loss: 0.5429, Test Loss: 0.5547\n",
      "Epoch 4257, Train Loss: 0.5972, Test Loss: 0.5513\n",
      "Epoch 4258, Train Loss: 0.5048, Test Loss: 0.5478\n",
      "Epoch 4259, Train Loss: 0.6376, Test Loss: 0.5448\n",
      "Epoch 4260, Train Loss: 0.5907, Test Loss: 0.5433\n",
      "Epoch 4261, Train Loss: 0.5246, Test Loss: 0.5429\n",
      "Epoch 4262, Train Loss: 0.6142, Test Loss: 0.5435\n",
      "Epoch 4263, Train Loss: 0.5693, Test Loss: 0.5448\n",
      "Epoch 4264, Train Loss: 0.4972, Test Loss: 0.5466\n",
      "Epoch 4265, Train Loss: 0.5541, Test Loss: 0.5477\n",
      "Epoch 4266, Train Loss: 0.6166, Test Loss: 0.5486\n",
      "Epoch 4267, Train Loss: 0.4430, Test Loss: 0.5501\n",
      "Epoch 4268, Train Loss: 0.4503, Test Loss: 0.5513\n",
      "Epoch 4269, Train Loss: 0.5945, Test Loss: 0.5518\n",
      "Epoch 4270, Train Loss: 0.4647, Test Loss: 0.5521\n",
      "Epoch 4271, Train Loss: 0.5167, Test Loss: 0.5509\n",
      "Epoch 4272, Train Loss: 0.5321, Test Loss: 0.5501\n",
      "Epoch 4273, Train Loss: 0.4749, Test Loss: 0.5504\n",
      "Epoch 4274, Train Loss: 0.4827, Test Loss: 0.5472\n",
      "Epoch 4275, Train Loss: 0.6121, Test Loss: 0.5429\n",
      "Epoch 4276, Train Loss: 0.5212, Test Loss: 0.5392\n",
      "Epoch 4277, Train Loss: 0.5504, Test Loss: 0.5365\n",
      "Epoch 4278, Train Loss: 0.5032, Test Loss: 0.5346\n",
      "Epoch 4279, Train Loss: 0.5186, Test Loss: 0.5335\n",
      "Epoch 4280, Train Loss: 0.6167, Test Loss: 0.5328\n",
      "Epoch 4281, Train Loss: 0.5372, Test Loss: 0.5318\n",
      "Epoch 4282, Train Loss: 0.4893, Test Loss: 0.5308\n",
      "Epoch 4283, Train Loss: 0.5465, Test Loss: 0.5302\n",
      "Epoch 4284, Train Loss: 0.6374, Test Loss: 0.5306\n",
      "Epoch 4285, Train Loss: 0.5764, Test Loss: 0.5325\n",
      "Epoch 4286, Train Loss: 0.5657, Test Loss: 0.5345\n",
      "Epoch 4287, Train Loss: 0.6201, Test Loss: 0.5363\n",
      "Epoch 4288, Train Loss: 0.5083, Test Loss: 0.5370\n",
      "Epoch 4289, Train Loss: 0.6215, Test Loss: 0.5378\n",
      "Epoch 4290, Train Loss: 0.5955, Test Loss: 0.5389\n",
      "Epoch 4291, Train Loss: 0.6166, Test Loss: 0.5402\n",
      "Epoch 4292, Train Loss: 0.4471, Test Loss: 0.5404\n",
      "Epoch 4293, Train Loss: 0.6631, Test Loss: 0.5406\n",
      "Epoch 4294, Train Loss: 0.4655, Test Loss: 0.5411\n",
      "Epoch 4295, Train Loss: 0.5456, Test Loss: 0.5421\n",
      "Epoch 4296, Train Loss: 0.5273, Test Loss: 0.5419\n",
      "Epoch 4297, Train Loss: 0.4574, Test Loss: 0.5422\n",
      "Epoch 4298, Train Loss: 0.6001, Test Loss: 0.5431\n",
      "Epoch 4299, Train Loss: 0.4715, Test Loss: 0.5439\n",
      "Epoch 4300, Train Loss: 0.6005, Test Loss: 0.5455\n",
      "Epoch 4301, Train Loss: 0.4683, Test Loss: 0.5468\n",
      "Epoch 4302, Train Loss: 0.4717, Test Loss: 0.5471\n",
      "Epoch 4303, Train Loss: 0.5597, Test Loss: 0.5467\n",
      "Epoch 4304, Train Loss: 0.6446, Test Loss: 0.5472\n",
      "Epoch 4305, Train Loss: 0.5317, Test Loss: 0.5474\n",
      "Epoch 4306, Train Loss: 0.5529, Test Loss: 0.5457\n",
      "Epoch 4307, Train Loss: 0.6992, Test Loss: 0.5433\n",
      "Epoch 4308, Train Loss: 0.4968, Test Loss: 0.5414\n",
      "Epoch 4309, Train Loss: 0.5165, Test Loss: 0.5399\n",
      "Epoch 4310, Train Loss: 0.6104, Test Loss: 0.5383\n",
      "Epoch 4311, Train Loss: 0.5270, Test Loss: 0.5369\n",
      "Epoch 4312, Train Loss: 0.6191, Test Loss: 0.5347\n",
      "Epoch 4313, Train Loss: 0.5289, Test Loss: 0.5322\n",
      "Epoch 4314, Train Loss: 0.5797, Test Loss: 0.5281\n",
      "Epoch 4315, Train Loss: 0.4872, Test Loss: 0.5259\n",
      "Epoch 4316, Train Loss: 0.6483, Test Loss: 0.5242\n",
      "Epoch 4317, Train Loss: 0.4685, Test Loss: 0.5226\n",
      "Epoch 4318, Train Loss: 0.4731, Test Loss: 0.5222\n",
      "Epoch 4319, Train Loss: 0.4929, Test Loss: 0.5218\n",
      "Epoch 4320, Train Loss: 0.6038, Test Loss: 0.5214\n",
      "Epoch 4321, Train Loss: 0.6228, Test Loss: 0.5218\n",
      "Epoch 4322, Train Loss: 0.5262, Test Loss: 0.5224\n",
      "Epoch 4323, Train Loss: 0.5002, Test Loss: 0.5229\n",
      "Epoch 4324, Train Loss: 0.4435, Test Loss: 0.5242\n",
      "Epoch 4325, Train Loss: 0.4749, Test Loss: 0.5256\n",
      "Epoch 4326, Train Loss: 0.6073, Test Loss: 0.5275\n",
      "Epoch 4327, Train Loss: 0.4476, Test Loss: 0.5301\n",
      "Epoch 4328, Train Loss: 0.5178, Test Loss: 0.5310\n",
      "Epoch 4329, Train Loss: 0.4559, Test Loss: 0.5329\n",
      "Epoch 4330, Train Loss: 0.5027, Test Loss: 0.5344\n",
      "Epoch 4331, Train Loss: 0.4625, Test Loss: 0.5348\n",
      "Epoch 4332, Train Loss: 0.6021, Test Loss: 0.5353\n",
      "Epoch 4333, Train Loss: 0.4121, Test Loss: 0.5349\n",
      "Epoch 4334, Train Loss: 0.5428, Test Loss: 0.5340\n",
      "Epoch 4335, Train Loss: 0.5154, Test Loss: 0.5334\n",
      "Epoch 4336, Train Loss: 0.4812, Test Loss: 0.5311\n",
      "Epoch 4337, Train Loss: 0.5731, Test Loss: 0.5304\n",
      "Epoch 4338, Train Loss: 0.5097, Test Loss: 0.5303\n",
      "Epoch 4339, Train Loss: 0.5638, Test Loss: 0.5298\n",
      "Epoch 4340, Train Loss: 0.5062, Test Loss: 0.5284\n",
      "Epoch 4341, Train Loss: 0.4686, Test Loss: 0.5277\n",
      "Epoch 4342, Train Loss: 0.5092, Test Loss: 0.5261\n",
      "Epoch 4343, Train Loss: 0.4728, Test Loss: 0.5240\n",
      "Epoch 4344, Train Loss: 0.5047, Test Loss: 0.5220\n",
      "Epoch 4345, Train Loss: 0.5298, Test Loss: 0.5210\n",
      "Epoch 4346, Train Loss: 0.4244, Test Loss: 0.5206\n",
      "Epoch 4347, Train Loss: 0.5153, Test Loss: 0.5212\n",
      "Epoch 4348, Train Loss: 0.4106, Test Loss: 0.5200\n",
      "Epoch 4349, Train Loss: 0.5511, Test Loss: 0.5184\n",
      "Epoch 4350, Train Loss: 0.5758, Test Loss: 0.5166\n",
      "Epoch 4351, Train Loss: 0.4168, Test Loss: 0.5145\n",
      "Epoch 4352, Train Loss: 0.5636, Test Loss: 0.5133\n",
      "Epoch 4353, Train Loss: 0.5127, Test Loss: 0.5127\n",
      "Epoch 4354, Train Loss: 0.4919, Test Loss: 0.5132\n",
      "Epoch 4355, Train Loss: 0.5115, Test Loss: 0.5144\n",
      "Epoch 4356, Train Loss: 0.5150, Test Loss: 0.5165\n",
      "Epoch 4357, Train Loss: 0.4699, Test Loss: 0.5176\n",
      "Epoch 4358, Train Loss: 0.5569, Test Loss: 0.5173\n",
      "Epoch 4359, Train Loss: 0.5123, Test Loss: 0.5172\n",
      "Epoch 4360, Train Loss: 0.4832, Test Loss: 0.5178\n",
      "Epoch 4361, Train Loss: 0.5667, Test Loss: 0.5183\n",
      "Epoch 4362, Train Loss: 0.4887, Test Loss: 0.5187\n",
      "Epoch 4363, Train Loss: 0.5132, Test Loss: 0.5189\n",
      "Epoch 4364, Train Loss: 0.4904, Test Loss: 0.5186\n",
      "Epoch 4365, Train Loss: 0.5320, Test Loss: 0.5181\n",
      "Epoch 4366, Train Loss: 0.5299, Test Loss: 0.5181\n",
      "Epoch 4367, Train Loss: 0.4974, Test Loss: 0.5184\n",
      "Epoch 4368, Train Loss: 0.4131, Test Loss: 0.5179\n",
      "Epoch 4369, Train Loss: 0.5078, Test Loss: 0.5174\n",
      "Epoch 4370, Train Loss: 0.4229, Test Loss: 0.5165\n",
      "Epoch 4371, Train Loss: 0.5139, Test Loss: 0.5159\n",
      "Epoch 4372, Train Loss: 0.4053, Test Loss: 0.5164\n",
      "Epoch 4373, Train Loss: 0.4787, Test Loss: 0.5172\n",
      "Epoch 4374, Train Loss: 0.4831, Test Loss: 0.5181\n",
      "Epoch 4375, Train Loss: 0.4948, Test Loss: 0.5194\n",
      "Epoch 4376, Train Loss: 0.4802, Test Loss: 0.5207\n",
      "Epoch 4377, Train Loss: 0.4866, Test Loss: 0.5215\n",
      "Epoch 4378, Train Loss: 0.4834, Test Loss: 0.5224\n",
      "Epoch 4379, Train Loss: 0.4546, Test Loss: 0.5224\n",
      "Epoch 4380, Train Loss: 0.6006, Test Loss: 0.5229\n",
      "Epoch 4381, Train Loss: 0.4581, Test Loss: 0.5228\n",
      "Epoch 4382, Train Loss: 0.5901, Test Loss: 0.5221\n",
      "Epoch 4383, Train Loss: 0.5324, Test Loss: 0.5209\n",
      "Epoch 4384, Train Loss: 0.4502, Test Loss: 0.5203\n",
      "Epoch 4385, Train Loss: 0.4505, Test Loss: 0.5192\n",
      "Epoch 4386, Train Loss: 0.4643, Test Loss: 0.5187\n",
      "Epoch 4387, Train Loss: 0.5426, Test Loss: 0.5176\n",
      "Epoch 4388, Train Loss: 0.4086, Test Loss: 0.5166\n",
      "Epoch 4389, Train Loss: 0.4676, Test Loss: 0.5159\n",
      "Epoch 4390, Train Loss: 0.4537, Test Loss: 0.5143\n",
      "Epoch 4391, Train Loss: 0.5022, Test Loss: 0.5128\n",
      "Epoch 4392, Train Loss: 0.4528, Test Loss: 0.5108\n",
      "Epoch 4393, Train Loss: 0.5032, Test Loss: 0.5091\n",
      "Epoch 4394, Train Loss: 0.4772, Test Loss: 0.5074\n",
      "Epoch 4395, Train Loss: 0.4299, Test Loss: 0.5066\n",
      "Epoch 4396, Train Loss: 0.4867, Test Loss: 0.5063\n",
      "Epoch 4397, Train Loss: 0.5314, Test Loss: 0.5064\n",
      "Epoch 4398, Train Loss: 0.4961, Test Loss: 0.5062\n",
      "Epoch 4399, Train Loss: 0.4799, Test Loss: 0.5059\n",
      "Epoch 4400, Train Loss: 0.4721, Test Loss: 0.5058\n",
      "Epoch 4401, Train Loss: 0.4020, Test Loss: 0.5062\n",
      "Epoch 4402, Train Loss: 0.4153, Test Loss: 0.5067\n",
      "Epoch 4403, Train Loss: 0.5650, Test Loss: 0.5079\n",
      "Epoch 4404, Train Loss: 0.4799, Test Loss: 0.5098\n",
      "Epoch 4405, Train Loss: 0.4508, Test Loss: 0.5126\n",
      "Epoch 4406, Train Loss: 0.4411, Test Loss: 0.5139\n",
      "Epoch 4407, Train Loss: 0.4155, Test Loss: 0.5143\n",
      "Epoch 4408, Train Loss: 0.4606, Test Loss: 0.5144\n",
      "Epoch 4409, Train Loss: 0.4691, Test Loss: 0.5150\n",
      "Epoch 4410, Train Loss: 0.4780, Test Loss: 0.5155\n",
      "Epoch 4411, Train Loss: 0.4032, Test Loss: 0.5154\n",
      "Epoch 4412, Train Loss: 0.5453, Test Loss: 0.5154\n",
      "Epoch 4413, Train Loss: 0.5620, Test Loss: 0.5143\n",
      "Epoch 4414, Train Loss: 0.4787, Test Loss: 0.5128\n",
      "Epoch 4415, Train Loss: 0.4023, Test Loss: 0.5117\n",
      "Epoch 4416, Train Loss: 0.4643, Test Loss: 0.5108\n",
      "Epoch 4417, Train Loss: 0.5436, Test Loss: 0.5090\n",
      "Epoch 4418, Train Loss: 0.4388, Test Loss: 0.5074\n",
      "Epoch 4419, Train Loss: 0.4923, Test Loss: 0.5069\n",
      "Epoch 4420, Train Loss: 0.5146, Test Loss: 0.5066\n",
      "Epoch 4421, Train Loss: 0.4503, Test Loss: 0.5067\n",
      "Epoch 4422, Train Loss: 0.4756, Test Loss: 0.5065\n",
      "Epoch 4423, Train Loss: 0.4460, Test Loss: 0.5060\n",
      "Epoch 4424, Train Loss: 0.4597, Test Loss: 0.5056\n",
      "Epoch 4425, Train Loss: 0.4282, Test Loss: 0.5069\n",
      "Epoch 4426, Train Loss: 0.5696, Test Loss: 0.5072\n",
      "Epoch 4427, Train Loss: 0.5284, Test Loss: 0.5079\n",
      "Epoch 4428, Train Loss: 0.4345, Test Loss: 0.5082\n",
      "Epoch 4429, Train Loss: 0.5602, Test Loss: 0.5090\n",
      "Epoch 4430, Train Loss: 0.4989, Test Loss: 0.5090\n",
      "Epoch 4431, Train Loss: 0.6119, Test Loss: 0.5090\n",
      "Epoch 4432, Train Loss: 0.4932, Test Loss: 0.5099\n",
      "Epoch 4433, Train Loss: 0.4669, Test Loss: 0.5106\n",
      "Epoch 4434, Train Loss: 0.4320, Test Loss: 0.5117\n",
      "Epoch 4435, Train Loss: 0.4680, Test Loss: 0.5121\n",
      "Epoch 4436, Train Loss: 0.4593, Test Loss: 0.5127\n",
      "Epoch 4437, Train Loss: 0.4938, Test Loss: 0.5124\n",
      "Epoch 4438, Train Loss: 0.4891, Test Loss: 0.5112\n",
      "Epoch 4439, Train Loss: 0.4811, Test Loss: 0.5104\n",
      "Epoch 4440, Train Loss: 0.4597, Test Loss: 0.5098\n",
      "Epoch 4441, Train Loss: 0.4597, Test Loss: 0.5078\n",
      "Epoch 4442, Train Loss: 0.4849, Test Loss: 0.5058\n",
      "Epoch 4443, Train Loss: 0.4820, Test Loss: 0.5030\n",
      "Epoch 4444, Train Loss: 0.4677, Test Loss: 0.5014\n",
      "Epoch 4445, Train Loss: 0.4137, Test Loss: 0.5013\n",
      "Epoch 4446, Train Loss: 0.5206, Test Loss: 0.5006\n",
      "Epoch 4447, Train Loss: 0.4284, Test Loss: 0.4996\n",
      "Epoch 4448, Train Loss: 0.4562, Test Loss: 0.4994\n",
      "Epoch 4449, Train Loss: 0.5043, Test Loss: 0.4992\n",
      "Epoch 4450, Train Loss: 0.4522, Test Loss: 0.4986\n",
      "Epoch 4451, Train Loss: 0.5056, Test Loss: 0.4969\n",
      "Epoch 4452, Train Loss: 0.4769, Test Loss: 0.4960\n",
      "Epoch 4453, Train Loss: 0.4560, Test Loss: 0.4960\n",
      "Epoch 4454, Train Loss: 0.5263, Test Loss: 0.4967\n",
      "Epoch 4455, Train Loss: 0.4360, Test Loss: 0.4974\n",
      "Epoch 4456, Train Loss: 0.4532, Test Loss: 0.4965\n",
      "Epoch 4457, Train Loss: 0.4283, Test Loss: 0.4962\n",
      "Epoch 4458, Train Loss: 0.5237, Test Loss: 0.4968\n",
      "Epoch 4459, Train Loss: 0.4217, Test Loss: 0.4967\n",
      "Epoch 4460, Train Loss: 0.4008, Test Loss: 0.4966\n",
      "Epoch 4461, Train Loss: 0.4476, Test Loss: 0.4967\n",
      "Epoch 4462, Train Loss: 0.4470, Test Loss: 0.4976\n",
      "Epoch 4463, Train Loss: 0.4653, Test Loss: 0.4980\n",
      "Epoch 4464, Train Loss: 0.5170, Test Loss: 0.4980\n",
      "Epoch 4465, Train Loss: 0.4781, Test Loss: 0.4980\n",
      "Epoch 4466, Train Loss: 0.3782, Test Loss: 0.4982\n",
      "Epoch 4467, Train Loss: 0.5622, Test Loss: 0.4986\n",
      "Epoch 4468, Train Loss: 0.4786, Test Loss: 0.4986\n",
      "Epoch 4469, Train Loss: 0.4311, Test Loss: 0.4987\n",
      "Epoch 4470, Train Loss: 0.4787, Test Loss: 0.4983\n",
      "Epoch 4471, Train Loss: 0.4482, Test Loss: 0.4979\n",
      "Epoch 4472, Train Loss: 0.4671, Test Loss: 0.4978\n",
      "Epoch 4473, Train Loss: 0.4869, Test Loss: 0.4977\n",
      "Epoch 4474, Train Loss: 0.4038, Test Loss: 0.4970\n",
      "Epoch 4475, Train Loss: 0.4810, Test Loss: 0.4963\n",
      "Epoch 4476, Train Loss: 0.4587, Test Loss: 0.4958\n",
      "Epoch 4477, Train Loss: 0.4298, Test Loss: 0.4945\n",
      "Epoch 4478, Train Loss: 0.5351, Test Loss: 0.4930\n",
      "Epoch 4479, Train Loss: 0.4065, Test Loss: 0.4916\n",
      "Epoch 4480, Train Loss: 0.4385, Test Loss: 0.4907\n",
      "Epoch 4481, Train Loss: 0.4224, Test Loss: 0.4901\n",
      "Epoch 4482, Train Loss: 0.4140, Test Loss: 0.4893\n",
      "Epoch 4483, Train Loss: 0.4373, Test Loss: 0.4888\n",
      "Epoch 4484, Train Loss: 0.3595, Test Loss: 0.4873\n",
      "Epoch 4485, Train Loss: 0.4155, Test Loss: 0.4859\n",
      "Epoch 4486, Train Loss: 0.4439, Test Loss: 0.4855\n",
      "Epoch 4487, Train Loss: 0.5243, Test Loss: 0.4848\n",
      "Epoch 4488, Train Loss: 0.3996, Test Loss: 0.4848\n",
      "Epoch 4489, Train Loss: 0.4488, Test Loss: 0.4842\n",
      "Epoch 4490, Train Loss: 0.3954, Test Loss: 0.4845\n",
      "Epoch 4491, Train Loss: 0.3937, Test Loss: 0.4849\n",
      "Epoch 4492, Train Loss: 0.4394, Test Loss: 0.4857\n",
      "Epoch 4493, Train Loss: 0.5037, Test Loss: 0.4870\n",
      "Epoch 4494, Train Loss: 0.4698, Test Loss: 0.4884\n",
      "Epoch 4495, Train Loss: 0.4384, Test Loss: 0.4888\n",
      "Epoch 4496, Train Loss: 0.4625, Test Loss: 0.4883\n",
      "Epoch 4497, Train Loss: 0.5301, Test Loss: 0.4886\n",
      "Epoch 4498, Train Loss: 0.4098, Test Loss: 0.4879\n",
      "Epoch 4499, Train Loss: 0.4787, Test Loss: 0.4871\n",
      "Epoch 4500, Train Loss: 0.4663, Test Loss: 0.4859\n",
      "Epoch 4501, Train Loss: 0.4226, Test Loss: 0.4845\n",
      "Epoch 4502, Train Loss: 0.4996, Test Loss: 0.4829\n",
      "Epoch 4503, Train Loss: 0.4556, Test Loss: 0.4814\n",
      "Epoch 4504, Train Loss: 0.4010, Test Loss: 0.4804\n",
      "Epoch 4505, Train Loss: 0.3439, Test Loss: 0.4797\n",
      "Epoch 4506, Train Loss: 0.3802, Test Loss: 0.4786\n",
      "Epoch 4507, Train Loss: 0.4337, Test Loss: 0.4778\n",
      "Epoch 4508, Train Loss: 0.3923, Test Loss: 0.4772\n",
      "Epoch 4509, Train Loss: 0.4965, Test Loss: 0.4774\n",
      "Epoch 4510, Train Loss: 0.3479, Test Loss: 0.4772\n",
      "Epoch 4511, Train Loss: 0.3768, Test Loss: 0.4768\n",
      "Epoch 4512, Train Loss: 0.4868, Test Loss: 0.4773\n",
      "Epoch 4513, Train Loss: 0.4034, Test Loss: 0.4779\n",
      "Epoch 4514, Train Loss: 0.4585, Test Loss: 0.4790\n",
      "Epoch 4515, Train Loss: 0.4403, Test Loss: 0.4800\n",
      "Epoch 4516, Train Loss: 0.3980, Test Loss: 0.4802\n",
      "Epoch 4517, Train Loss: 0.4702, Test Loss: 0.4815\n",
      "Epoch 4518, Train Loss: 0.4614, Test Loss: 0.4829\n",
      "Epoch 4519, Train Loss: 0.3743, Test Loss: 0.4840\n",
      "Epoch 4520, Train Loss: 0.4244, Test Loss: 0.4853\n",
      "Epoch 4521, Train Loss: 0.3921, Test Loss: 0.4864\n",
      "Epoch 4522, Train Loss: 0.4120, Test Loss: 0.4847\n",
      "Epoch 4523, Train Loss: 0.3962, Test Loss: 0.4827\n",
      "Epoch 4524, Train Loss: 0.3942, Test Loss: 0.4804\n",
      "Epoch 4525, Train Loss: 0.4467, Test Loss: 0.4783\n",
      "Epoch 4526, Train Loss: 0.4005, Test Loss: 0.4767\n",
      "Epoch 4527, Train Loss: 0.4235, Test Loss: 0.4752\n",
      "Epoch 4528, Train Loss: 0.3918, Test Loss: 0.4740\n",
      "Epoch 4529, Train Loss: 0.4250, Test Loss: 0.4728\n",
      "Epoch 4530, Train Loss: 0.4468, Test Loss: 0.4717\n",
      "Epoch 4531, Train Loss: 0.3892, Test Loss: 0.4713\n",
      "Epoch 4532, Train Loss: 0.3691, Test Loss: 0.4712\n",
      "Epoch 4533, Train Loss: 0.5336, Test Loss: 0.4717\n",
      "Epoch 4534, Train Loss: 0.4305, Test Loss: 0.4730\n",
      "Epoch 4535, Train Loss: 0.4247, Test Loss: 0.4746\n",
      "Epoch 4536, Train Loss: 0.3187, Test Loss: 0.4757\n",
      "Epoch 4537, Train Loss: 0.3326, Test Loss: 0.4765\n",
      "Epoch 4538, Train Loss: 0.5021, Test Loss: 0.4772\n",
      "Epoch 4539, Train Loss: 0.3968, Test Loss: 0.4779\n",
      "Epoch 4540, Train Loss: 0.4637, Test Loss: 0.4789\n",
      "Epoch 4541, Train Loss: 0.5189, Test Loss: 0.4789\n",
      "Epoch 4542, Train Loss: 0.4036, Test Loss: 0.4792\n",
      "Epoch 4543, Train Loss: 0.4869, Test Loss: 0.4798\n",
      "Epoch 4544, Train Loss: 0.4416, Test Loss: 0.4795\n",
      "Epoch 4545, Train Loss: 0.3655, Test Loss: 0.4781\n",
      "Epoch 4546, Train Loss: 0.3940, Test Loss: 0.4778\n",
      "Epoch 4547, Train Loss: 0.3435, Test Loss: 0.4778\n",
      "Epoch 4548, Train Loss: 0.3817, Test Loss: 0.4778\n",
      "Epoch 4549, Train Loss: 0.3719, Test Loss: 0.4779\n",
      "Epoch 4550, Train Loss: 0.4021, Test Loss: 0.4785\n",
      "Epoch 4551, Train Loss: 0.3981, Test Loss: 0.4781\n",
      "Epoch 4552, Train Loss: 0.4985, Test Loss: 0.4782\n",
      "Epoch 4553, Train Loss: 0.4510, Test Loss: 0.4779\n",
      "Epoch 4554, Train Loss: 0.3704, Test Loss: 0.4772\n",
      "Epoch 4555, Train Loss: 0.3867, Test Loss: 0.4765\n",
      "Epoch 4556, Train Loss: 0.4677, Test Loss: 0.4758\n",
      "Epoch 4557, Train Loss: 0.4750, Test Loss: 0.4742\n",
      "Epoch 4558, Train Loss: 0.3928, Test Loss: 0.4736\n",
      "Epoch 4559, Train Loss: 0.4750, Test Loss: 0.4733\n",
      "Epoch 4560, Train Loss: 0.3914, Test Loss: 0.4732\n",
      "Epoch 4561, Train Loss: 0.3449, Test Loss: 0.4733\n",
      "Epoch 4562, Train Loss: 0.4574, Test Loss: 0.4726\n",
      "Epoch 4563, Train Loss: 0.3820, Test Loss: 0.4714\n",
      "Epoch 4564, Train Loss: 0.4256, Test Loss: 0.4704\n",
      "Epoch 4565, Train Loss: 0.4759, Test Loss: 0.4694\n",
      "Epoch 4566, Train Loss: 0.4294, Test Loss: 0.4690\n",
      "Epoch 4567, Train Loss: 0.4656, Test Loss: 0.4686\n",
      "Epoch 4568, Train Loss: 0.3705, Test Loss: 0.4680\n",
      "Epoch 4569, Train Loss: 0.4902, Test Loss: 0.4673\n",
      "Epoch 4570, Train Loss: 0.4096, Test Loss: 0.4667\n",
      "Epoch 4571, Train Loss: 0.3776, Test Loss: 0.4664\n",
      "Epoch 4572, Train Loss: 0.4205, Test Loss: 0.4660\n",
      "Epoch 4573, Train Loss: 0.4056, Test Loss: 0.4656\n",
      "Epoch 4574, Train Loss: 0.3681, Test Loss: 0.4661\n",
      "Epoch 4575, Train Loss: 0.4137, Test Loss: 0.4666\n",
      "Epoch 4576, Train Loss: 0.4405, Test Loss: 0.4679\n",
      "Epoch 4577, Train Loss: 0.4791, Test Loss: 0.4690\n",
      "Epoch 4578, Train Loss: 0.3936, Test Loss: 0.4700\n",
      "Epoch 4579, Train Loss: 0.4462, Test Loss: 0.4695\n",
      "Epoch 4580, Train Loss: 0.4308, Test Loss: 0.4692\n",
      "Epoch 4581, Train Loss: 0.4299, Test Loss: 0.4695\n",
      "Epoch 4582, Train Loss: 0.2976, Test Loss: 0.4698\n",
      "Epoch 4583, Train Loss: 0.4384, Test Loss: 0.4698\n",
      "Epoch 4584, Train Loss: 0.3996, Test Loss: 0.4698\n",
      "Epoch 4585, Train Loss: 0.4594, Test Loss: 0.4695\n",
      "Epoch 4586, Train Loss: 0.4641, Test Loss: 0.4689\n",
      "Epoch 4587, Train Loss: 0.4341, Test Loss: 0.4681\n",
      "Epoch 4588, Train Loss: 0.3709, Test Loss: 0.4672\n",
      "Epoch 4589, Train Loss: 0.4226, Test Loss: 0.4665\n",
      "Epoch 4590, Train Loss: 0.3501, Test Loss: 0.4662\n",
      "Epoch 4591, Train Loss: 0.4481, Test Loss: 0.4658\n",
      "Epoch 4592, Train Loss: 0.3660, Test Loss: 0.4653\n",
      "Epoch 4593, Train Loss: 0.4182, Test Loss: 0.4649\n",
      "Epoch 4594, Train Loss: 0.4088, Test Loss: 0.4644\n",
      "Epoch 4595, Train Loss: 0.4045, Test Loss: 0.4641\n",
      "Epoch 4596, Train Loss: 0.4104, Test Loss: 0.4634\n",
      "Epoch 4597, Train Loss: 0.4458, Test Loss: 0.4632\n",
      "Epoch 4598, Train Loss: 0.4353, Test Loss: 0.4629\n",
      "Epoch 4599, Train Loss: 0.4642, Test Loss: 0.4628\n",
      "Epoch 4600, Train Loss: 0.3900, Test Loss: 0.4623\n",
      "Epoch 4601, Train Loss: 0.4187, Test Loss: 0.4622\n",
      "Epoch 4602, Train Loss: 0.3973, Test Loss: 0.4624\n",
      "Epoch 4603, Train Loss: 0.4401, Test Loss: 0.4627\n",
      "Epoch 4604, Train Loss: 0.4222, Test Loss: 0.4628\n",
      "Epoch 4605, Train Loss: 0.3567, Test Loss: 0.4628\n",
      "Epoch 4606, Train Loss: 0.3582, Test Loss: 0.4628\n",
      "Epoch 4607, Train Loss: 0.4313, Test Loss: 0.4625\n",
      "Epoch 4608, Train Loss: 0.3918, Test Loss: 0.4623\n",
      "Epoch 4609, Train Loss: 0.4194, Test Loss: 0.4629\n",
      "Epoch 4610, Train Loss: 0.3888, Test Loss: 0.4627\n",
      "Epoch 4611, Train Loss: 0.4381, Test Loss: 0.4624\n",
      "Epoch 4612, Train Loss: 0.3663, Test Loss: 0.4612\n",
      "Epoch 4613, Train Loss: 0.4314, Test Loss: 0.4603\n",
      "Epoch 4614, Train Loss: 0.3984, Test Loss: 0.4592\n",
      "Epoch 4615, Train Loss: 0.4244, Test Loss: 0.4587\n",
      "Epoch 4616, Train Loss: 0.3808, Test Loss: 0.4583\n",
      "Epoch 4617, Train Loss: 0.4374, Test Loss: 0.4588\n",
      "Epoch 4618, Train Loss: 0.4087, Test Loss: 0.4592\n",
      "Epoch 4619, Train Loss: 0.4152, Test Loss: 0.4593\n",
      "Epoch 4620, Train Loss: 0.4567, Test Loss: 0.4599\n",
      "Epoch 4621, Train Loss: 0.3146, Test Loss: 0.4605\n",
      "Epoch 4622, Train Loss: 0.4710, Test Loss: 0.4614\n",
      "Epoch 4623, Train Loss: 0.4533, Test Loss: 0.4630\n",
      "Epoch 4624, Train Loss: 0.3740, Test Loss: 0.4649\n",
      "Epoch 4625, Train Loss: 0.3871, Test Loss: 0.4661\n",
      "Epoch 4626, Train Loss: 0.3876, Test Loss: 0.4665\n",
      "Epoch 4627, Train Loss: 0.4427, Test Loss: 0.4672\n",
      "Epoch 4628, Train Loss: 0.3980, Test Loss: 0.4687\n",
      "Epoch 4629, Train Loss: 0.4487, Test Loss: 0.4690\n",
      "Epoch 4630, Train Loss: 0.4266, Test Loss: 0.4688\n",
      "Epoch 4631, Train Loss: 0.4183, Test Loss: 0.4670\n",
      "Epoch 4632, Train Loss: 0.3490, Test Loss: 0.4656\n",
      "Epoch 4633, Train Loss: 0.4263, Test Loss: 0.4636\n",
      "Epoch 4634, Train Loss: 0.3925, Test Loss: 0.4619\n",
      "Epoch 4635, Train Loss: 0.3853, Test Loss: 0.4604\n",
      "Epoch 4636, Train Loss: 0.3787, Test Loss: 0.4591\n",
      "Epoch 4637, Train Loss: 0.4270, Test Loss: 0.4584\n",
      "Epoch 4638, Train Loss: 0.3794, Test Loss: 0.4581\n",
      "Epoch 4639, Train Loss: 0.4390, Test Loss: 0.4582\n",
      "Epoch 4640, Train Loss: 0.3742, Test Loss: 0.4580\n",
      "Epoch 4641, Train Loss: 0.3603, Test Loss: 0.4578\n",
      "Epoch 4642, Train Loss: 0.3398, Test Loss: 0.4570\n",
      "Epoch 4643, Train Loss: 0.4161, Test Loss: 0.4564\n",
      "Epoch 4644, Train Loss: 0.4208, Test Loss: 0.4563\n",
      "Epoch 4645, Train Loss: 0.3905, Test Loss: 0.4561\n",
      "Epoch 4646, Train Loss: 0.3781, Test Loss: 0.4556\n",
      "Epoch 4647, Train Loss: 0.4408, Test Loss: 0.4558\n",
      "Epoch 4648, Train Loss: 0.3589, Test Loss: 0.4558\n",
      "Epoch 4649, Train Loss: 0.3701, Test Loss: 0.4561\n",
      "Epoch 4650, Train Loss: 0.4168, Test Loss: 0.4563\n",
      "Epoch 4651, Train Loss: 0.4193, Test Loss: 0.4566\n",
      "Epoch 4652, Train Loss: 0.3628, Test Loss: 0.4571\n",
      "Epoch 4653, Train Loss: 0.4325, Test Loss: 0.4583\n",
      "Epoch 4654, Train Loss: 0.4311, Test Loss: 0.4583\n",
      "Epoch 4655, Train Loss: 0.3335, Test Loss: 0.4586\n",
      "Epoch 4656, Train Loss: 0.4293, Test Loss: 0.4580\n",
      "Epoch 4657, Train Loss: 0.3892, Test Loss: 0.4578\n",
      "Epoch 4658, Train Loss: 0.4311, Test Loss: 0.4585\n",
      "Epoch 4659, Train Loss: 0.3252, Test Loss: 0.4586\n",
      "Epoch 4660, Train Loss: 0.3787, Test Loss: 0.4582\n",
      "Epoch 4661, Train Loss: 0.3791, Test Loss: 0.4576\n",
      "Epoch 4662, Train Loss: 0.3463, Test Loss: 0.4572\n",
      "Epoch 4663, Train Loss: 0.3522, Test Loss: 0.4568\n",
      "Epoch 4664, Train Loss: 0.3969, Test Loss: 0.4558\n",
      "Epoch 4665, Train Loss: 0.3077, Test Loss: 0.4550\n",
      "Epoch 4666, Train Loss: 0.3825, Test Loss: 0.4538\n",
      "Epoch 4667, Train Loss: 0.3512, Test Loss: 0.4523\n",
      "Epoch 4668, Train Loss: 0.3977, Test Loss: 0.4514\n",
      "Epoch 4669, Train Loss: 0.4389, Test Loss: 0.4509\n",
      "Epoch 4670, Train Loss: 0.3475, Test Loss: 0.4503\n",
      "Epoch 4671, Train Loss: 0.3495, Test Loss: 0.4500\n",
      "Epoch 4672, Train Loss: 0.4040, Test Loss: 0.4499\n",
      "Epoch 4673, Train Loss: 0.3469, Test Loss: 0.4502\n",
      "Epoch 4674, Train Loss: 0.4183, Test Loss: 0.4508\n",
      "Epoch 4675, Train Loss: 0.3310, Test Loss: 0.4520\n",
      "Epoch 4676, Train Loss: 0.3806, Test Loss: 0.4538\n",
      "Epoch 4677, Train Loss: 0.3895, Test Loss: 0.4551\n",
      "Epoch 4678, Train Loss: 0.4063, Test Loss: 0.4567\n",
      "Epoch 4679, Train Loss: 0.3894, Test Loss: 0.4580\n",
      "Epoch 4680, Train Loss: 0.4107, Test Loss: 0.4585\n",
      "Epoch 4681, Train Loss: 0.4027, Test Loss: 0.4578\n",
      "Epoch 4682, Train Loss: 0.4156, Test Loss: 0.4570\n",
      "Epoch 4683, Train Loss: 0.4341, Test Loss: 0.4550\n",
      "Epoch 4684, Train Loss: 0.4419, Test Loss: 0.4541\n",
      "Epoch 4685, Train Loss: 0.3315, Test Loss: 0.4537\n",
      "Epoch 4686, Train Loss: 0.3541, Test Loss: 0.4531\n",
      "Epoch 4687, Train Loss: 0.3412, Test Loss: 0.4523\n",
      "Epoch 4688, Train Loss: 0.4512, Test Loss: 0.4508\n",
      "Epoch 4689, Train Loss: 0.3276, Test Loss: 0.4495\n",
      "Epoch 4690, Train Loss: 0.3529, Test Loss: 0.4480\n",
      "Epoch 4691, Train Loss: 0.3721, Test Loss: 0.4475\n",
      "Epoch 4692, Train Loss: 0.3744, Test Loss: 0.4476\n",
      "Epoch 4693, Train Loss: 0.4171, Test Loss: 0.4488\n",
      "Epoch 4694, Train Loss: 0.4247, Test Loss: 0.4504\n",
      "Epoch 4695, Train Loss: 0.3485, Test Loss: 0.4510\n",
      "Epoch 4696, Train Loss: 0.4046, Test Loss: 0.4510\n",
      "Epoch 4697, Train Loss: 0.3388, Test Loss: 0.4509\n",
      "Epoch 4698, Train Loss: 0.3836, Test Loss: 0.4502\n",
      "Epoch 4699, Train Loss: 0.3887, Test Loss: 0.4494\n",
      "Epoch 4700, Train Loss: 0.4198, Test Loss: 0.4496\n",
      "Epoch 4701, Train Loss: 0.4699, Test Loss: 0.4496\n",
      "Epoch 4702, Train Loss: 0.3652, Test Loss: 0.4501\n",
      "Epoch 4703, Train Loss: 0.4259, Test Loss: 0.4512\n",
      "Epoch 4704, Train Loss: 0.3664, Test Loss: 0.4525\n",
      "Epoch 4705, Train Loss: 0.3757, Test Loss: 0.4530\n",
      "Epoch 4706, Train Loss: 0.3425, Test Loss: 0.4538\n",
      "Epoch 4707, Train Loss: 0.4166, Test Loss: 0.4530\n",
      "Epoch 4708, Train Loss: 0.4136, Test Loss: 0.4528\n",
      "Epoch 4709, Train Loss: 0.3175, Test Loss: 0.4526\n",
      "Epoch 4710, Train Loss: 0.3730, Test Loss: 0.4524\n",
      "Epoch 4711, Train Loss: 0.3068, Test Loss: 0.4521\n",
      "Epoch 4712, Train Loss: 0.4084, Test Loss: 0.4518\n",
      "Epoch 4713, Train Loss: 0.3156, Test Loss: 0.4518\n",
      "Epoch 4714, Train Loss: 0.3924, Test Loss: 0.4516\n",
      "Epoch 4715, Train Loss: 0.3588, Test Loss: 0.4507\n",
      "Epoch 4716, Train Loss: 0.3948, Test Loss: 0.4499\n",
      "Epoch 4717, Train Loss: 0.4125, Test Loss: 0.4488\n",
      "Epoch 4718, Train Loss: 0.3853, Test Loss: 0.4478\n",
      "Epoch 4719, Train Loss: 0.3986, Test Loss: 0.4468\n",
      "Epoch 4720, Train Loss: 0.3766, Test Loss: 0.4463\n",
      "Epoch 4721, Train Loss: 0.3789, Test Loss: 0.4463\n",
      "Epoch 4722, Train Loss: 0.3931, Test Loss: 0.4465\n",
      "Epoch 4723, Train Loss: 0.3439, Test Loss: 0.4472\n",
      "Epoch 4724, Train Loss: 0.3495, Test Loss: 0.4474\n",
      "Epoch 4725, Train Loss: 0.3633, Test Loss: 0.4474\n",
      "Epoch 4726, Train Loss: 0.3699, Test Loss: 0.4476\n",
      "Epoch 4727, Train Loss: 0.3959, Test Loss: 0.4477\n",
      "Epoch 4728, Train Loss: 0.3658, Test Loss: 0.4481\n",
      "Epoch 4729, Train Loss: 0.4464, Test Loss: 0.4492\n",
      "Epoch 4730, Train Loss: 0.3578, Test Loss: 0.4502\n",
      "Epoch 4731, Train Loss: 0.4164, Test Loss: 0.4513\n",
      "Epoch 4732, Train Loss: 0.3391, Test Loss: 0.4523\n",
      "Epoch 4733, Train Loss: 0.4121, Test Loss: 0.4530\n",
      "Epoch 4734, Train Loss: 0.3921, Test Loss: 0.4532\n",
      "Epoch 4735, Train Loss: 0.3549, Test Loss: 0.4533\n",
      "Epoch 4736, Train Loss: 0.3687, Test Loss: 0.4527\n",
      "Epoch 4737, Train Loss: 0.2915, Test Loss: 0.4527\n",
      "Epoch 4738, Train Loss: 0.2998, Test Loss: 0.4518\n",
      "Epoch 4739, Train Loss: 0.3836, Test Loss: 0.4507\n",
      "Epoch 4740, Train Loss: 0.3291, Test Loss: 0.4496\n",
      "Epoch 4741, Train Loss: 0.3975, Test Loss: 0.4490\n",
      "Epoch 4742, Train Loss: 0.3957, Test Loss: 0.4489\n",
      "Epoch 4743, Train Loss: 0.3810, Test Loss: 0.4486\n",
      "Epoch 4744, Train Loss: 0.2970, Test Loss: 0.4479\n",
      "Epoch 4745, Train Loss: 0.4247, Test Loss: 0.4483\n",
      "Epoch 4746, Train Loss: 0.3212, Test Loss: 0.4488\n",
      "Epoch 4747, Train Loss: 0.3444, Test Loss: 0.4493\n",
      "Epoch 4748, Train Loss: 0.3806, Test Loss: 0.4499\n",
      "Epoch 4749, Train Loss: 0.3613, Test Loss: 0.4500\n",
      "Epoch 4750, Train Loss: 0.3827, Test Loss: 0.4505\n",
      "Epoch 4751, Train Loss: 0.3129, Test Loss: 0.4506\n",
      "Epoch 4752, Train Loss: 0.3803, Test Loss: 0.4511\n",
      "Epoch 4753, Train Loss: 0.3449, Test Loss: 0.4513\n",
      "Epoch 4754, Train Loss: 0.3463, Test Loss: 0.4497\n",
      "Epoch 4755, Train Loss: 0.3343, Test Loss: 0.4477\n",
      "Epoch 4756, Train Loss: 0.3120, Test Loss: 0.4463\n",
      "Epoch 4757, Train Loss: 0.3624, Test Loss: 0.4455\n",
      "Epoch 4758, Train Loss: 0.3302, Test Loss: 0.4444\n",
      "Epoch 4759, Train Loss: 0.3472, Test Loss: 0.4440\n",
      "Epoch 4760, Train Loss: 0.3766, Test Loss: 0.4444\n",
      "Epoch 4761, Train Loss: 0.3864, Test Loss: 0.4446\n",
      "Epoch 4762, Train Loss: 0.3691, Test Loss: 0.4444\n",
      "Epoch 4763, Train Loss: 0.4074, Test Loss: 0.4442\n",
      "Epoch 4764, Train Loss: 0.3543, Test Loss: 0.4436\n",
      "Epoch 4765, Train Loss: 0.3693, Test Loss: 0.4424\n",
      "Epoch 4766, Train Loss: 0.3414, Test Loss: 0.4417\n",
      "Epoch 4767, Train Loss: 0.3240, Test Loss: 0.4412\n",
      "Epoch 4768, Train Loss: 0.3378, Test Loss: 0.4408\n",
      "Epoch 4769, Train Loss: 0.2900, Test Loss: 0.4406\n",
      "Epoch 4770, Train Loss: 0.3411, Test Loss: 0.4406\n",
      "Epoch 4771, Train Loss: 0.4010, Test Loss: 0.4402\n",
      "Epoch 4772, Train Loss: 0.3474, Test Loss: 0.4400\n",
      "Epoch 4773, Train Loss: 0.3518, Test Loss: 0.4400\n",
      "Epoch 4774, Train Loss: 0.3279, Test Loss: 0.4403\n",
      "Epoch 4775, Train Loss: 0.3313, Test Loss: 0.4406\n",
      "Epoch 4776, Train Loss: 0.3423, Test Loss: 0.4401\n",
      "Epoch 4777, Train Loss: 0.3677, Test Loss: 0.4402\n",
      "Epoch 4778, Train Loss: 0.3037, Test Loss: 0.4400\n",
      "Epoch 4779, Train Loss: 0.2872, Test Loss: 0.4401\n",
      "Epoch 4780, Train Loss: 0.3660, Test Loss: 0.4400\n",
      "Epoch 4781, Train Loss: 0.3535, Test Loss: 0.4395\n",
      "Epoch 4782, Train Loss: 0.3326, Test Loss: 0.4391\n",
      "Epoch 4783, Train Loss: 0.4103, Test Loss: 0.4378\n",
      "Epoch 4784, Train Loss: 0.3835, Test Loss: 0.4366\n",
      "Epoch 4785, Train Loss: 0.4400, Test Loss: 0.4355\n",
      "Epoch 4786, Train Loss: 0.3532, Test Loss: 0.4347\n",
      "Epoch 4787, Train Loss: 0.3374, Test Loss: 0.4341\n",
      "Epoch 4788, Train Loss: 0.3554, Test Loss: 0.4337\n",
      "Epoch 4789, Train Loss: 0.3520, Test Loss: 0.4337\n",
      "Epoch 4790, Train Loss: 0.3618, Test Loss: 0.4343\n",
      "Epoch 4791, Train Loss: 0.3148, Test Loss: 0.4350\n",
      "Epoch 4792, Train Loss: 0.3190, Test Loss: 0.4357\n",
      "Epoch 4793, Train Loss: 0.3408, Test Loss: 0.4365\n",
      "Epoch 4794, Train Loss: 0.3418, Test Loss: 0.4374\n",
      "Epoch 4795, Train Loss: 0.3765, Test Loss: 0.4378\n",
      "Epoch 4796, Train Loss: 0.3233, Test Loss: 0.4382\n",
      "Epoch 4797, Train Loss: 0.3367, Test Loss: 0.4373\n",
      "Epoch 4798, Train Loss: 0.3361, Test Loss: 0.4365\n",
      "Epoch 4799, Train Loss: 0.3319, Test Loss: 0.4360\n",
      "Epoch 4800, Train Loss: 0.3036, Test Loss: 0.4355\n",
      "Epoch 4801, Train Loss: 0.3315, Test Loss: 0.4350\n",
      "Epoch 4802, Train Loss: 0.4004, Test Loss: 0.4347\n",
      "Epoch 4803, Train Loss: 0.3677, Test Loss: 0.4340\n",
      "Epoch 4804, Train Loss: 0.3764, Test Loss: 0.4338\n",
      "Epoch 4805, Train Loss: 0.3753, Test Loss: 0.4333\n",
      "Epoch 4806, Train Loss: 0.3413, Test Loss: 0.4332\n",
      "Epoch 4807, Train Loss: 0.3425, Test Loss: 0.4329\n",
      "Epoch 4808, Train Loss: 0.3220, Test Loss: 0.4326\n",
      "Epoch 4809, Train Loss: 0.3405, Test Loss: 0.4319\n",
      "Epoch 4810, Train Loss: 0.3644, Test Loss: 0.4314\n",
      "Epoch 4811, Train Loss: 0.3299, Test Loss: 0.4312\n",
      "Epoch 4812, Train Loss: 0.3912, Test Loss: 0.4315\n",
      "Epoch 4813, Train Loss: 0.4222, Test Loss: 0.4319\n",
      "Epoch 4814, Train Loss: 0.3074, Test Loss: 0.4318\n",
      "Epoch 4815, Train Loss: 0.3760, Test Loss: 0.4315\n",
      "Epoch 4816, Train Loss: 0.3945, Test Loss: 0.4311\n",
      "Epoch 4817, Train Loss: 0.2853, Test Loss: 0.4310\n",
      "Epoch 4818, Train Loss: 0.3660, Test Loss: 0.4312\n",
      "Epoch 4819, Train Loss: 0.3445, Test Loss: 0.4322\n",
      "Epoch 4820, Train Loss: 0.4327, Test Loss: 0.4329\n",
      "Epoch 4821, Train Loss: 0.3395, Test Loss: 0.4338\n",
      "Epoch 4822, Train Loss: 0.3613, Test Loss: 0.4341\n",
      "Epoch 4823, Train Loss: 0.3355, Test Loss: 0.4344\n",
      "Epoch 4824, Train Loss: 0.3615, Test Loss: 0.4346\n",
      "Epoch 4825, Train Loss: 0.3039, Test Loss: 0.4345\n",
      "Epoch 4826, Train Loss: 0.3155, Test Loss: 0.4345\n",
      "Epoch 4827, Train Loss: 0.3232, Test Loss: 0.4347\n",
      "Epoch 4828, Train Loss: 0.3569, Test Loss: 0.4350\n",
      "Epoch 4829, Train Loss: 0.3562, Test Loss: 0.4351\n",
      "Epoch 4830, Train Loss: 0.2826, Test Loss: 0.4356\n",
      "Epoch 4831, Train Loss: 0.2974, Test Loss: 0.4361\n",
      "Epoch 4832, Train Loss: 0.2719, Test Loss: 0.4356\n",
      "Epoch 4833, Train Loss: 0.3442, Test Loss: 0.4342\n",
      "Epoch 4834, Train Loss: 0.2855, Test Loss: 0.4331\n",
      "Epoch 4835, Train Loss: 0.2901, Test Loss: 0.4323\n",
      "Epoch 4836, Train Loss: 0.3738, Test Loss: 0.4316\n",
      "Epoch 4837, Train Loss: 0.3466, Test Loss: 0.4310\n",
      "Epoch 4838, Train Loss: 0.3620, Test Loss: 0.4307\n",
      "Epoch 4839, Train Loss: 0.3754, Test Loss: 0.4305\n",
      "Epoch 4840, Train Loss: 0.3787, Test Loss: 0.4313\n",
      "Epoch 4841, Train Loss: 0.3205, Test Loss: 0.4323\n",
      "Epoch 4842, Train Loss: 0.2857, Test Loss: 0.4340\n",
      "Epoch 4843, Train Loss: 0.3185, Test Loss: 0.4351\n",
      "Epoch 4844, Train Loss: 0.3905, Test Loss: 0.4352\n",
      "Epoch 4845, Train Loss: 0.3293, Test Loss: 0.4350\n",
      "Epoch 4846, Train Loss: 0.3079, Test Loss: 0.4339\n",
      "Epoch 4847, Train Loss: 0.3808, Test Loss: 0.4333\n",
      "Epoch 4848, Train Loss: 0.3338, Test Loss: 0.4330\n",
      "Epoch 4849, Train Loss: 0.3405, Test Loss: 0.4336\n",
      "Epoch 4850, Train Loss: 0.4007, Test Loss: 0.4347\n",
      "Epoch 4851, Train Loss: 0.3427, Test Loss: 0.4346\n",
      "Epoch 4852, Train Loss: 0.2977, Test Loss: 0.4340\n",
      "Epoch 4853, Train Loss: 0.3514, Test Loss: 0.4332\n",
      "Epoch 4854, Train Loss: 0.3428, Test Loss: 0.4320\n",
      "Epoch 4855, Train Loss: 0.3523, Test Loss: 0.4309\n",
      "Epoch 4856, Train Loss: 0.3354, Test Loss: 0.4297\n",
      "Epoch 4857, Train Loss: 0.3174, Test Loss: 0.4288\n",
      "Epoch 4858, Train Loss: 0.3425, Test Loss: 0.4282\n",
      "Epoch 4859, Train Loss: 0.3369, Test Loss: 0.4275\n",
      "Epoch 4860, Train Loss: 0.2842, Test Loss: 0.4271\n",
      "Epoch 4861, Train Loss: 0.3623, Test Loss: 0.4271\n",
      "Epoch 4862, Train Loss: 0.3501, Test Loss: 0.4266\n",
      "Epoch 4863, Train Loss: 0.3136, Test Loss: 0.4262\n",
      "Epoch 4864, Train Loss: 0.3056, Test Loss: 0.4259\n",
      "Epoch 4865, Train Loss: 0.2944, Test Loss: 0.4260\n",
      "Epoch 4866, Train Loss: 0.3993, Test Loss: 0.4262\n",
      "Epoch 4867, Train Loss: 0.3518, Test Loss: 0.4267\n",
      "Epoch 4868, Train Loss: 0.3280, Test Loss: 0.4274\n",
      "Epoch 4869, Train Loss: 0.3414, Test Loss: 0.4285\n",
      "Epoch 4870, Train Loss: 0.3210, Test Loss: 0.4296\n",
      "Epoch 4871, Train Loss: 0.3390, Test Loss: 0.4305\n",
      "Epoch 4872, Train Loss: 0.3802, Test Loss: 0.4313\n",
      "Epoch 4873, Train Loss: 0.3028, Test Loss: 0.4322\n",
      "Epoch 4874, Train Loss: 0.3662, Test Loss: 0.4329\n",
      "Epoch 4875, Train Loss: 0.2939, Test Loss: 0.4327\n",
      "Epoch 4876, Train Loss: 0.3326, Test Loss: 0.4322\n",
      "Epoch 4877, Train Loss: 0.3313, Test Loss: 0.4318\n",
      "Epoch 4878, Train Loss: 0.2750, Test Loss: 0.4301\n",
      "Epoch 4879, Train Loss: 0.2820, Test Loss: 0.4287\n",
      "Epoch 4880, Train Loss: 0.3200, Test Loss: 0.4274\n",
      "Epoch 4881, Train Loss: 0.3347, Test Loss: 0.4262\n",
      "Epoch 4882, Train Loss: 0.3091, Test Loss: 0.4249\n",
      "Epoch 4883, Train Loss: 0.2831, Test Loss: 0.4243\n",
      "Epoch 4884, Train Loss: 0.3419, Test Loss: 0.4235\n",
      "Epoch 4885, Train Loss: 0.3070, Test Loss: 0.4228\n",
      "Epoch 4886, Train Loss: 0.3347, Test Loss: 0.4228\n",
      "Epoch 4887, Train Loss: 0.3480, Test Loss: 0.4228\n",
      "Epoch 4888, Train Loss: 0.3175, Test Loss: 0.4225\n",
      "Epoch 4889, Train Loss: 0.2637, Test Loss: 0.4222\n",
      "Epoch 4890, Train Loss: 0.3079, Test Loss: 0.4224\n",
      "Epoch 4891, Train Loss: 0.3395, Test Loss: 0.4231\n",
      "Epoch 4892, Train Loss: 0.2798, Test Loss: 0.4241\n",
      "Epoch 4893, Train Loss: 0.3145, Test Loss: 0.4253\n",
      "Epoch 4894, Train Loss: 0.3533, Test Loss: 0.4264\n",
      "Epoch 4895, Train Loss: 0.4163, Test Loss: 0.4264\n",
      "Epoch 4896, Train Loss: 0.3480, Test Loss: 0.4261\n",
      "Epoch 4897, Train Loss: 0.3166, Test Loss: 0.4254\n",
      "Epoch 4898, Train Loss: 0.3407, Test Loss: 0.4249\n",
      "Epoch 4899, Train Loss: 0.3122, Test Loss: 0.4240\n",
      "Epoch 4900, Train Loss: 0.2937, Test Loss: 0.4228\n",
      "Epoch 4901, Train Loss: 0.3354, Test Loss: 0.4216\n",
      "Epoch 4902, Train Loss: 0.3449, Test Loss: 0.4207\n",
      "Epoch 4903, Train Loss: 0.3213, Test Loss: 0.4204\n",
      "Epoch 4904, Train Loss: 0.3259, Test Loss: 0.4200\n",
      "Epoch 4905, Train Loss: 0.3291, Test Loss: 0.4200\n",
      "Epoch 4906, Train Loss: 0.3992, Test Loss: 0.4200\n",
      "Epoch 4907, Train Loss: 0.3246, Test Loss: 0.4194\n",
      "Epoch 4908, Train Loss: 0.2877, Test Loss: 0.4188\n",
      "Epoch 4909, Train Loss: 0.2974, Test Loss: 0.4188\n",
      "Epoch 4910, Train Loss: 0.3646, Test Loss: 0.4191\n",
      "Epoch 4911, Train Loss: 0.3455, Test Loss: 0.4202\n",
      "Epoch 4912, Train Loss: 0.3714, Test Loss: 0.4216\n",
      "Epoch 4913, Train Loss: 0.3231, Test Loss: 0.4234\n",
      "Epoch 4914, Train Loss: 0.3726, Test Loss: 0.4255\n",
      "Epoch 4915, Train Loss: 0.2988, Test Loss: 0.4268\n",
      "Epoch 4916, Train Loss: 0.3307, Test Loss: 0.4275\n",
      "Epoch 4917, Train Loss: 0.3021, Test Loss: 0.4281\n",
      "Epoch 4918, Train Loss: 0.3906, Test Loss: 0.4287\n",
      "Epoch 4919, Train Loss: 0.3328, Test Loss: 0.4294\n",
      "Epoch 4920, Train Loss: 0.2952, Test Loss: 0.4294\n",
      "Epoch 4921, Train Loss: 0.2837, Test Loss: 0.4279\n",
      "Epoch 4922, Train Loss: 0.3265, Test Loss: 0.4256\n",
      "Epoch 4923, Train Loss: 0.3385, Test Loss: 0.4233\n",
      "Epoch 4924, Train Loss: 0.3212, Test Loss: 0.4216\n",
      "Epoch 4925, Train Loss: 0.3097, Test Loss: 0.4203\n",
      "Epoch 4926, Train Loss: 0.3028, Test Loss: 0.4197\n",
      "Epoch 4927, Train Loss: 0.3004, Test Loss: 0.4190\n",
      "Epoch 4928, Train Loss: 0.3310, Test Loss: 0.4186\n",
      "Epoch 4929, Train Loss: 0.3095, Test Loss: 0.4181\n",
      "Epoch 4930, Train Loss: 0.2995, Test Loss: 0.4181\n",
      "Epoch 4931, Train Loss: 0.3173, Test Loss: 0.4184\n",
      "Epoch 4932, Train Loss: 0.3102, Test Loss: 0.4192\n",
      "Epoch 4933, Train Loss: 0.3407, Test Loss: 0.4200\n",
      "Epoch 4934, Train Loss: 0.3267, Test Loss: 0.4207\n",
      "Epoch 4935, Train Loss: 0.2811, Test Loss: 0.4214\n",
      "Epoch 4936, Train Loss: 0.3532, Test Loss: 0.4225\n",
      "Epoch 4937, Train Loss: 0.3228, Test Loss: 0.4233\n",
      "Epoch 4938, Train Loss: 0.3058, Test Loss: 0.4239\n",
      "Epoch 4939, Train Loss: 0.3397, Test Loss: 0.4233\n",
      "Epoch 4940, Train Loss: 0.2837, Test Loss: 0.4227\n",
      "Epoch 4941, Train Loss: 0.3643, Test Loss: 0.4221\n",
      "Epoch 4942, Train Loss: 0.2846, Test Loss: 0.4210\n",
      "Epoch 4943, Train Loss: 0.2979, Test Loss: 0.4195\n",
      "Epoch 4944, Train Loss: 0.3437, Test Loss: 0.4180\n",
      "Epoch 4945, Train Loss: 0.2949, Test Loss: 0.4170\n",
      "Epoch 4946, Train Loss: 0.2956, Test Loss: 0.4163\n",
      "Epoch 4947, Train Loss: 0.2661, Test Loss: 0.4162\n",
      "Epoch 4948, Train Loss: 0.2881, Test Loss: 0.4160\n",
      "Epoch 4949, Train Loss: 0.3235, Test Loss: 0.4158\n",
      "Epoch 4950, Train Loss: 0.3393, Test Loss: 0.4156\n",
      "Epoch 4951, Train Loss: 0.3726, Test Loss: 0.4161\n",
      "Epoch 4952, Train Loss: 0.3480, Test Loss: 0.4167\n",
      "Epoch 4953, Train Loss: 0.4072, Test Loss: 0.4172\n",
      "Epoch 4954, Train Loss: 0.2757, Test Loss: 0.4177\n",
      "Epoch 4955, Train Loss: 0.3712, Test Loss: 0.4179\n",
      "Epoch 4956, Train Loss: 0.2562, Test Loss: 0.4181\n",
      "Epoch 4957, Train Loss: 0.3491, Test Loss: 0.4182\n",
      "Epoch 4958, Train Loss: 0.2868, Test Loss: 0.4177\n",
      "Epoch 4959, Train Loss: 0.2693, Test Loss: 0.4177\n",
      "Epoch 4960, Train Loss: 0.2969, Test Loss: 0.4173\n",
      "Epoch 4961, Train Loss: 0.2803, Test Loss: 0.4171\n",
      "Epoch 4962, Train Loss: 0.3150, Test Loss: 0.4168\n",
      "Epoch 4963, Train Loss: 0.2846, Test Loss: 0.4166\n",
      "Epoch 4964, Train Loss: 0.3219, Test Loss: 0.4165\n",
      "Epoch 4965, Train Loss: 0.2536, Test Loss: 0.4162\n",
      "Epoch 4966, Train Loss: 0.3924, Test Loss: 0.4155\n",
      "Epoch 4967, Train Loss: 0.2924, Test Loss: 0.4147\n",
      "Epoch 4968, Train Loss: 0.2721, Test Loss: 0.4143\n",
      "Epoch 4969, Train Loss: 0.3562, Test Loss: 0.4141\n",
      "Epoch 4970, Train Loss: 0.3463, Test Loss: 0.4143\n",
      "Epoch 4971, Train Loss: 0.2858, Test Loss: 0.4141\n",
      "Epoch 4972, Train Loss: 0.3265, Test Loss: 0.4136\n",
      "Epoch 4973, Train Loss: 0.2715, Test Loss: 0.4130\n",
      "Epoch 4974, Train Loss: 0.2703, Test Loss: 0.4123\n",
      "Epoch 4975, Train Loss: 0.3388, Test Loss: 0.4119\n",
      "Epoch 4976, Train Loss: 0.3160, Test Loss: 0.4117\n",
      "Epoch 4977, Train Loss: 0.3271, Test Loss: 0.4117\n",
      "Epoch 4978, Train Loss: 0.3100, Test Loss: 0.4123\n",
      "Epoch 4979, Train Loss: 0.3922, Test Loss: 0.4128\n",
      "Epoch 4980, Train Loss: 0.3628, Test Loss: 0.4134\n",
      "Epoch 4981, Train Loss: 0.2727, Test Loss: 0.4137\n",
      "Epoch 4982, Train Loss: 0.3453, Test Loss: 0.4134\n",
      "Epoch 4983, Train Loss: 0.2785, Test Loss: 0.4134\n",
      "Epoch 4984, Train Loss: 0.2992, Test Loss: 0.4138\n",
      "Epoch 4985, Train Loss: 0.2763, Test Loss: 0.4137\n",
      "Epoch 4986, Train Loss: 0.2608, Test Loss: 0.4132\n",
      "Epoch 4987, Train Loss: 0.3645, Test Loss: 0.4128\n",
      "Epoch 4988, Train Loss: 0.2778, Test Loss: 0.4126\n",
      "Epoch 4989, Train Loss: 0.3261, Test Loss: 0.4124\n",
      "Epoch 4990, Train Loss: 0.3232, Test Loss: 0.4123\n",
      "Epoch 4991, Train Loss: 0.3729, Test Loss: 0.4120\n",
      "Epoch 4992, Train Loss: 0.3336, Test Loss: 0.4121\n",
      "Epoch 4993, Train Loss: 0.3685, Test Loss: 0.4122\n",
      "Epoch 4994, Train Loss: 0.3592, Test Loss: 0.4123\n",
      "Epoch 4995, Train Loss: 0.3349, Test Loss: 0.4124\n",
      "Epoch 4996, Train Loss: 0.3082, Test Loss: 0.4125\n",
      "Epoch 4997, Train Loss: 0.2995, Test Loss: 0.4121\n",
      "Epoch 4998, Train Loss: 0.3500, Test Loss: 0.4121\n",
      "Epoch 4999, Train Loss: 0.3535, Test Loss: 0.4127\n",
      "Epoch 5000, Train Loss: 0.3447, Test Loss: 0.4135\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Training\n",
    "model = EEG_GNN(in_dim=160, hidden_dim=64, out_dim=4)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Lists to store loss values\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(5000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Training loss\n",
    "    output = model(graph_data.x, graph_data.edge_index)[graph_data.train_mask]\n",
    "    loss = criterion(output, graph_data.y[graph_data.train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Store train loss\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(graph_data.x, graph_data.edge_index)[graph_data.test_mask]\n",
    "        test_loss = criterion(test_output, graph_data.y[graph_data.test_mask])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "#  Now you can plot the loss curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.4135\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(graph_data.x, graph_data.edge_index)[graph_data.test_mask]\n",
    "    test_loss = criterion(test_output, graph_data.y[graph_data.test_mask])\n",
    "\n",
    "print(f\"Test Loss: {test_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Labels Shape: (1280,)\n",
      "Class Distribution: [304 270 329 377]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_names = [\"valence\", \"arousal\", \"dominance\", \"liking\"]\n",
    "\n",
    "# Convert each sample to a single class (0=valence, 1=arousal, 2=dominance, 3=liking)\n",
    "labels_single_class = np.argmax(labels, axis=1)\n",
    "\n",
    "print(\"Converted Labels Shape:\", labels_single_class.shape)  # Should be (1280,)\n",
    "print(\"Class Distribution:\", np.bincount(labels_single_class))  # Check class balance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class EEG_GNN_MultiClass(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, out_dim=4):  # 4 classes: valence, arousal, dominance, liking\n",
    "        super(EEG_GNN_MultiClass, self).__init__()\n",
    "        self.conv1 = GCNConv(in_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, out_dim)  # Output 4 class logits\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.relu(self.conv1(x, edge_index))\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.relu(self.conv2(x, edge_index))\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x, dim=1)  # Apply softmax for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()  # Standard for multi-class classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 3.1487, Test Loss: 3.1321\n",
      "Epoch 2, Train Loss: 3.1392, Test Loss: 3.1327\n",
      "Epoch 3, Train Loss: 3.1474, Test Loss: 3.1337\n",
      "Epoch 4, Train Loss: 3.1404, Test Loss: 3.1342\n",
      "Epoch 5, Train Loss: 3.1073, Test Loss: 3.1345\n",
      "Epoch 6, Train Loss: 3.1164, Test Loss: 3.1344\n",
      "Epoch 7, Train Loss: 3.1262, Test Loss: 3.1339\n",
      "Epoch 8, Train Loss: 3.0942, Test Loss: 3.1334\n",
      "Epoch 9, Train Loss: 3.0974, Test Loss: 3.1336\n",
      "Epoch 10, Train Loss: 3.1371, Test Loss: 3.1340\n",
      "Epoch 11, Train Loss: 3.1343, Test Loss: 3.1349\n",
      "Epoch 12, Train Loss: 3.1159, Test Loss: 3.1360\n",
      "Epoch 13, Train Loss: 3.0957, Test Loss: 3.1374\n",
      "Epoch 14, Train Loss: 3.1177, Test Loss: 3.1381\n",
      "Epoch 15, Train Loss: 3.1246, Test Loss: 3.1380\n",
      "Epoch 16, Train Loss: 3.1109, Test Loss: 3.1374\n",
      "Epoch 17, Train Loss: 3.1073, Test Loss: 3.1368\n",
      "Epoch 18, Train Loss: 3.1448, Test Loss: 3.1371\n",
      "Epoch 19, Train Loss: 3.1149, Test Loss: 3.1369\n",
      "Epoch 20, Train Loss: 3.1385, Test Loss: 3.1370\n",
      "Epoch 21, Train Loss: 3.1464, Test Loss: 3.1371\n",
      "Epoch 22, Train Loss: 3.1344, Test Loss: 3.1371\n",
      "Epoch 23, Train Loss: 3.1260, Test Loss: 3.1368\n",
      "Epoch 24, Train Loss: 3.1455, Test Loss: 3.1362\n",
      "Epoch 25, Train Loss: 3.1395, Test Loss: 3.1356\n",
      "Epoch 26, Train Loss: 3.1149, Test Loss: 3.1349\n",
      "Epoch 27, Train Loss: 3.1288, Test Loss: 3.1343\n",
      "Epoch 28, Train Loss: 3.1258, Test Loss: 3.1334\n",
      "Epoch 29, Train Loss: 3.1320, Test Loss: 3.1333\n",
      "Epoch 30, Train Loss: 3.1199, Test Loss: 3.1338\n",
      "Epoch 31, Train Loss: 3.1046, Test Loss: 3.1341\n",
      "Epoch 32, Train Loss: 3.1085, Test Loss: 3.1344\n",
      "Epoch 33, Train Loss: 3.0934, Test Loss: 3.1341\n",
      "Epoch 34, Train Loss: 3.1682, Test Loss: 3.1334\n",
      "Epoch 35, Train Loss: 3.1278, Test Loss: 3.1330\n",
      "Epoch 36, Train Loss: 3.1269, Test Loss: 3.1323\n",
      "Epoch 37, Train Loss: 3.1040, Test Loss: 3.1321\n",
      "Epoch 38, Train Loss: 3.1197, Test Loss: 3.1319\n",
      "Epoch 39, Train Loss: 3.1264, Test Loss: 3.1323\n",
      "Epoch 40, Train Loss: 3.1462, Test Loss: 3.1329\n",
      "Epoch 41, Train Loss: 3.1178, Test Loss: 3.1336\n",
      "Epoch 42, Train Loss: 3.1414, Test Loss: 3.1337\n",
      "Epoch 43, Train Loss: 3.1338, Test Loss: 3.1327\n",
      "Epoch 44, Train Loss: 3.1193, Test Loss: 3.1319\n",
      "Epoch 45, Train Loss: 3.1166, Test Loss: 3.1318\n",
      "Epoch 46, Train Loss: 3.1161, Test Loss: 3.1318\n",
      "Epoch 47, Train Loss: 3.1154, Test Loss: 3.1316\n",
      "Epoch 48, Train Loss: 3.1052, Test Loss: 3.1317\n",
      "Epoch 49, Train Loss: 3.1357, Test Loss: 3.1321\n",
      "Epoch 50, Train Loss: 3.1399, Test Loss: 3.1329\n",
      "Epoch 51, Train Loss: 3.1138, Test Loss: 3.1331\n",
      "Epoch 52, Train Loss: 3.1041, Test Loss: 3.1318\n",
      "Epoch 53, Train Loss: 3.1293, Test Loss: 3.1305\n",
      "Epoch 54, Train Loss: 3.1243, Test Loss: 3.1293\n",
      "Epoch 55, Train Loss: 3.1266, Test Loss: 3.1287\n",
      "Epoch 56, Train Loss: 3.1208, Test Loss: 3.1282\n",
      "Epoch 57, Train Loss: 3.1378, Test Loss: 3.1276\n",
      "Epoch 58, Train Loss: 3.1173, Test Loss: 3.1276\n",
      "Epoch 59, Train Loss: 3.1197, Test Loss: 3.1280\n",
      "Epoch 60, Train Loss: 3.1198, Test Loss: 3.1291\n",
      "Epoch 61, Train Loss: 3.1575, Test Loss: 3.1301\n",
      "Epoch 62, Train Loss: 3.1149, Test Loss: 3.1302\n",
      "Epoch 63, Train Loss: 3.1152, Test Loss: 3.1296\n",
      "Epoch 64, Train Loss: 3.1319, Test Loss: 3.1290\n",
      "Epoch 65, Train Loss: 3.1125, Test Loss: 3.1283\n",
      "Epoch 66, Train Loss: 3.1271, Test Loss: 3.1280\n",
      "Epoch 67, Train Loss: 3.0977, Test Loss: 3.1283\n",
      "Epoch 68, Train Loss: 3.1415, Test Loss: 3.1290\n",
      "Epoch 69, Train Loss: 3.1134, Test Loss: 3.1293\n",
      "Epoch 70, Train Loss: 3.1117, Test Loss: 3.1293\n",
      "Epoch 71, Train Loss: 3.1301, Test Loss: 3.1295\n",
      "Epoch 72, Train Loss: 3.1115, Test Loss: 3.1297\n",
      "Epoch 73, Train Loss: 3.1047, Test Loss: 3.1294\n",
      "Epoch 74, Train Loss: 3.1428, Test Loss: 3.1291\n",
      "Epoch 75, Train Loss: 3.1086, Test Loss: 3.1289\n",
      "Epoch 76, Train Loss: 3.1147, Test Loss: 3.1284\n",
      "Epoch 77, Train Loss: 3.1208, Test Loss: 3.1279\n",
      "Epoch 78, Train Loss: 3.1479, Test Loss: 3.1275\n",
      "Epoch 79, Train Loss: 3.1529, Test Loss: 3.1273\n",
      "Epoch 80, Train Loss: 3.1187, Test Loss: 3.1276\n",
      "Epoch 81, Train Loss: 3.1043, Test Loss: 3.1284\n",
      "Epoch 82, Train Loss: 3.0982, Test Loss: 3.1287\n",
      "Epoch 83, Train Loss: 3.1480, Test Loss: 3.1283\n",
      "Epoch 84, Train Loss: 3.1144, Test Loss: 3.1281\n",
      "Epoch 85, Train Loss: 3.1486, Test Loss: 3.1280\n",
      "Epoch 86, Train Loss: 3.1207, Test Loss: 3.1277\n",
      "Epoch 87, Train Loss: 3.1250, Test Loss: 3.1277\n",
      "Epoch 88, Train Loss: 3.1257, Test Loss: 3.1280\n",
      "Epoch 89, Train Loss: 3.1140, Test Loss: 3.1283\n",
      "Epoch 90, Train Loss: 3.1515, Test Loss: 3.1289\n",
      "Epoch 91, Train Loss: 3.1324, Test Loss: 3.1295\n",
      "Epoch 92, Train Loss: 3.1011, Test Loss: 3.1300\n",
      "Epoch 93, Train Loss: 3.1015, Test Loss: 3.1303\n",
      "Epoch 94, Train Loss: 3.1392, Test Loss: 3.1300\n",
      "Epoch 95, Train Loss: 3.1425, Test Loss: 3.1297\n",
      "Epoch 96, Train Loss: 3.1136, Test Loss: 3.1291\n",
      "Epoch 97, Train Loss: 3.1143, Test Loss: 3.1286\n",
      "Epoch 98, Train Loss: 3.1164, Test Loss: 3.1282\n",
      "Epoch 99, Train Loss: 3.1408, Test Loss: 3.1285\n",
      "Epoch 100, Train Loss: 3.1130, Test Loss: 3.1292\n",
      "Epoch 101, Train Loss: 3.1068, Test Loss: 3.1305\n",
      "Epoch 102, Train Loss: 3.1372, Test Loss: 3.1295\n",
      "Epoch 103, Train Loss: 3.1224, Test Loss: 3.1284\n",
      "Epoch 104, Train Loss: 3.1305, Test Loss: 3.1274\n",
      "Epoch 105, Train Loss: 3.1178, Test Loss: 3.1270\n",
      "Epoch 106, Train Loss: 3.1063, Test Loss: 3.1269\n",
      "Epoch 107, Train Loss: 3.1368, Test Loss: 3.1272\n",
      "Epoch 108, Train Loss: 3.0917, Test Loss: 3.1275\n",
      "Epoch 109, Train Loss: 3.1191, Test Loss: 3.1282\n",
      "Epoch 110, Train Loss: 3.0949, Test Loss: 3.1286\n",
      "Epoch 111, Train Loss: 3.1282, Test Loss: 3.1294\n",
      "Epoch 112, Train Loss: 3.1287, Test Loss: 3.1301\n",
      "Epoch 113, Train Loss: 3.1346, Test Loss: 3.1305\n",
      "Epoch 114, Train Loss: 3.0972, Test Loss: 3.1303\n",
      "Epoch 115, Train Loss: 3.1122, Test Loss: 3.1299\n",
      "Epoch 116, Train Loss: 3.1059, Test Loss: 3.1297\n",
      "Epoch 117, Train Loss: 3.1289, Test Loss: 3.1295\n",
      "Epoch 118, Train Loss: 3.1494, Test Loss: 3.1295\n",
      "Epoch 119, Train Loss: 3.1109, Test Loss: 3.1298\n",
      "Epoch 120, Train Loss: 3.1151, Test Loss: 3.1300\n",
      "Epoch 121, Train Loss: 3.1264, Test Loss: 3.1293\n",
      "Epoch 122, Train Loss: 3.1380, Test Loss: 3.1282\n",
      "Epoch 123, Train Loss: 3.1114, Test Loss: 3.1272\n",
      "Epoch 124, Train Loss: 3.1093, Test Loss: 3.1262\n",
      "Epoch 125, Train Loss: 3.1110, Test Loss: 3.1256\n",
      "Epoch 126, Train Loss: 3.1193, Test Loss: 3.1253\n",
      "Epoch 127, Train Loss: 3.1076, Test Loss: 3.1252\n",
      "Epoch 128, Train Loss: 3.1123, Test Loss: 3.1249\n",
      "Epoch 129, Train Loss: 3.1307, Test Loss: 3.1249\n",
      "Epoch 130, Train Loss: 3.1160, Test Loss: 3.1250\n",
      "Epoch 131, Train Loss: 3.1538, Test Loss: 3.1255\n",
      "Epoch 132, Train Loss: 3.1015, Test Loss: 3.1257\n",
      "Epoch 133, Train Loss: 3.1307, Test Loss: 3.1259\n",
      "Epoch 134, Train Loss: 3.1278, Test Loss: 3.1256\n",
      "Epoch 135, Train Loss: 3.1107, Test Loss: 3.1258\n",
      "Epoch 136, Train Loss: 3.1196, Test Loss: 3.1261\n",
      "Epoch 137, Train Loss: 3.0872, Test Loss: 3.1266\n",
      "Epoch 138, Train Loss: 3.0888, Test Loss: 3.1272\n",
      "Epoch 139, Train Loss: 3.1235, Test Loss: 3.1277\n",
      "Epoch 140, Train Loss: 3.1226, Test Loss: 3.1279\n",
      "Epoch 141, Train Loss: 3.1180, Test Loss: 3.1275\n",
      "Epoch 142, Train Loss: 3.1102, Test Loss: 3.1274\n",
      "Epoch 143, Train Loss: 3.1141, Test Loss: 3.1277\n",
      "Epoch 144, Train Loss: 3.1201, Test Loss: 3.1277\n",
      "Epoch 145, Train Loss: 3.1235, Test Loss: 3.1280\n",
      "Epoch 146, Train Loss: 3.1148, Test Loss: 3.1285\n",
      "Epoch 147, Train Loss: 3.1205, Test Loss: 3.1287\n",
      "Epoch 148, Train Loss: 3.1321, Test Loss: 3.1292\n",
      "Epoch 149, Train Loss: 3.1147, Test Loss: 3.1289\n",
      "Epoch 150, Train Loss: 3.1012, Test Loss: 3.1282\n",
      "Epoch 151, Train Loss: 3.1254, Test Loss: 3.1276\n",
      "Epoch 152, Train Loss: 3.1142, Test Loss: 3.1276\n",
      "Epoch 153, Train Loss: 3.1269, Test Loss: 3.1274\n",
      "Epoch 154, Train Loss: 3.0985, Test Loss: 3.1270\n",
      "Epoch 155, Train Loss: 3.0993, Test Loss: 3.1270\n",
      "Epoch 156, Train Loss: 3.1038, Test Loss: 3.1274\n",
      "Epoch 157, Train Loss: 3.1016, Test Loss: 3.1278\n",
      "Epoch 158, Train Loss: 3.1147, Test Loss: 3.1275\n",
      "Epoch 159, Train Loss: 3.1343, Test Loss: 3.1268\n",
      "Epoch 160, Train Loss: 3.1148, Test Loss: 3.1259\n",
      "Epoch 161, Train Loss: 3.1152, Test Loss: 3.1255\n",
      "Epoch 162, Train Loss: 3.1205, Test Loss: 3.1254\n",
      "Epoch 163, Train Loss: 3.1102, Test Loss: 3.1250\n",
      "Epoch 164, Train Loss: 3.1049, Test Loss: 3.1244\n",
      "Epoch 165, Train Loss: 3.1103, Test Loss: 3.1241\n",
      "Epoch 166, Train Loss: 3.1200, Test Loss: 3.1239\n",
      "Epoch 167, Train Loss: 3.0920, Test Loss: 3.1239\n",
      "Epoch 168, Train Loss: 3.1055, Test Loss: 3.1242\n",
      "Epoch 169, Train Loss: 3.0943, Test Loss: 3.1243\n",
      "Epoch 170, Train Loss: 3.1248, Test Loss: 3.1240\n",
      "Epoch 171, Train Loss: 3.1097, Test Loss: 3.1241\n",
      "Epoch 172, Train Loss: 3.1150, Test Loss: 3.1239\n",
      "Epoch 173, Train Loss: 3.1231, Test Loss: 3.1240\n",
      "Epoch 174, Train Loss: 3.1106, Test Loss: 3.1245\n",
      "Epoch 175, Train Loss: 3.1025, Test Loss: 3.1250\n",
      "Epoch 176, Train Loss: 3.1223, Test Loss: 3.1257\n",
      "Epoch 177, Train Loss: 3.1175, Test Loss: 3.1265\n",
      "Epoch 178, Train Loss: 3.1079, Test Loss: 3.1275\n",
      "Epoch 179, Train Loss: 3.0884, Test Loss: 3.1283\n",
      "Epoch 180, Train Loss: 3.1090, Test Loss: 3.1289\n",
      "Epoch 181, Train Loss: 3.1227, Test Loss: 3.1286\n",
      "Epoch 182, Train Loss: 3.1357, Test Loss: 3.1281\n",
      "Epoch 183, Train Loss: 3.0959, Test Loss: 3.1274\n",
      "Epoch 184, Train Loss: 3.1028, Test Loss: 3.1260\n",
      "Epoch 185, Train Loss: 3.1114, Test Loss: 3.1248\n",
      "Epoch 186, Train Loss: 3.0995, Test Loss: 3.1239\n",
      "Epoch 187, Train Loss: 3.0969, Test Loss: 3.1233\n",
      "Epoch 188, Train Loss: 3.1524, Test Loss: 3.1234\n",
      "Epoch 189, Train Loss: 3.1111, Test Loss: 3.1236\n",
      "Epoch 190, Train Loss: 3.1153, Test Loss: 3.1232\n",
      "Epoch 191, Train Loss: 3.1122, Test Loss: 3.1237\n",
      "Epoch 192, Train Loss: 3.1150, Test Loss: 3.1242\n",
      "Epoch 193, Train Loss: 3.1077, Test Loss: 3.1248\n",
      "Epoch 194, Train Loss: 3.1118, Test Loss: 3.1253\n",
      "Epoch 195, Train Loss: 3.1096, Test Loss: 3.1257\n",
      "Epoch 196, Train Loss: 3.1297, Test Loss: 3.1265\n",
      "Epoch 197, Train Loss: 3.1113, Test Loss: 3.1274\n",
      "Epoch 198, Train Loss: 3.1411, Test Loss: 3.1280\n",
      "Epoch 199, Train Loss: 3.1207, Test Loss: 3.1273\n",
      "Epoch 200, Train Loss: 3.0904, Test Loss: 3.1264\n",
      "Epoch 201, Train Loss: 3.1087, Test Loss: 3.1255\n",
      "Epoch 202, Train Loss: 3.1024, Test Loss: 3.1247\n",
      "Epoch 203, Train Loss: 3.1239, Test Loss: 3.1240\n",
      "Epoch 204, Train Loss: 3.0943, Test Loss: 3.1238\n",
      "Epoch 205, Train Loss: 3.1077, Test Loss: 3.1239\n",
      "Epoch 206, Train Loss: 3.1088, Test Loss: 3.1241\n",
      "Epoch 207, Train Loss: 3.0998, Test Loss: 3.1241\n",
      "Epoch 208, Train Loss: 3.0951, Test Loss: 3.1239\n",
      "Epoch 209, Train Loss: 3.1108, Test Loss: 3.1238\n",
      "Epoch 210, Train Loss: 3.1218, Test Loss: 3.1238\n",
      "Epoch 211, Train Loss: 3.0992, Test Loss: 3.1236\n",
      "Epoch 212, Train Loss: 3.1165, Test Loss: 3.1236\n",
      "Epoch 213, Train Loss: 3.1027, Test Loss: 3.1239\n",
      "Epoch 214, Train Loss: 3.1231, Test Loss: 3.1251\n",
      "Epoch 215, Train Loss: 3.1439, Test Loss: 3.1263\n",
      "Epoch 216, Train Loss: 3.1019, Test Loss: 3.1269\n",
      "Epoch 217, Train Loss: 3.1091, Test Loss: 3.1256\n",
      "Epoch 218, Train Loss: 3.1207, Test Loss: 3.1245\n",
      "Epoch 219, Train Loss: 3.1199, Test Loss: 3.1242\n",
      "Epoch 220, Train Loss: 3.0901, Test Loss: 3.1243\n",
      "Epoch 221, Train Loss: 3.1087, Test Loss: 3.1248\n",
      "Epoch 222, Train Loss: 3.1102, Test Loss: 3.1257\n",
      "Epoch 223, Train Loss: 3.0920, Test Loss: 3.1267\n",
      "Epoch 224, Train Loss: 3.1285, Test Loss: 3.1268\n",
      "Epoch 225, Train Loss: 3.1004, Test Loss: 3.1268\n",
      "Epoch 226, Train Loss: 3.1047, Test Loss: 3.1262\n",
      "Epoch 227, Train Loss: 3.0818, Test Loss: 3.1258\n",
      "Epoch 228, Train Loss: 3.1114, Test Loss: 3.1261\n",
      "Epoch 229, Train Loss: 3.1239, Test Loss: 3.1264\n",
      "Epoch 230, Train Loss: 3.1043, Test Loss: 3.1272\n",
      "Epoch 231, Train Loss: 3.1173, Test Loss: 3.1279\n",
      "Epoch 232, Train Loss: 3.1304, Test Loss: 3.1273\n",
      "Epoch 233, Train Loss: 3.1309, Test Loss: 3.1261\n",
      "Epoch 234, Train Loss: 3.1212, Test Loss: 3.1246\n",
      "Epoch 235, Train Loss: 3.1161, Test Loss: 3.1239\n",
      "Epoch 236, Train Loss: 3.1128, Test Loss: 3.1237\n",
      "Epoch 237, Train Loss: 3.1027, Test Loss: 3.1236\n",
      "Epoch 238, Train Loss: 3.0849, Test Loss: 3.1239\n",
      "Epoch 239, Train Loss: 3.1363, Test Loss: 3.1248\n",
      "Epoch 240, Train Loss: 3.1233, Test Loss: 3.1257\n",
      "Epoch 241, Train Loss: 3.1216, Test Loss: 3.1263\n",
      "Epoch 242, Train Loss: 3.0814, Test Loss: 3.1262\n",
      "Epoch 243, Train Loss: 3.1117, Test Loss: 3.1258\n",
      "Epoch 244, Train Loss: 3.1011, Test Loss: 3.1258\n",
      "Epoch 245, Train Loss: 3.0954, Test Loss: 3.1256\n",
      "Epoch 246, Train Loss: 3.0809, Test Loss: 3.1255\n",
      "Epoch 247, Train Loss: 3.0815, Test Loss: 3.1255\n",
      "Epoch 248, Train Loss: 3.1103, Test Loss: 3.1258\n",
      "Epoch 249, Train Loss: 3.0915, Test Loss: 3.1257\n",
      "Epoch 250, Train Loss: 3.0893, Test Loss: 3.1258\n",
      "Epoch 251, Train Loss: 3.1035, Test Loss: 3.1251\n",
      "Epoch 252, Train Loss: 3.1227, Test Loss: 3.1241\n",
      "Epoch 253, Train Loss: 3.0999, Test Loss: 3.1238\n",
      "Epoch 254, Train Loss: 3.1135, Test Loss: 3.1238\n",
      "Epoch 255, Train Loss: 3.0905, Test Loss: 3.1237\n",
      "Epoch 256, Train Loss: 3.1326, Test Loss: 3.1232\n",
      "Epoch 257, Train Loss: 3.1219, Test Loss: 3.1231\n",
      "Epoch 258, Train Loss: 3.1173, Test Loss: 3.1230\n",
      "Epoch 259, Train Loss: 3.1092, Test Loss: 3.1236\n",
      "Epoch 260, Train Loss: 3.0838, Test Loss: 3.1246\n",
      "Epoch 261, Train Loss: 3.1183, Test Loss: 3.1259\n",
      "Epoch 262, Train Loss: 3.0953, Test Loss: 3.1268\n",
      "Epoch 263, Train Loss: 3.1092, Test Loss: 3.1275\n",
      "Epoch 264, Train Loss: 3.0760, Test Loss: 3.1277\n",
      "Epoch 265, Train Loss: 3.1010, Test Loss: 3.1275\n",
      "Epoch 266, Train Loss: 3.1043, Test Loss: 3.1271\n",
      "Epoch 267, Train Loss: 3.1186, Test Loss: 3.1267\n",
      "Epoch 268, Train Loss: 3.1348, Test Loss: 3.1259\n",
      "Epoch 269, Train Loss: 3.1000, Test Loss: 3.1251\n",
      "Epoch 270, Train Loss: 3.1040, Test Loss: 3.1239\n",
      "Epoch 271, Train Loss: 3.1152, Test Loss: 3.1231\n",
      "Epoch 272, Train Loss: 3.1021, Test Loss: 3.1228\n",
      "Epoch 273, Train Loss: 3.0767, Test Loss: 3.1228\n",
      "Epoch 274, Train Loss: 3.1278, Test Loss: 3.1232\n",
      "Epoch 275, Train Loss: 3.0946, Test Loss: 3.1238\n",
      "Epoch 276, Train Loss: 3.1159, Test Loss: 3.1248\n",
      "Epoch 277, Train Loss: 3.1120, Test Loss: 3.1251\n",
      "Epoch 278, Train Loss: 3.0908, Test Loss: 3.1242\n",
      "Epoch 279, Train Loss: 3.1124, Test Loss: 3.1237\n",
      "Epoch 280, Train Loss: 3.1051, Test Loss: 3.1234\n",
      "Epoch 281, Train Loss: 3.1156, Test Loss: 3.1235\n",
      "Epoch 282, Train Loss: 3.1162, Test Loss: 3.1241\n",
      "Epoch 283, Train Loss: 3.1023, Test Loss: 3.1249\n",
      "Epoch 284, Train Loss: 3.0849, Test Loss: 3.1258\n",
      "Epoch 285, Train Loss: 3.0930, Test Loss: 3.1266\n",
      "Epoch 286, Train Loss: 3.0847, Test Loss: 3.1273\n",
      "Epoch 287, Train Loss: 3.0851, Test Loss: 3.1274\n",
      "Epoch 288, Train Loss: 3.0807, Test Loss: 3.1269\n",
      "Epoch 289, Train Loss: 3.0987, Test Loss: 3.1264\n",
      "Epoch 290, Train Loss: 3.1302, Test Loss: 3.1257\n",
      "Epoch 291, Train Loss: 3.0891, Test Loss: 3.1249\n",
      "Epoch 292, Train Loss: 3.1314, Test Loss: 3.1244\n",
      "Epoch 293, Train Loss: 3.0937, Test Loss: 3.1242\n",
      "Epoch 294, Train Loss: 3.1062, Test Loss: 3.1237\n",
      "Epoch 295, Train Loss: 3.0786, Test Loss: 3.1229\n",
      "Epoch 296, Train Loss: 3.1154, Test Loss: 3.1219\n",
      "Epoch 297, Train Loss: 3.1101, Test Loss: 3.1207\n",
      "Epoch 298, Train Loss: 3.0884, Test Loss: 3.1200\n",
      "Epoch 299, Train Loss: 3.1002, Test Loss: 3.1200\n",
      "Epoch 300, Train Loss: 3.1128, Test Loss: 3.1206\n",
      "Epoch 301, Train Loss: 3.0886, Test Loss: 3.1219\n",
      "Epoch 302, Train Loss: 3.0963, Test Loss: 3.1228\n",
      "Epoch 303, Train Loss: 3.0987, Test Loss: 3.1235\n",
      "Epoch 304, Train Loss: 3.1077, Test Loss: 3.1240\n",
      "Epoch 305, Train Loss: 3.0842, Test Loss: 3.1247\n",
      "Epoch 306, Train Loss: 3.0893, Test Loss: 3.1250\n",
      "Epoch 307, Train Loss: 3.0936, Test Loss: 3.1255\n",
      "Epoch 308, Train Loss: 3.1013, Test Loss: 3.1259\n",
      "Epoch 309, Train Loss: 3.1254, Test Loss: 3.1264\n",
      "Epoch 310, Train Loss: 3.0984, Test Loss: 3.1266\n",
      "Epoch 311, Train Loss: 3.1256, Test Loss: 3.1266\n",
      "Epoch 312, Train Loss: 3.0973, Test Loss: 3.1263\n",
      "Epoch 313, Train Loss: 3.0865, Test Loss: 3.1260\n",
      "Epoch 314, Train Loss: 3.1158, Test Loss: 3.1255\n",
      "Epoch 315, Train Loss: 3.0917, Test Loss: 3.1248\n",
      "Epoch 316, Train Loss: 3.1106, Test Loss: 3.1238\n",
      "Epoch 317, Train Loss: 3.0899, Test Loss: 3.1233\n",
      "Epoch 318, Train Loss: 3.0933, Test Loss: 3.1226\n",
      "Epoch 319, Train Loss: 3.1198, Test Loss: 3.1227\n",
      "Epoch 320, Train Loss: 3.0996, Test Loss: 3.1232\n",
      "Epoch 321, Train Loss: 3.1011, Test Loss: 3.1233\n",
      "Epoch 322, Train Loss: 3.1037, Test Loss: 3.1229\n",
      "Epoch 323, Train Loss: 3.1247, Test Loss: 3.1224\n",
      "Epoch 324, Train Loss: 3.0851, Test Loss: 3.1224\n",
      "Epoch 325, Train Loss: 3.1029, Test Loss: 3.1228\n",
      "Epoch 326, Train Loss: 3.1181, Test Loss: 3.1240\n",
      "Epoch 327, Train Loss: 3.1138, Test Loss: 3.1254\n",
      "Epoch 328, Train Loss: 3.1071, Test Loss: 3.1258\n",
      "Epoch 329, Train Loss: 3.1085, Test Loss: 3.1254\n",
      "Epoch 330, Train Loss: 3.1197, Test Loss: 3.1248\n",
      "Epoch 331, Train Loss: 3.1118, Test Loss: 3.1243\n",
      "Epoch 332, Train Loss: 3.1148, Test Loss: 3.1244\n",
      "Epoch 333, Train Loss: 3.1005, Test Loss: 3.1244\n",
      "Epoch 334, Train Loss: 3.0999, Test Loss: 3.1242\n",
      "Epoch 335, Train Loss: 3.1084, Test Loss: 3.1238\n",
      "Epoch 336, Train Loss: 3.1183, Test Loss: 3.1239\n",
      "Epoch 337, Train Loss: 3.1033, Test Loss: 3.1232\n",
      "Epoch 338, Train Loss: 3.0998, Test Loss: 3.1225\n",
      "Epoch 339, Train Loss: 3.1062, Test Loss: 3.1219\n",
      "Epoch 340, Train Loss: 3.0820, Test Loss: 3.1215\n",
      "Epoch 341, Train Loss: 3.1052, Test Loss: 3.1219\n",
      "Epoch 342, Train Loss: 3.0888, Test Loss: 3.1227\n",
      "Epoch 343, Train Loss: 3.0951, Test Loss: 3.1238\n",
      "Epoch 344, Train Loss: 3.1153, Test Loss: 3.1249\n",
      "Epoch 345, Train Loss: 3.0970, Test Loss: 3.1253\n",
      "Epoch 346, Train Loss: 3.1084, Test Loss: 3.1250\n",
      "Epoch 347, Train Loss: 3.1005, Test Loss: 3.1250\n",
      "Epoch 348, Train Loss: 3.0942, Test Loss: 3.1251\n",
      "Epoch 349, Train Loss: 3.0912, Test Loss: 3.1254\n",
      "Epoch 350, Train Loss: 3.1039, Test Loss: 3.1251\n",
      "Epoch 351, Train Loss: 3.0839, Test Loss: 3.1246\n",
      "Epoch 352, Train Loss: 3.1165, Test Loss: 3.1239\n",
      "Epoch 353, Train Loss: 3.1274, Test Loss: 3.1227\n",
      "Epoch 354, Train Loss: 3.0784, Test Loss: 3.1217\n",
      "Epoch 355, Train Loss: 3.0707, Test Loss: 3.1206\n",
      "Epoch 356, Train Loss: 3.0936, Test Loss: 3.1198\n",
      "Epoch 357, Train Loss: 3.0966, Test Loss: 3.1198\n",
      "Epoch 358, Train Loss: 3.1148, Test Loss: 3.1199\n",
      "Epoch 359, Train Loss: 3.0952, Test Loss: 3.1207\n",
      "Epoch 360, Train Loss: 3.0850, Test Loss: 3.1214\n",
      "Epoch 361, Train Loss: 3.1226, Test Loss: 3.1220\n",
      "Epoch 362, Train Loss: 3.0878, Test Loss: 3.1220\n",
      "Epoch 363, Train Loss: 3.1036, Test Loss: 3.1223\n",
      "Epoch 364, Train Loss: 3.1113, Test Loss: 3.1231\n",
      "Epoch 365, Train Loss: 3.0944, Test Loss: 3.1240\n",
      "Epoch 366, Train Loss: 3.0860, Test Loss: 3.1249\n",
      "Epoch 367, Train Loss: 3.1020, Test Loss: 3.1257\n",
      "Epoch 368, Train Loss: 3.1021, Test Loss: 3.1258\n",
      "Epoch 369, Train Loss: 3.0963, Test Loss: 3.1249\n",
      "Epoch 370, Train Loss: 3.0833, Test Loss: 3.1237\n",
      "Epoch 371, Train Loss: 3.1276, Test Loss: 3.1227\n",
      "Epoch 372, Train Loss: 3.0962, Test Loss: 3.1223\n",
      "Epoch 373, Train Loss: 3.0909, Test Loss: 3.1220\n",
      "Epoch 374, Train Loss: 3.0882, Test Loss: 3.1222\n",
      "Epoch 375, Train Loss: 3.1118, Test Loss: 3.1226\n",
      "Epoch 376, Train Loss: 3.0914, Test Loss: 3.1225\n",
      "Epoch 377, Train Loss: 3.1019, Test Loss: 3.1217\n",
      "Epoch 378, Train Loss: 3.0933, Test Loss: 3.1214\n",
      "Epoch 379, Train Loss: 3.1100, Test Loss: 3.1212\n",
      "Epoch 380, Train Loss: 3.0895, Test Loss: 3.1216\n",
      "Epoch 381, Train Loss: 3.0905, Test Loss: 3.1222\n",
      "Epoch 382, Train Loss: 3.1089, Test Loss: 3.1231\n",
      "Epoch 383, Train Loss: 3.1179, Test Loss: 3.1234\n",
      "Epoch 384, Train Loss: 3.0963, Test Loss: 3.1230\n",
      "Epoch 385, Train Loss: 3.1058, Test Loss: 3.1228\n",
      "Epoch 386, Train Loss: 3.0750, Test Loss: 3.1228\n",
      "Epoch 387, Train Loss: 3.1004, Test Loss: 3.1225\n",
      "Epoch 388, Train Loss: 3.0871, Test Loss: 3.1223\n",
      "Epoch 389, Train Loss: 3.1065, Test Loss: 3.1223\n",
      "Epoch 390, Train Loss: 3.1164, Test Loss: 3.1224\n",
      "Epoch 391, Train Loss: 3.0962, Test Loss: 3.1223\n",
      "Epoch 392, Train Loss: 3.0897, Test Loss: 3.1222\n",
      "Epoch 393, Train Loss: 3.1076, Test Loss: 3.1224\n",
      "Epoch 394, Train Loss: 3.0776, Test Loss: 3.1231\n",
      "Epoch 395, Train Loss: 3.0962, Test Loss: 3.1232\n",
      "Epoch 396, Train Loss: 3.0973, Test Loss: 3.1232\n",
      "Epoch 397, Train Loss: 3.1004, Test Loss: 3.1237\n",
      "Epoch 398, Train Loss: 3.0890, Test Loss: 3.1243\n",
      "Epoch 399, Train Loss: 3.0775, Test Loss: 3.1249\n",
      "Epoch 400, Train Loss: 3.0904, Test Loss: 3.1253\n",
      "Epoch 401, Train Loss: 3.1019, Test Loss: 3.1252\n",
      "Epoch 402, Train Loss: 3.1078, Test Loss: 3.1247\n",
      "Epoch 403, Train Loss: 3.1034, Test Loss: 3.1238\n",
      "Epoch 404, Train Loss: 3.0975, Test Loss: 3.1227\n",
      "Epoch 405, Train Loss: 3.0965, Test Loss: 3.1216\n",
      "Epoch 406, Train Loss: 3.0915, Test Loss: 3.1209\n",
      "Epoch 407, Train Loss: 3.1025, Test Loss: 3.1206\n",
      "Epoch 408, Train Loss: 3.1100, Test Loss: 3.1213\n",
      "Epoch 409, Train Loss: 3.1212, Test Loss: 3.1219\n",
      "Epoch 410, Train Loss: 3.1091, Test Loss: 3.1216\n",
      "Epoch 411, Train Loss: 3.0807, Test Loss: 3.1202\n",
      "Epoch 412, Train Loss: 3.0934, Test Loss: 3.1190\n",
      "Epoch 413, Train Loss: 3.1148, Test Loss: 3.1191\n",
      "Epoch 414, Train Loss: 3.0899, Test Loss: 3.1193\n",
      "Epoch 415, Train Loss: 3.1002, Test Loss: 3.1205\n",
      "Epoch 416, Train Loss: 3.0718, Test Loss: 3.1221\n",
      "Epoch 417, Train Loss: 3.1011, Test Loss: 3.1233\n",
      "Epoch 418, Train Loss: 3.1214, Test Loss: 3.1230\n",
      "Epoch 419, Train Loss: 3.1052, Test Loss: 3.1217\n",
      "Epoch 420, Train Loss: 3.0824, Test Loss: 3.1209\n",
      "Epoch 421, Train Loss: 3.0907, Test Loss: 3.1207\n",
      "Epoch 422, Train Loss: 3.1061, Test Loss: 3.1204\n",
      "Epoch 423, Train Loss: 3.0961, Test Loss: 3.1207\n",
      "Epoch 424, Train Loss: 3.1068, Test Loss: 3.1216\n",
      "Epoch 425, Train Loss: 3.0865, Test Loss: 3.1226\n",
      "Epoch 426, Train Loss: 3.0858, Test Loss: 3.1225\n",
      "Epoch 427, Train Loss: 3.1093, Test Loss: 3.1220\n",
      "Epoch 428, Train Loss: 3.0869, Test Loss: 3.1221\n",
      "Epoch 429, Train Loss: 3.1066, Test Loss: 3.1221\n",
      "Epoch 430, Train Loss: 3.0946, Test Loss: 3.1220\n",
      "Epoch 431, Train Loss: 3.0788, Test Loss: 3.1223\n",
      "Epoch 432, Train Loss: 3.0940, Test Loss: 3.1225\n",
      "Epoch 433, Train Loss: 3.0942, Test Loss: 3.1230\n",
      "Epoch 434, Train Loss: 3.1232, Test Loss: 3.1239\n",
      "Epoch 435, Train Loss: 3.0985, Test Loss: 3.1249\n",
      "Epoch 436, Train Loss: 3.0965, Test Loss: 3.1246\n",
      "Epoch 437, Train Loss: 3.1114, Test Loss: 3.1239\n",
      "Epoch 438, Train Loss: 3.0922, Test Loss: 3.1233\n",
      "Epoch 439, Train Loss: 3.0957, Test Loss: 3.1227\n",
      "Epoch 440, Train Loss: 3.0984, Test Loss: 3.1223\n",
      "Epoch 441, Train Loss: 3.0994, Test Loss: 3.1223\n",
      "Epoch 442, Train Loss: 3.1044, Test Loss: 3.1230\n",
      "Epoch 443, Train Loss: 3.1155, Test Loss: 3.1230\n",
      "Epoch 444, Train Loss: 3.0808, Test Loss: 3.1224\n",
      "Epoch 445, Train Loss: 3.0792, Test Loss: 3.1211\n",
      "Epoch 446, Train Loss: 3.0674, Test Loss: 3.1202\n",
      "Epoch 447, Train Loss: 3.0917, Test Loss: 3.1196\n",
      "Epoch 448, Train Loss: 3.0952, Test Loss: 3.1192\n",
      "Epoch 449, Train Loss: 3.1249, Test Loss: 3.1192\n",
      "Epoch 450, Train Loss: 3.0797, Test Loss: 3.1198\n",
      "Epoch 451, Train Loss: 3.0907, Test Loss: 3.1211\n",
      "Epoch 452, Train Loss: 3.1003, Test Loss: 3.1219\n",
      "Epoch 453, Train Loss: 3.0871, Test Loss: 3.1221\n",
      "Epoch 454, Train Loss: 3.0885, Test Loss: 3.1215\n",
      "Epoch 455, Train Loss: 3.0863, Test Loss: 3.1211\n",
      "Epoch 456, Train Loss: 3.0724, Test Loss: 3.1214\n",
      "Epoch 457, Train Loss: 3.0895, Test Loss: 3.1218\n",
      "Epoch 458, Train Loss: 3.1003, Test Loss: 3.1223\n",
      "Epoch 459, Train Loss: 3.1015, Test Loss: 3.1234\n",
      "Epoch 460, Train Loss: 3.1022, Test Loss: 3.1240\n",
      "Epoch 461, Train Loss: 3.0827, Test Loss: 3.1236\n",
      "Epoch 462, Train Loss: 3.0803, Test Loss: 3.1229\n",
      "Epoch 463, Train Loss: 3.0927, Test Loss: 3.1219\n",
      "Epoch 464, Train Loss: 3.0786, Test Loss: 3.1211\n",
      "Epoch 465, Train Loss: 3.0916, Test Loss: 3.1206\n",
      "Epoch 466, Train Loss: 3.0814, Test Loss: 3.1201\n",
      "Epoch 467, Train Loss: 3.0797, Test Loss: 3.1195\n",
      "Epoch 468, Train Loss: 3.0851, Test Loss: 3.1191\n",
      "Epoch 469, Train Loss: 3.0937, Test Loss: 3.1194\n",
      "Epoch 470, Train Loss: 3.0901, Test Loss: 3.1200\n",
      "Epoch 471, Train Loss: 3.1082, Test Loss: 3.1200\n",
      "Epoch 472, Train Loss: 3.0877, Test Loss: 3.1201\n",
      "Epoch 473, Train Loss: 3.0956, Test Loss: 3.1203\n",
      "Epoch 474, Train Loss: 3.1031, Test Loss: 3.1209\n",
      "Epoch 475, Train Loss: 3.0916, Test Loss: 3.1224\n",
      "Epoch 476, Train Loss: 3.0688, Test Loss: 3.1239\n",
      "Epoch 477, Train Loss: 3.0928, Test Loss: 3.1241\n",
      "Epoch 478, Train Loss: 3.0832, Test Loss: 3.1233\n",
      "Epoch 479, Train Loss: 3.0848, Test Loss: 3.1222\n",
      "Epoch 480, Train Loss: 3.0847, Test Loss: 3.1215\n",
      "Epoch 481, Train Loss: 3.0980, Test Loss: 3.1213\n",
      "Epoch 482, Train Loss: 3.1161, Test Loss: 3.1216\n",
      "Epoch 483, Train Loss: 3.0846, Test Loss: 3.1216\n",
      "Epoch 484, Train Loss: 3.1060, Test Loss: 3.1211\n",
      "Epoch 485, Train Loss: 3.0929, Test Loss: 3.1204\n",
      "Epoch 486, Train Loss: 3.0877, Test Loss: 3.1198\n",
      "Epoch 487, Train Loss: 3.0954, Test Loss: 3.1194\n",
      "Epoch 488, Train Loss: 3.0907, Test Loss: 3.1192\n",
      "Epoch 489, Train Loss: 3.0819, Test Loss: 3.1194\n",
      "Epoch 490, Train Loss: 3.1031, Test Loss: 3.1200\n",
      "Epoch 491, Train Loss: 3.0758, Test Loss: 3.1205\n",
      "Epoch 492, Train Loss: 3.0807, Test Loss: 3.1202\n",
      "Epoch 493, Train Loss: 3.0814, Test Loss: 3.1194\n",
      "Epoch 494, Train Loss: 3.0768, Test Loss: 3.1187\n",
      "Epoch 495, Train Loss: 3.1005, Test Loss: 3.1184\n",
      "Epoch 496, Train Loss: 3.0874, Test Loss: 3.1185\n",
      "Epoch 497, Train Loss: 3.0922, Test Loss: 3.1189\n",
      "Epoch 498, Train Loss: 3.0792, Test Loss: 3.1198\n",
      "Epoch 499, Train Loss: 3.1031, Test Loss: 3.1208\n",
      "Epoch 500, Train Loss: 3.0712, Test Loss: 3.1219\n",
      "Epoch 501, Train Loss: 3.0823, Test Loss: 3.1222\n",
      "Epoch 502, Train Loss: 3.1058, Test Loss: 3.1209\n",
      "Epoch 503, Train Loss: 3.0977, Test Loss: 3.1200\n",
      "Epoch 504, Train Loss: 3.0735, Test Loss: 3.1196\n",
      "Epoch 505, Train Loss: 3.0793, Test Loss: 3.1192\n",
      "Epoch 506, Train Loss: 3.0785, Test Loss: 3.1187\n",
      "Epoch 507, Train Loss: 3.1199, Test Loss: 3.1186\n",
      "Epoch 508, Train Loss: 3.1382, Test Loss: 3.1192\n",
      "Epoch 509, Train Loss: 3.0769, Test Loss: 3.1206\n",
      "Epoch 510, Train Loss: 3.1063, Test Loss: 3.1209\n",
      "Epoch 511, Train Loss: 3.0841, Test Loss: 3.1206\n",
      "Epoch 512, Train Loss: 3.0774, Test Loss: 3.1196\n",
      "Epoch 513, Train Loss: 3.0927, Test Loss: 3.1192\n",
      "Epoch 514, Train Loss: 3.1005, Test Loss: 3.1195\n",
      "Epoch 515, Train Loss: 3.0975, Test Loss: 3.1200\n",
      "Epoch 516, Train Loss: 3.0996, Test Loss: 3.1207\n",
      "Epoch 517, Train Loss: 3.0924, Test Loss: 3.1219\n",
      "Epoch 518, Train Loss: 3.1072, Test Loss: 3.1233\n",
      "Epoch 519, Train Loss: 3.0868, Test Loss: 3.1233\n",
      "Epoch 520, Train Loss: 3.0693, Test Loss: 3.1229\n",
      "Epoch 521, Train Loss: 3.0913, Test Loss: 3.1220\n",
      "Epoch 522, Train Loss: 3.0901, Test Loss: 3.1214\n",
      "Epoch 523, Train Loss: 3.0930, Test Loss: 3.1205\n",
      "Epoch 524, Train Loss: 3.0924, Test Loss: 3.1198\n",
      "Epoch 525, Train Loss: 3.0712, Test Loss: 3.1199\n",
      "Epoch 526, Train Loss: 3.0905, Test Loss: 3.1200\n",
      "Epoch 527, Train Loss: 3.0941, Test Loss: 3.1196\n",
      "Epoch 528, Train Loss: 3.1029, Test Loss: 3.1188\n",
      "Epoch 529, Train Loss: 3.0719, Test Loss: 3.1182\n",
      "Epoch 530, Train Loss: 3.0841, Test Loss: 3.1183\n",
      "Epoch 531, Train Loss: 3.0959, Test Loss: 3.1189\n",
      "Epoch 532, Train Loss: 3.0767, Test Loss: 3.1199\n",
      "Epoch 533, Train Loss: 3.0644, Test Loss: 3.1203\n",
      "Epoch 534, Train Loss: 3.0623, Test Loss: 3.1203\n",
      "Epoch 535, Train Loss: 3.0599, Test Loss: 3.1200\n",
      "Epoch 536, Train Loss: 3.0669, Test Loss: 3.1191\n",
      "Epoch 537, Train Loss: 3.0789, Test Loss: 3.1187\n",
      "Epoch 538, Train Loss: 3.0933, Test Loss: 3.1185\n",
      "Epoch 539, Train Loss: 3.0755, Test Loss: 3.1182\n",
      "Epoch 540, Train Loss: 3.0828, Test Loss: 3.1182\n",
      "Epoch 541, Train Loss: 3.0756, Test Loss: 3.1184\n",
      "Epoch 542, Train Loss: 3.0839, Test Loss: 3.1185\n",
      "Epoch 543, Train Loss: 3.0772, Test Loss: 3.1182\n",
      "Epoch 544, Train Loss: 3.0960, Test Loss: 3.1180\n",
      "Epoch 545, Train Loss: 3.0952, Test Loss: 3.1181\n",
      "Epoch 546, Train Loss: 3.0835, Test Loss: 3.1184\n",
      "Epoch 547, Train Loss: 3.0887, Test Loss: 3.1189\n",
      "Epoch 548, Train Loss: 3.0815, Test Loss: 3.1196\n",
      "Epoch 549, Train Loss: 3.1007, Test Loss: 3.1200\n",
      "Epoch 550, Train Loss: 3.0860, Test Loss: 3.1203\n",
      "Epoch 551, Train Loss: 3.0658, Test Loss: 3.1211\n",
      "Epoch 552, Train Loss: 3.0765, Test Loss: 3.1219\n",
      "Epoch 553, Train Loss: 3.0817, Test Loss: 3.1222\n",
      "Epoch 554, Train Loss: 3.1062, Test Loss: 3.1218\n",
      "Epoch 555, Train Loss: 3.0955, Test Loss: 3.1215\n",
      "Epoch 556, Train Loss: 3.0937, Test Loss: 3.1209\n",
      "Epoch 557, Train Loss: 3.1014, Test Loss: 3.1203\n",
      "Epoch 558, Train Loss: 3.0745, Test Loss: 3.1199\n",
      "Epoch 559, Train Loss: 3.0831, Test Loss: 3.1199\n",
      "Epoch 560, Train Loss: 3.0907, Test Loss: 3.1196\n",
      "Epoch 561, Train Loss: 3.0898, Test Loss: 3.1191\n",
      "Epoch 562, Train Loss: 3.0946, Test Loss: 3.1185\n",
      "Epoch 563, Train Loss: 3.0921, Test Loss: 3.1184\n",
      "Epoch 564, Train Loss: 3.0818, Test Loss: 3.1183\n",
      "Epoch 565, Train Loss: 3.0986, Test Loss: 3.1186\n",
      "Epoch 566, Train Loss: 3.0877, Test Loss: 3.1194\n",
      "Epoch 567, Train Loss: 3.0917, Test Loss: 3.1207\n",
      "Epoch 568, Train Loss: 3.0959, Test Loss: 3.1209\n",
      "Epoch 569, Train Loss: 3.0980, Test Loss: 3.1199\n",
      "Epoch 570, Train Loss: 3.0735, Test Loss: 3.1188\n",
      "Epoch 571, Train Loss: 3.0806, Test Loss: 3.1182\n",
      "Epoch 572, Train Loss: 3.0814, Test Loss: 3.1179\n",
      "Epoch 573, Train Loss: 3.0935, Test Loss: 3.1179\n",
      "Epoch 574, Train Loss: 3.0993, Test Loss: 3.1180\n",
      "Epoch 575, Train Loss: 3.0826, Test Loss: 3.1183\n",
      "Epoch 576, Train Loss: 3.0838, Test Loss: 3.1191\n",
      "Epoch 577, Train Loss: 3.0940, Test Loss: 3.1186\n",
      "Epoch 578, Train Loss: 3.1125, Test Loss: 3.1175\n",
      "Epoch 579, Train Loss: 3.0959, Test Loss: 3.1167\n",
      "Epoch 580, Train Loss: 3.0819, Test Loss: 3.1165\n",
      "Epoch 581, Train Loss: 3.0997, Test Loss: 3.1167\n",
      "Epoch 582, Train Loss: 3.1008, Test Loss: 3.1175\n",
      "Epoch 583, Train Loss: 3.1026, Test Loss: 3.1191\n",
      "Epoch 584, Train Loss: 3.0919, Test Loss: 3.1202\n",
      "Epoch 585, Train Loss: 3.0769, Test Loss: 3.1204\n",
      "Epoch 586, Train Loss: 3.0892, Test Loss: 3.1204\n",
      "Epoch 587, Train Loss: 3.0780, Test Loss: 3.1201\n",
      "Epoch 588, Train Loss: 3.0736, Test Loss: 3.1198\n",
      "Epoch 589, Train Loss: 3.1018, Test Loss: 3.1191\n",
      "Epoch 590, Train Loss: 3.0715, Test Loss: 3.1191\n",
      "Epoch 591, Train Loss: 3.0687, Test Loss: 3.1190\n",
      "Epoch 592, Train Loss: 3.0823, Test Loss: 3.1190\n",
      "Epoch 593, Train Loss: 3.0933, Test Loss: 3.1189\n",
      "Epoch 594, Train Loss: 3.0999, Test Loss: 3.1187\n",
      "Epoch 595, Train Loss: 3.0988, Test Loss: 3.1185\n",
      "Epoch 596, Train Loss: 3.0716, Test Loss: 3.1183\n",
      "Epoch 597, Train Loss: 3.0931, Test Loss: 3.1179\n",
      "Epoch 598, Train Loss: 3.0780, Test Loss: 3.1181\n",
      "Epoch 599, Train Loss: 3.0817, Test Loss: 3.1179\n",
      "Epoch 600, Train Loss: 3.0838, Test Loss: 3.1181\n",
      "Epoch 601, Train Loss: 3.0697, Test Loss: 3.1185\n",
      "Epoch 602, Train Loss: 3.0834, Test Loss: 3.1190\n",
      "Epoch 603, Train Loss: 3.0696, Test Loss: 3.1194\n",
      "Epoch 604, Train Loss: 3.0773, Test Loss: 3.1197\n",
      "Epoch 605, Train Loss: 3.0982, Test Loss: 3.1204\n",
      "Epoch 606, Train Loss: 3.0899, Test Loss: 3.1208\n",
      "Epoch 607, Train Loss: 3.0692, Test Loss: 3.1209\n",
      "Epoch 608, Train Loss: 3.0581, Test Loss: 3.1212\n",
      "Epoch 609, Train Loss: 3.0950, Test Loss: 3.1217\n",
      "Epoch 610, Train Loss: 3.0720, Test Loss: 3.1219\n",
      "Epoch 611, Train Loss: 3.0924, Test Loss: 3.1216\n",
      "Epoch 612, Train Loss: 3.0914, Test Loss: 3.1207\n",
      "Epoch 613, Train Loss: 3.0854, Test Loss: 3.1195\n",
      "Epoch 614, Train Loss: 3.0694, Test Loss: 3.1193\n",
      "Epoch 615, Train Loss: 3.0714, Test Loss: 3.1192\n",
      "Epoch 616, Train Loss: 3.0936, Test Loss: 3.1188\n",
      "Epoch 617, Train Loss: 3.1006, Test Loss: 3.1192\n",
      "Epoch 618, Train Loss: 3.0996, Test Loss: 3.1206\n",
      "Epoch 619, Train Loss: 3.1050, Test Loss: 3.1220\n",
      "Epoch 620, Train Loss: 3.0812, Test Loss: 3.1215\n",
      "Epoch 621, Train Loss: 3.0877, Test Loss: 3.1193\n",
      "Epoch 622, Train Loss: 3.0925, Test Loss: 3.1182\n",
      "Epoch 623, Train Loss: 3.1084, Test Loss: 3.1188\n",
      "Epoch 624, Train Loss: 3.0665, Test Loss: 3.1187\n",
      "Epoch 625, Train Loss: 3.0938, Test Loss: 3.1197\n",
      "Epoch 626, Train Loss: 3.0748, Test Loss: 3.1206\n",
      "Epoch 627, Train Loss: 3.0708, Test Loss: 3.1208\n",
      "Epoch 628, Train Loss: 3.0841, Test Loss: 3.1193\n",
      "Epoch 629, Train Loss: 3.0822, Test Loss: 3.1182\n",
      "Epoch 630, Train Loss: 3.0901, Test Loss: 3.1176\n",
      "Epoch 631, Train Loss: 3.0904, Test Loss: 3.1176\n",
      "Epoch 632, Train Loss: 3.0669, Test Loss: 3.1182\n",
      "Epoch 633, Train Loss: 3.0974, Test Loss: 3.1192\n",
      "Epoch 634, Train Loss: 3.0786, Test Loss: 3.1201\n",
      "Epoch 635, Train Loss: 3.0497, Test Loss: 3.1202\n",
      "Epoch 636, Train Loss: 3.0863, Test Loss: 3.1192\n",
      "Epoch 637, Train Loss: 3.1041, Test Loss: 3.1188\n",
      "Epoch 638, Train Loss: 3.0764, Test Loss: 3.1187\n",
      "Epoch 639, Train Loss: 3.1005, Test Loss: 3.1188\n",
      "Epoch 640, Train Loss: 3.0831, Test Loss: 3.1195\n",
      "Epoch 641, Train Loss: 3.0933, Test Loss: 3.1204\n",
      "Epoch 642, Train Loss: 3.0777, Test Loss: 3.1207\n",
      "Epoch 643, Train Loss: 3.0806, Test Loss: 3.1193\n",
      "Epoch 644, Train Loss: 3.0746, Test Loss: 3.1181\n",
      "Epoch 645, Train Loss: 3.0951, Test Loss: 3.1177\n",
      "Epoch 646, Train Loss: 3.0674, Test Loss: 3.1176\n",
      "Epoch 647, Train Loss: 3.0773, Test Loss: 3.1177\n",
      "Epoch 648, Train Loss: 3.0869, Test Loss: 3.1177\n",
      "Epoch 649, Train Loss: 3.0990, Test Loss: 3.1178\n",
      "Epoch 650, Train Loss: 3.0924, Test Loss: 3.1179\n",
      "Epoch 651, Train Loss: 3.0715, Test Loss: 3.1177\n",
      "Epoch 652, Train Loss: 3.0732, Test Loss: 3.1172\n",
      "Epoch 653, Train Loss: 3.0778, Test Loss: 3.1164\n",
      "Epoch 654, Train Loss: 3.0877, Test Loss: 3.1158\n",
      "Epoch 655, Train Loss: 3.1060, Test Loss: 3.1157\n",
      "Epoch 656, Train Loss: 3.0681, Test Loss: 3.1159\n",
      "Epoch 657, Train Loss: 3.0771, Test Loss: 3.1165\n",
      "Epoch 658, Train Loss: 3.0882, Test Loss: 3.1171\n",
      "Epoch 659, Train Loss: 3.0932, Test Loss: 3.1181\n",
      "Epoch 660, Train Loss: 3.0725, Test Loss: 3.1182\n",
      "Epoch 661, Train Loss: 3.0896, Test Loss: 3.1183\n",
      "Epoch 662, Train Loss: 3.0872, Test Loss: 3.1183\n",
      "Epoch 663, Train Loss: 3.1003, Test Loss: 3.1184\n",
      "Epoch 664, Train Loss: 3.0754, Test Loss: 3.1187\n",
      "Epoch 665, Train Loss: 3.0802, Test Loss: 3.1189\n",
      "Epoch 666, Train Loss: 3.0848, Test Loss: 3.1195\n",
      "Epoch 667, Train Loss: 3.0743, Test Loss: 3.1201\n",
      "Epoch 668, Train Loss: 3.0814, Test Loss: 3.1198\n",
      "Epoch 669, Train Loss: 3.0930, Test Loss: 3.1191\n",
      "Epoch 670, Train Loss: 3.0715, Test Loss: 3.1177\n",
      "Epoch 671, Train Loss: 3.0864, Test Loss: 3.1171\n",
      "Epoch 672, Train Loss: 3.0947, Test Loss: 3.1170\n",
      "Epoch 673, Train Loss: 3.0975, Test Loss: 3.1172\n",
      "Epoch 674, Train Loss: 3.0741, Test Loss: 3.1174\n",
      "Epoch 675, Train Loss: 3.0835, Test Loss: 3.1173\n",
      "Epoch 676, Train Loss: 3.0749, Test Loss: 3.1170\n",
      "Epoch 677, Train Loss: 3.0787, Test Loss: 3.1164\n",
      "Epoch 678, Train Loss: 3.0760, Test Loss: 3.1160\n",
      "Epoch 679, Train Loss: 3.0618, Test Loss: 3.1161\n",
      "Epoch 680, Train Loss: 3.0980, Test Loss: 3.1167\n",
      "Epoch 681, Train Loss: 3.0673, Test Loss: 3.1176\n",
      "Epoch 682, Train Loss: 3.0845, Test Loss: 3.1187\n",
      "Epoch 683, Train Loss: 3.0696, Test Loss: 3.1194\n",
      "Epoch 684, Train Loss: 3.0789, Test Loss: 3.1192\n",
      "Epoch 685, Train Loss: 3.0530, Test Loss: 3.1189\n",
      "Epoch 686, Train Loss: 3.1034, Test Loss: 3.1191\n",
      "Epoch 687, Train Loss: 3.1015, Test Loss: 3.1198\n",
      "Epoch 688, Train Loss: 3.0984, Test Loss: 3.1206\n",
      "Epoch 689, Train Loss: 3.0898, Test Loss: 3.1210\n",
      "Epoch 690, Train Loss: 3.0594, Test Loss: 3.1207\n",
      "Epoch 691, Train Loss: 3.0868, Test Loss: 3.1199\n",
      "Epoch 692, Train Loss: 3.0776, Test Loss: 3.1195\n",
      "Epoch 693, Train Loss: 3.0786, Test Loss: 3.1192\n",
      "Epoch 694, Train Loss: 3.0970, Test Loss: 3.1191\n",
      "Epoch 695, Train Loss: 3.0642, Test Loss: 3.1187\n",
      "Epoch 696, Train Loss: 3.0713, Test Loss: 3.1179\n",
      "Epoch 697, Train Loss: 3.0694, Test Loss: 3.1173\n",
      "Epoch 698, Train Loss: 3.0776, Test Loss: 3.1172\n",
      "Epoch 699, Train Loss: 3.0677, Test Loss: 3.1174\n",
      "Epoch 700, Train Loss: 3.0848, Test Loss: 3.1176\n",
      "Epoch 701, Train Loss: 3.0951, Test Loss: 3.1176\n",
      "Epoch 702, Train Loss: 3.0879, Test Loss: 3.1174\n",
      "Epoch 703, Train Loss: 3.0677, Test Loss: 3.1172\n",
      "Epoch 704, Train Loss: 3.0717, Test Loss: 3.1172\n",
      "Epoch 705, Train Loss: 3.0730, Test Loss: 3.1174\n",
      "Epoch 706, Train Loss: 3.0833, Test Loss: 3.1179\n",
      "Epoch 707, Train Loss: 3.0860, Test Loss: 3.1186\n",
      "Epoch 708, Train Loss: 3.0839, Test Loss: 3.1192\n",
      "Epoch 709, Train Loss: 3.0819, Test Loss: 3.1183\n",
      "Epoch 710, Train Loss: 3.0777, Test Loss: 3.1174\n",
      "Epoch 711, Train Loss: 3.0876, Test Loss: 3.1166\n",
      "Epoch 712, Train Loss: 3.0813, Test Loss: 3.1165\n",
      "Epoch 713, Train Loss: 3.0876, Test Loss: 3.1169\n",
      "Epoch 714, Train Loss: 3.0778, Test Loss: 3.1181\n",
      "Epoch 715, Train Loss: 3.0911, Test Loss: 3.1195\n",
      "Epoch 716, Train Loss: 3.0813, Test Loss: 3.1196\n",
      "Epoch 717, Train Loss: 3.0796, Test Loss: 3.1192\n",
      "Epoch 718, Train Loss: 3.0831, Test Loss: 3.1185\n",
      "Epoch 719, Train Loss: 3.0702, Test Loss: 3.1180\n",
      "Epoch 720, Train Loss: 3.1061, Test Loss: 3.1180\n",
      "Epoch 721, Train Loss: 3.0875, Test Loss: 3.1178\n",
      "Epoch 722, Train Loss: 3.0907, Test Loss: 3.1183\n",
      "Epoch 723, Train Loss: 3.0885, Test Loss: 3.1190\n",
      "Epoch 724, Train Loss: 3.1083, Test Loss: 3.1193\n",
      "Epoch 725, Train Loss: 3.0685, Test Loss: 3.1178\n",
      "Epoch 726, Train Loss: 3.0917, Test Loss: 3.1169\n",
      "Epoch 727, Train Loss: 3.0859, Test Loss: 3.1163\n",
      "Epoch 728, Train Loss: 3.0813, Test Loss: 3.1162\n",
      "Epoch 729, Train Loss: 3.0766, Test Loss: 3.1163\n",
      "Epoch 730, Train Loss: 3.0608, Test Loss: 3.1164\n",
      "Epoch 731, Train Loss: 3.0809, Test Loss: 3.1160\n",
      "Epoch 732, Train Loss: 3.0819, Test Loss: 3.1151\n",
      "Epoch 733, Train Loss: 3.0577, Test Loss: 3.1147\n",
      "Epoch 734, Train Loss: 3.0744, Test Loss: 3.1148\n",
      "Epoch 735, Train Loss: 3.0736, Test Loss: 3.1153\n",
      "Epoch 736, Train Loss: 3.0816, Test Loss: 3.1165\n",
      "Epoch 737, Train Loss: 3.0775, Test Loss: 3.1187\n",
      "Epoch 738, Train Loss: 3.0816, Test Loss: 3.1201\n",
      "Epoch 739, Train Loss: 3.0820, Test Loss: 3.1192\n",
      "Epoch 740, Train Loss: 3.1027, Test Loss: 3.1171\n",
      "Epoch 741, Train Loss: 3.0961, Test Loss: 3.1172\n",
      "Epoch 742, Train Loss: 3.0637, Test Loss: 3.1170\n",
      "Epoch 743, Train Loss: 3.0692, Test Loss: 3.1165\n",
      "Epoch 744, Train Loss: 3.0653, Test Loss: 3.1170\n",
      "Epoch 745, Train Loss: 3.0964, Test Loss: 3.1181\n",
      "Epoch 746, Train Loss: 3.0815, Test Loss: 3.1170\n",
      "Epoch 747, Train Loss: 3.0833, Test Loss: 3.1150\n",
      "Epoch 748, Train Loss: 3.0795, Test Loss: 3.1138\n",
      "Epoch 749, Train Loss: 3.0949, Test Loss: 3.1140\n",
      "Epoch 750, Train Loss: 3.0692, Test Loss: 3.1147\n",
      "Epoch 751, Train Loss: 3.0805, Test Loss: 3.1152\n",
      "Epoch 752, Train Loss: 3.0792, Test Loss: 3.1168\n",
      "Epoch 753, Train Loss: 3.0620, Test Loss: 3.1193\n",
      "Epoch 754, Train Loss: 3.0812, Test Loss: 3.1200\n",
      "Epoch 755, Train Loss: 3.0711, Test Loss: 3.1193\n",
      "Epoch 756, Train Loss: 3.0641, Test Loss: 3.1187\n",
      "Epoch 757, Train Loss: 3.0860, Test Loss: 3.1186\n",
      "Epoch 758, Train Loss: 3.0633, Test Loss: 3.1185\n",
      "Epoch 759, Train Loss: 3.0907, Test Loss: 3.1183\n",
      "Epoch 760, Train Loss: 3.0643, Test Loss: 3.1183\n",
      "Epoch 761, Train Loss: 3.0620, Test Loss: 3.1185\n",
      "Epoch 762, Train Loss: 3.0606, Test Loss: 3.1185\n",
      "Epoch 763, Train Loss: 3.0744, Test Loss: 3.1185\n",
      "Epoch 764, Train Loss: 3.0821, Test Loss: 3.1176\n",
      "Epoch 765, Train Loss: 3.0624, Test Loss: 3.1163\n",
      "Epoch 766, Train Loss: 3.0883, Test Loss: 3.1158\n",
      "Epoch 767, Train Loss: 3.0762, Test Loss: 3.1156\n",
      "Epoch 768, Train Loss: 3.0781, Test Loss: 3.1155\n",
      "Epoch 769, Train Loss: 3.0936, Test Loss: 3.1160\n",
      "Epoch 770, Train Loss: 3.0696, Test Loss: 3.1161\n",
      "Epoch 771, Train Loss: 3.0647, Test Loss: 3.1161\n",
      "Epoch 772, Train Loss: 3.0795, Test Loss: 3.1162\n",
      "Epoch 773, Train Loss: 3.0586, Test Loss: 3.1166\n",
      "Epoch 774, Train Loss: 3.0665, Test Loss: 3.1163\n",
      "Epoch 775, Train Loss: 3.0721, Test Loss: 3.1160\n",
      "Epoch 776, Train Loss: 3.0819, Test Loss: 3.1154\n",
      "Epoch 777, Train Loss: 3.0788, Test Loss: 3.1152\n",
      "Epoch 778, Train Loss: 3.0771, Test Loss: 3.1157\n",
      "Epoch 779, Train Loss: 3.0549, Test Loss: 3.1165\n",
      "Epoch 780, Train Loss: 3.0772, Test Loss: 3.1172\n",
      "Epoch 781, Train Loss: 3.0895, Test Loss: 3.1172\n",
      "Epoch 782, Train Loss: 3.0808, Test Loss: 3.1165\n",
      "Epoch 783, Train Loss: 3.0873, Test Loss: 3.1160\n",
      "Epoch 784, Train Loss: 3.0769, Test Loss: 3.1161\n",
      "Epoch 785, Train Loss: 3.0751, Test Loss: 3.1162\n",
      "Epoch 786, Train Loss: 3.0872, Test Loss: 3.1164\n",
      "Epoch 787, Train Loss: 3.0842, Test Loss: 3.1164\n",
      "Epoch 788, Train Loss: 3.0562, Test Loss: 3.1168\n",
      "Epoch 789, Train Loss: 3.0682, Test Loss: 3.1169\n",
      "Epoch 790, Train Loss: 3.0673, Test Loss: 3.1165\n",
      "Epoch 791, Train Loss: 3.0672, Test Loss: 3.1153\n",
      "Epoch 792, Train Loss: 3.0697, Test Loss: 3.1146\n",
      "Epoch 793, Train Loss: 3.0817, Test Loss: 3.1143\n",
      "Epoch 794, Train Loss: 3.0772, Test Loss: 3.1144\n",
      "Epoch 795, Train Loss: 3.0777, Test Loss: 3.1145\n",
      "Epoch 796, Train Loss: 3.0662, Test Loss: 3.1146\n",
      "Epoch 797, Train Loss: 3.0946, Test Loss: 3.1149\n",
      "Epoch 798, Train Loss: 3.0713, Test Loss: 3.1155\n",
      "Epoch 799, Train Loss: 3.0630, Test Loss: 3.1160\n",
      "Epoch 800, Train Loss: 3.0860, Test Loss: 3.1163\n",
      "Epoch 801, Train Loss: 3.0812, Test Loss: 3.1161\n",
      "Epoch 802, Train Loss: 3.0799, Test Loss: 3.1162\n",
      "Epoch 803, Train Loss: 3.0814, Test Loss: 3.1155\n",
      "Epoch 804, Train Loss: 3.0508, Test Loss: 3.1153\n",
      "Epoch 805, Train Loss: 3.0762, Test Loss: 3.1152\n",
      "Epoch 806, Train Loss: 3.0705, Test Loss: 3.1151\n",
      "Epoch 807, Train Loss: 3.0741, Test Loss: 3.1151\n",
      "Epoch 808, Train Loss: 3.0880, Test Loss: 3.1151\n",
      "Epoch 809, Train Loss: 3.0813, Test Loss: 3.1156\n",
      "Epoch 810, Train Loss: 3.0611, Test Loss: 3.1156\n",
      "Epoch 811, Train Loss: 3.0799, Test Loss: 3.1156\n",
      "Epoch 812, Train Loss: 3.0640, Test Loss: 3.1157\n",
      "Epoch 813, Train Loss: 3.0843, Test Loss: 3.1159\n",
      "Epoch 814, Train Loss: 3.0709, Test Loss: 3.1162\n",
      "Epoch 815, Train Loss: 3.0738, Test Loss: 3.1166\n",
      "Epoch 816, Train Loss: 3.0651, Test Loss: 3.1166\n",
      "Epoch 817, Train Loss: 3.0584, Test Loss: 3.1165\n",
      "Epoch 818, Train Loss: 3.0877, Test Loss: 3.1164\n",
      "Epoch 819, Train Loss: 3.0702, Test Loss: 3.1163\n",
      "Epoch 820, Train Loss: 3.0798, Test Loss: 3.1164\n",
      "Epoch 821, Train Loss: 3.0829, Test Loss: 3.1161\n",
      "Epoch 822, Train Loss: 3.0694, Test Loss: 3.1155\n",
      "Epoch 823, Train Loss: 3.0776, Test Loss: 3.1151\n",
      "Epoch 824, Train Loss: 3.0954, Test Loss: 3.1148\n",
      "Epoch 825, Train Loss: 3.0644, Test Loss: 3.1149\n",
      "Epoch 826, Train Loss: 3.0699, Test Loss: 3.1147\n",
      "Epoch 827, Train Loss: 3.0579, Test Loss: 3.1150\n",
      "Epoch 828, Train Loss: 3.0716, Test Loss: 3.1143\n",
      "Epoch 829, Train Loss: 3.0521, Test Loss: 3.1138\n",
      "Epoch 830, Train Loss: 3.0647, Test Loss: 3.1135\n",
      "Epoch 831, Train Loss: 3.0728, Test Loss: 3.1139\n",
      "Epoch 832, Train Loss: 3.0537, Test Loss: 3.1146\n",
      "Epoch 833, Train Loss: 3.0705, Test Loss: 3.1153\n",
      "Epoch 834, Train Loss: 3.0763, Test Loss: 3.1150\n",
      "Epoch 835, Train Loss: 3.0836, Test Loss: 3.1145\n",
      "Epoch 836, Train Loss: 3.0657, Test Loss: 3.1145\n",
      "Epoch 837, Train Loss: 3.0665, Test Loss: 3.1145\n",
      "Epoch 838, Train Loss: 3.0644, Test Loss: 3.1148\n",
      "Epoch 839, Train Loss: 3.0608, Test Loss: 3.1151\n",
      "Epoch 840, Train Loss: 3.0695, Test Loss: 3.1159\n",
      "Epoch 841, Train Loss: 3.0657, Test Loss: 3.1163\n",
      "Epoch 842, Train Loss: 3.0681, Test Loss: 3.1158\n",
      "Epoch 843, Train Loss: 3.0635, Test Loss: 3.1148\n",
      "Epoch 844, Train Loss: 3.0785, Test Loss: 3.1140\n",
      "Epoch 845, Train Loss: 3.0638, Test Loss: 3.1137\n",
      "Epoch 846, Train Loss: 3.0687, Test Loss: 3.1138\n",
      "Epoch 847, Train Loss: 3.0720, Test Loss: 3.1139\n",
      "Epoch 848, Train Loss: 3.0860, Test Loss: 3.1149\n",
      "Epoch 849, Train Loss: 3.0744, Test Loss: 3.1149\n",
      "Epoch 850, Train Loss: 3.0660, Test Loss: 3.1140\n",
      "Epoch 851, Train Loss: 3.0654, Test Loss: 3.1131\n",
      "Epoch 852, Train Loss: 3.0932, Test Loss: 3.1125\n",
      "Epoch 853, Train Loss: 3.0853, Test Loss: 3.1129\n",
      "Epoch 854, Train Loss: 3.0683, Test Loss: 3.1135\n",
      "Epoch 855, Train Loss: 3.0744, Test Loss: 3.1149\n",
      "Epoch 856, Train Loss: 3.0695, Test Loss: 3.1165\n",
      "Epoch 857, Train Loss: 3.0757, Test Loss: 3.1177\n",
      "Epoch 858, Train Loss: 3.0693, Test Loss: 3.1169\n",
      "Epoch 859, Train Loss: 3.0696, Test Loss: 3.1163\n",
      "Epoch 860, Train Loss: 3.0597, Test Loss: 3.1159\n",
      "Epoch 861, Train Loss: 3.0577, Test Loss: 3.1159\n",
      "Epoch 862, Train Loss: 3.0588, Test Loss: 3.1159\n",
      "Epoch 863, Train Loss: 3.0558, Test Loss: 3.1164\n",
      "Epoch 864, Train Loss: 3.0792, Test Loss: 3.1168\n",
      "Epoch 865, Train Loss: 3.0935, Test Loss: 3.1165\n",
      "Epoch 866, Train Loss: 3.0914, Test Loss: 3.1152\n",
      "Epoch 867, Train Loss: 3.0607, Test Loss: 3.1147\n",
      "Epoch 868, Train Loss: 3.0643, Test Loss: 3.1146\n",
      "Epoch 869, Train Loss: 3.0605, Test Loss: 3.1148\n",
      "Epoch 870, Train Loss: 3.0656, Test Loss: 3.1154\n",
      "Epoch 871, Train Loss: 3.0622, Test Loss: 3.1162\n",
      "Epoch 872, Train Loss: 3.0628, Test Loss: 3.1166\n",
      "Epoch 873, Train Loss: 3.0506, Test Loss: 3.1170\n",
      "Epoch 874, Train Loss: 3.0914, Test Loss: 3.1168\n",
      "Epoch 875, Train Loss: 3.0616, Test Loss: 3.1164\n",
      "Epoch 876, Train Loss: 3.0696, Test Loss: 3.1164\n",
      "Epoch 877, Train Loss: 3.0558, Test Loss: 3.1170\n",
      "Epoch 878, Train Loss: 3.0647, Test Loss: 3.1184\n",
      "Epoch 879, Train Loss: 3.0686, Test Loss: 3.1188\n",
      "Epoch 880, Train Loss: 3.0699, Test Loss: 3.1177\n",
      "Epoch 881, Train Loss: 3.0595, Test Loss: 3.1162\n",
      "Epoch 882, Train Loss: 3.0762, Test Loss: 3.1156\n",
      "Epoch 883, Train Loss: 3.0726, Test Loss: 3.1152\n",
      "Epoch 884, Train Loss: 3.0562, Test Loss: 3.1149\n",
      "Epoch 885, Train Loss: 3.0879, Test Loss: 3.1147\n",
      "Epoch 886, Train Loss: 3.0649, Test Loss: 3.1147\n",
      "Epoch 887, Train Loss: 3.0768, Test Loss: 3.1146\n",
      "Epoch 888, Train Loss: 3.0789, Test Loss: 3.1145\n",
      "Epoch 889, Train Loss: 3.0729, Test Loss: 3.1144\n",
      "Epoch 890, Train Loss: 3.0577, Test Loss: 3.1144\n",
      "Epoch 891, Train Loss: 3.0616, Test Loss: 3.1148\n",
      "Epoch 892, Train Loss: 3.0677, Test Loss: 3.1150\n",
      "Epoch 893, Train Loss: 3.0629, Test Loss: 3.1148\n",
      "Epoch 894, Train Loss: 3.0722, Test Loss: 3.1150\n",
      "Epoch 895, Train Loss: 3.0720, Test Loss: 3.1151\n",
      "Epoch 896, Train Loss: 3.0458, Test Loss: 3.1153\n",
      "Epoch 897, Train Loss: 3.0890, Test Loss: 3.1162\n",
      "Epoch 898, Train Loss: 3.0720, Test Loss: 3.1173\n",
      "Epoch 899, Train Loss: 3.0874, Test Loss: 3.1160\n",
      "Epoch 900, Train Loss: 3.0532, Test Loss: 3.1146\n",
      "Epoch 901, Train Loss: 3.0778, Test Loss: 3.1141\n",
      "Epoch 902, Train Loss: 3.0580, Test Loss: 3.1138\n",
      "Epoch 903, Train Loss: 3.0781, Test Loss: 3.1144\n",
      "Epoch 904, Train Loss: 3.0527, Test Loss: 3.1154\n",
      "Epoch 905, Train Loss: 3.0703, Test Loss: 3.1151\n",
      "Epoch 906, Train Loss: 3.0855, Test Loss: 3.1148\n",
      "Epoch 907, Train Loss: 3.0625, Test Loss: 3.1150\n",
      "Epoch 908, Train Loss: 3.0659, Test Loss: 3.1157\n",
      "Epoch 909, Train Loss: 3.0781, Test Loss: 3.1166\n",
      "Epoch 910, Train Loss: 3.0756, Test Loss: 3.1170\n",
      "Epoch 911, Train Loss: 3.0549, Test Loss: 3.1170\n",
      "Epoch 912, Train Loss: 3.0717, Test Loss: 3.1168\n",
      "Epoch 913, Train Loss: 3.0475, Test Loss: 3.1165\n",
      "Epoch 914, Train Loss: 3.0565, Test Loss: 3.1162\n",
      "Epoch 915, Train Loss: 3.0574, Test Loss: 3.1157\n",
      "Epoch 916, Train Loss: 3.0673, Test Loss: 3.1149\n",
      "Epoch 917, Train Loss: 3.0773, Test Loss: 3.1143\n",
      "Epoch 918, Train Loss: 3.0889, Test Loss: 3.1143\n",
      "Epoch 919, Train Loss: 3.0822, Test Loss: 3.1146\n",
      "Epoch 920, Train Loss: 3.0747, Test Loss: 3.1151\n",
      "Epoch 921, Train Loss: 3.0484, Test Loss: 3.1144\n",
      "Epoch 922, Train Loss: 3.0641, Test Loss: 3.1127\n",
      "Epoch 923, Train Loss: 3.0694, Test Loss: 3.1126\n",
      "Epoch 924, Train Loss: 3.0784, Test Loss: 3.1133\n",
      "Epoch 925, Train Loss: 3.0762, Test Loss: 3.1143\n",
      "Epoch 926, Train Loss: 3.0469, Test Loss: 3.1154\n",
      "Epoch 927, Train Loss: 3.0663, Test Loss: 3.1166\n",
      "Epoch 928, Train Loss: 3.0654, Test Loss: 3.1163\n",
      "Epoch 929, Train Loss: 3.0650, Test Loss: 3.1157\n",
      "Epoch 930, Train Loss: 3.0937, Test Loss: 3.1151\n",
      "Epoch 931, Train Loss: 3.0504, Test Loss: 3.1145\n",
      "Epoch 932, Train Loss: 3.0639, Test Loss: 3.1147\n",
      "Epoch 933, Train Loss: 3.0748, Test Loss: 3.1150\n",
      "Epoch 934, Train Loss: 3.0717, Test Loss: 3.1151\n",
      "Epoch 935, Train Loss: 3.0747, Test Loss: 3.1149\n",
      "Epoch 936, Train Loss: 3.0873, Test Loss: 3.1150\n",
      "Epoch 937, Train Loss: 3.0771, Test Loss: 3.1146\n",
      "Epoch 938, Train Loss: 3.0686, Test Loss: 3.1143\n",
      "Epoch 939, Train Loss: 3.0801, Test Loss: 3.1142\n",
      "Epoch 940, Train Loss: 3.0689, Test Loss: 3.1144\n",
      "Epoch 941, Train Loss: 3.0684, Test Loss: 3.1145\n",
      "Epoch 942, Train Loss: 3.0740, Test Loss: 3.1150\n",
      "Epoch 943, Train Loss: 3.0641, Test Loss: 3.1154\n",
      "Epoch 944, Train Loss: 3.0694, Test Loss: 3.1145\n",
      "Epoch 945, Train Loss: 3.0663, Test Loss: 3.1138\n",
      "Epoch 946, Train Loss: 3.0672, Test Loss: 3.1135\n",
      "Epoch 947, Train Loss: 3.0875, Test Loss: 3.1136\n",
      "Epoch 948, Train Loss: 3.0941, Test Loss: 3.1138\n",
      "Epoch 949, Train Loss: 3.0685, Test Loss: 3.1139\n",
      "Epoch 950, Train Loss: 3.0765, Test Loss: 3.1143\n",
      "Epoch 951, Train Loss: 3.0619, Test Loss: 3.1149\n",
      "Epoch 952, Train Loss: 3.0576, Test Loss: 3.1145\n",
      "Epoch 953, Train Loss: 3.0545, Test Loss: 3.1139\n",
      "Epoch 954, Train Loss: 3.0531, Test Loss: 3.1139\n",
      "Epoch 955, Train Loss: 3.0906, Test Loss: 3.1138\n",
      "Epoch 956, Train Loss: 3.0684, Test Loss: 3.1144\n",
      "Epoch 957, Train Loss: 3.0557, Test Loss: 3.1153\n",
      "Epoch 958, Train Loss: 3.0644, Test Loss: 3.1153\n",
      "Epoch 959, Train Loss: 3.0566, Test Loss: 3.1146\n",
      "Epoch 960, Train Loss: 3.0731, Test Loss: 3.1138\n",
      "Epoch 961, Train Loss: 3.0767, Test Loss: 3.1135\n",
      "Epoch 962, Train Loss: 3.0634, Test Loss: 3.1133\n",
      "Epoch 963, Train Loss: 3.0684, Test Loss: 3.1135\n",
      "Epoch 964, Train Loss: 3.0576, Test Loss: 3.1132\n",
      "Epoch 965, Train Loss: 3.0520, Test Loss: 3.1125\n",
      "Epoch 966, Train Loss: 3.0594, Test Loss: 3.1121\n",
      "Epoch 967, Train Loss: 3.0576, Test Loss: 3.1117\n",
      "Epoch 968, Train Loss: 3.0621, Test Loss: 3.1121\n",
      "Epoch 969, Train Loss: 3.0804, Test Loss: 3.1126\n",
      "Epoch 970, Train Loss: 3.0735, Test Loss: 3.1132\n",
      "Epoch 971, Train Loss: 3.0528, Test Loss: 3.1137\n",
      "Epoch 972, Train Loss: 3.0669, Test Loss: 3.1145\n",
      "Epoch 973, Train Loss: 3.0646, Test Loss: 3.1153\n",
      "Epoch 974, Train Loss: 3.0646, Test Loss: 3.1149\n",
      "Epoch 975, Train Loss: 3.0717, Test Loss: 3.1148\n",
      "Epoch 976, Train Loss: 3.0533, Test Loss: 3.1146\n",
      "Epoch 977, Train Loss: 3.0723, Test Loss: 3.1143\n",
      "Epoch 978, Train Loss: 3.0625, Test Loss: 3.1139\n",
      "Epoch 979, Train Loss: 3.0740, Test Loss: 3.1133\n",
      "Epoch 980, Train Loss: 3.0624, Test Loss: 3.1130\n",
      "Epoch 981, Train Loss: 3.0504, Test Loss: 3.1129\n",
      "Epoch 982, Train Loss: 3.0601, Test Loss: 3.1134\n",
      "Epoch 983, Train Loss: 3.0642, Test Loss: 3.1141\n",
      "Epoch 984, Train Loss: 3.0493, Test Loss: 3.1128\n",
      "Epoch 985, Train Loss: 3.0492, Test Loss: 3.1120\n",
      "Epoch 986, Train Loss: 3.0657, Test Loss: 3.1118\n",
      "Epoch 987, Train Loss: 3.0640, Test Loss: 3.1122\n",
      "Epoch 988, Train Loss: 3.0640, Test Loss: 3.1129\n",
      "Epoch 989, Train Loss: 3.0523, Test Loss: 3.1145\n",
      "Epoch 990, Train Loss: 3.0726, Test Loss: 3.1162\n",
      "Epoch 991, Train Loss: 3.0569, Test Loss: 3.1159\n",
      "Epoch 992, Train Loss: 3.0609, Test Loss: 3.1150\n",
      "Epoch 993, Train Loss: 3.0534, Test Loss: 3.1133\n",
      "Epoch 994, Train Loss: 3.0916, Test Loss: 3.1124\n",
      "Epoch 995, Train Loss: 3.0658, Test Loss: 3.1122\n",
      "Epoch 996, Train Loss: 3.0718, Test Loss: 3.1120\n",
      "Epoch 997, Train Loss: 3.0609, Test Loss: 3.1122\n",
      "Epoch 998, Train Loss: 3.0724, Test Loss: 3.1125\n",
      "Epoch 999, Train Loss: 3.0668, Test Loss: 3.1130\n",
      "Epoch 1000, Train Loss: 3.0683, Test Loss: 3.1124\n",
      "Epoch 1001, Train Loss: 3.0579, Test Loss: 3.1115\n",
      "Epoch 1002, Train Loss: 3.0657, Test Loss: 3.1109\n",
      "Epoch 1003, Train Loss: 3.0649, Test Loss: 3.1106\n",
      "Epoch 1004, Train Loss: 3.0825, Test Loss: 3.1111\n",
      "Epoch 1005, Train Loss: 3.0767, Test Loss: 3.1121\n",
      "Epoch 1006, Train Loss: 3.0658, Test Loss: 3.1128\n",
      "Epoch 1007, Train Loss: 3.0866, Test Loss: 3.1135\n",
      "Epoch 1008, Train Loss: 3.0535, Test Loss: 3.1138\n",
      "Epoch 1009, Train Loss: 3.0845, Test Loss: 3.1139\n",
      "Epoch 1010, Train Loss: 3.0650, Test Loss: 3.1140\n",
      "Epoch 1011, Train Loss: 3.0701, Test Loss: 3.1142\n",
      "Epoch 1012, Train Loss: 3.0736, Test Loss: 3.1140\n",
      "Epoch 1013, Train Loss: 3.0798, Test Loss: 3.1142\n",
      "Epoch 1014, Train Loss: 3.0653, Test Loss: 3.1141\n",
      "Epoch 1015, Train Loss: 3.0731, Test Loss: 3.1135\n",
      "Epoch 1016, Train Loss: 3.0620, Test Loss: 3.1131\n",
      "Epoch 1017, Train Loss: 3.0685, Test Loss: 3.1131\n",
      "Epoch 1018, Train Loss: 3.0640, Test Loss: 3.1130\n",
      "Epoch 1019, Train Loss: 3.0524, Test Loss: 3.1136\n",
      "Epoch 1020, Train Loss: 3.0667, Test Loss: 3.1140\n",
      "Epoch 1021, Train Loss: 3.0637, Test Loss: 3.1142\n",
      "Epoch 1022, Train Loss: 3.0708, Test Loss: 3.1136\n",
      "Epoch 1023, Train Loss: 3.0593, Test Loss: 3.1129\n",
      "Epoch 1024, Train Loss: 3.0652, Test Loss: 3.1126\n",
      "Epoch 1025, Train Loss: 3.0683, Test Loss: 3.1123\n",
      "Epoch 1026, Train Loss: 3.0672, Test Loss: 3.1121\n",
      "Epoch 1027, Train Loss: 3.0582, Test Loss: 3.1123\n",
      "Epoch 1028, Train Loss: 3.0848, Test Loss: 3.1128\n",
      "Epoch 1029, Train Loss: 3.0788, Test Loss: 3.1128\n",
      "Epoch 1030, Train Loss: 3.0604, Test Loss: 3.1129\n",
      "Epoch 1031, Train Loss: 3.0824, Test Loss: 3.1130\n",
      "Epoch 1032, Train Loss: 3.0844, Test Loss: 3.1133\n",
      "Epoch 1033, Train Loss: 3.0745, Test Loss: 3.1145\n",
      "Epoch 1034, Train Loss: 3.0570, Test Loss: 3.1157\n",
      "Epoch 1035, Train Loss: 3.0371, Test Loss: 3.1154\n",
      "Epoch 1036, Train Loss: 3.0441, Test Loss: 3.1144\n",
      "Epoch 1037, Train Loss: 3.0664, Test Loss: 3.1137\n",
      "Epoch 1038, Train Loss: 3.0678, Test Loss: 3.1134\n",
      "Epoch 1039, Train Loss: 3.0769, Test Loss: 3.1131\n",
      "Epoch 1040, Train Loss: 3.0717, Test Loss: 3.1126\n",
      "Epoch 1041, Train Loss: 3.0737, Test Loss: 3.1129\n",
      "Epoch 1042, Train Loss: 3.0861, Test Loss: 3.1132\n",
      "Epoch 1043, Train Loss: 3.0628, Test Loss: 3.1136\n",
      "Epoch 1044, Train Loss: 3.0607, Test Loss: 3.1135\n",
      "Epoch 1045, Train Loss: 3.0689, Test Loss: 3.1135\n",
      "Epoch 1046, Train Loss: 3.0646, Test Loss: 3.1139\n",
      "Epoch 1047, Train Loss: 3.0684, Test Loss: 3.1146\n",
      "Epoch 1048, Train Loss: 3.0649, Test Loss: 3.1148\n",
      "Epoch 1049, Train Loss: 3.0547, Test Loss: 3.1143\n",
      "Epoch 1050, Train Loss: 3.0671, Test Loss: 3.1137\n",
      "Epoch 1051, Train Loss: 3.0800, Test Loss: 3.1135\n",
      "Epoch 1052, Train Loss: 3.0634, Test Loss: 3.1137\n",
      "Epoch 1053, Train Loss: 3.0802, Test Loss: 3.1138\n",
      "Epoch 1054, Train Loss: 3.0767, Test Loss: 3.1135\n",
      "Epoch 1055, Train Loss: 3.0431, Test Loss: 3.1134\n",
      "Epoch 1056, Train Loss: 3.0634, Test Loss: 3.1131\n",
      "Epoch 1057, Train Loss: 3.0546, Test Loss: 3.1123\n",
      "Epoch 1058, Train Loss: 3.0636, Test Loss: 3.1119\n",
      "Epoch 1059, Train Loss: 3.0520, Test Loss: 3.1122\n",
      "Epoch 1060, Train Loss: 3.0504, Test Loss: 3.1129\n",
      "Epoch 1061, Train Loss: 3.0580, Test Loss: 3.1133\n",
      "Epoch 1062, Train Loss: 3.0609, Test Loss: 3.1144\n",
      "Epoch 1063, Train Loss: 3.0666, Test Loss: 3.1146\n",
      "Epoch 1064, Train Loss: 3.0647, Test Loss: 3.1142\n",
      "Epoch 1065, Train Loss: 3.0513, Test Loss: 3.1141\n",
      "Epoch 1066, Train Loss: 3.0699, Test Loss: 3.1138\n",
      "Epoch 1067, Train Loss: 3.0668, Test Loss: 3.1139\n",
      "Epoch 1068, Train Loss: 3.0710, Test Loss: 3.1140\n",
      "Epoch 1069, Train Loss: 3.0667, Test Loss: 3.1139\n",
      "Epoch 1070, Train Loss: 3.0522, Test Loss: 3.1136\n",
      "Epoch 1071, Train Loss: 3.0667, Test Loss: 3.1135\n",
      "Epoch 1072, Train Loss: 3.0728, Test Loss: 3.1138\n",
      "Epoch 1073, Train Loss: 3.0506, Test Loss: 3.1137\n",
      "Epoch 1074, Train Loss: 3.0797, Test Loss: 3.1132\n",
      "Epoch 1075, Train Loss: 3.0663, Test Loss: 3.1133\n",
      "Epoch 1076, Train Loss: 3.0687, Test Loss: 3.1126\n",
      "Epoch 1077, Train Loss: 3.0587, Test Loss: 3.1122\n",
      "Epoch 1078, Train Loss: 3.0646, Test Loss: 3.1123\n",
      "Epoch 1079, Train Loss: 3.0721, Test Loss: 3.1129\n",
      "Epoch 1080, Train Loss: 3.0707, Test Loss: 3.1133\n",
      "Epoch 1081, Train Loss: 3.0693, Test Loss: 3.1132\n",
      "Epoch 1082, Train Loss: 3.0842, Test Loss: 3.1129\n",
      "Epoch 1083, Train Loss: 3.0654, Test Loss: 3.1123\n",
      "Epoch 1084, Train Loss: 3.0769, Test Loss: 3.1122\n",
      "Epoch 1085, Train Loss: 3.0935, Test Loss: 3.1117\n",
      "Epoch 1086, Train Loss: 3.0660, Test Loss: 3.1119\n",
      "Epoch 1087, Train Loss: 3.0804, Test Loss: 3.1122\n",
      "Epoch 1088, Train Loss: 3.0635, Test Loss: 3.1118\n",
      "Epoch 1089, Train Loss: 3.0631, Test Loss: 3.1114\n",
      "Epoch 1090, Train Loss: 3.0614, Test Loss: 3.1114\n",
      "Epoch 1091, Train Loss: 3.0694, Test Loss: 3.1108\n",
      "Epoch 1092, Train Loss: 3.0489, Test Loss: 3.1102\n",
      "Epoch 1093, Train Loss: 3.0783, Test Loss: 3.1101\n",
      "Epoch 1094, Train Loss: 3.0449, Test Loss: 3.1102\n",
      "Epoch 1095, Train Loss: 3.0656, Test Loss: 3.1109\n",
      "Epoch 1096, Train Loss: 3.0701, Test Loss: 3.1121\n",
      "Epoch 1097, Train Loss: 3.0590, Test Loss: 3.1129\n",
      "Epoch 1098, Train Loss: 3.0544, Test Loss: 3.1123\n",
      "Epoch 1099, Train Loss: 3.0408, Test Loss: 3.1114\n",
      "Epoch 1100, Train Loss: 3.0543, Test Loss: 3.1112\n",
      "Epoch 1101, Train Loss: 3.0611, Test Loss: 3.1119\n",
      "Epoch 1102, Train Loss: 3.0649, Test Loss: 3.1130\n",
      "Epoch 1103, Train Loss: 3.0460, Test Loss: 3.1143\n",
      "Epoch 1104, Train Loss: 3.0496, Test Loss: 3.1137\n",
      "Epoch 1105, Train Loss: 3.0553, Test Loss: 3.1121\n",
      "Epoch 1106, Train Loss: 3.0523, Test Loss: 3.1104\n",
      "Epoch 1107, Train Loss: 3.0428, Test Loss: 3.1099\n",
      "Epoch 1108, Train Loss: 3.0576, Test Loss: 3.1099\n",
      "Epoch 1109, Train Loss: 3.0627, Test Loss: 3.1100\n",
      "Epoch 1110, Train Loss: 3.0592, Test Loss: 3.1115\n",
      "Epoch 1111, Train Loss: 3.0696, Test Loss: 3.1130\n",
      "Epoch 1112, Train Loss: 3.0686, Test Loss: 3.1127\n",
      "Epoch 1113, Train Loss: 3.0645, Test Loss: 3.1118\n",
      "Epoch 1114, Train Loss: 3.0726, Test Loss: 3.1109\n",
      "Epoch 1115, Train Loss: 3.0567, Test Loss: 3.1109\n",
      "Epoch 1116, Train Loss: 3.0579, Test Loss: 3.1112\n",
      "Epoch 1117, Train Loss: 3.0595, Test Loss: 3.1115\n",
      "Epoch 1118, Train Loss: 3.0643, Test Loss: 3.1116\n",
      "Epoch 1119, Train Loss: 3.0430, Test Loss: 3.1121\n",
      "Epoch 1120, Train Loss: 3.0561, Test Loss: 3.1124\n",
      "Epoch 1121, Train Loss: 3.0490, Test Loss: 3.1120\n",
      "Epoch 1122, Train Loss: 3.0711, Test Loss: 3.1111\n",
      "Epoch 1123, Train Loss: 3.0550, Test Loss: 3.1105\n",
      "Epoch 1124, Train Loss: 3.0427, Test Loss: 3.1102\n",
      "Epoch 1125, Train Loss: 3.0712, Test Loss: 3.1097\n",
      "Epoch 1126, Train Loss: 3.0436, Test Loss: 3.1096\n",
      "Epoch 1127, Train Loss: 3.0805, Test Loss: 3.1106\n",
      "Epoch 1128, Train Loss: 3.0722, Test Loss: 3.1109\n",
      "Epoch 1129, Train Loss: 3.0562, Test Loss: 3.1108\n",
      "Epoch 1130, Train Loss: 3.0505, Test Loss: 3.1105\n",
      "Epoch 1131, Train Loss: 3.0565, Test Loss: 3.1103\n",
      "Epoch 1132, Train Loss: 3.0782, Test Loss: 3.1104\n",
      "Epoch 1133, Train Loss: 3.0635, Test Loss: 3.1107\n",
      "Epoch 1134, Train Loss: 3.0555, Test Loss: 3.1110\n",
      "Epoch 1135, Train Loss: 3.0382, Test Loss: 3.1111\n",
      "Epoch 1136, Train Loss: 3.0664, Test Loss: 3.1108\n",
      "Epoch 1137, Train Loss: 3.0568, Test Loss: 3.1106\n",
      "Epoch 1138, Train Loss: 3.0629, Test Loss: 3.1103\n",
      "Epoch 1139, Train Loss: 3.0689, Test Loss: 3.1105\n",
      "Epoch 1140, Train Loss: 3.0760, Test Loss: 3.1111\n",
      "Epoch 1141, Train Loss: 3.0484, Test Loss: 3.1115\n",
      "Epoch 1142, Train Loss: 3.0650, Test Loss: 3.1120\n",
      "Epoch 1143, Train Loss: 3.0645, Test Loss: 3.1121\n",
      "Epoch 1144, Train Loss: 3.0696, Test Loss: 3.1121\n",
      "Epoch 1145, Train Loss: 3.0588, Test Loss: 3.1125\n",
      "Epoch 1146, Train Loss: 3.0590, Test Loss: 3.1126\n",
      "Epoch 1147, Train Loss: 3.0544, Test Loss: 3.1124\n",
      "Epoch 1148, Train Loss: 3.0643, Test Loss: 3.1122\n",
      "Epoch 1149, Train Loss: 3.0569, Test Loss: 3.1122\n",
      "Epoch 1150, Train Loss: 3.0746, Test Loss: 3.1117\n",
      "Epoch 1151, Train Loss: 3.0793, Test Loss: 3.1114\n",
      "Epoch 1152, Train Loss: 3.0761, Test Loss: 3.1111\n",
      "Epoch 1153, Train Loss: 3.0750, Test Loss: 3.1110\n",
      "Epoch 1154, Train Loss: 3.0774, Test Loss: 3.1113\n",
      "Epoch 1155, Train Loss: 3.0681, Test Loss: 3.1117\n",
      "Epoch 1156, Train Loss: 3.0502, Test Loss: 3.1113\n",
      "Epoch 1157, Train Loss: 3.0643, Test Loss: 3.1107\n",
      "Epoch 1158, Train Loss: 3.0545, Test Loss: 3.1103\n",
      "Epoch 1159, Train Loss: 3.0725, Test Loss: 3.1106\n",
      "Epoch 1160, Train Loss: 3.0714, Test Loss: 3.1116\n",
      "Epoch 1161, Train Loss: 3.0511, Test Loss: 3.1118\n",
      "Epoch 1162, Train Loss: 3.0553, Test Loss: 3.1116\n",
      "Epoch 1163, Train Loss: 3.0519, Test Loss: 3.1120\n",
      "Epoch 1164, Train Loss: 3.0662, Test Loss: 3.1119\n",
      "Epoch 1165, Train Loss: 3.0408, Test Loss: 3.1117\n",
      "Epoch 1166, Train Loss: 3.0688, Test Loss: 3.1115\n",
      "Epoch 1167, Train Loss: 3.0511, Test Loss: 3.1111\n",
      "Epoch 1168, Train Loss: 3.0616, Test Loss: 3.1108\n",
      "Epoch 1169, Train Loss: 3.0676, Test Loss: 3.1110\n",
      "Epoch 1170, Train Loss: 3.0705, Test Loss: 3.1108\n",
      "Epoch 1171, Train Loss: 3.0649, Test Loss: 3.1106\n",
      "Epoch 1172, Train Loss: 3.0675, Test Loss: 3.1105\n",
      "Epoch 1173, Train Loss: 3.0591, Test Loss: 3.1104\n",
      "Epoch 1174, Train Loss: 3.0723, Test Loss: 3.1101\n",
      "Epoch 1175, Train Loss: 3.0687, Test Loss: 3.1101\n",
      "Epoch 1176, Train Loss: 3.0668, Test Loss: 3.1096\n",
      "Epoch 1177, Train Loss: 3.0466, Test Loss: 3.1094\n",
      "Epoch 1178, Train Loss: 3.0692, Test Loss: 3.1100\n",
      "Epoch 1179, Train Loss: 3.0702, Test Loss: 3.1106\n",
      "Epoch 1180, Train Loss: 3.0625, Test Loss: 3.1109\n",
      "Epoch 1181, Train Loss: 3.0630, Test Loss: 3.1111\n",
      "Epoch 1182, Train Loss: 3.0461, Test Loss: 3.1102\n",
      "Epoch 1183, Train Loss: 3.0614, Test Loss: 3.1102\n",
      "Epoch 1184, Train Loss: 3.0569, Test Loss: 3.1108\n",
      "Epoch 1185, Train Loss: 3.0562, Test Loss: 3.1120\n",
      "Epoch 1186, Train Loss: 3.0509, Test Loss: 3.1133\n",
      "Epoch 1187, Train Loss: 3.0613, Test Loss: 3.1137\n",
      "Epoch 1188, Train Loss: 3.0677, Test Loss: 3.1132\n",
      "Epoch 1189, Train Loss: 3.0669, Test Loss: 3.1116\n",
      "Epoch 1190, Train Loss: 3.0570, Test Loss: 3.1102\n",
      "Epoch 1191, Train Loss: 3.0581, Test Loss: 3.1093\n",
      "Epoch 1192, Train Loss: 3.0589, Test Loss: 3.1092\n",
      "Epoch 1193, Train Loss: 3.0667, Test Loss: 3.1094\n",
      "Epoch 1194, Train Loss: 3.0439, Test Loss: 3.1097\n",
      "Epoch 1195, Train Loss: 3.0566, Test Loss: 3.1096\n",
      "Epoch 1196, Train Loss: 3.0551, Test Loss: 3.1091\n",
      "Epoch 1197, Train Loss: 3.0866, Test Loss: 3.1087\n",
      "Epoch 1198, Train Loss: 3.0573, Test Loss: 3.1089\n",
      "Epoch 1199, Train Loss: 3.0607, Test Loss: 3.1088\n",
      "Epoch 1200, Train Loss: 3.0599, Test Loss: 3.1094\n",
      "Epoch 1201, Train Loss: 3.0493, Test Loss: 3.1101\n",
      "Epoch 1202, Train Loss: 3.0639, Test Loss: 3.1107\n",
      "Epoch 1203, Train Loss: 3.0655, Test Loss: 3.1115\n",
      "Epoch 1204, Train Loss: 3.0632, Test Loss: 3.1123\n",
      "Epoch 1205, Train Loss: 3.0545, Test Loss: 3.1125\n",
      "Epoch 1206, Train Loss: 3.0513, Test Loss: 3.1127\n",
      "Epoch 1207, Train Loss: 3.0506, Test Loss: 3.1130\n",
      "Epoch 1208, Train Loss: 3.0379, Test Loss: 3.1130\n",
      "Epoch 1209, Train Loss: 3.0614, Test Loss: 3.1123\n",
      "Epoch 1210, Train Loss: 3.0401, Test Loss: 3.1116\n",
      "Epoch 1211, Train Loss: 3.0602, Test Loss: 3.1113\n",
      "Epoch 1212, Train Loss: 3.0623, Test Loss: 3.1115\n",
      "Epoch 1213, Train Loss: 3.0464, Test Loss: 3.1117\n",
      "Epoch 1214, Train Loss: 3.0706, Test Loss: 3.1105\n",
      "Epoch 1215, Train Loss: 3.0514, Test Loss: 3.1096\n",
      "Epoch 1216, Train Loss: 3.0559, Test Loss: 3.1090\n",
      "Epoch 1217, Train Loss: 3.0453, Test Loss: 3.1091\n",
      "Epoch 1218, Train Loss: 3.0602, Test Loss: 3.1097\n",
      "Epoch 1219, Train Loss: 3.0656, Test Loss: 3.1103\n",
      "Epoch 1220, Train Loss: 3.0540, Test Loss: 3.1107\n",
      "Epoch 1221, Train Loss: 3.0514, Test Loss: 3.1109\n",
      "Epoch 1222, Train Loss: 3.0402, Test Loss: 3.1115\n",
      "Epoch 1223, Train Loss: 3.0522, Test Loss: 3.1119\n",
      "Epoch 1224, Train Loss: 3.0431, Test Loss: 3.1113\n",
      "Epoch 1225, Train Loss: 3.0555, Test Loss: 3.1109\n",
      "Epoch 1226, Train Loss: 3.0596, Test Loss: 3.1106\n",
      "Epoch 1227, Train Loss: 3.0703, Test Loss: 3.1105\n",
      "Epoch 1228, Train Loss: 3.0632, Test Loss: 3.1103\n",
      "Epoch 1229, Train Loss: 3.0705, Test Loss: 3.1104\n",
      "Epoch 1230, Train Loss: 3.0565, Test Loss: 3.1108\n",
      "Epoch 1231, Train Loss: 3.0436, Test Loss: 3.1110\n",
      "Epoch 1232, Train Loss: 3.0451, Test Loss: 3.1112\n",
      "Epoch 1233, Train Loss: 3.0615, Test Loss: 3.1108\n",
      "Epoch 1234, Train Loss: 3.0587, Test Loss: 3.1096\n",
      "Epoch 1235, Train Loss: 3.0547, Test Loss: 3.1090\n",
      "Epoch 1236, Train Loss: 3.0496, Test Loss: 3.1090\n",
      "Epoch 1237, Train Loss: 3.0488, Test Loss: 3.1089\n",
      "Epoch 1238, Train Loss: 3.0712, Test Loss: 3.1091\n",
      "Epoch 1239, Train Loss: 3.0668, Test Loss: 3.1099\n",
      "Epoch 1240, Train Loss: 3.0426, Test Loss: 3.1099\n",
      "Epoch 1241, Train Loss: 3.0825, Test Loss: 3.1093\n",
      "Epoch 1242, Train Loss: 3.0608, Test Loss: 3.1087\n",
      "Epoch 1243, Train Loss: 3.0642, Test Loss: 3.1090\n",
      "Epoch 1244, Train Loss: 3.0632, Test Loss: 3.1096\n",
      "Epoch 1245, Train Loss: 3.0553, Test Loss: 3.1103\n",
      "Epoch 1246, Train Loss: 3.0698, Test Loss: 3.1106\n",
      "Epoch 1247, Train Loss: 3.0430, Test Loss: 3.1104\n",
      "Epoch 1248, Train Loss: 3.0552, Test Loss: 3.1097\n",
      "Epoch 1249, Train Loss: 3.0510, Test Loss: 3.1096\n",
      "Epoch 1250, Train Loss: 3.0576, Test Loss: 3.1094\n",
      "Epoch 1251, Train Loss: 3.0456, Test Loss: 3.1089\n",
      "Epoch 1252, Train Loss: 3.0645, Test Loss: 3.1086\n",
      "Epoch 1253, Train Loss: 3.0631, Test Loss: 3.1084\n",
      "Epoch 1254, Train Loss: 3.0659, Test Loss: 3.1087\n",
      "Epoch 1255, Train Loss: 3.0703, Test Loss: 3.1092\n",
      "Epoch 1256, Train Loss: 3.0445, Test Loss: 3.1094\n",
      "Epoch 1257, Train Loss: 3.0522, Test Loss: 3.1106\n",
      "Epoch 1258, Train Loss: 3.0767, Test Loss: 3.1113\n",
      "Epoch 1259, Train Loss: 3.0590, Test Loss: 3.1111\n",
      "Epoch 1260, Train Loss: 3.0445, Test Loss: 3.1098\n",
      "Epoch 1261, Train Loss: 3.0666, Test Loss: 3.1095\n",
      "Epoch 1262, Train Loss: 3.0472, Test Loss: 3.1099\n",
      "Epoch 1263, Train Loss: 3.0404, Test Loss: 3.1099\n",
      "Epoch 1264, Train Loss: 3.0626, Test Loss: 3.1101\n",
      "Epoch 1265, Train Loss: 3.0571, Test Loss: 3.1102\n",
      "Epoch 1266, Train Loss: 3.0599, Test Loss: 3.1104\n",
      "Epoch 1267, Train Loss: 3.0500, Test Loss: 3.1097\n",
      "Epoch 1268, Train Loss: 3.0518, Test Loss: 3.1090\n",
      "Epoch 1269, Train Loss: 3.0689, Test Loss: 3.1083\n",
      "Epoch 1270, Train Loss: 3.0673, Test Loss: 3.1082\n",
      "Epoch 1271, Train Loss: 3.0681, Test Loss: 3.1078\n",
      "Epoch 1272, Train Loss: 3.0673, Test Loss: 3.1081\n",
      "Epoch 1273, Train Loss: 3.0596, Test Loss: 3.1087\n",
      "Epoch 1274, Train Loss: 3.0534, Test Loss: 3.1089\n",
      "Epoch 1275, Train Loss: 3.0492, Test Loss: 3.1086\n",
      "Epoch 1276, Train Loss: 3.0452, Test Loss: 3.1087\n",
      "Epoch 1277, Train Loss: 3.0600, Test Loss: 3.1085\n",
      "Epoch 1278, Train Loss: 3.0527, Test Loss: 3.1089\n",
      "Epoch 1279, Train Loss: 3.0547, Test Loss: 3.1093\n",
      "Epoch 1280, Train Loss: 3.0675, Test Loss: 3.1094\n",
      "Epoch 1281, Train Loss: 3.0720, Test Loss: 3.1099\n",
      "Epoch 1282, Train Loss: 3.0600, Test Loss: 3.1106\n",
      "Epoch 1283, Train Loss: 3.0520, Test Loss: 3.1113\n",
      "Epoch 1284, Train Loss: 3.0590, Test Loss: 3.1113\n",
      "Epoch 1285, Train Loss: 3.0586, Test Loss: 3.1100\n",
      "Epoch 1286, Train Loss: 3.0507, Test Loss: 3.1084\n",
      "Epoch 1287, Train Loss: 3.0645, Test Loss: 3.1076\n",
      "Epoch 1288, Train Loss: 3.0464, Test Loss: 3.1074\n",
      "Epoch 1289, Train Loss: 3.0567, Test Loss: 3.1079\n",
      "Epoch 1290, Train Loss: 3.0513, Test Loss: 3.1087\n",
      "Epoch 1291, Train Loss: 3.0602, Test Loss: 3.1093\n",
      "Epoch 1292, Train Loss: 3.0475, Test Loss: 3.1094\n",
      "Epoch 1293, Train Loss: 3.0503, Test Loss: 3.1080\n",
      "Epoch 1294, Train Loss: 3.0603, Test Loss: 3.1075\n",
      "Epoch 1295, Train Loss: 3.0499, Test Loss: 3.1079\n",
      "Epoch 1296, Train Loss: 3.0659, Test Loss: 3.1087\n",
      "Epoch 1297, Train Loss: 3.0714, Test Loss: 3.1093\n",
      "Epoch 1298, Train Loss: 3.0608, Test Loss: 3.1098\n",
      "Epoch 1299, Train Loss: 3.0464, Test Loss: 3.1101\n",
      "Epoch 1300, Train Loss: 3.0419, Test Loss: 3.1096\n",
      "Epoch 1301, Train Loss: 3.0550, Test Loss: 3.1088\n",
      "Epoch 1302, Train Loss: 3.0434, Test Loss: 3.1084\n",
      "Epoch 1303, Train Loss: 3.0694, Test Loss: 3.1083\n",
      "Epoch 1304, Train Loss: 3.0565, Test Loss: 3.1084\n",
      "Epoch 1305, Train Loss: 3.0575, Test Loss: 3.1090\n",
      "Epoch 1306, Train Loss: 3.0412, Test Loss: 3.1098\n",
      "Epoch 1307, Train Loss: 3.0704, Test Loss: 3.1110\n",
      "Epoch 1308, Train Loss: 3.0439, Test Loss: 3.1088\n",
      "Epoch 1309, Train Loss: 3.0509, Test Loss: 3.1078\n",
      "Epoch 1310, Train Loss: 3.0516, Test Loss: 3.1080\n",
      "Epoch 1311, Train Loss: 3.0610, Test Loss: 3.1086\n",
      "Epoch 1312, Train Loss: 3.0488, Test Loss: 3.1092\n",
      "Epoch 1313, Train Loss: 3.0661, Test Loss: 3.1104\n",
      "Epoch 1314, Train Loss: 3.0519, Test Loss: 3.1120\n",
      "Epoch 1315, Train Loss: 3.0541, Test Loss: 3.1107\n",
      "Epoch 1316, Train Loss: 3.0601, Test Loss: 3.1086\n",
      "Epoch 1317, Train Loss: 3.0521, Test Loss: 3.1076\n",
      "Epoch 1318, Train Loss: 3.0417, Test Loss: 3.1074\n",
      "Epoch 1319, Train Loss: 3.0531, Test Loss: 3.1075\n",
      "Epoch 1320, Train Loss: 3.0668, Test Loss: 3.1080\n",
      "Epoch 1321, Train Loss: 3.0464, Test Loss: 3.1084\n",
      "Epoch 1322, Train Loss: 3.0504, Test Loss: 3.1082\n",
      "Epoch 1323, Train Loss: 3.0800, Test Loss: 3.1077\n",
      "Epoch 1324, Train Loss: 3.0536, Test Loss: 3.1072\n",
      "Epoch 1325, Train Loss: 3.0585, Test Loss: 3.1071\n",
      "Epoch 1326, Train Loss: 3.0448, Test Loss: 3.1071\n",
      "Epoch 1327, Train Loss: 3.0602, Test Loss: 3.1073\n",
      "Epoch 1328, Train Loss: 3.0496, Test Loss: 3.1078\n",
      "Epoch 1329, Train Loss: 3.0569, Test Loss: 3.1089\n",
      "Epoch 1330, Train Loss: 3.0624, Test Loss: 3.1094\n",
      "Epoch 1331, Train Loss: 3.0624, Test Loss: 3.1084\n",
      "Epoch 1332, Train Loss: 3.0540, Test Loss: 3.1075\n",
      "Epoch 1333, Train Loss: 3.0575, Test Loss: 3.1074\n",
      "Epoch 1334, Train Loss: 3.0786, Test Loss: 3.1078\n",
      "Epoch 1335, Train Loss: 3.0503, Test Loss: 3.1095\n",
      "Epoch 1336, Train Loss: 3.0485, Test Loss: 3.1094\n",
      "Epoch 1337, Train Loss: 3.0571, Test Loss: 3.1085\n",
      "Epoch 1338, Train Loss: 3.0424, Test Loss: 3.1084\n",
      "Epoch 1339, Train Loss: 3.0542, Test Loss: 3.1086\n",
      "Epoch 1340, Train Loss: 3.0666, Test Loss: 3.1094\n",
      "Epoch 1341, Train Loss: 3.0337, Test Loss: 3.1107\n",
      "Epoch 1342, Train Loss: 3.0520, Test Loss: 3.1111\n",
      "Epoch 1343, Train Loss: 3.0633, Test Loss: 3.1108\n",
      "Epoch 1344, Train Loss: 3.0520, Test Loss: 3.1102\n",
      "Epoch 1345, Train Loss: 3.0532, Test Loss: 3.1095\n",
      "Epoch 1346, Train Loss: 3.0498, Test Loss: 3.1087\n",
      "Epoch 1347, Train Loss: 3.0623, Test Loss: 3.1084\n",
      "Epoch 1348, Train Loss: 3.0367, Test Loss: 3.1083\n",
      "Epoch 1349, Train Loss: 3.0712, Test Loss: 3.1086\n",
      "Epoch 1350, Train Loss: 3.0508, Test Loss: 3.1087\n",
      "Epoch 1351, Train Loss: 3.0488, Test Loss: 3.1081\n",
      "Epoch 1352, Train Loss: 3.0494, Test Loss: 3.1077\n",
      "Epoch 1353, Train Loss: 3.0699, Test Loss: 3.1078\n",
      "Epoch 1354, Train Loss: 3.0629, Test Loss: 3.1083\n",
      "Epoch 1355, Train Loss: 3.0568, Test Loss: 3.1088\n",
      "Epoch 1356, Train Loss: 3.0623, Test Loss: 3.1090\n",
      "Epoch 1357, Train Loss: 3.0568, Test Loss: 3.1092\n",
      "Epoch 1358, Train Loss: 3.0333, Test Loss: 3.1089\n",
      "Epoch 1359, Train Loss: 3.0519, Test Loss: 3.1081\n",
      "Epoch 1360, Train Loss: 3.0550, Test Loss: 3.1077\n",
      "Epoch 1361, Train Loss: 3.0390, Test Loss: 3.1065\n",
      "Epoch 1362, Train Loss: 3.0650, Test Loss: 3.1060\n",
      "Epoch 1363, Train Loss: 3.0685, Test Loss: 3.1063\n",
      "Epoch 1364, Train Loss: 3.0289, Test Loss: 3.1072\n",
      "Epoch 1365, Train Loss: 3.0606, Test Loss: 3.1078\n",
      "Epoch 1366, Train Loss: 3.0616, Test Loss: 3.1080\n",
      "Epoch 1367, Train Loss: 3.0463, Test Loss: 3.1084\n",
      "Epoch 1368, Train Loss: 3.0443, Test Loss: 3.1086\n",
      "Epoch 1369, Train Loss: 3.0511, Test Loss: 3.1091\n",
      "Epoch 1370, Train Loss: 3.0586, Test Loss: 3.1093\n",
      "Epoch 1371, Train Loss: 3.0472, Test Loss: 3.1089\n",
      "Epoch 1372, Train Loss: 3.0519, Test Loss: 3.1087\n",
      "Epoch 1373, Train Loss: 3.0443, Test Loss: 3.1084\n",
      "Epoch 1374, Train Loss: 3.0580, Test Loss: 3.1072\n",
      "Epoch 1375, Train Loss: 3.0624, Test Loss: 3.1069\n",
      "Epoch 1376, Train Loss: 3.0606, Test Loss: 3.1071\n",
      "Epoch 1377, Train Loss: 3.0553, Test Loss: 3.1073\n",
      "Epoch 1378, Train Loss: 3.0546, Test Loss: 3.1075\n",
      "Epoch 1379, Train Loss: 3.0492, Test Loss: 3.1077\n",
      "Epoch 1380, Train Loss: 3.0780, Test Loss: 3.1083\n",
      "Epoch 1381, Train Loss: 3.0422, Test Loss: 3.1090\n",
      "Epoch 1382, Train Loss: 3.0312, Test Loss: 3.1081\n",
      "Epoch 1383, Train Loss: 3.0527, Test Loss: 3.1073\n",
      "Epoch 1384, Train Loss: 3.0593, Test Loss: 3.1071\n",
      "Epoch 1385, Train Loss: 3.0621, Test Loss: 3.1074\n",
      "Epoch 1386, Train Loss: 3.0473, Test Loss: 3.1084\n",
      "Epoch 1387, Train Loss: 3.0361, Test Loss: 3.1100\n",
      "Epoch 1388, Train Loss: 3.0466, Test Loss: 3.1112\n",
      "Epoch 1389, Train Loss: 3.0611, Test Loss: 3.1095\n",
      "Epoch 1390, Train Loss: 3.0504, Test Loss: 3.1082\n",
      "Epoch 1391, Train Loss: 3.0559, Test Loss: 3.1076\n",
      "Epoch 1392, Train Loss: 3.0533, Test Loss: 3.1072\n",
      "Epoch 1393, Train Loss: 3.0532, Test Loss: 3.1073\n",
      "Epoch 1394, Train Loss: 3.0366, Test Loss: 3.1082\n",
      "Epoch 1395, Train Loss: 3.0419, Test Loss: 3.1083\n",
      "Epoch 1396, Train Loss: 3.0520, Test Loss: 3.1076\n",
      "Epoch 1397, Train Loss: 3.0594, Test Loss: 3.1067\n",
      "Epoch 1398, Train Loss: 3.0551, Test Loss: 3.1063\n",
      "Epoch 1399, Train Loss: 3.0482, Test Loss: 3.1061\n",
      "Epoch 1400, Train Loss: 3.0479, Test Loss: 3.1063\n",
      "Epoch 1401, Train Loss: 3.0503, Test Loss: 3.1067\n",
      "Epoch 1402, Train Loss: 3.0528, Test Loss: 3.1071\n",
      "Epoch 1403, Train Loss: 3.0481, Test Loss: 3.1078\n",
      "Epoch 1404, Train Loss: 3.0412, Test Loss: 3.1080\n",
      "Epoch 1405, Train Loss: 3.0448, Test Loss: 3.1081\n",
      "Epoch 1406, Train Loss: 3.0399, Test Loss: 3.1084\n",
      "Epoch 1407, Train Loss: 3.0588, Test Loss: 3.1084\n",
      "Epoch 1408, Train Loss: 3.0549, Test Loss: 3.1080\n",
      "Epoch 1409, Train Loss: 3.0315, Test Loss: 3.1073\n",
      "Epoch 1410, Train Loss: 3.0447, Test Loss: 3.1071\n",
      "Epoch 1411, Train Loss: 3.0544, Test Loss: 3.1073\n",
      "Epoch 1412, Train Loss: 3.0571, Test Loss: 3.1080\n",
      "Epoch 1413, Train Loss: 3.0597, Test Loss: 3.1076\n",
      "Epoch 1414, Train Loss: 3.0541, Test Loss: 3.1071\n",
      "Epoch 1415, Train Loss: 3.0714, Test Loss: 3.1068\n",
      "Epoch 1416, Train Loss: 3.0459, Test Loss: 3.1064\n",
      "Epoch 1417, Train Loss: 3.0389, Test Loss: 3.1060\n",
      "Epoch 1418, Train Loss: 3.0411, Test Loss: 3.1056\n",
      "Epoch 1419, Train Loss: 3.0488, Test Loss: 3.1057\n",
      "Epoch 1420, Train Loss: 3.0454, Test Loss: 3.1061\n",
      "Epoch 1421, Train Loss: 3.0454, Test Loss: 3.1072\n",
      "Epoch 1422, Train Loss: 3.0386, Test Loss: 3.1078\n",
      "Epoch 1423, Train Loss: 3.0584, Test Loss: 3.1083\n",
      "Epoch 1424, Train Loss: 3.0626, Test Loss: 3.1083\n",
      "Epoch 1425, Train Loss: 3.0387, Test Loss: 3.1080\n",
      "Epoch 1426, Train Loss: 3.0493, Test Loss: 3.1075\n",
      "Epoch 1427, Train Loss: 3.0557, Test Loss: 3.1074\n",
      "Epoch 1428, Train Loss: 3.0452, Test Loss: 3.1078\n",
      "Epoch 1429, Train Loss: 3.0575, Test Loss: 3.1067\n",
      "Epoch 1430, Train Loss: 3.0606, Test Loss: 3.1061\n",
      "Epoch 1431, Train Loss: 3.0568, Test Loss: 3.1062\n",
      "Epoch 1432, Train Loss: 3.0297, Test Loss: 3.1062\n",
      "Epoch 1433, Train Loss: 3.0510, Test Loss: 3.1067\n",
      "Epoch 1434, Train Loss: 3.0383, Test Loss: 3.1075\n",
      "Epoch 1435, Train Loss: 3.0489, Test Loss: 3.1082\n",
      "Epoch 1436, Train Loss: 3.0527, Test Loss: 3.1083\n",
      "Epoch 1437, Train Loss: 3.0586, Test Loss: 3.1084\n",
      "Epoch 1438, Train Loss: 3.0557, Test Loss: 3.1086\n",
      "Epoch 1439, Train Loss: 3.0486, Test Loss: 3.1092\n",
      "Epoch 1440, Train Loss: 3.0492, Test Loss: 3.1108\n",
      "Epoch 1441, Train Loss: 3.0536, Test Loss: 3.1125\n",
      "Epoch 1442, Train Loss: 3.0574, Test Loss: 3.1124\n",
      "Epoch 1443, Train Loss: 3.0512, Test Loss: 3.1109\n",
      "Epoch 1444, Train Loss: 3.0721, Test Loss: 3.1093\n",
      "Epoch 1445, Train Loss: 3.0501, Test Loss: 3.1080\n",
      "Epoch 1446, Train Loss: 3.0394, Test Loss: 3.1068\n",
      "Epoch 1447, Train Loss: 3.0392, Test Loss: 3.1063\n",
      "Epoch 1448, Train Loss: 3.0442, Test Loss: 3.1062\n",
      "Epoch 1449, Train Loss: 3.0435, Test Loss: 3.1065\n",
      "Epoch 1450, Train Loss: 3.0636, Test Loss: 3.1065\n",
      "Epoch 1451, Train Loss: 3.0501, Test Loss: 3.1065\n",
      "Epoch 1452, Train Loss: 3.0585, Test Loss: 3.1069\n",
      "Epoch 1453, Train Loss: 3.0394, Test Loss: 3.1071\n",
      "Epoch 1454, Train Loss: 3.0408, Test Loss: 3.1075\n",
      "Epoch 1455, Train Loss: 3.0527, Test Loss: 3.1086\n",
      "Epoch 1456, Train Loss: 3.0534, Test Loss: 3.1090\n",
      "Epoch 1457, Train Loss: 3.0521, Test Loss: 3.1082\n",
      "Epoch 1458, Train Loss: 3.0447, Test Loss: 3.1071\n",
      "Epoch 1459, Train Loss: 3.0600, Test Loss: 3.1080\n",
      "Epoch 1460, Train Loss: 3.0520, Test Loss: 3.1083\n",
      "Epoch 1461, Train Loss: 3.0620, Test Loss: 3.1087\n",
      "Epoch 1462, Train Loss: 3.0600, Test Loss: 3.1090\n",
      "Epoch 1463, Train Loss: 3.0467, Test Loss: 3.1091\n",
      "Epoch 1464, Train Loss: 3.0541, Test Loss: 3.1089\n",
      "Epoch 1465, Train Loss: 3.0449, Test Loss: 3.1077\n",
      "Epoch 1466, Train Loss: 3.0470, Test Loss: 3.1073\n",
      "Epoch 1467, Train Loss: 3.0489, Test Loss: 3.1082\n",
      "Epoch 1468, Train Loss: 3.0543, Test Loss: 3.1091\n",
      "Epoch 1469, Train Loss: 3.0614, Test Loss: 3.1096\n",
      "Epoch 1470, Train Loss: 3.0469, Test Loss: 3.1088\n",
      "Epoch 1471, Train Loss: 3.0515, Test Loss: 3.1076\n",
      "Epoch 1472, Train Loss: 3.0548, Test Loss: 3.1061\n",
      "Epoch 1473, Train Loss: 3.0496, Test Loss: 3.1056\n",
      "Epoch 1474, Train Loss: 3.0603, Test Loss: 3.1058\n",
      "Epoch 1475, Train Loss: 3.0550, Test Loss: 3.1067\n",
      "Epoch 1476, Train Loss: 3.0445, Test Loss: 3.1071\n",
      "Epoch 1477, Train Loss: 3.0437, Test Loss: 3.1070\n",
      "Epoch 1478, Train Loss: 3.0577, Test Loss: 3.1061\n",
      "Epoch 1479, Train Loss: 3.0643, Test Loss: 3.1057\n",
      "Epoch 1480, Train Loss: 3.0466, Test Loss: 3.1053\n",
      "Epoch 1481, Train Loss: 3.0338, Test Loss: 3.1055\n",
      "Epoch 1482, Train Loss: 3.0536, Test Loss: 3.1057\n",
      "Epoch 1483, Train Loss: 3.0421, Test Loss: 3.1056\n",
      "Epoch 1484, Train Loss: 3.0461, Test Loss: 3.1062\n",
      "Epoch 1485, Train Loss: 3.0579, Test Loss: 3.1067\n",
      "Epoch 1486, Train Loss: 3.0504, Test Loss: 3.1069\n",
      "Epoch 1487, Train Loss: 3.0461, Test Loss: 3.1065\n",
      "Epoch 1488, Train Loss: 3.0441, Test Loss: 3.1067\n",
      "Epoch 1489, Train Loss: 3.0542, Test Loss: 3.1060\n",
      "Epoch 1490, Train Loss: 3.0505, Test Loss: 3.1054\n",
      "Epoch 1491, Train Loss: 3.0557, Test Loss: 3.1052\n",
      "Epoch 1492, Train Loss: 3.0528, Test Loss: 3.1057\n",
      "Epoch 1493, Train Loss: 3.0491, Test Loss: 3.1075\n",
      "Epoch 1494, Train Loss: 3.0570, Test Loss: 3.1082\n",
      "Epoch 1495, Train Loss: 3.0647, Test Loss: 3.1064\n",
      "Epoch 1496, Train Loss: 3.0598, Test Loss: 3.1060\n",
      "Epoch 1497, Train Loss: 3.0569, Test Loss: 3.1064\n",
      "Epoch 1498, Train Loss: 3.0591, Test Loss: 3.1067\n",
      "Epoch 1499, Train Loss: 3.0801, Test Loss: 3.1067\n",
      "Epoch 1500, Train Loss: 3.0440, Test Loss: 3.1070\n",
      "Epoch 1501, Train Loss: 3.0599, Test Loss: 3.1080\n",
      "Epoch 1502, Train Loss: 3.0325, Test Loss: 3.1075\n",
      "Epoch 1503, Train Loss: 3.0463, Test Loss: 3.1055\n",
      "Epoch 1504, Train Loss: 3.0479, Test Loss: 3.1046\n",
      "Epoch 1505, Train Loss: 3.0508, Test Loss: 3.1048\n",
      "Epoch 1506, Train Loss: 3.0661, Test Loss: 3.1045\n",
      "Epoch 1507, Train Loss: 3.0512, Test Loss: 3.1048\n",
      "Epoch 1508, Train Loss: 3.0373, Test Loss: 3.1057\n",
      "Epoch 1509, Train Loss: 3.0484, Test Loss: 3.1073\n",
      "Epoch 1510, Train Loss: 3.0546, Test Loss: 3.1076\n",
      "Epoch 1511, Train Loss: 3.0380, Test Loss: 3.1065\n",
      "Epoch 1512, Train Loss: 3.0561, Test Loss: 3.1054\n",
      "Epoch 1513, Train Loss: 3.0519, Test Loss: 3.1055\n",
      "Epoch 1514, Train Loss: 3.0446, Test Loss: 3.1058\n",
      "Epoch 1515, Train Loss: 3.0580, Test Loss: 3.1059\n",
      "Epoch 1516, Train Loss: 3.0445, Test Loss: 3.1075\n",
      "Epoch 1517, Train Loss: 3.0380, Test Loss: 3.1094\n",
      "Epoch 1518, Train Loss: 3.0483, Test Loss: 3.1099\n",
      "Epoch 1519, Train Loss: 3.0422, Test Loss: 3.1086\n",
      "Epoch 1520, Train Loss: 3.0576, Test Loss: 3.1064\n",
      "Epoch 1521, Train Loss: 3.0526, Test Loss: 3.1049\n",
      "Epoch 1522, Train Loss: 3.0506, Test Loss: 3.1041\n",
      "Epoch 1523, Train Loss: 3.0423, Test Loss: 3.1035\n",
      "Epoch 1524, Train Loss: 3.0431, Test Loss: 3.1042\n",
      "Epoch 1525, Train Loss: 3.0447, Test Loss: 3.1054\n",
      "Epoch 1526, Train Loss: 3.0461, Test Loss: 3.1060\n",
      "Epoch 1527, Train Loss: 3.0397, Test Loss: 3.1052\n",
      "Epoch 1528, Train Loss: 3.0402, Test Loss: 3.1046\n",
      "Epoch 1529, Train Loss: 3.0449, Test Loss: 3.1050\n",
      "Epoch 1530, Train Loss: 3.0561, Test Loss: 3.1057\n",
      "Epoch 1531, Train Loss: 3.0476, Test Loss: 3.1068\n",
      "Epoch 1532, Train Loss: 3.0504, Test Loss: 3.1086\n",
      "Epoch 1533, Train Loss: 3.0647, Test Loss: 3.1079\n",
      "Epoch 1534, Train Loss: 3.0437, Test Loss: 3.1069\n",
      "Epoch 1535, Train Loss: 3.0409, Test Loss: 3.1062\n",
      "Epoch 1536, Train Loss: 3.0440, Test Loss: 3.1060\n",
      "Epoch 1537, Train Loss: 3.0317, Test Loss: 3.1057\n",
      "Epoch 1538, Train Loss: 3.0694, Test Loss: 3.1058\n",
      "Epoch 1539, Train Loss: 3.0345, Test Loss: 3.1059\n",
      "Epoch 1540, Train Loss: 3.0691, Test Loss: 3.1054\n",
      "Epoch 1541, Train Loss: 3.0631, Test Loss: 3.1044\n",
      "Epoch 1542, Train Loss: 3.0438, Test Loss: 3.1041\n",
      "Epoch 1543, Train Loss: 3.0452, Test Loss: 3.1048\n",
      "Epoch 1544, Train Loss: 3.0436, Test Loss: 3.1053\n",
      "Epoch 1545, Train Loss: 3.0363, Test Loss: 3.1054\n",
      "Epoch 1546, Train Loss: 3.0393, Test Loss: 3.1047\n",
      "Epoch 1547, Train Loss: 3.0608, Test Loss: 3.1044\n",
      "Epoch 1548, Train Loss: 3.0365, Test Loss: 3.1047\n",
      "Epoch 1549, Train Loss: 3.0565, Test Loss: 3.1052\n",
      "Epoch 1550, Train Loss: 3.0325, Test Loss: 3.1065\n",
      "Epoch 1551, Train Loss: 3.0435, Test Loss: 3.1072\n",
      "Epoch 1552, Train Loss: 3.0435, Test Loss: 3.1073\n",
      "Epoch 1553, Train Loss: 3.0314, Test Loss: 3.1073\n",
      "Epoch 1554, Train Loss: 3.0519, Test Loss: 3.1067\n",
      "Epoch 1555, Train Loss: 3.0563, Test Loss: 3.1067\n",
      "Epoch 1556, Train Loss: 3.0460, Test Loss: 3.1072\n",
      "Epoch 1557, Train Loss: 3.0510, Test Loss: 3.1077\n",
      "Epoch 1558, Train Loss: 3.0418, Test Loss: 3.1076\n",
      "Epoch 1559, Train Loss: 3.0495, Test Loss: 3.1067\n",
      "Epoch 1560, Train Loss: 3.0455, Test Loss: 3.1060\n",
      "Epoch 1561, Train Loss: 3.0456, Test Loss: 3.1058\n",
      "Epoch 1562, Train Loss: 3.0439, Test Loss: 3.1052\n",
      "Epoch 1563, Train Loss: 3.0468, Test Loss: 3.1050\n",
      "Epoch 1564, Train Loss: 3.0329, Test Loss: 3.1046\n",
      "Epoch 1565, Train Loss: 3.0661, Test Loss: 3.1047\n",
      "Epoch 1566, Train Loss: 3.0497, Test Loss: 3.1047\n",
      "Epoch 1567, Train Loss: 3.0688, Test Loss: 3.1046\n",
      "Epoch 1568, Train Loss: 3.0430, Test Loss: 3.1044\n",
      "Epoch 1569, Train Loss: 3.0444, Test Loss: 3.1042\n",
      "Epoch 1570, Train Loss: 3.0456, Test Loss: 3.1045\n",
      "Epoch 1571, Train Loss: 3.0461, Test Loss: 3.1051\n",
      "Epoch 1572, Train Loss: 3.0558, Test Loss: 3.1064\n",
      "Epoch 1573, Train Loss: 3.0456, Test Loss: 3.1073\n",
      "Epoch 1574, Train Loss: 3.0302, Test Loss: 3.1067\n",
      "Epoch 1575, Train Loss: 3.0593, Test Loss: 3.1061\n",
      "Epoch 1576, Train Loss: 3.0482, Test Loss: 3.1058\n",
      "Epoch 1577, Train Loss: 3.0425, Test Loss: 3.1051\n",
      "Epoch 1578, Train Loss: 3.0467, Test Loss: 3.1047\n",
      "Epoch 1579, Train Loss: 3.0676, Test Loss: 3.1050\n",
      "Epoch 1580, Train Loss: 3.0536, Test Loss: 3.1053\n",
      "Epoch 1581, Train Loss: 3.0638, Test Loss: 3.1052\n",
      "Epoch 1582, Train Loss: 3.0515, Test Loss: 3.1049\n",
      "Epoch 1583, Train Loss: 3.0415, Test Loss: 3.1052\n",
      "Epoch 1584, Train Loss: 3.0358, Test Loss: 3.1054\n",
      "Epoch 1585, Train Loss: 3.0522, Test Loss: 3.1057\n",
      "Epoch 1586, Train Loss: 3.0443, Test Loss: 3.1054\n",
      "Epoch 1587, Train Loss: 3.0306, Test Loss: 3.1053\n",
      "Epoch 1588, Train Loss: 3.0379, Test Loss: 3.1050\n",
      "Epoch 1589, Train Loss: 3.0448, Test Loss: 3.1046\n",
      "Epoch 1590, Train Loss: 3.0423, Test Loss: 3.1044\n",
      "Epoch 1591, Train Loss: 3.0424, Test Loss: 3.1042\n",
      "Epoch 1592, Train Loss: 3.0582, Test Loss: 3.1043\n",
      "Epoch 1593, Train Loss: 3.0322, Test Loss: 3.1045\n",
      "Epoch 1594, Train Loss: 3.0794, Test Loss: 3.1046\n",
      "Epoch 1595, Train Loss: 3.0383, Test Loss: 3.1048\n",
      "Epoch 1596, Train Loss: 3.0287, Test Loss: 3.1051\n",
      "Epoch 1597, Train Loss: 3.0497, Test Loss: 3.1057\n",
      "Epoch 1598, Train Loss: 3.0473, Test Loss: 3.1051\n",
      "Epoch 1599, Train Loss: 3.0520, Test Loss: 3.1045\n",
      "Epoch 1600, Train Loss: 3.0479, Test Loss: 3.1045\n",
      "Epoch 1601, Train Loss: 3.0464, Test Loss: 3.1039\n",
      "Epoch 1602, Train Loss: 3.0390, Test Loss: 3.1032\n",
      "Epoch 1603, Train Loss: 3.0449, Test Loss: 3.1031\n",
      "Epoch 1604, Train Loss: 3.0419, Test Loss: 3.1039\n",
      "Epoch 1605, Train Loss: 3.0640, Test Loss: 3.1050\n",
      "Epoch 1606, Train Loss: 3.0295, Test Loss: 3.1056\n",
      "Epoch 1607, Train Loss: 3.0437, Test Loss: 3.1059\n",
      "Epoch 1608, Train Loss: 3.0414, Test Loss: 3.1060\n",
      "Epoch 1609, Train Loss: 3.0366, Test Loss: 3.1064\n",
      "Epoch 1610, Train Loss: 3.0350, Test Loss: 3.1067\n",
      "Epoch 1611, Train Loss: 3.0461, Test Loss: 3.1068\n",
      "Epoch 1612, Train Loss: 3.0506, Test Loss: 3.1067\n",
      "Epoch 1613, Train Loss: 3.0610, Test Loss: 3.1057\n",
      "Epoch 1614, Train Loss: 3.0516, Test Loss: 3.1053\n",
      "Epoch 1615, Train Loss: 3.0323, Test Loss: 3.1048\n",
      "Epoch 1616, Train Loss: 3.0351, Test Loss: 3.1042\n",
      "Epoch 1617, Train Loss: 3.0475, Test Loss: 3.1040\n",
      "Epoch 1618, Train Loss: 3.0372, Test Loss: 3.1038\n",
      "Epoch 1619, Train Loss: 3.0494, Test Loss: 3.1036\n",
      "Epoch 1620, Train Loss: 3.0433, Test Loss: 3.1043\n",
      "Epoch 1621, Train Loss: 3.0446, Test Loss: 3.1049\n",
      "Epoch 1622, Train Loss: 3.0502, Test Loss: 3.1042\n",
      "Epoch 1623, Train Loss: 3.0533, Test Loss: 3.1040\n",
      "Epoch 1624, Train Loss: 3.0414, Test Loss: 3.1039\n",
      "Epoch 1625, Train Loss: 3.0288, Test Loss: 3.1035\n",
      "Epoch 1626, Train Loss: 3.0521, Test Loss: 3.1028\n",
      "Epoch 1627, Train Loss: 3.0499, Test Loss: 3.1025\n",
      "Epoch 1628, Train Loss: 3.0344, Test Loss: 3.1026\n",
      "Epoch 1629, Train Loss: 3.0550, Test Loss: 3.1025\n",
      "Epoch 1630, Train Loss: 3.0487, Test Loss: 3.1026\n",
      "Epoch 1631, Train Loss: 3.0474, Test Loss: 3.1024\n",
      "Epoch 1632, Train Loss: 3.0432, Test Loss: 3.1029\n",
      "Epoch 1633, Train Loss: 3.0441, Test Loss: 3.1037\n",
      "Epoch 1634, Train Loss: 3.0555, Test Loss: 3.1039\n",
      "Epoch 1635, Train Loss: 3.0558, Test Loss: 3.1042\n",
      "Epoch 1636, Train Loss: 3.0382, Test Loss: 3.1047\n",
      "Epoch 1637, Train Loss: 3.0410, Test Loss: 3.1049\n",
      "Epoch 1638, Train Loss: 3.0216, Test Loss: 3.1049\n",
      "Epoch 1639, Train Loss: 3.0375, Test Loss: 3.1040\n",
      "Epoch 1640, Train Loss: 3.0543, Test Loss: 3.1036\n",
      "Epoch 1641, Train Loss: 3.0511, Test Loss: 3.1032\n",
      "Epoch 1642, Train Loss: 3.0467, Test Loss: 3.1030\n",
      "Epoch 1643, Train Loss: 3.0439, Test Loss: 3.1038\n",
      "Epoch 1644, Train Loss: 3.0393, Test Loss: 3.1040\n",
      "Epoch 1645, Train Loss: 3.0452, Test Loss: 3.1039\n",
      "Epoch 1646, Train Loss: 3.0590, Test Loss: 3.1037\n",
      "Epoch 1647, Train Loss: 3.0438, Test Loss: 3.1046\n",
      "Epoch 1648, Train Loss: 3.0430, Test Loss: 3.1050\n",
      "Epoch 1649, Train Loss: 3.0503, Test Loss: 3.1057\n",
      "Epoch 1650, Train Loss: 3.0315, Test Loss: 3.1055\n",
      "Epoch 1651, Train Loss: 3.0477, Test Loss: 3.1052\n",
      "Epoch 1652, Train Loss: 3.0454, Test Loss: 3.1046\n",
      "Epoch 1653, Train Loss: 3.0416, Test Loss: 3.1049\n",
      "Epoch 1654, Train Loss: 3.0349, Test Loss: 3.1051\n",
      "Epoch 1655, Train Loss: 3.0555, Test Loss: 3.1052\n",
      "Epoch 1656, Train Loss: 3.0573, Test Loss: 3.1052\n",
      "Epoch 1657, Train Loss: 3.0522, Test Loss: 3.1052\n",
      "Epoch 1658, Train Loss: 3.0473, Test Loss: 3.1057\n",
      "Epoch 1659, Train Loss: 3.0526, Test Loss: 3.1066\n",
      "Epoch 1660, Train Loss: 3.0501, Test Loss: 3.1072\n",
      "Epoch 1661, Train Loss: 3.0495, Test Loss: 3.1077\n",
      "Epoch 1662, Train Loss: 3.0391, Test Loss: 3.1066\n",
      "Epoch 1663, Train Loss: 3.0593, Test Loss: 3.1060\n",
      "Epoch 1664, Train Loss: 3.0404, Test Loss: 3.1061\n",
      "Epoch 1665, Train Loss: 3.0545, Test Loss: 3.1066\n",
      "Epoch 1666, Train Loss: 3.0523, Test Loss: 3.1075\n",
      "Epoch 1667, Train Loss: 3.0458, Test Loss: 3.1078\n",
      "Epoch 1668, Train Loss: 3.0523, Test Loss: 3.1061\n",
      "Epoch 1669, Train Loss: 3.0510, Test Loss: 3.1049\n",
      "Epoch 1670, Train Loss: 3.0618, Test Loss: 3.1045\n",
      "Epoch 1671, Train Loss: 3.0587, Test Loss: 3.1039\n",
      "Epoch 1672, Train Loss: 3.0250, Test Loss: 3.1039\n",
      "Epoch 1673, Train Loss: 3.0392, Test Loss: 3.1047\n",
      "Epoch 1674, Train Loss: 3.0489, Test Loss: 3.1048\n",
      "Epoch 1675, Train Loss: 3.0392, Test Loss: 3.1036\n",
      "Epoch 1676, Train Loss: 3.0411, Test Loss: 3.1027\n",
      "Epoch 1677, Train Loss: 3.0456, Test Loss: 3.1028\n",
      "Epoch 1678, Train Loss: 3.0429, Test Loss: 3.1037\n",
      "Epoch 1679, Train Loss: 3.0460, Test Loss: 3.1047\n",
      "Epoch 1680, Train Loss: 3.0444, Test Loss: 3.1050\n",
      "Epoch 1681, Train Loss: 3.0334, Test Loss: 3.1044\n",
      "Epoch 1682, Train Loss: 3.0524, Test Loss: 3.1041\n",
      "Epoch 1683, Train Loss: 3.0339, Test Loss: 3.1041\n",
      "Epoch 1684, Train Loss: 3.0647, Test Loss: 3.1047\n",
      "Epoch 1685, Train Loss: 3.0479, Test Loss: 3.1057\n",
      "Epoch 1686, Train Loss: 3.0390, Test Loss: 3.1060\n",
      "Epoch 1687, Train Loss: 3.0398, Test Loss: 3.1053\n",
      "Epoch 1688, Train Loss: 3.0492, Test Loss: 3.1044\n",
      "Epoch 1689, Train Loss: 3.0369, Test Loss: 3.1043\n",
      "Epoch 1690, Train Loss: 3.0382, Test Loss: 3.1040\n",
      "Epoch 1691, Train Loss: 3.0345, Test Loss: 3.1037\n",
      "Epoch 1692, Train Loss: 3.0424, Test Loss: 3.1036\n",
      "Epoch 1693, Train Loss: 3.0388, Test Loss: 3.1038\n",
      "Epoch 1694, Train Loss: 3.0426, Test Loss: 3.1041\n",
      "Epoch 1695, Train Loss: 3.0398, Test Loss: 3.1034\n",
      "Epoch 1696, Train Loss: 3.0445, Test Loss: 3.1030\n",
      "Epoch 1697, Train Loss: 3.0323, Test Loss: 3.1026\n",
      "Epoch 1698, Train Loss: 3.0424, Test Loss: 3.1030\n",
      "Epoch 1699, Train Loss: 3.0409, Test Loss: 3.1046\n",
      "Epoch 1700, Train Loss: 3.0368, Test Loss: 3.1052\n",
      "Epoch 1701, Train Loss: 3.0422, Test Loss: 3.1043\n",
      "Epoch 1702, Train Loss: 3.0331, Test Loss: 3.1028\n",
      "Epoch 1703, Train Loss: 3.0406, Test Loss: 3.1028\n",
      "Epoch 1704, Train Loss: 3.0465, Test Loss: 3.1032\n",
      "Epoch 1705, Train Loss: 3.0491, Test Loss: 3.1037\n",
      "Epoch 1706, Train Loss: 3.0423, Test Loss: 3.1041\n",
      "Epoch 1707, Train Loss: 3.0388, Test Loss: 3.1043\n",
      "Epoch 1708, Train Loss: 3.0218, Test Loss: 3.1040\n",
      "Epoch 1709, Train Loss: 3.0531, Test Loss: 3.1035\n",
      "Epoch 1710, Train Loss: 3.0286, Test Loss: 3.1035\n",
      "Epoch 1711, Train Loss: 3.0542, Test Loss: 3.1042\n",
      "Epoch 1712, Train Loss: 3.0348, Test Loss: 3.1043\n",
      "Epoch 1713, Train Loss: 3.0387, Test Loss: 3.1041\n",
      "Epoch 1714, Train Loss: 3.0423, Test Loss: 3.1043\n",
      "Epoch 1715, Train Loss: 3.0442, Test Loss: 3.1047\n",
      "Epoch 1716, Train Loss: 3.0540, Test Loss: 3.1055\n",
      "Epoch 1717, Train Loss: 3.0320, Test Loss: 3.1049\n",
      "Epoch 1718, Train Loss: 3.0323, Test Loss: 3.1034\n",
      "Epoch 1719, Train Loss: 3.0548, Test Loss: 3.1030\n",
      "Epoch 1720, Train Loss: 3.0236, Test Loss: 3.1029\n",
      "Epoch 1721, Train Loss: 3.0329, Test Loss: 3.1036\n",
      "Epoch 1722, Train Loss: 3.0575, Test Loss: 3.1054\n",
      "Epoch 1723, Train Loss: 3.0353, Test Loss: 3.1056\n",
      "Epoch 1724, Train Loss: 3.0329, Test Loss: 3.1055\n",
      "Epoch 1725, Train Loss: 3.0404, Test Loss: 3.1044\n",
      "Epoch 1726, Train Loss: 3.0324, Test Loss: 3.1035\n",
      "Epoch 1727, Train Loss: 3.0465, Test Loss: 3.1035\n",
      "Epoch 1728, Train Loss: 3.0393, Test Loss: 3.1036\n",
      "Epoch 1729, Train Loss: 3.0446, Test Loss: 3.1039\n",
      "Epoch 1730, Train Loss: 3.0544, Test Loss: 3.1035\n",
      "Epoch 1731, Train Loss: 3.0389, Test Loss: 3.1031\n",
      "Epoch 1732, Train Loss: 3.0363, Test Loss: 3.1028\n",
      "Epoch 1733, Train Loss: 3.0496, Test Loss: 3.1027\n",
      "Epoch 1734, Train Loss: 3.0577, Test Loss: 3.1030\n",
      "Epoch 1735, Train Loss: 3.0399, Test Loss: 3.1037\n",
      "Epoch 1736, Train Loss: 3.0351, Test Loss: 3.1039\n",
      "Epoch 1737, Train Loss: 3.0400, Test Loss: 3.1022\n",
      "Epoch 1738, Train Loss: 3.0326, Test Loss: 3.1011\n",
      "Epoch 1739, Train Loss: 3.0569, Test Loss: 3.1009\n",
      "Epoch 1740, Train Loss: 3.0482, Test Loss: 3.1012\n",
      "Epoch 1741, Train Loss: 3.0459, Test Loss: 3.1033\n",
      "Epoch 1742, Train Loss: 3.0330, Test Loss: 3.1035\n",
      "Epoch 1743, Train Loss: 3.0422, Test Loss: 3.1031\n",
      "Epoch 1744, Train Loss: 3.0553, Test Loss: 3.1030\n",
      "Epoch 1745, Train Loss: 3.0372, Test Loss: 3.1030\n",
      "Epoch 1746, Train Loss: 3.0472, Test Loss: 3.1040\n",
      "Epoch 1747, Train Loss: 3.0310, Test Loss: 3.1046\n",
      "Epoch 1748, Train Loss: 3.0643, Test Loss: 3.1043\n",
      "Epoch 1749, Train Loss: 3.0470, Test Loss: 3.1033\n",
      "Epoch 1750, Train Loss: 3.0360, Test Loss: 3.1034\n",
      "Epoch 1751, Train Loss: 3.0481, Test Loss: 3.1033\n",
      "Epoch 1752, Train Loss: 3.0197, Test Loss: 3.1027\n",
      "Epoch 1753, Train Loss: 3.0367, Test Loss: 3.1024\n",
      "Epoch 1754, Train Loss: 3.0296, Test Loss: 3.1020\n",
      "Epoch 1755, Train Loss: 3.0287, Test Loss: 3.1018\n",
      "Epoch 1756, Train Loss: 3.0479, Test Loss: 3.1021\n",
      "Epoch 1757, Train Loss: 3.0506, Test Loss: 3.1036\n",
      "Epoch 1758, Train Loss: 3.0361, Test Loss: 3.1054\n",
      "Epoch 1759, Train Loss: 3.0426, Test Loss: 3.1051\n",
      "Epoch 1760, Train Loss: 3.0300, Test Loss: 3.1037\n",
      "Epoch 1761, Train Loss: 3.0601, Test Loss: 3.1031\n",
      "Epoch 1762, Train Loss: 3.0516, Test Loss: 3.1024\n",
      "Epoch 1763, Train Loss: 3.0376, Test Loss: 3.1024\n",
      "Epoch 1764, Train Loss: 3.0382, Test Loss: 3.1041\n",
      "Epoch 1765, Train Loss: 3.0481, Test Loss: 3.1025\n",
      "Epoch 1766, Train Loss: 3.0527, Test Loss: 3.1006\n",
      "Epoch 1767, Train Loss: 3.0444, Test Loss: 3.1002\n",
      "Epoch 1768, Train Loss: 3.0421, Test Loss: 3.0999\n",
      "Epoch 1769, Train Loss: 3.0327, Test Loss: 3.1015\n",
      "Epoch 1770, Train Loss: 3.0420, Test Loss: 3.1038\n",
      "Epoch 1771, Train Loss: 3.0430, Test Loss: 3.1039\n",
      "Epoch 1772, Train Loss: 3.0439, Test Loss: 3.1026\n",
      "Epoch 1773, Train Loss: 3.0325, Test Loss: 3.1034\n",
      "Epoch 1774, Train Loss: 3.0426, Test Loss: 3.1037\n",
      "Epoch 1775, Train Loss: 3.0479, Test Loss: 3.1033\n",
      "Epoch 1776, Train Loss: 3.0375, Test Loss: 3.1061\n",
      "Epoch 1777, Train Loss: 3.0578, Test Loss: 3.1051\n",
      "Epoch 1778, Train Loss: 3.0506, Test Loss: 3.1022\n",
      "Epoch 1779, Train Loss: 3.0387, Test Loss: 3.1018\n",
      "Epoch 1780, Train Loss: 3.0425, Test Loss: 3.1024\n",
      "Epoch 1781, Train Loss: 3.0372, Test Loss: 3.1023\n",
      "Epoch 1782, Train Loss: 3.0415, Test Loss: 3.1035\n",
      "Epoch 1783, Train Loss: 3.0429, Test Loss: 3.1027\n",
      "Epoch 1784, Train Loss: 3.0385, Test Loss: 3.1007\n",
      "Epoch 1785, Train Loss: 3.0512, Test Loss: 3.1003\n",
      "Epoch 1786, Train Loss: 3.0726, Test Loss: 3.1002\n",
      "Epoch 1787, Train Loss: 3.0481, Test Loss: 3.1014\n",
      "Epoch 1788, Train Loss: 3.0511, Test Loss: 3.1034\n",
      "Epoch 1789, Train Loss: 3.0419, Test Loss: 3.1040\n",
      "Epoch 1790, Train Loss: 3.0499, Test Loss: 3.1010\n",
      "Epoch 1791, Train Loss: 3.0427, Test Loss: 3.1011\n",
      "Epoch 1792, Train Loss: 3.0486, Test Loss: 3.1006\n",
      "Epoch 1793, Train Loss: 3.0420, Test Loss: 3.1019\n",
      "Epoch 1794, Train Loss: 3.0325, Test Loss: 3.1058\n",
      "Epoch 1795, Train Loss: 3.0496, Test Loss: 3.1056\n",
      "Epoch 1796, Train Loss: 3.0668, Test Loss: 3.1012\n",
      "Epoch 1797, Train Loss: 3.0364, Test Loss: 3.1015\n",
      "Epoch 1798, Train Loss: 3.0372, Test Loss: 3.1007\n",
      "Epoch 1799, Train Loss: 3.0483, Test Loss: 3.1021\n",
      "Epoch 1800, Train Loss: 3.0337, Test Loss: 3.1060\n",
      "Epoch 1801, Train Loss: 3.0331, Test Loss: 3.1054\n",
      "Epoch 1802, Train Loss: 3.0481, Test Loss: 3.1028\n",
      "Epoch 1803, Train Loss: 3.0350, Test Loss: 3.1034\n",
      "Epoch 1804, Train Loss: 3.0485, Test Loss: 3.1023\n",
      "Epoch 1805, Train Loss: 3.0495, Test Loss: 3.1036\n",
      "Epoch 1806, Train Loss: 3.0239, Test Loss: 3.1076\n",
      "Epoch 1807, Train Loss: 3.0476, Test Loss: 3.1074\n",
      "Epoch 1808, Train Loss: 3.0467, Test Loss: 3.1031\n",
      "Epoch 1809, Train Loss: 3.0310, Test Loss: 3.1006\n",
      "Epoch 1810, Train Loss: 3.0344, Test Loss: 3.1017\n",
      "Epoch 1811, Train Loss: 3.0322, Test Loss: 3.1004\n",
      "Epoch 1812, Train Loss: 3.0388, Test Loss: 3.1021\n",
      "Epoch 1813, Train Loss: 3.0520, Test Loss: 3.1037\n",
      "Epoch 1814, Train Loss: 3.0448, Test Loss: 3.1025\n",
      "Epoch 1815, Train Loss: 3.0420, Test Loss: 3.1009\n",
      "Epoch 1816, Train Loss: 3.0486, Test Loss: 3.1028\n",
      "Epoch 1817, Train Loss: 3.0476, Test Loss: 3.1026\n",
      "Epoch 1818, Train Loss: 3.0405, Test Loss: 3.1063\n",
      "Epoch 1819, Train Loss: 3.0507, Test Loss: 3.1088\n",
      "Epoch 1820, Train Loss: 3.0480, Test Loss: 3.1066\n",
      "Epoch 1821, Train Loss: 3.0549, Test Loss: 3.1021\n",
      "Epoch 1822, Train Loss: 3.0533, Test Loss: 3.1021\n",
      "Epoch 1823, Train Loss: 3.0507, Test Loss: 3.1018\n",
      "Epoch 1824, Train Loss: 3.0673, Test Loss: 3.1004\n",
      "Epoch 1825, Train Loss: 3.0498, Test Loss: 3.1021\n",
      "Epoch 1826, Train Loss: 3.0384, Test Loss: 3.1055\n",
      "Epoch 1827, Train Loss: 3.0488, Test Loss: 3.1042\n",
      "Epoch 1828, Train Loss: 3.0462, Test Loss: 3.1004\n",
      "Epoch 1829, Train Loss: 3.0373, Test Loss: 3.1002\n",
      "Epoch 1830, Train Loss: 3.0332, Test Loss: 3.1006\n",
      "Epoch 1831, Train Loss: 3.0511, Test Loss: 3.1018\n",
      "Epoch 1832, Train Loss: 3.0512, Test Loss: 3.1056\n",
      "Epoch 1833, Train Loss: 3.0437, Test Loss: 3.1050\n",
      "Epoch 1834, Train Loss: 3.0287, Test Loss: 3.1039\n",
      "Epoch 1835, Train Loss: 3.0382, Test Loss: 3.1035\n",
      "Epoch 1836, Train Loss: 3.0409, Test Loss: 3.1033\n",
      "Epoch 1837, Train Loss: 3.0301, Test Loss: 3.1040\n",
      "Epoch 1838, Train Loss: 3.0379, Test Loss: 3.1052\n",
      "Epoch 1839, Train Loss: 3.0326, Test Loss: 3.1037\n",
      "Epoch 1840, Train Loss: 3.0319, Test Loss: 3.1016\n",
      "Epoch 1841, Train Loss: 3.0426, Test Loss: 3.1008\n",
      "Epoch 1842, Train Loss: 3.0562, Test Loss: 3.1004\n",
      "Epoch 1843, Train Loss: 3.0320, Test Loss: 3.1011\n",
      "Epoch 1844, Train Loss: 3.0394, Test Loss: 3.1022\n",
      "Epoch 1845, Train Loss: 3.0484, Test Loss: 3.1015\n",
      "Epoch 1846, Train Loss: 3.0343, Test Loss: 3.1009\n",
      "Epoch 1847, Train Loss: 3.0387, Test Loss: 3.1003\n",
      "Epoch 1848, Train Loss: 3.0563, Test Loss: 3.1006\n",
      "Epoch 1849, Train Loss: 3.0262, Test Loss: 3.1012\n",
      "Epoch 1850, Train Loss: 3.0498, Test Loss: 3.1024\n",
      "Epoch 1851, Train Loss: 3.0410, Test Loss: 3.1034\n",
      "Epoch 1852, Train Loss: 3.0429, Test Loss: 3.1032\n",
      "Epoch 1853, Train Loss: 3.0359, Test Loss: 3.1018\n",
      "Epoch 1854, Train Loss: 3.0358, Test Loss: 3.1014\n",
      "Epoch 1855, Train Loss: 3.0505, Test Loss: 3.1016\n",
      "Epoch 1856, Train Loss: 3.0342, Test Loss: 3.1023\n",
      "Epoch 1857, Train Loss: 3.0331, Test Loss: 3.1021\n",
      "Epoch 1858, Train Loss: 3.0431, Test Loss: 3.1009\n",
      "Epoch 1859, Train Loss: 3.0388, Test Loss: 3.0995\n",
      "Epoch 1860, Train Loss: 3.0266, Test Loss: 3.0989\n",
      "Epoch 1861, Train Loss: 3.0416, Test Loss: 3.0988\n",
      "Epoch 1862, Train Loss: 3.0358, Test Loss: 3.0999\n",
      "Epoch 1863, Train Loss: 3.0380, Test Loss: 3.1000\n",
      "Epoch 1864, Train Loss: 3.0367, Test Loss: 3.0996\n",
      "Epoch 1865, Train Loss: 3.0585, Test Loss: 3.0990\n",
      "Epoch 1866, Train Loss: 3.0423, Test Loss: 3.0998\n",
      "Epoch 1867, Train Loss: 3.0375, Test Loss: 3.0998\n",
      "Epoch 1868, Train Loss: 3.0374, Test Loss: 3.1016\n",
      "Epoch 1869, Train Loss: 3.0414, Test Loss: 3.1023\n",
      "Epoch 1870, Train Loss: 3.0447, Test Loss: 3.1007\n",
      "Epoch 1871, Train Loss: 3.0504, Test Loss: 3.1005\n",
      "Epoch 1872, Train Loss: 3.0427, Test Loss: 3.1008\n",
      "Epoch 1873, Train Loss: 3.0396, Test Loss: 3.1007\n",
      "Epoch 1874, Train Loss: 3.0409, Test Loss: 3.1011\n",
      "Epoch 1875, Train Loss: 3.0437, Test Loss: 3.1014\n",
      "Epoch 1876, Train Loss: 3.0485, Test Loss: 3.1001\n",
      "Epoch 1877, Train Loss: 3.0448, Test Loss: 3.0997\n",
      "Epoch 1878, Train Loss: 3.0388, Test Loss: 3.1000\n",
      "Epoch 1879, Train Loss: 3.0283, Test Loss: 3.1009\n",
      "Epoch 1880, Train Loss: 3.0391, Test Loss: 3.1028\n",
      "Epoch 1881, Train Loss: 3.0354, Test Loss: 3.1025\n",
      "Epoch 1882, Train Loss: 3.0342, Test Loss: 3.1010\n",
      "Epoch 1883, Train Loss: 3.0339, Test Loss: 3.1002\n",
      "Epoch 1884, Train Loss: 3.0371, Test Loss: 3.1007\n",
      "Epoch 1885, Train Loss: 3.0449, Test Loss: 3.1009\n",
      "Epoch 1886, Train Loss: 3.0319, Test Loss: 3.1017\n",
      "Epoch 1887, Train Loss: 3.0338, Test Loss: 3.1021\n",
      "Epoch 1888, Train Loss: 3.0337, Test Loss: 3.1011\n",
      "Epoch 1889, Train Loss: 3.0424, Test Loss: 3.1003\n",
      "Epoch 1890, Train Loss: 3.0440, Test Loss: 3.1004\n",
      "Epoch 1891, Train Loss: 3.0371, Test Loss: 3.1004\n",
      "Epoch 1892, Train Loss: 3.0463, Test Loss: 3.1006\n",
      "Epoch 1893, Train Loss: 3.0479, Test Loss: 3.1015\n",
      "Epoch 1894, Train Loss: 3.0400, Test Loss: 3.1017\n",
      "Epoch 1895, Train Loss: 3.0540, Test Loss: 3.1012\n",
      "Epoch 1896, Train Loss: 3.0328, Test Loss: 3.1004\n",
      "Epoch 1897, Train Loss: 3.0264, Test Loss: 3.1000\n",
      "Epoch 1898, Train Loss: 3.0417, Test Loss: 3.1001\n",
      "Epoch 1899, Train Loss: 3.0288, Test Loss: 3.1007\n",
      "Epoch 1900, Train Loss: 3.0469, Test Loss: 3.1009\n",
      "Epoch 1901, Train Loss: 3.0397, Test Loss: 3.1000\n",
      "Epoch 1902, Train Loss: 3.0314, Test Loss: 3.0990\n",
      "Epoch 1903, Train Loss: 3.0426, Test Loss: 3.0997\n",
      "Epoch 1904, Train Loss: 3.0435, Test Loss: 3.1004\n",
      "Epoch 1905, Train Loss: 3.0492, Test Loss: 3.1010\n",
      "Epoch 1906, Train Loss: 3.0477, Test Loss: 3.1014\n",
      "Epoch 1907, Train Loss: 3.0348, Test Loss: 3.1021\n",
      "Epoch 1908, Train Loss: 3.0400, Test Loss: 3.1019\n",
      "Epoch 1909, Train Loss: 3.0358, Test Loss: 3.1031\n",
      "Epoch 1910, Train Loss: 3.0368, Test Loss: 3.1034\n",
      "Epoch 1911, Train Loss: 3.0434, Test Loss: 3.1031\n",
      "Epoch 1912, Train Loss: 3.0374, Test Loss: 3.1019\n",
      "Epoch 1913, Train Loss: 3.0557, Test Loss: 3.1013\n",
      "Epoch 1914, Train Loss: 3.0297, Test Loss: 3.1016\n",
      "Epoch 1915, Train Loss: 3.0389, Test Loss: 3.1014\n",
      "Epoch 1916, Train Loss: 3.0474, Test Loss: 3.1014\n",
      "Epoch 1917, Train Loss: 3.0424, Test Loss: 3.1003\n",
      "Epoch 1918, Train Loss: 3.0412, Test Loss: 3.0998\n",
      "Epoch 1919, Train Loss: 3.0412, Test Loss: 3.1001\n",
      "Epoch 1920, Train Loss: 3.0371, Test Loss: 3.1006\n",
      "Epoch 1921, Train Loss: 3.0327, Test Loss: 3.1011\n",
      "Epoch 1922, Train Loss: 3.0560, Test Loss: 3.1015\n",
      "Epoch 1923, Train Loss: 3.0446, Test Loss: 3.1016\n",
      "Epoch 1924, Train Loss: 3.0279, Test Loss: 3.1012\n",
      "Epoch 1925, Train Loss: 3.0452, Test Loss: 3.1012\n",
      "Epoch 1926, Train Loss: 3.0296, Test Loss: 3.1018\n",
      "Epoch 1927, Train Loss: 3.0312, Test Loss: 3.1027\n",
      "Epoch 1928, Train Loss: 3.0503, Test Loss: 3.1034\n",
      "Epoch 1929, Train Loss: 3.0398, Test Loss: 3.1034\n",
      "Epoch 1930, Train Loss: 3.0456, Test Loss: 3.1035\n",
      "Epoch 1931, Train Loss: 3.0420, Test Loss: 3.1024\n",
      "Epoch 1932, Train Loss: 3.0280, Test Loss: 3.1016\n",
      "Epoch 1933, Train Loss: 3.0461, Test Loss: 3.1014\n",
      "Epoch 1934, Train Loss: 3.0315, Test Loss: 3.1017\n",
      "Epoch 1935, Train Loss: 3.0276, Test Loss: 3.1023\n",
      "Epoch 1936, Train Loss: 3.0294, Test Loss: 3.1021\n",
      "Epoch 1937, Train Loss: 3.0406, Test Loss: 3.1013\n",
      "Epoch 1938, Train Loss: 3.0391, Test Loss: 3.1004\n",
      "Epoch 1939, Train Loss: 3.0490, Test Loss: 3.1004\n",
      "Epoch 1940, Train Loss: 3.0382, Test Loss: 3.1009\n",
      "Epoch 1941, Train Loss: 3.0258, Test Loss: 3.1024\n",
      "Epoch 1942, Train Loss: 3.0525, Test Loss: 3.1033\n",
      "Epoch 1943, Train Loss: 3.0449, Test Loss: 3.1029\n",
      "Epoch 1944, Train Loss: 3.0332, Test Loss: 3.1012\n",
      "Epoch 1945, Train Loss: 3.0366, Test Loss: 3.1006\n",
      "Epoch 1946, Train Loss: 3.0327, Test Loss: 3.1005\n",
      "Epoch 1947, Train Loss: 3.0383, Test Loss: 3.1017\n",
      "Epoch 1948, Train Loss: 3.0421, Test Loss: 3.1028\n",
      "Epoch 1949, Train Loss: 3.0565, Test Loss: 3.1018\n",
      "Epoch 1950, Train Loss: 3.0577, Test Loss: 3.1009\n",
      "Epoch 1951, Train Loss: 3.0317, Test Loss: 3.1000\n",
      "Epoch 1952, Train Loss: 3.0336, Test Loss: 3.1000\n",
      "Epoch 1953, Train Loss: 3.0318, Test Loss: 3.1000\n",
      "Epoch 1954, Train Loss: 3.0432, Test Loss: 3.1006\n",
      "Epoch 1955, Train Loss: 3.0521, Test Loss: 3.1010\n",
      "Epoch 1956, Train Loss: 3.0255, Test Loss: 3.1004\n",
      "Epoch 1957, Train Loss: 3.0377, Test Loss: 3.1001\n",
      "Epoch 1958, Train Loss: 3.0374, Test Loss: 3.1007\n",
      "Epoch 1959, Train Loss: 3.0416, Test Loss: 3.1007\n",
      "Epoch 1960, Train Loss: 3.0386, Test Loss: 3.1009\n",
      "Epoch 1961, Train Loss: 3.0456, Test Loss: 3.1014\n",
      "Epoch 1962, Train Loss: 3.0444, Test Loss: 3.1022\n",
      "Epoch 1963, Train Loss: 3.0432, Test Loss: 3.1024\n",
      "Epoch 1964, Train Loss: 3.0386, Test Loss: 3.1012\n",
      "Epoch 1965, Train Loss: 3.0486, Test Loss: 3.1000\n",
      "Epoch 1966, Train Loss: 3.0475, Test Loss: 3.0993\n",
      "Epoch 1967, Train Loss: 3.0265, Test Loss: 3.0989\n",
      "Epoch 1968, Train Loss: 3.0409, Test Loss: 3.0990\n",
      "Epoch 1969, Train Loss: 3.0223, Test Loss: 3.0990\n",
      "Epoch 1970, Train Loss: 3.0426, Test Loss: 3.0990\n",
      "Epoch 1971, Train Loss: 3.0295, Test Loss: 3.0984\n",
      "Epoch 1972, Train Loss: 3.0421, Test Loss: 3.0991\n",
      "Epoch 1973, Train Loss: 3.0412, Test Loss: 3.1004\n",
      "Epoch 1974, Train Loss: 3.0489, Test Loss: 3.1009\n",
      "Epoch 1975, Train Loss: 3.0177, Test Loss: 3.1017\n",
      "Epoch 1976, Train Loss: 3.0413, Test Loss: 3.1027\n",
      "Epoch 1977, Train Loss: 3.0408, Test Loss: 3.1023\n",
      "Epoch 1978, Train Loss: 3.0419, Test Loss: 3.1015\n",
      "Epoch 1979, Train Loss: 3.0450, Test Loss: 3.1010\n",
      "Epoch 1980, Train Loss: 3.0328, Test Loss: 3.1006\n",
      "Epoch 1981, Train Loss: 3.0479, Test Loss: 3.1002\n",
      "Epoch 1982, Train Loss: 3.0253, Test Loss: 3.1000\n",
      "Epoch 1983, Train Loss: 3.0483, Test Loss: 3.1001\n",
      "Epoch 1984, Train Loss: 3.0330, Test Loss: 3.1002\n",
      "Epoch 1985, Train Loss: 3.0470, Test Loss: 3.1013\n",
      "Epoch 1986, Train Loss: 3.0337, Test Loss: 3.1020\n",
      "Epoch 1987, Train Loss: 3.0423, Test Loss: 3.1004\n",
      "Epoch 1988, Train Loss: 3.0216, Test Loss: 3.0995\n",
      "Epoch 1989, Train Loss: 3.0188, Test Loss: 3.0991\n",
      "Epoch 1990, Train Loss: 3.0417, Test Loss: 3.0991\n",
      "Epoch 1991, Train Loss: 3.0599, Test Loss: 3.0999\n",
      "Epoch 1992, Train Loss: 3.0510, Test Loss: 3.1020\n",
      "Epoch 1993, Train Loss: 3.0240, Test Loss: 3.1023\n",
      "Epoch 1994, Train Loss: 3.0521, Test Loss: 3.1003\n",
      "Epoch 1995, Train Loss: 3.0245, Test Loss: 3.0986\n",
      "Epoch 1996, Train Loss: 3.0421, Test Loss: 3.0984\n",
      "Epoch 1997, Train Loss: 3.0407, Test Loss: 3.0983\n",
      "Epoch 1998, Train Loss: 3.0504, Test Loss: 3.0999\n",
      "Epoch 1999, Train Loss: 3.0498, Test Loss: 3.1011\n",
      "Epoch 2000, Train Loss: 3.0284, Test Loss: 3.1012\n",
      "Epoch 2001, Train Loss: 3.0336, Test Loss: 3.1001\n",
      "Epoch 2002, Train Loss: 3.0396, Test Loss: 3.0998\n",
      "Epoch 2003, Train Loss: 3.0472, Test Loss: 3.1006\n",
      "Epoch 2004, Train Loss: 3.0454, Test Loss: 3.1011\n",
      "Epoch 2005, Train Loss: 3.0281, Test Loss: 3.1015\n",
      "Epoch 2006, Train Loss: 3.0294, Test Loss: 3.1004\n",
      "Epoch 2007, Train Loss: 3.0333, Test Loss: 3.0999\n",
      "Epoch 2008, Train Loss: 3.0387, Test Loss: 3.1004\n",
      "Epoch 2009, Train Loss: 3.0462, Test Loss: 3.1014\n",
      "Epoch 2010, Train Loss: 3.0347, Test Loss: 3.1016\n",
      "Epoch 2011, Train Loss: 3.0349, Test Loss: 3.0996\n",
      "Epoch 2012, Train Loss: 3.0465, Test Loss: 3.0989\n",
      "Epoch 2013, Train Loss: 3.0436, Test Loss: 3.0991\n",
      "Epoch 2014, Train Loss: 3.0417, Test Loss: 3.1002\n",
      "Epoch 2015, Train Loss: 3.0392, Test Loss: 3.1016\n",
      "Epoch 2016, Train Loss: 3.0334, Test Loss: 3.1012\n",
      "Epoch 2017, Train Loss: 3.0494, Test Loss: 3.1007\n",
      "Epoch 2018, Train Loss: 3.0438, Test Loss: 3.1002\n",
      "Epoch 2019, Train Loss: 3.0341, Test Loss: 3.1004\n",
      "Epoch 2020, Train Loss: 3.0277, Test Loss: 3.1005\n",
      "Epoch 2021, Train Loss: 3.0384, Test Loss: 3.1007\n",
      "Epoch 2022, Train Loss: 3.0423, Test Loss: 3.1008\n",
      "Epoch 2023, Train Loss: 3.0235, Test Loss: 3.1006\n",
      "Epoch 2024, Train Loss: 3.0227, Test Loss: 3.1009\n",
      "Epoch 2025, Train Loss: 3.0358, Test Loss: 3.1022\n",
      "Epoch 2026, Train Loss: 3.0458, Test Loss: 3.1032\n",
      "Epoch 2027, Train Loss: 3.0345, Test Loss: 3.1021\n",
      "Epoch 2028, Train Loss: 3.0361, Test Loss: 3.1005\n",
      "Epoch 2029, Train Loss: 3.0460, Test Loss: 3.1001\n",
      "Epoch 2030, Train Loss: 3.0325, Test Loss: 3.0994\n",
      "Epoch 2031, Train Loss: 3.0369, Test Loss: 3.0997\n",
      "Epoch 2032, Train Loss: 3.0411, Test Loss: 3.1015\n",
      "Epoch 2033, Train Loss: 3.0395, Test Loss: 3.1016\n",
      "Epoch 2034, Train Loss: 3.0430, Test Loss: 3.1016\n",
      "Epoch 2035, Train Loss: 3.0430, Test Loss: 3.1020\n",
      "Epoch 2036, Train Loss: 3.0468, Test Loss: 3.1017\n",
      "Epoch 2037, Train Loss: 3.0499, Test Loss: 3.1023\n",
      "Epoch 2038, Train Loss: 3.0462, Test Loss: 3.1014\n",
      "Epoch 2039, Train Loss: 3.0325, Test Loss: 3.1002\n",
      "Epoch 2040, Train Loss: 3.0175, Test Loss: 3.1001\n",
      "Epoch 2041, Train Loss: 3.0284, Test Loss: 3.1006\n",
      "Epoch 2042, Train Loss: 3.0389, Test Loss: 3.1014\n",
      "Epoch 2043, Train Loss: 3.0341, Test Loss: 3.1012\n",
      "Epoch 2044, Train Loss: 3.0427, Test Loss: 3.1007\n",
      "Epoch 2045, Train Loss: 3.0315, Test Loss: 3.1005\n",
      "Epoch 2046, Train Loss: 3.0524, Test Loss: 3.1002\n",
      "Epoch 2047, Train Loss: 3.0406, Test Loss: 3.1004\n",
      "Epoch 2048, Train Loss: 3.0339, Test Loss: 3.1014\n",
      "Epoch 2049, Train Loss: 3.0447, Test Loss: 3.1009\n",
      "Epoch 2050, Train Loss: 3.0421, Test Loss: 3.0997\n",
      "Epoch 2051, Train Loss: 3.0310, Test Loss: 3.0997\n",
      "Epoch 2052, Train Loss: 3.0439, Test Loss: 3.0994\n",
      "Epoch 2053, Train Loss: 3.0390, Test Loss: 3.1000\n",
      "Epoch 2054, Train Loss: 3.0241, Test Loss: 3.1005\n",
      "Epoch 2055, Train Loss: 3.0286, Test Loss: 3.1001\n",
      "Epoch 2056, Train Loss: 3.0252, Test Loss: 3.1002\n",
      "Epoch 2057, Train Loss: 3.0344, Test Loss: 3.1007\n",
      "Epoch 2058, Train Loss: 3.0415, Test Loss: 3.1007\n",
      "Epoch 2059, Train Loss: 3.0318, Test Loss: 3.1013\n",
      "Epoch 2060, Train Loss: 3.0382, Test Loss: 3.1014\n",
      "Epoch 2061, Train Loss: 3.0463, Test Loss: 3.0997\n",
      "Epoch 2062, Train Loss: 3.0293, Test Loss: 3.0989\n",
      "Epoch 2063, Train Loss: 3.0225, Test Loss: 3.0994\n",
      "Epoch 2064, Train Loss: 3.0427, Test Loss: 3.0996\n",
      "Epoch 2065, Train Loss: 3.0228, Test Loss: 3.0999\n",
      "Epoch 2066, Train Loss: 3.0303, Test Loss: 3.0991\n",
      "Epoch 2067, Train Loss: 3.0460, Test Loss: 3.0987\n",
      "Epoch 2068, Train Loss: 3.0344, Test Loss: 3.0989\n",
      "Epoch 2069, Train Loss: 3.0370, Test Loss: 3.1000\n",
      "Epoch 2070, Train Loss: 3.0328, Test Loss: 3.1006\n",
      "Epoch 2071, Train Loss: 3.0291, Test Loss: 3.1003\n",
      "Epoch 2072, Train Loss: 3.0466, Test Loss: 3.1001\n",
      "Epoch 2073, Train Loss: 3.0373, Test Loss: 3.1012\n",
      "Epoch 2074, Train Loss: 3.0431, Test Loss: 3.1013\n",
      "Epoch 2075, Train Loss: 3.0313, Test Loss: 3.1010\n",
      "Epoch 2076, Train Loss: 3.0444, Test Loss: 3.1012\n",
      "Epoch 2077, Train Loss: 3.0232, Test Loss: 3.1010\n",
      "Epoch 2078, Train Loss: 3.0372, Test Loss: 3.0999\n",
      "Epoch 2079, Train Loss: 3.0329, Test Loss: 3.0994\n",
      "Epoch 2080, Train Loss: 3.0378, Test Loss: 3.0991\n",
      "Epoch 2081, Train Loss: 3.0315, Test Loss: 3.0993\n",
      "Epoch 2082, Train Loss: 3.0472, Test Loss: 3.0997\n",
      "Epoch 2083, Train Loss: 3.0545, Test Loss: 3.1002\n",
      "Epoch 2084, Train Loss: 3.0435, Test Loss: 3.0994\n",
      "Epoch 2085, Train Loss: 3.0311, Test Loss: 3.0989\n",
      "Epoch 2086, Train Loss: 3.0210, Test Loss: 3.0990\n",
      "Epoch 2087, Train Loss: 3.0225, Test Loss: 3.0997\n",
      "Epoch 2088, Train Loss: 3.0238, Test Loss: 3.1014\n",
      "Epoch 2089, Train Loss: 3.0233, Test Loss: 3.1008\n",
      "Epoch 2090, Train Loss: 3.0285, Test Loss: 3.1003\n",
      "Epoch 2091, Train Loss: 3.0357, Test Loss: 3.1007\n",
      "Epoch 2092, Train Loss: 3.0283, Test Loss: 3.1015\n",
      "Epoch 2093, Train Loss: 3.0286, Test Loss: 3.1031\n",
      "Epoch 2094, Train Loss: 3.0338, Test Loss: 3.1023\n",
      "Epoch 2095, Train Loss: 3.0398, Test Loss: 3.1011\n",
      "Epoch 2096, Train Loss: 3.0517, Test Loss: 3.1012\n",
      "Epoch 2097, Train Loss: 3.0326, Test Loss: 3.1013\n",
      "Epoch 2098, Train Loss: 3.0285, Test Loss: 3.1002\n",
      "Epoch 2099, Train Loss: 3.0434, Test Loss: 3.0994\n",
      "Epoch 2100, Train Loss: 3.0463, Test Loss: 3.0988\n",
      "Epoch 2101, Train Loss: 3.0260, Test Loss: 3.0990\n",
      "Epoch 2102, Train Loss: 3.0395, Test Loss: 3.0991\n",
      "Epoch 2103, Train Loss: 3.0302, Test Loss: 3.0995\n",
      "Epoch 2104, Train Loss: 3.0311, Test Loss: 3.0989\n",
      "Epoch 2105, Train Loss: 3.0377, Test Loss: 3.0985\n",
      "Epoch 2106, Train Loss: 3.0201, Test Loss: 3.0987\n",
      "Epoch 2107, Train Loss: 3.0303, Test Loss: 3.0988\n",
      "Epoch 2108, Train Loss: 3.0235, Test Loss: 3.0991\n",
      "Epoch 2109, Train Loss: 3.0418, Test Loss: 3.0998\n",
      "Epoch 2110, Train Loss: 3.0357, Test Loss: 3.0988\n",
      "Epoch 2111, Train Loss: 3.0270, Test Loss: 3.0972\n",
      "Epoch 2112, Train Loss: 3.0478, Test Loss: 3.0968\n",
      "Epoch 2113, Train Loss: 3.0364, Test Loss: 3.0971\n",
      "Epoch 2114, Train Loss: 3.0373, Test Loss: 3.0986\n",
      "Epoch 2115, Train Loss: 3.0548, Test Loss: 3.0999\n",
      "Epoch 2116, Train Loss: 3.0304, Test Loss: 3.0986\n",
      "Epoch 2117, Train Loss: 3.0365, Test Loss: 3.0982\n",
      "Epoch 2118, Train Loss: 3.0364, Test Loss: 3.0981\n",
      "Epoch 2119, Train Loss: 3.0235, Test Loss: 3.0988\n",
      "Epoch 2120, Train Loss: 3.0374, Test Loss: 3.0992\n",
      "Epoch 2121, Train Loss: 3.0496, Test Loss: 3.0985\n",
      "Epoch 2122, Train Loss: 3.0430, Test Loss: 3.0979\n",
      "Epoch 2123, Train Loss: 3.0241, Test Loss: 3.0971\n",
      "Epoch 2124, Train Loss: 3.0309, Test Loss: 3.0978\n",
      "Epoch 2125, Train Loss: 3.0222, Test Loss: 3.0986\n",
      "Epoch 2126, Train Loss: 3.0318, Test Loss: 3.0991\n",
      "Epoch 2127, Train Loss: 3.0288, Test Loss: 3.1003\n",
      "Epoch 2128, Train Loss: 3.0327, Test Loss: 3.1011\n",
      "Epoch 2129, Train Loss: 3.0433, Test Loss: 3.1014\n",
      "Epoch 2130, Train Loss: 3.0411, Test Loss: 3.1015\n",
      "Epoch 2131, Train Loss: 3.0317, Test Loss: 3.1008\n",
      "Epoch 2132, Train Loss: 3.0365, Test Loss: 3.1001\n",
      "Epoch 2133, Train Loss: 3.0282, Test Loss: 3.1005\n",
      "Epoch 2134, Train Loss: 3.0180, Test Loss: 3.1003\n",
      "Epoch 2135, Train Loss: 3.0379, Test Loss: 3.0999\n",
      "Epoch 2136, Train Loss: 3.0462, Test Loss: 3.0997\n",
      "Epoch 2137, Train Loss: 3.0439, Test Loss: 3.0993\n",
      "Epoch 2138, Train Loss: 3.0359, Test Loss: 3.0993\n",
      "Epoch 2139, Train Loss: 3.0309, Test Loss: 3.0999\n",
      "Epoch 2140, Train Loss: 3.0442, Test Loss: 3.0998\n",
      "Epoch 2141, Train Loss: 3.0361, Test Loss: 3.0999\n",
      "Epoch 2142, Train Loss: 3.0263, Test Loss: 3.1011\n",
      "Epoch 2143, Train Loss: 3.0337, Test Loss: 3.1008\n",
      "Epoch 2144, Train Loss: 3.0296, Test Loss: 3.0982\n",
      "Epoch 2145, Train Loss: 3.0407, Test Loss: 3.0971\n",
      "Epoch 2146, Train Loss: 3.0407, Test Loss: 3.0972\n",
      "Epoch 2147, Train Loss: 3.0467, Test Loss: 3.0984\n",
      "Epoch 2148, Train Loss: 3.0371, Test Loss: 3.0997\n",
      "Epoch 2149, Train Loss: 3.0338, Test Loss: 3.1007\n",
      "Epoch 2150, Train Loss: 3.0363, Test Loss: 3.1020\n",
      "Epoch 2151, Train Loss: 3.0371, Test Loss: 3.1010\n",
      "Epoch 2152, Train Loss: 3.0334, Test Loss: 3.0997\n",
      "Epoch 2153, Train Loss: 3.0262, Test Loss: 3.1023\n",
      "Epoch 2154, Train Loss: 3.0301, Test Loss: 3.1005\n",
      "Epoch 2155, Train Loss: 3.0334, Test Loss: 3.0979\n",
      "Epoch 2156, Train Loss: 3.0280, Test Loss: 3.1008\n",
      "Epoch 2157, Train Loss: 3.0474, Test Loss: 3.1008\n",
      "Epoch 2158, Train Loss: 3.0296, Test Loss: 3.1008\n",
      "Epoch 2159, Train Loss: 3.0340, Test Loss: 3.1014\n",
      "Epoch 2160, Train Loss: 3.0439, Test Loss: 3.0996\n",
      "Epoch 2161, Train Loss: 3.0320, Test Loss: 3.1002\n",
      "Epoch 2162, Train Loss: 3.0248, Test Loss: 3.1012\n",
      "Epoch 2163, Train Loss: 3.0428, Test Loss: 3.1047\n",
      "Epoch 2164, Train Loss: 3.0367, Test Loss: 3.1041\n",
      "Epoch 2165, Train Loss: 3.0321, Test Loss: 3.1001\n",
      "Epoch 2166, Train Loss: 3.0215, Test Loss: 3.0993\n",
      "Epoch 2167, Train Loss: 3.0348, Test Loss: 3.0996\n",
      "Epoch 2168, Train Loss: 3.0348, Test Loss: 3.1010\n",
      "Epoch 2169, Train Loss: 3.0353, Test Loss: 3.1048\n",
      "Epoch 2170, Train Loss: 3.0399, Test Loss: 3.1005\n",
      "Epoch 2171, Train Loss: 3.0335, Test Loss: 3.0985\n",
      "Epoch 2172, Train Loss: 3.0311, Test Loss: 3.0982\n",
      "Epoch 2173, Train Loss: 3.0245, Test Loss: 3.0984\n",
      "Epoch 2174, Train Loss: 3.0368, Test Loss: 3.1008\n",
      "Epoch 2175, Train Loss: 3.0346, Test Loss: 3.1027\n",
      "Epoch 2176, Train Loss: 3.0431, Test Loss: 3.0997\n",
      "Epoch 2177, Train Loss: 3.0335, Test Loss: 3.0978\n",
      "Epoch 2178, Train Loss: 3.0335, Test Loss: 3.0981\n",
      "Epoch 2179, Train Loss: 3.0325, Test Loss: 3.0982\n",
      "Epoch 2180, Train Loss: 3.0330, Test Loss: 3.0992\n",
      "Epoch 2181, Train Loss: 3.0220, Test Loss: 3.0998\n",
      "Epoch 2182, Train Loss: 3.0423, Test Loss: 3.0993\n",
      "Epoch 2183, Train Loss: 3.0268, Test Loss: 3.0992\n",
      "Epoch 2184, Train Loss: 3.0296, Test Loss: 3.1000\n",
      "Epoch 2185, Train Loss: 3.0510, Test Loss: 3.1017\n",
      "Epoch 2186, Train Loss: 3.0308, Test Loss: 3.1014\n",
      "Epoch 2187, Train Loss: 3.0411, Test Loss: 3.1012\n",
      "Epoch 2188, Train Loss: 3.0255, Test Loss: 3.1006\n",
      "Epoch 2189, Train Loss: 3.0409, Test Loss: 3.1007\n",
      "Epoch 2190, Train Loss: 3.0325, Test Loss: 3.1009\n",
      "Epoch 2191, Train Loss: 3.0358, Test Loss: 3.0993\n",
      "Epoch 2192, Train Loss: 3.0158, Test Loss: 3.0974\n",
      "Epoch 2193, Train Loss: 3.0503, Test Loss: 3.0966\n",
      "Epoch 2194, Train Loss: 3.0247, Test Loss: 3.0962\n",
      "Epoch 2195, Train Loss: 3.0484, Test Loss: 3.0971\n",
      "Epoch 2196, Train Loss: 3.0380, Test Loss: 3.0996\n",
      "Epoch 2197, Train Loss: 3.0304, Test Loss: 3.0995\n",
      "Epoch 2198, Train Loss: 3.0294, Test Loss: 3.0982\n",
      "Epoch 2199, Train Loss: 3.0336, Test Loss: 3.0979\n",
      "Epoch 2200, Train Loss: 3.0336, Test Loss: 3.0983\n",
      "Epoch 2201, Train Loss: 3.0212, Test Loss: 3.0995\n",
      "Epoch 2202, Train Loss: 3.0345, Test Loss: 3.1006\n",
      "Epoch 2203, Train Loss: 3.0449, Test Loss: 3.0998\n",
      "Epoch 2204, Train Loss: 3.0374, Test Loss: 3.1003\n",
      "Epoch 2205, Train Loss: 3.0330, Test Loss: 3.1000\n",
      "Epoch 2206, Train Loss: 3.0286, Test Loss: 3.0987\n",
      "Epoch 2207, Train Loss: 3.0371, Test Loss: 3.0993\n",
      "Epoch 2208, Train Loss: 3.0402, Test Loss: 3.0999\n",
      "Epoch 2209, Train Loss: 3.0340, Test Loss: 3.0994\n",
      "Epoch 2210, Train Loss: 3.0374, Test Loss: 3.0988\n",
      "Epoch 2211, Train Loss: 3.0291, Test Loss: 3.0996\n",
      "Epoch 2212, Train Loss: 3.0335, Test Loss: 3.1005\n",
      "Epoch 2213, Train Loss: 3.0350, Test Loss: 3.0996\n",
      "Epoch 2214, Train Loss: 3.0403, Test Loss: 3.1005\n",
      "Epoch 2215, Train Loss: 3.0301, Test Loss: 3.1006\n",
      "Epoch 2216, Train Loss: 3.0288, Test Loss: 3.1000\n",
      "Epoch 2217, Train Loss: 3.0292, Test Loss: 3.1009\n",
      "Epoch 2218, Train Loss: 3.0363, Test Loss: 3.1014\n",
      "Epoch 2219, Train Loss: 3.0468, Test Loss: 3.0998\n",
      "Epoch 2220, Train Loss: 3.0309, Test Loss: 3.0980\n",
      "Epoch 2221, Train Loss: 3.0227, Test Loss: 3.0969\n",
      "Epoch 2222, Train Loss: 3.0242, Test Loss: 3.0965\n",
      "Epoch 2223, Train Loss: 3.0292, Test Loss: 3.0983\n",
      "Epoch 2224, Train Loss: 3.0366, Test Loss: 3.0973\n",
      "Epoch 2225, Train Loss: 3.0297, Test Loss: 3.0968\n",
      "Epoch 2226, Train Loss: 3.0287, Test Loss: 3.0970\n",
      "Epoch 2227, Train Loss: 3.0462, Test Loss: 3.0987\n",
      "Epoch 2228, Train Loss: 3.0446, Test Loss: 3.1021\n",
      "Epoch 2229, Train Loss: 3.0431, Test Loss: 3.1027\n",
      "Epoch 2230, Train Loss: 3.0332, Test Loss: 3.1023\n",
      "Epoch 2231, Train Loss: 3.0265, Test Loss: 3.1018\n",
      "Epoch 2232, Train Loss: 3.0244, Test Loss: 3.1017\n",
      "Epoch 2233, Train Loss: 3.0327, Test Loss: 3.1016\n",
      "Epoch 2234, Train Loss: 3.0172, Test Loss: 3.0997\n",
      "Epoch 2235, Train Loss: 3.0281, Test Loss: 3.0993\n",
      "Epoch 2236, Train Loss: 3.0433, Test Loss: 3.0983\n",
      "Epoch 2237, Train Loss: 3.0376, Test Loss: 3.0976\n",
      "Epoch 2238, Train Loss: 3.0468, Test Loss: 3.0976\n",
      "Epoch 2239, Train Loss: 3.0274, Test Loss: 3.0974\n",
      "Epoch 2240, Train Loss: 3.0397, Test Loss: 3.0977\n",
      "Epoch 2241, Train Loss: 3.0245, Test Loss: 3.0988\n",
      "Epoch 2242, Train Loss: 3.0328, Test Loss: 3.0991\n",
      "Epoch 2243, Train Loss: 3.0473, Test Loss: 3.0983\n",
      "Epoch 2244, Train Loss: 3.0262, Test Loss: 3.0991\n",
      "Epoch 2245, Train Loss: 3.0292, Test Loss: 3.1000\n",
      "Epoch 2246, Train Loss: 3.0356, Test Loss: 3.0997\n",
      "Epoch 2247, Train Loss: 3.0264, Test Loss: 3.0996\n",
      "Epoch 2248, Train Loss: 3.0402, Test Loss: 3.0989\n",
      "Epoch 2249, Train Loss: 3.0245, Test Loss: 3.0985\n",
      "Epoch 2250, Train Loss: 3.0293, Test Loss: 3.0974\n",
      "Epoch 2251, Train Loss: 3.0256, Test Loss: 3.0963\n",
      "Epoch 2252, Train Loss: 3.0324, Test Loss: 3.0959\n",
      "Epoch 2253, Train Loss: 3.0305, Test Loss: 3.0962\n",
      "Epoch 2254, Train Loss: 3.0209, Test Loss: 3.0971\n",
      "Epoch 2255, Train Loss: 3.0292, Test Loss: 3.0990\n",
      "Epoch 2256, Train Loss: 3.0409, Test Loss: 3.0982\n",
      "Epoch 2257, Train Loss: 3.0275, Test Loss: 3.0978\n",
      "Epoch 2258, Train Loss: 3.0283, Test Loss: 3.0984\n",
      "Epoch 2259, Train Loss: 3.0367, Test Loss: 3.0985\n",
      "Epoch 2260, Train Loss: 3.0325, Test Loss: 3.1002\n",
      "Epoch 2261, Train Loss: 3.0284, Test Loss: 3.1007\n",
      "Epoch 2262, Train Loss: 3.0396, Test Loss: 3.0983\n",
      "Epoch 2263, Train Loss: 3.0349, Test Loss: 3.0973\n",
      "Epoch 2264, Train Loss: 3.0282, Test Loss: 3.0973\n",
      "Epoch 2265, Train Loss: 3.0321, Test Loss: 3.0981\n",
      "Epoch 2266, Train Loss: 3.0356, Test Loss: 3.0978\n",
      "Epoch 2267, Train Loss: 3.0325, Test Loss: 3.0965\n",
      "Epoch 2268, Train Loss: 3.0251, Test Loss: 3.0960\n",
      "Epoch 2269, Train Loss: 3.0284, Test Loss: 3.0962\n",
      "Epoch 2270, Train Loss: 3.0422, Test Loss: 3.0977\n",
      "Epoch 2271, Train Loss: 3.0339, Test Loss: 3.0993\n",
      "Epoch 2272, Train Loss: 3.0219, Test Loss: 3.0991\n",
      "Epoch 2273, Train Loss: 3.0364, Test Loss: 3.0983\n",
      "Epoch 2274, Train Loss: 3.0363, Test Loss: 3.0979\n",
      "Epoch 2275, Train Loss: 3.0350, Test Loss: 3.0977\n",
      "Epoch 2276, Train Loss: 3.0282, Test Loss: 3.0985\n",
      "Epoch 2277, Train Loss: 3.0391, Test Loss: 3.0996\n",
      "Epoch 2278, Train Loss: 3.0381, Test Loss: 3.0977\n",
      "Epoch 2279, Train Loss: 3.0359, Test Loss: 3.0977\n",
      "Epoch 2280, Train Loss: 3.0410, Test Loss: 3.0984\n",
      "Epoch 2281, Train Loss: 3.0345, Test Loss: 3.0991\n",
      "Epoch 2282, Train Loss: 3.0392, Test Loss: 3.0998\n",
      "Epoch 2283, Train Loss: 3.0298, Test Loss: 3.0997\n",
      "Epoch 2284, Train Loss: 3.0278, Test Loss: 3.0984\n",
      "Epoch 2285, Train Loss: 3.0237, Test Loss: 3.0988\n",
      "Epoch 2286, Train Loss: 3.0535, Test Loss: 3.0998\n",
      "Epoch 2287, Train Loss: 3.0209, Test Loss: 3.1000\n",
      "Epoch 2288, Train Loss: 3.0430, Test Loss: 3.1002\n",
      "Epoch 2289, Train Loss: 3.0245, Test Loss: 3.1006\n",
      "Epoch 2290, Train Loss: 3.0428, Test Loss: 3.1004\n",
      "Epoch 2291, Train Loss: 3.0324, Test Loss: 3.1000\n",
      "Epoch 2292, Train Loss: 3.0369, Test Loss: 3.1003\n",
      "Epoch 2293, Train Loss: 3.0410, Test Loss: 3.1003\n",
      "Epoch 2294, Train Loss: 3.0301, Test Loss: 3.0984\n",
      "Epoch 2295, Train Loss: 3.0348, Test Loss: 3.0974\n",
      "Epoch 2296, Train Loss: 3.0249, Test Loss: 3.0973\n",
      "Epoch 2297, Train Loss: 3.0272, Test Loss: 3.0981\n",
      "Epoch 2298, Train Loss: 3.0214, Test Loss: 3.0984\n",
      "Epoch 2299, Train Loss: 3.0357, Test Loss: 3.0977\n",
      "Epoch 2300, Train Loss: 3.0228, Test Loss: 3.0980\n",
      "Epoch 2301, Train Loss: 3.0290, Test Loss: 3.0987\n",
      "Epoch 2302, Train Loss: 3.0232, Test Loss: 3.0998\n",
      "Epoch 2303, Train Loss: 3.0391, Test Loss: 3.0997\n",
      "Epoch 2304, Train Loss: 3.0316, Test Loss: 3.0986\n",
      "Epoch 2305, Train Loss: 3.0338, Test Loss: 3.0976\n",
      "Epoch 2306, Train Loss: 3.0372, Test Loss: 3.0971\n",
      "Epoch 2307, Train Loss: 3.0408, Test Loss: 3.0981\n",
      "Epoch 2308, Train Loss: 3.0329, Test Loss: 3.0991\n",
      "Epoch 2309, Train Loss: 3.0201, Test Loss: 3.0983\n",
      "Epoch 2310, Train Loss: 3.0339, Test Loss: 3.0973\n",
      "Epoch 2311, Train Loss: 3.0279, Test Loss: 3.0974\n",
      "Epoch 2312, Train Loss: 3.0231, Test Loss: 3.0972\n",
      "Epoch 2313, Train Loss: 3.0339, Test Loss: 3.0978\n",
      "Epoch 2314, Train Loss: 3.0239, Test Loss: 3.0986\n",
      "Epoch 2315, Train Loss: 3.0278, Test Loss: 3.0981\n",
      "Epoch 2316, Train Loss: 3.0286, Test Loss: 3.0972\n",
      "Epoch 2317, Train Loss: 3.0317, Test Loss: 3.0971\n",
      "Epoch 2318, Train Loss: 3.0340, Test Loss: 3.0968\n",
      "Epoch 2319, Train Loss: 3.0262, Test Loss: 3.0982\n",
      "Epoch 2320, Train Loss: 3.0344, Test Loss: 3.0986\n",
      "Epoch 2321, Train Loss: 3.0239, Test Loss: 3.0982\n",
      "Epoch 2322, Train Loss: 3.0234, Test Loss: 3.0969\n",
      "Epoch 2323, Train Loss: 3.0247, Test Loss: 3.0966\n",
      "Epoch 2324, Train Loss: 3.0388, Test Loss: 3.0973\n",
      "Epoch 2325, Train Loss: 3.0253, Test Loss: 3.0984\n",
      "Epoch 2326, Train Loss: 3.0276, Test Loss: 3.0991\n",
      "Epoch 2327, Train Loss: 3.0364, Test Loss: 3.0991\n",
      "Epoch 2328, Train Loss: 3.0391, Test Loss: 3.0985\n",
      "Epoch 2329, Train Loss: 3.0408, Test Loss: 3.0974\n",
      "Epoch 2330, Train Loss: 3.0154, Test Loss: 3.0984\n",
      "Epoch 2331, Train Loss: 3.0225, Test Loss: 3.0988\n",
      "Epoch 2332, Train Loss: 3.0250, Test Loss: 3.0981\n",
      "Epoch 2333, Train Loss: 3.0313, Test Loss: 3.0972\n",
      "Epoch 2334, Train Loss: 3.0210, Test Loss: 3.0965\n",
      "Epoch 2335, Train Loss: 3.0299, Test Loss: 3.0968\n",
      "Epoch 2336, Train Loss: 3.0280, Test Loss: 3.0974\n",
      "Epoch 2337, Train Loss: 3.0323, Test Loss: 3.0970\n",
      "Epoch 2338, Train Loss: 3.0277, Test Loss: 3.0973\n",
      "Epoch 2339, Train Loss: 3.0254, Test Loss: 3.0979\n",
      "Epoch 2340, Train Loss: 3.0325, Test Loss: 3.0976\n",
      "Epoch 2341, Train Loss: 3.0267, Test Loss: 3.0972\n",
      "Epoch 2342, Train Loss: 3.0333, Test Loss: 3.0974\n",
      "Epoch 2343, Train Loss: 3.0337, Test Loss: 3.0976\n",
      "Epoch 2344, Train Loss: 3.0343, Test Loss: 3.0977\n",
      "Epoch 2345, Train Loss: 3.0345, Test Loss: 3.0988\n",
      "Epoch 2346, Train Loss: 3.0267, Test Loss: 3.0989\n",
      "Epoch 2347, Train Loss: 3.0229, Test Loss: 3.0983\n",
      "Epoch 2348, Train Loss: 3.0249, Test Loss: 3.0974\n",
      "Epoch 2349, Train Loss: 3.0274, Test Loss: 3.0970\n",
      "Epoch 2350, Train Loss: 3.0259, Test Loss: 3.0980\n",
      "Epoch 2351, Train Loss: 3.0527, Test Loss: 3.0979\n",
      "Epoch 2352, Train Loss: 3.0371, Test Loss: 3.0982\n",
      "Epoch 2353, Train Loss: 3.0295, Test Loss: 3.0981\n",
      "Epoch 2354, Train Loss: 3.0193, Test Loss: 3.0977\n",
      "Epoch 2355, Train Loss: 3.0170, Test Loss: 3.0973\n",
      "Epoch 2356, Train Loss: 3.0312, Test Loss: 3.0967\n",
      "Epoch 2357, Train Loss: 3.0270, Test Loss: 3.0959\n",
      "Epoch 2358, Train Loss: 3.0352, Test Loss: 3.0967\n",
      "Epoch 2359, Train Loss: 3.0343, Test Loss: 3.0974\n",
      "Epoch 2360, Train Loss: 3.0318, Test Loss: 3.0982\n",
      "Epoch 2361, Train Loss: 3.0285, Test Loss: 3.0985\n",
      "Epoch 2362, Train Loss: 3.0208, Test Loss: 3.0967\n",
      "Epoch 2363, Train Loss: 3.0238, Test Loss: 3.0979\n",
      "Epoch 2364, Train Loss: 3.0294, Test Loss: 3.0991\n",
      "Epoch 2365, Train Loss: 3.0317, Test Loss: 3.1008\n",
      "Epoch 2366, Train Loss: 3.0354, Test Loss: 3.0995\n",
      "Epoch 2367, Train Loss: 3.0185, Test Loss: 3.0984\n",
      "Epoch 2368, Train Loss: 3.0398, Test Loss: 3.0972\n",
      "Epoch 2369, Train Loss: 3.0386, Test Loss: 3.0975\n",
      "Epoch 2370, Train Loss: 3.0273, Test Loss: 3.0989\n",
      "Epoch 2371, Train Loss: 3.0283, Test Loss: 3.1005\n",
      "Epoch 2372, Train Loss: 3.0176, Test Loss: 3.0982\n",
      "Epoch 2373, Train Loss: 3.0282, Test Loss: 3.0977\n",
      "Epoch 2374, Train Loss: 3.0233, Test Loss: 3.0980\n",
      "Epoch 2375, Train Loss: 3.0344, Test Loss: 3.0986\n",
      "Epoch 2376, Train Loss: 3.0317, Test Loss: 3.1006\n",
      "Epoch 2377, Train Loss: 3.0324, Test Loss: 3.1001\n",
      "Epoch 2378, Train Loss: 3.0259, Test Loss: 3.0992\n",
      "Epoch 2379, Train Loss: 3.0259, Test Loss: 3.0985\n",
      "Epoch 2380, Train Loss: 3.0448, Test Loss: 3.0980\n",
      "Epoch 2381, Train Loss: 3.0356, Test Loss: 3.0973\n",
      "Epoch 2382, Train Loss: 3.0327, Test Loss: 3.0981\n",
      "Epoch 2383, Train Loss: 3.0337, Test Loss: 3.0986\n",
      "Epoch 2384, Train Loss: 3.0268, Test Loss: 3.0990\n",
      "Epoch 2385, Train Loss: 3.0277, Test Loss: 3.0980\n",
      "Epoch 2386, Train Loss: 3.0430, Test Loss: 3.0976\n",
      "Epoch 2387, Train Loss: 3.0322, Test Loss: 3.0983\n",
      "Epoch 2388, Train Loss: 3.0216, Test Loss: 3.0994\n",
      "Epoch 2389, Train Loss: 3.0490, Test Loss: 3.1012\n",
      "Epoch 2390, Train Loss: 3.0259, Test Loss: 3.1016\n",
      "Epoch 2391, Train Loss: 3.0368, Test Loss: 3.1001\n",
      "Epoch 2392, Train Loss: 3.0244, Test Loss: 3.0987\n",
      "Epoch 2393, Train Loss: 3.0250, Test Loss: 3.0983\n",
      "Epoch 2394, Train Loss: 3.0242, Test Loss: 3.0981\n",
      "Epoch 2395, Train Loss: 3.0424, Test Loss: 3.0988\n",
      "Epoch 2396, Train Loss: 3.0180, Test Loss: 3.0986\n",
      "Epoch 2397, Train Loss: 3.0348, Test Loss: 3.0976\n",
      "Epoch 2398, Train Loss: 3.0407, Test Loss: 3.0978\n",
      "Epoch 2399, Train Loss: 3.0263, Test Loss: 3.0982\n",
      "Epoch 2400, Train Loss: 3.0277, Test Loss: 3.1000\n",
      "Epoch 2401, Train Loss: 3.0306, Test Loss: 3.1000\n",
      "Epoch 2402, Train Loss: 3.0401, Test Loss: 3.1000\n",
      "Epoch 2403, Train Loss: 3.0404, Test Loss: 3.0997\n",
      "Epoch 2404, Train Loss: 3.0390, Test Loss: 3.0988\n",
      "Epoch 2405, Train Loss: 3.0262, Test Loss: 3.0989\n",
      "Epoch 2406, Train Loss: 3.0261, Test Loss: 3.0988\n",
      "Epoch 2407, Train Loss: 3.0254, Test Loss: 3.0980\n",
      "Epoch 2408, Train Loss: 3.0319, Test Loss: 3.0972\n",
      "Epoch 2409, Train Loss: 3.0173, Test Loss: 3.0970\n",
      "Epoch 2410, Train Loss: 3.0301, Test Loss: 3.0974\n",
      "Epoch 2411, Train Loss: 3.0267, Test Loss: 3.0965\n",
      "Epoch 2412, Train Loss: 3.0241, Test Loss: 3.0974\n",
      "Epoch 2413, Train Loss: 3.0327, Test Loss: 3.0981\n",
      "Epoch 2414, Train Loss: 3.0295, Test Loss: 3.0990\n",
      "Epoch 2415, Train Loss: 3.0279, Test Loss: 3.0987\n",
      "Epoch 2416, Train Loss: 3.0298, Test Loss: 3.0973\n",
      "Epoch 2417, Train Loss: 3.0216, Test Loss: 3.0974\n",
      "Epoch 2418, Train Loss: 3.0300, Test Loss: 3.0983\n",
      "Epoch 2419, Train Loss: 3.0223, Test Loss: 3.0987\n",
      "Epoch 2420, Train Loss: 3.0340, Test Loss: 3.0978\n",
      "Epoch 2421, Train Loss: 3.0298, Test Loss: 3.0981\n",
      "Epoch 2422, Train Loss: 3.0458, Test Loss: 3.0988\n",
      "Epoch 2423, Train Loss: 3.0350, Test Loss: 3.0993\n",
      "Epoch 2424, Train Loss: 3.0392, Test Loss: 3.0998\n",
      "Epoch 2425, Train Loss: 3.0452, Test Loss: 3.1000\n",
      "Epoch 2426, Train Loss: 3.0251, Test Loss: 3.0981\n",
      "Epoch 2427, Train Loss: 3.0194, Test Loss: 3.0968\n",
      "Epoch 2428, Train Loss: 3.0292, Test Loss: 3.0961\n",
      "Epoch 2429, Train Loss: 3.0284, Test Loss: 3.0961\n",
      "Epoch 2430, Train Loss: 3.0317, Test Loss: 3.0964\n",
      "Epoch 2431, Train Loss: 3.0327, Test Loss: 3.0949\n",
      "Epoch 2432, Train Loss: 3.0246, Test Loss: 3.0944\n",
      "Epoch 2433, Train Loss: 3.0343, Test Loss: 3.0954\n",
      "Epoch 2434, Train Loss: 3.0252, Test Loss: 3.0966\n",
      "Epoch 2435, Train Loss: 3.0221, Test Loss: 3.0958\n",
      "Epoch 2436, Train Loss: 3.0322, Test Loss: 3.0949\n",
      "Epoch 2437, Train Loss: 3.0347, Test Loss: 3.0950\n",
      "Epoch 2438, Train Loss: 3.0339, Test Loss: 3.0955\n",
      "Epoch 2439, Train Loss: 3.0479, Test Loss: 3.0970\n",
      "Epoch 2440, Train Loss: 3.0230, Test Loss: 3.0970\n",
      "Epoch 2441, Train Loss: 3.0356, Test Loss: 3.0972\n",
      "Epoch 2442, Train Loss: 3.0473, Test Loss: 3.0968\n",
      "Epoch 2443, Train Loss: 3.0251, Test Loss: 3.0965\n",
      "Epoch 2444, Train Loss: 3.0374, Test Loss: 3.0971\n",
      "Epoch 2445, Train Loss: 3.0204, Test Loss: 3.0995\n",
      "Epoch 2446, Train Loss: 3.0272, Test Loss: 3.0999\n",
      "Epoch 2447, Train Loss: 3.0283, Test Loss: 3.0982\n",
      "Epoch 2448, Train Loss: 3.0366, Test Loss: 3.0976\n",
      "Epoch 2449, Train Loss: 3.0279, Test Loss: 3.0977\n",
      "Epoch 2450, Train Loss: 3.0292, Test Loss: 3.1008\n",
      "Epoch 2451, Train Loss: 3.0337, Test Loss: 3.1005\n",
      "Epoch 2452, Train Loss: 3.0260, Test Loss: 3.0980\n",
      "Epoch 2453, Train Loss: 3.0288, Test Loss: 3.0965\n",
      "Epoch 2454, Train Loss: 3.0316, Test Loss: 3.0958\n",
      "Epoch 2455, Train Loss: 3.0193, Test Loss: 3.0970\n",
      "Epoch 2456, Train Loss: 3.0268, Test Loss: 3.0986\n",
      "Epoch 2457, Train Loss: 3.0267, Test Loss: 3.0976\n",
      "Epoch 2458, Train Loss: 3.0315, Test Loss: 3.0966\n",
      "Epoch 2459, Train Loss: 3.0325, Test Loss: 3.0963\n",
      "Epoch 2460, Train Loss: 3.0297, Test Loss: 3.0965\n",
      "Epoch 2461, Train Loss: 3.0289, Test Loss: 3.0972\n",
      "Epoch 2462, Train Loss: 3.0261, Test Loss: 3.0988\n",
      "Epoch 2463, Train Loss: 3.0246, Test Loss: 3.0973\n",
      "Epoch 2464, Train Loss: 3.0111, Test Loss: 3.0955\n",
      "Epoch 2465, Train Loss: 3.0323, Test Loss: 3.0954\n",
      "Epoch 2466, Train Loss: 3.0276, Test Loss: 3.0957\n",
      "Epoch 2467, Train Loss: 3.0407, Test Loss: 3.0973\n",
      "Epoch 2468, Train Loss: 3.0262, Test Loss: 3.0979\n",
      "Epoch 2469, Train Loss: 3.0234, Test Loss: 3.0965\n",
      "Epoch 2470, Train Loss: 3.0208, Test Loss: 3.0959\n",
      "Epoch 2471, Train Loss: 3.0290, Test Loss: 3.0958\n",
      "Epoch 2472, Train Loss: 3.0343, Test Loss: 3.0961\n",
      "Epoch 2473, Train Loss: 3.0249, Test Loss: 3.0969\n",
      "Epoch 2474, Train Loss: 3.0338, Test Loss: 3.0981\n",
      "Epoch 2475, Train Loss: 3.0369, Test Loss: 3.0960\n",
      "Epoch 2476, Train Loss: 3.0320, Test Loss: 3.0952\n",
      "Epoch 2477, Train Loss: 3.0255, Test Loss: 3.0957\n",
      "Epoch 2478, Train Loss: 3.0245, Test Loss: 3.0966\n",
      "Epoch 2479, Train Loss: 3.0317, Test Loss: 3.0982\n",
      "Epoch 2480, Train Loss: 3.0300, Test Loss: 3.0976\n",
      "Epoch 2481, Train Loss: 3.0383, Test Loss: 3.0969\n",
      "Epoch 2482, Train Loss: 3.0272, Test Loss: 3.0980\n",
      "Epoch 2483, Train Loss: 3.0261, Test Loss: 3.1001\n",
      "Epoch 2484, Train Loss: 3.0301, Test Loss: 3.0982\n",
      "Epoch 2485, Train Loss: 3.0184, Test Loss: 3.0957\n",
      "Epoch 2486, Train Loss: 3.0222, Test Loss: 3.0949\n",
      "Epoch 2487, Train Loss: 3.0332, Test Loss: 3.0950\n",
      "Epoch 2488, Train Loss: 3.0213, Test Loss: 3.0961\n",
      "Epoch 2489, Train Loss: 3.0314, Test Loss: 3.0974\n",
      "Epoch 2490, Train Loss: 3.0319, Test Loss: 3.0962\n",
      "Epoch 2491, Train Loss: 3.0293, Test Loss: 3.0960\n",
      "Epoch 2492, Train Loss: 3.0266, Test Loss: 3.0959\n",
      "Epoch 2493, Train Loss: 3.0282, Test Loss: 3.0967\n",
      "Epoch 2494, Train Loss: 3.0314, Test Loss: 3.0981\n",
      "Epoch 2495, Train Loss: 3.0363, Test Loss: 3.0987\n",
      "Epoch 2496, Train Loss: 3.0286, Test Loss: 3.0980\n",
      "Epoch 2497, Train Loss: 3.0298, Test Loss: 3.0980\n",
      "Epoch 2498, Train Loss: 3.0174, Test Loss: 3.0986\n",
      "Epoch 2499, Train Loss: 3.0302, Test Loss: 3.0978\n",
      "Epoch 2500, Train Loss: 3.0232, Test Loss: 3.0979\n",
      "Epoch 2501, Train Loss: 3.0303, Test Loss: 3.0989\n",
      "Epoch 2502, Train Loss: 3.0288, Test Loss: 3.0981\n",
      "Epoch 2503, Train Loss: 3.0261, Test Loss: 3.0978\n",
      "Epoch 2504, Train Loss: 3.0311, Test Loss: 3.0978\n",
      "Epoch 2505, Train Loss: 3.0384, Test Loss: 3.0960\n",
      "Epoch 2506, Train Loss: 3.0250, Test Loss: 3.0971\n",
      "Epoch 2507, Train Loss: 3.0196, Test Loss: 3.0989\n",
      "Epoch 2508, Train Loss: 3.0412, Test Loss: 3.0993\n",
      "Epoch 2509, Train Loss: 3.0266, Test Loss: 3.0963\n",
      "Epoch 2510, Train Loss: 3.0287, Test Loss: 3.0949\n",
      "Epoch 2511, Train Loss: 3.0253, Test Loss: 3.0952\n",
      "Epoch 2512, Train Loss: 3.0181, Test Loss: 3.0978\n",
      "Epoch 2513, Train Loss: 3.0225, Test Loss: 3.0994\n",
      "Epoch 2514, Train Loss: 3.0273, Test Loss: 3.0964\n",
      "Epoch 2515, Train Loss: 3.0401, Test Loss: 3.0960\n",
      "Epoch 2516, Train Loss: 3.0303, Test Loss: 3.0958\n",
      "Epoch 2517, Train Loss: 3.0324, Test Loss: 3.0973\n",
      "Epoch 2518, Train Loss: 3.0243, Test Loss: 3.0981\n",
      "Epoch 2519, Train Loss: 3.0331, Test Loss: 3.0952\n",
      "Epoch 2520, Train Loss: 3.0291, Test Loss: 3.0946\n",
      "Epoch 2521, Train Loss: 3.0325, Test Loss: 3.0945\n",
      "Epoch 2522, Train Loss: 3.0386, Test Loss: 3.0949\n",
      "Epoch 2523, Train Loss: 3.0184, Test Loss: 3.0966\n",
      "Epoch 2524, Train Loss: 3.0250, Test Loss: 3.0973\n",
      "Epoch 2525, Train Loss: 3.0243, Test Loss: 3.0950\n",
      "Epoch 2526, Train Loss: 3.0147, Test Loss: 3.0949\n",
      "Epoch 2527, Train Loss: 3.0208, Test Loss: 3.0945\n",
      "Epoch 2528, Train Loss: 3.0287, Test Loss: 3.0947\n",
      "Epoch 2529, Train Loss: 3.0269, Test Loss: 3.0963\n",
      "Epoch 2530, Train Loss: 3.0303, Test Loss: 3.0942\n",
      "Epoch 2531, Train Loss: 3.0393, Test Loss: 3.0939\n",
      "Epoch 2532, Train Loss: 3.0260, Test Loss: 3.0941\n",
      "Epoch 2533, Train Loss: 3.0200, Test Loss: 3.0951\n",
      "Epoch 2534, Train Loss: 3.0267, Test Loss: 3.0970\n",
      "Epoch 2535, Train Loss: 3.0246, Test Loss: 3.0974\n",
      "Epoch 2536, Train Loss: 3.0224, Test Loss: 3.0953\n",
      "Epoch 2537, Train Loss: 3.0290, Test Loss: 3.0951\n",
      "Epoch 2538, Train Loss: 3.0191, Test Loss: 3.0955\n",
      "Epoch 2539, Train Loss: 3.0339, Test Loss: 3.0967\n",
      "Epoch 2540, Train Loss: 3.0292, Test Loss: 3.0992\n",
      "Epoch 2541, Train Loss: 3.0214, Test Loss: 3.0956\n",
      "Epoch 2542, Train Loss: 3.0389, Test Loss: 3.0947\n",
      "Epoch 2543, Train Loss: 3.0353, Test Loss: 3.0938\n",
      "Epoch 2544, Train Loss: 3.0211, Test Loss: 3.0963\n",
      "Epoch 2545, Train Loss: 3.0311, Test Loss: 3.0985\n",
      "Epoch 2546, Train Loss: 3.0235, Test Loss: 3.0945\n",
      "Epoch 2547, Train Loss: 3.0465, Test Loss: 3.0958\n",
      "Epoch 2548, Train Loss: 3.0244, Test Loss: 3.0952\n",
      "Epoch 2549, Train Loss: 3.0207, Test Loss: 3.0962\n",
      "Epoch 2550, Train Loss: 3.0205, Test Loss: 3.1000\n",
      "Epoch 2551, Train Loss: 3.0289, Test Loss: 3.1003\n",
      "Epoch 2552, Train Loss: 3.0332, Test Loss: 3.0961\n",
      "Epoch 2553, Train Loss: 3.0192, Test Loss: 3.0979\n",
      "Epoch 2554, Train Loss: 3.0232, Test Loss: 3.0973\n",
      "Epoch 2555, Train Loss: 3.0251, Test Loss: 3.1013\n",
      "Epoch 2556, Train Loss: 3.0386, Test Loss: 3.0968\n",
      "Epoch 2557, Train Loss: 3.0302, Test Loss: 3.0953\n",
      "Epoch 2558, Train Loss: 3.0275, Test Loss: 3.0961\n",
      "Epoch 2559, Train Loss: 3.0403, Test Loss: 3.0958\n",
      "Epoch 2560, Train Loss: 3.0297, Test Loss: 3.0974\n",
      "Epoch 2561, Train Loss: 3.0195, Test Loss: 3.0964\n",
      "Epoch 2562, Train Loss: 3.0256, Test Loss: 3.0950\n",
      "Epoch 2563, Train Loss: 3.0194, Test Loss: 3.0956\n",
      "Epoch 2564, Train Loss: 3.0257, Test Loss: 3.0972\n",
      "Epoch 2565, Train Loss: 3.0216, Test Loss: 3.0990\n",
      "Epoch 2566, Train Loss: 3.0345, Test Loss: 3.0965\n",
      "Epoch 2567, Train Loss: 3.0292, Test Loss: 3.0964\n",
      "Epoch 2568, Train Loss: 3.0309, Test Loss: 3.0962\n",
      "Epoch 2569, Train Loss: 3.0164, Test Loss: 3.0959\n",
      "Epoch 2570, Train Loss: 3.0282, Test Loss: 3.0980\n",
      "Epoch 2571, Train Loss: 3.0304, Test Loss: 3.0961\n",
      "Epoch 2572, Train Loss: 3.0311, Test Loss: 3.0959\n",
      "Epoch 2573, Train Loss: 3.0268, Test Loss: 3.0965\n",
      "Epoch 2574, Train Loss: 3.0395, Test Loss: 3.0949\n",
      "Epoch 2575, Train Loss: 3.0284, Test Loss: 3.0967\n",
      "Epoch 2576, Train Loss: 3.0317, Test Loss: 3.0974\n",
      "Epoch 2577, Train Loss: 3.0482, Test Loss: 3.0959\n",
      "Epoch 2578, Train Loss: 3.0349, Test Loss: 3.0965\n",
      "Epoch 2579, Train Loss: 3.0362, Test Loss: 3.0964\n",
      "Epoch 2580, Train Loss: 3.0294, Test Loss: 3.0991\n",
      "Epoch 2581, Train Loss: 3.0367, Test Loss: 3.0987\n",
      "Epoch 2582, Train Loss: 3.0222, Test Loss: 3.0966\n",
      "Epoch 2583, Train Loss: 3.0171, Test Loss: 3.0970\n",
      "Epoch 2584, Train Loss: 3.0243, Test Loss: 3.0969\n",
      "Epoch 2585, Train Loss: 3.0361, Test Loss: 3.0983\n",
      "Epoch 2586, Train Loss: 3.0321, Test Loss: 3.0949\n",
      "Epoch 2587, Train Loss: 3.0228, Test Loss: 3.0966\n",
      "Epoch 2588, Train Loss: 3.0296, Test Loss: 3.0956\n",
      "Epoch 2589, Train Loss: 3.0202, Test Loss: 3.0988\n",
      "Epoch 2590, Train Loss: 3.0295, Test Loss: 3.0977\n",
      "Epoch 2591, Train Loss: 3.0194, Test Loss: 3.0940\n",
      "Epoch 2592, Train Loss: 3.0259, Test Loss: 3.0954\n",
      "Epoch 2593, Train Loss: 3.0166, Test Loss: 3.0950\n",
      "Epoch 2594, Train Loss: 3.0309, Test Loss: 3.0964\n",
      "Epoch 2595, Train Loss: 3.0311, Test Loss: 3.0985\n",
      "Epoch 2596, Train Loss: 3.0311, Test Loss: 3.0963\n",
      "Epoch 2597, Train Loss: 3.0291, Test Loss: 3.0946\n",
      "Epoch 2598, Train Loss: 3.0294, Test Loss: 3.0947\n",
      "Epoch 2599, Train Loss: 3.0300, Test Loss: 3.0955\n",
      "Epoch 2600, Train Loss: 3.0269, Test Loss: 3.0960\n",
      "Epoch 2601, Train Loss: 3.0243, Test Loss: 3.0939\n",
      "Epoch 2602, Train Loss: 3.0182, Test Loss: 3.0938\n",
      "Epoch 2603, Train Loss: 3.0267, Test Loss: 3.0937\n",
      "Epoch 2604, Train Loss: 3.0237, Test Loss: 3.0960\n",
      "Epoch 2605, Train Loss: 3.0290, Test Loss: 3.0957\n",
      "Epoch 2606, Train Loss: 3.0344, Test Loss: 3.0949\n",
      "Epoch 2607, Train Loss: 3.0276, Test Loss: 3.0961\n",
      "Epoch 2608, Train Loss: 3.0359, Test Loss: 3.0970\n",
      "Epoch 2609, Train Loss: 3.0260, Test Loss: 3.0993\n",
      "Epoch 2610, Train Loss: 3.0271, Test Loss: 3.1010\n",
      "Epoch 2611, Train Loss: 3.0291, Test Loss: 3.0989\n",
      "Epoch 2612, Train Loss: 3.0290, Test Loss: 3.0978\n",
      "Epoch 2613, Train Loss: 3.0212, Test Loss: 3.0969\n",
      "Epoch 2614, Train Loss: 3.0229, Test Loss: 3.0972\n",
      "Epoch 2615, Train Loss: 3.0291, Test Loss: 3.0965\n",
      "Epoch 2616, Train Loss: 3.0255, Test Loss: 3.0958\n",
      "Epoch 2617, Train Loss: 3.0268, Test Loss: 3.0948\n",
      "Epoch 2618, Train Loss: 3.0264, Test Loss: 3.0952\n",
      "Epoch 2619, Train Loss: 3.0214, Test Loss: 3.0951\n",
      "Epoch 2620, Train Loss: 3.0342, Test Loss: 3.0961\n",
      "Epoch 2621, Train Loss: 3.0382, Test Loss: 3.0958\n",
      "Epoch 2622, Train Loss: 3.0336, Test Loss: 3.0964\n",
      "Epoch 2623, Train Loss: 3.0338, Test Loss: 3.0973\n",
      "Epoch 2624, Train Loss: 3.0320, Test Loss: 3.1000\n",
      "Epoch 2625, Train Loss: 3.0288, Test Loss: 3.0974\n",
      "Epoch 2626, Train Loss: 3.0190, Test Loss: 3.0958\n",
      "Epoch 2627, Train Loss: 3.0319, Test Loss: 3.0947\n",
      "Epoch 2628, Train Loss: 3.0213, Test Loss: 3.0946\n",
      "Epoch 2629, Train Loss: 3.0255, Test Loss: 3.0963\n",
      "Epoch 2630, Train Loss: 3.0382, Test Loss: 3.0944\n",
      "Epoch 2631, Train Loss: 3.0328, Test Loss: 3.0943\n",
      "Epoch 2632, Train Loss: 3.0452, Test Loss: 3.0948\n",
      "Epoch 2633, Train Loss: 3.0249, Test Loss: 3.0975\n",
      "Epoch 2634, Train Loss: 3.0182, Test Loss: 3.0986\n",
      "Epoch 2635, Train Loss: 3.0332, Test Loss: 3.0962\n",
      "Epoch 2636, Train Loss: 3.0300, Test Loss: 3.0959\n",
      "Epoch 2637, Train Loss: 3.0264, Test Loss: 3.0964\n",
      "Epoch 2638, Train Loss: 3.0222, Test Loss: 3.0970\n",
      "Epoch 2639, Train Loss: 3.0225, Test Loss: 3.0976\n",
      "Epoch 2640, Train Loss: 3.0246, Test Loss: 3.0960\n",
      "Epoch 2641, Train Loss: 3.0245, Test Loss: 3.0948\n",
      "Epoch 2642, Train Loss: 3.0411, Test Loss: 3.0952\n",
      "Epoch 2643, Train Loss: 3.0399, Test Loss: 3.0972\n",
      "Epoch 2644, Train Loss: 3.0115, Test Loss: 3.0965\n",
      "Epoch 2645, Train Loss: 3.0335, Test Loss: 3.0960\n",
      "Epoch 2646, Train Loss: 3.0233, Test Loss: 3.0963\n",
      "Epoch 2647, Train Loss: 3.0229, Test Loss: 3.0967\n",
      "Epoch 2648, Train Loss: 3.0231, Test Loss: 3.0974\n",
      "Epoch 2649, Train Loss: 3.0179, Test Loss: 3.0985\n",
      "Epoch 2650, Train Loss: 3.0294, Test Loss: 3.0965\n",
      "Epoch 2651, Train Loss: 3.0317, Test Loss: 3.0949\n",
      "Epoch 2652, Train Loss: 3.0252, Test Loss: 3.0949\n",
      "Epoch 2653, Train Loss: 3.0227, Test Loss: 3.0953\n",
      "Epoch 2654, Train Loss: 3.0260, Test Loss: 3.0951\n",
      "Epoch 2655, Train Loss: 3.0396, Test Loss: 3.0953\n",
      "Epoch 2656, Train Loss: 3.0369, Test Loss: 3.0959\n",
      "Epoch 2657, Train Loss: 3.0161, Test Loss: 3.0965\n",
      "Epoch 2658, Train Loss: 3.0283, Test Loss: 3.0961\n",
      "Epoch 2659, Train Loss: 3.0314, Test Loss: 3.0960\n",
      "Epoch 2660, Train Loss: 3.0276, Test Loss: 3.0965\n",
      "Epoch 2661, Train Loss: 3.0199, Test Loss: 3.0962\n",
      "Epoch 2662, Train Loss: 3.0249, Test Loss: 3.0944\n",
      "Epoch 2663, Train Loss: 3.0309, Test Loss: 3.0942\n",
      "Epoch 2664, Train Loss: 3.0335, Test Loss: 3.0943\n",
      "Epoch 2665, Train Loss: 3.0218, Test Loss: 3.0945\n",
      "Epoch 2666, Train Loss: 3.0228, Test Loss: 3.0953\n",
      "Epoch 2667, Train Loss: 3.0227, Test Loss: 3.0955\n",
      "Epoch 2668, Train Loss: 3.0233, Test Loss: 3.0959\n",
      "Epoch 2669, Train Loss: 3.0155, Test Loss: 3.0956\n",
      "Epoch 2670, Train Loss: 3.0351, Test Loss: 3.0951\n",
      "Epoch 2671, Train Loss: 3.0313, Test Loss: 3.0950\n",
      "Epoch 2672, Train Loss: 3.0319, Test Loss: 3.0942\n",
      "Epoch 2673, Train Loss: 3.0324, Test Loss: 3.0945\n",
      "Epoch 2674, Train Loss: 3.0354, Test Loss: 3.0958\n",
      "Epoch 2675, Train Loss: 3.0251, Test Loss: 3.0976\n",
      "Epoch 2676, Train Loss: 3.0281, Test Loss: 3.0970\n",
      "Epoch 2677, Train Loss: 3.0174, Test Loss: 3.0973\n",
      "Epoch 2678, Train Loss: 3.0262, Test Loss: 3.0973\n",
      "Epoch 2679, Train Loss: 3.0376, Test Loss: 3.0984\n",
      "Epoch 2680, Train Loss: 3.0126, Test Loss: 3.0972\n",
      "Epoch 2681, Train Loss: 3.0334, Test Loss: 3.0948\n",
      "Epoch 2682, Train Loss: 3.0274, Test Loss: 3.0929\n",
      "Epoch 2683, Train Loss: 3.0246, Test Loss: 3.0927\n",
      "Epoch 2684, Train Loss: 3.0425, Test Loss: 3.0926\n",
      "Epoch 2685, Train Loss: 3.0220, Test Loss: 3.0930\n",
      "Epoch 2686, Train Loss: 3.0274, Test Loss: 3.0947\n",
      "Epoch 2687, Train Loss: 3.0425, Test Loss: 3.0945\n",
      "Epoch 2688, Train Loss: 3.0243, Test Loss: 3.0956\n",
      "Epoch 2689, Train Loss: 3.0324, Test Loss: 3.0969\n",
      "Epoch 2690, Train Loss: 3.0174, Test Loss: 3.0971\n",
      "Epoch 2691, Train Loss: 3.0238, Test Loss: 3.0973\n",
      "Epoch 2692, Train Loss: 3.0276, Test Loss: 3.0982\n",
      "Epoch 2693, Train Loss: 3.0323, Test Loss: 3.0965\n",
      "Epoch 2694, Train Loss: 3.0260, Test Loss: 3.0942\n",
      "Epoch 2695, Train Loss: 3.0265, Test Loss: 3.0935\n",
      "Epoch 2696, Train Loss: 3.0317, Test Loss: 3.0929\n",
      "Epoch 2697, Train Loss: 3.0288, Test Loss: 3.0942\n",
      "Epoch 2698, Train Loss: 3.0334, Test Loss: 3.0931\n",
      "Epoch 2699, Train Loss: 3.0310, Test Loss: 3.0925\n",
      "Epoch 2700, Train Loss: 3.0319, Test Loss: 3.0934\n",
      "Epoch 2701, Train Loss: 3.0357, Test Loss: 3.0950\n",
      "Epoch 2702, Train Loss: 3.0227, Test Loss: 3.0966\n",
      "Epoch 2703, Train Loss: 3.0326, Test Loss: 3.0959\n",
      "Epoch 2704, Train Loss: 3.0251, Test Loss: 3.0950\n",
      "Epoch 2705, Train Loss: 3.0222, Test Loss: 3.0942\n",
      "Epoch 2706, Train Loss: 3.0186, Test Loss: 3.0938\n",
      "Epoch 2707, Train Loss: 3.0221, Test Loss: 3.0929\n",
      "Epoch 2708, Train Loss: 3.0202, Test Loss: 3.0924\n",
      "Epoch 2709, Train Loss: 3.0487, Test Loss: 3.0930\n",
      "Epoch 2710, Train Loss: 3.0250, Test Loss: 3.0943\n",
      "Epoch 2711, Train Loss: 3.0194, Test Loss: 3.0969\n",
      "Epoch 2712, Train Loss: 3.0336, Test Loss: 3.0987\n",
      "Epoch 2713, Train Loss: 3.0273, Test Loss: 3.0968\n",
      "Epoch 2714, Train Loss: 3.0241, Test Loss: 3.0977\n",
      "Epoch 2715, Train Loss: 3.0293, Test Loss: 3.0966\n",
      "Epoch 2716, Train Loss: 3.0297, Test Loss: 3.0969\n",
      "Epoch 2717, Train Loss: 3.0151, Test Loss: 3.0981\n",
      "Epoch 2718, Train Loss: 3.0297, Test Loss: 3.0959\n",
      "Epoch 2719, Train Loss: 3.0253, Test Loss: 3.0938\n",
      "Epoch 2720, Train Loss: 3.0244, Test Loss: 3.0932\n",
      "Epoch 2721, Train Loss: 3.0292, Test Loss: 3.0949\n",
      "Epoch 2722, Train Loss: 3.0306, Test Loss: 3.0930\n",
      "Epoch 2723, Train Loss: 3.0271, Test Loss: 3.0924\n",
      "Epoch 2724, Train Loss: 3.0187, Test Loss: 3.0926\n",
      "Epoch 2725, Train Loss: 3.0211, Test Loss: 3.0947\n",
      "Epoch 2726, Train Loss: 3.0279, Test Loss: 3.0940\n",
      "Epoch 2727, Train Loss: 3.0226, Test Loss: 3.0939\n",
      "Epoch 2728, Train Loss: 3.0284, Test Loss: 3.0960\n",
      "Epoch 2729, Train Loss: 3.0277, Test Loss: 3.0956\n",
      "Epoch 2730, Train Loss: 3.0224, Test Loss: 3.0974\n",
      "Epoch 2731, Train Loss: 3.0248, Test Loss: 3.0961\n",
      "Epoch 2732, Train Loss: 3.0223, Test Loss: 3.0943\n",
      "Epoch 2733, Train Loss: 3.0282, Test Loss: 3.0941\n",
      "Epoch 2734, Train Loss: 3.0284, Test Loss: 3.0942\n",
      "Epoch 2735, Train Loss: 3.0250, Test Loss: 3.0950\n",
      "Epoch 2736, Train Loss: 3.0260, Test Loss: 3.0956\n",
      "Epoch 2737, Train Loss: 3.0256, Test Loss: 3.0926\n",
      "Epoch 2738, Train Loss: 3.0274, Test Loss: 3.0922\n",
      "Epoch 2739, Train Loss: 3.0265, Test Loss: 3.0933\n",
      "Epoch 2740, Train Loss: 3.0267, Test Loss: 3.0958\n",
      "Epoch 2741, Train Loss: 3.0392, Test Loss: 3.0956\n",
      "Epoch 2742, Train Loss: 3.0212, Test Loss: 3.0934\n",
      "Epoch 2743, Train Loss: 3.0272, Test Loss: 3.0946\n",
      "Epoch 2744, Train Loss: 3.0187, Test Loss: 3.0948\n",
      "Epoch 2745, Train Loss: 3.0212, Test Loss: 3.0962\n",
      "Epoch 2746, Train Loss: 3.0239, Test Loss: 3.0966\n",
      "Epoch 2747, Train Loss: 3.0200, Test Loss: 3.0934\n",
      "Epoch 2748, Train Loss: 3.0292, Test Loss: 3.0922\n",
      "Epoch 2749, Train Loss: 3.0188, Test Loss: 3.0925\n",
      "Epoch 2750, Train Loss: 3.0255, Test Loss: 3.0919\n",
      "Epoch 2751, Train Loss: 3.0206, Test Loss: 3.0936\n",
      "Epoch 2752, Train Loss: 3.0206, Test Loss: 3.0934\n",
      "Epoch 2753, Train Loss: 3.0209, Test Loss: 3.0936\n",
      "Epoch 2754, Train Loss: 3.0268, Test Loss: 3.0942\n",
      "Epoch 2755, Train Loss: 3.0329, Test Loss: 3.0926\n",
      "Epoch 2756, Train Loss: 3.0368, Test Loss: 3.0925\n",
      "Epoch 2757, Train Loss: 3.0224, Test Loss: 3.0930\n",
      "Epoch 2758, Train Loss: 3.0245, Test Loss: 3.0945\n",
      "Epoch 2759, Train Loss: 3.0254, Test Loss: 3.0946\n",
      "Epoch 2760, Train Loss: 3.0241, Test Loss: 3.0926\n",
      "Epoch 2761, Train Loss: 3.0221, Test Loss: 3.0937\n",
      "Epoch 2762, Train Loss: 3.0309, Test Loss: 3.0947\n",
      "Epoch 2763, Train Loss: 3.0240, Test Loss: 3.0985\n",
      "Epoch 2764, Train Loss: 3.0227, Test Loss: 3.1000\n",
      "Epoch 2765, Train Loss: 3.0300, Test Loss: 3.0985\n",
      "Epoch 2766, Train Loss: 3.0191, Test Loss: 3.0963\n",
      "Epoch 2767, Train Loss: 3.0239, Test Loss: 3.0945\n",
      "Epoch 2768, Train Loss: 3.0218, Test Loss: 3.0953\n",
      "Epoch 2769, Train Loss: 3.0233, Test Loss: 3.0958\n",
      "Epoch 2770, Train Loss: 3.0369, Test Loss: 3.0935\n",
      "Epoch 2771, Train Loss: 3.0209, Test Loss: 3.0931\n",
      "Epoch 2772, Train Loss: 3.0334, Test Loss: 3.0923\n",
      "Epoch 2773, Train Loss: 3.0349, Test Loss: 3.0936\n",
      "Epoch 2774, Train Loss: 3.0321, Test Loss: 3.0941\n",
      "Epoch 2775, Train Loss: 3.0272, Test Loss: 3.0922\n",
      "Epoch 2776, Train Loss: 3.0248, Test Loss: 3.0945\n",
      "Epoch 2777, Train Loss: 3.0366, Test Loss: 3.0941\n",
      "Epoch 2778, Train Loss: 3.0227, Test Loss: 3.0973\n",
      "Epoch 2779, Train Loss: 3.0199, Test Loss: 3.0981\n",
      "Epoch 2780, Train Loss: 3.0252, Test Loss: 3.0961\n",
      "Epoch 2781, Train Loss: 3.0168, Test Loss: 3.0971\n",
      "Epoch 2782, Train Loss: 3.0293, Test Loss: 3.0956\n",
      "Epoch 2783, Train Loss: 3.0329, Test Loss: 3.0968\n",
      "Epoch 2784, Train Loss: 3.0157, Test Loss: 3.0984\n",
      "Epoch 2785, Train Loss: 3.0369, Test Loss: 3.0948\n",
      "Epoch 2786, Train Loss: 3.0241, Test Loss: 3.0941\n",
      "Epoch 2787, Train Loss: 3.0260, Test Loss: 3.0942\n",
      "Epoch 2788, Train Loss: 3.0266, Test Loss: 3.0969\n",
      "Epoch 2789, Train Loss: 3.0198, Test Loss: 3.0986\n",
      "Epoch 2790, Train Loss: 3.0344, Test Loss: 3.0967\n",
      "Epoch 2791, Train Loss: 3.0249, Test Loss: 3.0954\n",
      "Epoch 2792, Train Loss: 3.0341, Test Loss: 3.0951\n",
      "Epoch 2793, Train Loss: 3.0280, Test Loss: 3.0978\n",
      "Epoch 2794, Train Loss: 3.0318, Test Loss: 3.0981\n",
      "Epoch 2795, Train Loss: 3.0243, Test Loss: 3.0932\n",
      "Epoch 2796, Train Loss: 3.0208, Test Loss: 3.0922\n",
      "Epoch 2797, Train Loss: 3.0331, Test Loss: 3.0932\n",
      "Epoch 2798, Train Loss: 3.0316, Test Loss: 3.0942\n",
      "Epoch 2799, Train Loss: 3.0203, Test Loss: 3.0947\n",
      "Epoch 2800, Train Loss: 3.0280, Test Loss: 3.0938\n",
      "Epoch 2801, Train Loss: 3.0208, Test Loss: 3.0926\n",
      "Epoch 2802, Train Loss: 3.0252, Test Loss: 3.0930\n",
      "Epoch 2803, Train Loss: 3.0231, Test Loss: 3.0939\n",
      "Epoch 2804, Train Loss: 3.0232, Test Loss: 3.0945\n",
      "Epoch 2805, Train Loss: 3.0199, Test Loss: 3.0959\n",
      "Epoch 2806, Train Loss: 3.0237, Test Loss: 3.0941\n",
      "Epoch 2807, Train Loss: 3.0351, Test Loss: 3.0927\n",
      "Epoch 2808, Train Loss: 3.0183, Test Loss: 3.0932\n",
      "Epoch 2809, Train Loss: 3.0294, Test Loss: 3.0933\n",
      "Epoch 2810, Train Loss: 3.0290, Test Loss: 3.0945\n",
      "Epoch 2811, Train Loss: 3.0290, Test Loss: 3.0958\n",
      "Epoch 2812, Train Loss: 3.0187, Test Loss: 3.0946\n",
      "Epoch 2813, Train Loss: 3.0179, Test Loss: 3.0958\n",
      "Epoch 2814, Train Loss: 3.0370, Test Loss: 3.0963\n",
      "Epoch 2815, Train Loss: 3.0347, Test Loss: 3.0952\n",
      "Epoch 2816, Train Loss: 3.0309, Test Loss: 3.0946\n",
      "Epoch 2817, Train Loss: 3.0238, Test Loss: 3.0932\n",
      "Epoch 2818, Train Loss: 3.0236, Test Loss: 3.0935\n",
      "Epoch 2819, Train Loss: 3.0240, Test Loss: 3.0935\n",
      "Epoch 2820, Train Loss: 3.0195, Test Loss: 3.0920\n",
      "Epoch 2821, Train Loss: 3.0118, Test Loss: 3.0928\n",
      "Epoch 2822, Train Loss: 3.0297, Test Loss: 3.0924\n",
      "Epoch 2823, Train Loss: 3.0204, Test Loss: 3.0930\n",
      "Epoch 2824, Train Loss: 3.0291, Test Loss: 3.0929\n",
      "Epoch 2825, Train Loss: 3.0276, Test Loss: 3.0929\n",
      "Epoch 2826, Train Loss: 3.0328, Test Loss: 3.0941\n",
      "Epoch 2827, Train Loss: 3.0216, Test Loss: 3.0950\n",
      "Epoch 2828, Train Loss: 3.0227, Test Loss: 3.0951\n",
      "Epoch 2829, Train Loss: 3.0269, Test Loss: 3.0934\n",
      "Epoch 2830, Train Loss: 3.0187, Test Loss: 3.0923\n",
      "Epoch 2831, Train Loss: 3.0205, Test Loss: 3.0926\n",
      "Epoch 2832, Train Loss: 3.0226, Test Loss: 3.0942\n",
      "Epoch 2833, Train Loss: 3.0238, Test Loss: 3.0944\n",
      "Epoch 2834, Train Loss: 3.0355, Test Loss: 3.0942\n",
      "Epoch 2835, Train Loss: 3.0214, Test Loss: 3.0928\n",
      "Epoch 2836, Train Loss: 3.0253, Test Loss: 3.0930\n",
      "Epoch 2837, Train Loss: 3.0215, Test Loss: 3.0939\n",
      "Epoch 2838, Train Loss: 3.0283, Test Loss: 3.0962\n",
      "Epoch 2839, Train Loss: 3.0186, Test Loss: 3.0941\n",
      "Epoch 2840, Train Loss: 3.0248, Test Loss: 3.0935\n",
      "Epoch 2841, Train Loss: 3.0167, Test Loss: 3.0940\n",
      "Epoch 2842, Train Loss: 3.0178, Test Loss: 3.0952\n",
      "Epoch 2843, Train Loss: 3.0175, Test Loss: 3.0935\n",
      "Epoch 2844, Train Loss: 3.0253, Test Loss: 3.0922\n",
      "Epoch 2845, Train Loss: 3.0218, Test Loss: 3.0920\n",
      "Epoch 2846, Train Loss: 3.0219, Test Loss: 3.0934\n",
      "Epoch 2847, Train Loss: 3.0217, Test Loss: 3.0958\n",
      "Epoch 2848, Train Loss: 3.0129, Test Loss: 3.0922\n",
      "Epoch 2849, Train Loss: 3.0269, Test Loss: 3.0919\n",
      "Epoch 2850, Train Loss: 3.0306, Test Loss: 3.0917\n",
      "Epoch 2851, Train Loss: 3.0314, Test Loss: 3.0934\n",
      "Epoch 2852, Train Loss: 3.0211, Test Loss: 3.0966\n",
      "Epoch 2853, Train Loss: 3.0199, Test Loss: 3.0952\n",
      "Epoch 2854, Train Loss: 3.0201, Test Loss: 3.0937\n",
      "Epoch 2855, Train Loss: 3.0317, Test Loss: 3.0925\n",
      "Epoch 2856, Train Loss: 3.0092, Test Loss: 3.0937\n",
      "Epoch 2857, Train Loss: 3.0264, Test Loss: 3.0938\n",
      "Epoch 2858, Train Loss: 3.0280, Test Loss: 3.0935\n",
      "Epoch 2859, Train Loss: 3.0248, Test Loss: 3.0933\n",
      "Epoch 2860, Train Loss: 3.0303, Test Loss: 3.0926\n",
      "Epoch 2861, Train Loss: 3.0269, Test Loss: 3.0922\n",
      "Epoch 2862, Train Loss: 3.0288, Test Loss: 3.0912\n",
      "Epoch 2863, Train Loss: 3.0218, Test Loss: 3.0915\n",
      "Epoch 2864, Train Loss: 3.0157, Test Loss: 3.0925\n",
      "Epoch 2865, Train Loss: 3.0192, Test Loss: 3.0939\n",
      "Epoch 2866, Train Loss: 3.0201, Test Loss: 3.0936\n",
      "Epoch 2867, Train Loss: 3.0370, Test Loss: 3.0913\n",
      "Epoch 2868, Train Loss: 3.0278, Test Loss: 3.0919\n",
      "Epoch 2869, Train Loss: 3.0360, Test Loss: 3.0920\n",
      "Epoch 2870, Train Loss: 3.0265, Test Loss: 3.0950\n",
      "Epoch 2871, Train Loss: 3.0172, Test Loss: 3.0923\n",
      "Epoch 2872, Train Loss: 3.0240, Test Loss: 3.0909\n",
      "Epoch 2873, Train Loss: 3.0293, Test Loss: 3.0914\n",
      "Epoch 2874, Train Loss: 3.0160, Test Loss: 3.0943\n",
      "Epoch 2875, Train Loss: 3.0385, Test Loss: 3.0928\n",
      "Epoch 2876, Train Loss: 3.0193, Test Loss: 3.0927\n",
      "Epoch 2877, Train Loss: 3.0223, Test Loss: 3.0932\n",
      "Epoch 2878, Train Loss: 3.0323, Test Loss: 3.0938\n",
      "Epoch 2879, Train Loss: 3.0171, Test Loss: 3.0944\n",
      "Epoch 2880, Train Loss: 3.0252, Test Loss: 3.0919\n",
      "Epoch 2881, Train Loss: 3.0064, Test Loss: 3.0913\n",
      "Epoch 2882, Train Loss: 3.0189, Test Loss: 3.0920\n",
      "Epoch 2883, Train Loss: 3.0295, Test Loss: 3.0922\n",
      "Epoch 2884, Train Loss: 3.0218, Test Loss: 3.0923\n",
      "Epoch 2885, Train Loss: 3.0164, Test Loss: 3.0921\n",
      "Epoch 2886, Train Loss: 3.0286, Test Loss: 3.0925\n",
      "Epoch 2887, Train Loss: 3.0275, Test Loss: 3.0931\n",
      "Epoch 2888, Train Loss: 3.0154, Test Loss: 3.0928\n",
      "Epoch 2889, Train Loss: 3.0135, Test Loss: 3.0915\n",
      "Epoch 2890, Train Loss: 3.0203, Test Loss: 3.0923\n",
      "Epoch 2891, Train Loss: 3.0152, Test Loss: 3.0932\n",
      "Epoch 2892, Train Loss: 3.0181, Test Loss: 3.0915\n",
      "Epoch 2893, Train Loss: 3.0165, Test Loss: 3.0915\n",
      "Epoch 2894, Train Loss: 3.0183, Test Loss: 3.0922\n",
      "Epoch 2895, Train Loss: 3.0391, Test Loss: 3.0949\n",
      "Epoch 2896, Train Loss: 3.0392, Test Loss: 3.1001\n",
      "Epoch 2897, Train Loss: 3.0253, Test Loss: 3.0942\n",
      "Epoch 2898, Train Loss: 3.0178, Test Loss: 3.0942\n",
      "Epoch 2899, Train Loss: 3.0283, Test Loss: 3.0929\n",
      "Epoch 2900, Train Loss: 3.0155, Test Loss: 3.0947\n",
      "Epoch 2901, Train Loss: 3.0134, Test Loss: 3.0948\n",
      "Epoch 2902, Train Loss: 3.0205, Test Loss: 3.0934\n",
      "Epoch 2903, Train Loss: 3.0253, Test Loss: 3.0917\n",
      "Epoch 2904, Train Loss: 3.0405, Test Loss: 3.0926\n",
      "Epoch 2905, Train Loss: 3.0194, Test Loss: 3.0939\n",
      "Epoch 2906, Train Loss: 3.0245, Test Loss: 3.0913\n",
      "Epoch 2907, Train Loss: 3.0236, Test Loss: 3.0956\n",
      "Epoch 2908, Train Loss: 3.0334, Test Loss: 3.0949\n",
      "Epoch 2909, Train Loss: 3.0340, Test Loss: 3.0970\n",
      "Epoch 2910, Train Loss: 3.0202, Test Loss: 3.0964\n",
      "Epoch 2911, Train Loss: 3.0305, Test Loss: 3.0932\n",
      "Epoch 2912, Train Loss: 3.0141, Test Loss: 3.0954\n",
      "Epoch 2913, Train Loss: 3.0326, Test Loss: 3.0965\n",
      "Epoch 2914, Train Loss: 3.0240, Test Loss: 3.0923\n",
      "Epoch 2915, Train Loss: 3.0349, Test Loss: 3.0925\n",
      "Epoch 2916, Train Loss: 3.0303, Test Loss: 3.0905\n",
      "Epoch 2917, Train Loss: 3.0228, Test Loss: 3.0911\n",
      "Epoch 2918, Train Loss: 3.0200, Test Loss: 3.0970\n",
      "Epoch 2919, Train Loss: 3.0221, Test Loss: 3.0930\n",
      "Epoch 2920, Train Loss: 3.0230, Test Loss: 3.0921\n",
      "Epoch 2921, Train Loss: 3.0279, Test Loss: 3.0950\n",
      "Epoch 2922, Train Loss: 3.0305, Test Loss: 3.0915\n",
      "Epoch 2923, Train Loss: 3.0184, Test Loss: 3.0973\n",
      "Epoch 2924, Train Loss: 3.0336, Test Loss: 3.0976\n",
      "Epoch 2925, Train Loss: 3.0176, Test Loss: 3.0921\n",
      "Epoch 2926, Train Loss: 3.0242, Test Loss: 3.0931\n",
      "Epoch 2927, Train Loss: 3.0272, Test Loss: 3.0928\n",
      "Epoch 2928, Train Loss: 3.0405, Test Loss: 3.0921\n",
      "Epoch 2929, Train Loss: 3.0237, Test Loss: 3.0984\n",
      "Epoch 2930, Train Loss: 3.0368, Test Loss: 3.0959\n",
      "Epoch 2931, Train Loss: 3.0198, Test Loss: 3.0920\n",
      "Epoch 2932, Train Loss: 3.0311, Test Loss: 3.0934\n",
      "Epoch 2933, Train Loss: 3.0261, Test Loss: 3.0924\n",
      "Epoch 2934, Train Loss: 3.0222, Test Loss: 3.0939\n",
      "Epoch 2935, Train Loss: 3.0174, Test Loss: 3.0959\n",
      "Epoch 2936, Train Loss: 3.0250, Test Loss: 3.0914\n",
      "Epoch 2937, Train Loss: 3.0253, Test Loss: 3.0889\n",
      "Epoch 2938, Train Loss: 3.0277, Test Loss: 3.0903\n",
      "Epoch 2939, Train Loss: 3.0190, Test Loss: 3.0895\n",
      "Epoch 2940, Train Loss: 3.0250, Test Loss: 3.0907\n",
      "Epoch 2941, Train Loss: 3.0101, Test Loss: 3.0932\n",
      "Epoch 2942, Train Loss: 3.0282, Test Loss: 3.0938\n",
      "Epoch 2943, Train Loss: 3.0342, Test Loss: 3.0933\n",
      "Epoch 2944, Train Loss: 3.0330, Test Loss: 3.0930\n",
      "Epoch 2945, Train Loss: 3.0145, Test Loss: 3.0919\n",
      "Epoch 2946, Train Loss: 3.0303, Test Loss: 3.0934\n",
      "Epoch 2947, Train Loss: 3.0186, Test Loss: 3.0950\n",
      "Epoch 2948, Train Loss: 3.0272, Test Loss: 3.0897\n",
      "Epoch 2949, Train Loss: 3.0186, Test Loss: 3.0903\n",
      "Epoch 2950, Train Loss: 3.0254, Test Loss: 3.0903\n",
      "Epoch 2951, Train Loss: 3.0143, Test Loss: 3.0926\n",
      "Epoch 2952, Train Loss: 3.0159, Test Loss: 3.0967\n",
      "Epoch 2953, Train Loss: 3.0249, Test Loss: 3.0980\n",
      "Epoch 2954, Train Loss: 3.0282, Test Loss: 3.0934\n",
      "Epoch 2955, Train Loss: 3.0329, Test Loss: 3.0940\n",
      "Epoch 2956, Train Loss: 3.0300, Test Loss: 3.0925\n",
      "Epoch 2957, Train Loss: 3.0281, Test Loss: 3.0929\n",
      "Epoch 2958, Train Loss: 3.0302, Test Loss: 3.0926\n",
      "Epoch 2959, Train Loss: 3.0327, Test Loss: 3.0916\n",
      "Epoch 2960, Train Loss: 3.0253, Test Loss: 3.0930\n",
      "Epoch 2961, Train Loss: 3.0325, Test Loss: 3.0915\n",
      "Epoch 2962, Train Loss: 3.0232, Test Loss: 3.0913\n",
      "Epoch 2963, Train Loss: 3.0176, Test Loss: 3.0923\n",
      "Epoch 2964, Train Loss: 3.0371, Test Loss: 3.0916\n",
      "Epoch 2965, Train Loss: 3.0236, Test Loss: 3.0918\n",
      "Epoch 2966, Train Loss: 3.0388, Test Loss: 3.0920\n",
      "Epoch 2967, Train Loss: 3.0264, Test Loss: 3.0942\n",
      "Epoch 2968, Train Loss: 3.0213, Test Loss: 3.0969\n",
      "Epoch 2969, Train Loss: 3.0377, Test Loss: 3.0944\n",
      "Epoch 2970, Train Loss: 3.0328, Test Loss: 3.0926\n",
      "Epoch 2971, Train Loss: 3.0193, Test Loss: 3.0913\n",
      "Epoch 2972, Train Loss: 3.0168, Test Loss: 3.0925\n",
      "Epoch 2973, Train Loss: 3.0253, Test Loss: 3.0921\n",
      "Epoch 2974, Train Loss: 3.0259, Test Loss: 3.0918\n",
      "Epoch 2975, Train Loss: 3.0159, Test Loss: 3.0897\n",
      "Epoch 2976, Train Loss: 3.0338, Test Loss: 3.0902\n",
      "Epoch 2977, Train Loss: 3.0215, Test Loss: 3.0904\n",
      "Epoch 2978, Train Loss: 3.0240, Test Loss: 3.0922\n",
      "Epoch 2979, Train Loss: 3.0127, Test Loss: 3.0917\n",
      "Epoch 2980, Train Loss: 3.0260, Test Loss: 3.0909\n",
      "Epoch 2981, Train Loss: 3.0311, Test Loss: 3.0908\n",
      "Epoch 2982, Train Loss: 3.0222, Test Loss: 3.0908\n",
      "Epoch 2983, Train Loss: 3.0286, Test Loss: 3.0910\n",
      "Epoch 2984, Train Loss: 3.0268, Test Loss: 3.0920\n",
      "Epoch 2985, Train Loss: 3.0131, Test Loss: 3.0918\n",
      "Epoch 2986, Train Loss: 3.0225, Test Loss: 3.0899\n",
      "Epoch 2987, Train Loss: 3.0386, Test Loss: 3.0900\n",
      "Epoch 2988, Train Loss: 3.0225, Test Loss: 3.0910\n",
      "Epoch 2989, Train Loss: 3.0291, Test Loss: 3.0919\n",
      "Epoch 2990, Train Loss: 3.0189, Test Loss: 3.0912\n",
      "Epoch 2991, Train Loss: 3.0264, Test Loss: 3.0905\n",
      "Epoch 2992, Train Loss: 3.0199, Test Loss: 3.0908\n",
      "Epoch 2993, Train Loss: 3.0220, Test Loss: 3.0923\n",
      "Epoch 2994, Train Loss: 3.0240, Test Loss: 3.0932\n",
      "Epoch 2995, Train Loss: 3.0236, Test Loss: 3.0924\n",
      "Epoch 2996, Train Loss: 3.0293, Test Loss: 3.0920\n",
      "Epoch 2997, Train Loss: 3.0240, Test Loss: 3.0919\n",
      "Epoch 2998, Train Loss: 3.0124, Test Loss: 3.0913\n",
      "Epoch 2999, Train Loss: 3.0224, Test Loss: 3.0912\n",
      "Epoch 3000, Train Loss: 3.0198, Test Loss: 3.0908\n",
      "Epoch 3001, Train Loss: 3.0346, Test Loss: 3.0917\n",
      "Epoch 3002, Train Loss: 3.0157, Test Loss: 3.0919\n",
      "Epoch 3003, Train Loss: 3.0100, Test Loss: 3.0919\n",
      "Epoch 3004, Train Loss: 3.0153, Test Loss: 3.0924\n",
      "Epoch 3005, Train Loss: 3.0204, Test Loss: 3.0927\n",
      "Epoch 3006, Train Loss: 3.0319, Test Loss: 3.0925\n",
      "Epoch 3007, Train Loss: 3.0164, Test Loss: 3.0916\n",
      "Epoch 3008, Train Loss: 3.0227, Test Loss: 3.0918\n",
      "Epoch 3009, Train Loss: 3.0213, Test Loss: 3.0912\n",
      "Epoch 3010, Train Loss: 3.0205, Test Loss: 3.0913\n",
      "Epoch 3011, Train Loss: 3.0176, Test Loss: 3.0914\n",
      "Epoch 3012, Train Loss: 3.0206, Test Loss: 3.0911\n",
      "Epoch 3013, Train Loss: 3.0162, Test Loss: 3.0916\n",
      "Epoch 3014, Train Loss: 3.0169, Test Loss: 3.0904\n",
      "Epoch 3015, Train Loss: 3.0171, Test Loss: 3.0911\n",
      "Epoch 3016, Train Loss: 3.0187, Test Loss: 3.0914\n",
      "Epoch 3017, Train Loss: 3.0237, Test Loss: 3.0933\n",
      "Epoch 3018, Train Loss: 3.0163, Test Loss: 3.0932\n",
      "Epoch 3019, Train Loss: 3.0217, Test Loss: 3.0922\n",
      "Epoch 3020, Train Loss: 3.0146, Test Loss: 3.0922\n",
      "Epoch 3021, Train Loss: 3.0181, Test Loss: 3.0917\n",
      "Epoch 3022, Train Loss: 3.0241, Test Loss: 3.0919\n",
      "Epoch 3023, Train Loss: 3.0277, Test Loss: 3.0900\n",
      "Epoch 3024, Train Loss: 3.0263, Test Loss: 3.0904\n",
      "Epoch 3025, Train Loss: 3.0260, Test Loss: 3.0908\n",
      "Epoch 3026, Train Loss: 3.0232, Test Loss: 3.0928\n",
      "Epoch 3027, Train Loss: 3.0211, Test Loss: 3.0933\n",
      "Epoch 3028, Train Loss: 3.0143, Test Loss: 3.0945\n",
      "Epoch 3029, Train Loss: 3.0297, Test Loss: 3.0949\n",
      "Epoch 3030, Train Loss: 3.0344, Test Loss: 3.0952\n",
      "Epoch 3031, Train Loss: 3.0248, Test Loss: 3.0955\n",
      "Epoch 3032, Train Loss: 3.0151, Test Loss: 3.0919\n",
      "Epoch 3033, Train Loss: 3.0352, Test Loss: 3.0916\n",
      "Epoch 3034, Train Loss: 3.0225, Test Loss: 3.0911\n",
      "Epoch 3035, Train Loss: 3.0376, Test Loss: 3.0931\n",
      "Epoch 3036, Train Loss: 3.0307, Test Loss: 3.0931\n",
      "Epoch 3037, Train Loss: 3.0244, Test Loss: 3.0920\n",
      "Epoch 3038, Train Loss: 3.0234, Test Loss: 3.0929\n",
      "Epoch 3039, Train Loss: 3.0233, Test Loss: 3.0938\n",
      "Epoch 3040, Train Loss: 3.0166, Test Loss: 3.0945\n",
      "Epoch 3041, Train Loss: 3.0240, Test Loss: 3.0972\n",
      "Epoch 3042, Train Loss: 3.0288, Test Loss: 3.0946\n",
      "Epoch 3043, Train Loss: 3.0120, Test Loss: 3.0916\n",
      "Epoch 3044, Train Loss: 3.0152, Test Loss: 3.0920\n",
      "Epoch 3045, Train Loss: 3.0209, Test Loss: 3.0936\n",
      "Epoch 3046, Train Loss: 3.0254, Test Loss: 3.0931\n",
      "Epoch 3047, Train Loss: 3.0169, Test Loss: 3.0935\n",
      "Epoch 3048, Train Loss: 3.0344, Test Loss: 3.0926\n",
      "Epoch 3049, Train Loss: 3.0317, Test Loss: 3.0934\n",
      "Epoch 3050, Train Loss: 3.0171, Test Loss: 3.0954\n",
      "Epoch 3051, Train Loss: 3.0318, Test Loss: 3.0927\n",
      "Epoch 3052, Train Loss: 3.0273, Test Loss: 3.0932\n",
      "Epoch 3053, Train Loss: 3.0214, Test Loss: 3.0921\n",
      "Epoch 3054, Train Loss: 3.0227, Test Loss: 3.0927\n",
      "Epoch 3055, Train Loss: 3.0147, Test Loss: 3.0945\n",
      "Epoch 3056, Train Loss: 3.0227, Test Loss: 3.0909\n",
      "Epoch 3057, Train Loss: 3.0158, Test Loss: 3.0943\n",
      "Epoch 3058, Train Loss: 3.0277, Test Loss: 3.0952\n",
      "Epoch 3059, Train Loss: 3.0203, Test Loss: 3.1010\n",
      "Epoch 3060, Train Loss: 3.0240, Test Loss: 3.0962\n",
      "Epoch 3061, Train Loss: 3.0234, Test Loss: 3.0934\n",
      "Epoch 3062, Train Loss: 3.0199, Test Loss: 3.0923\n",
      "Epoch 3063, Train Loss: 3.0276, Test Loss: 3.0931\n",
      "Epoch 3064, Train Loss: 3.0184, Test Loss: 3.0952\n",
      "Epoch 3065, Train Loss: 3.0238, Test Loss: 3.0923\n",
      "Epoch 3066, Train Loss: 3.0224, Test Loss: 3.0913\n",
      "Epoch 3067, Train Loss: 3.0234, Test Loss: 3.0907\n",
      "Epoch 3068, Train Loss: 3.0221, Test Loss: 3.0937\n",
      "Epoch 3069, Train Loss: 3.0223, Test Loss: 3.0926\n",
      "Epoch 3070, Train Loss: 3.0272, Test Loss: 3.0934\n",
      "Epoch 3071, Train Loss: 3.0231, Test Loss: 3.0932\n",
      "Epoch 3072, Train Loss: 3.0219, Test Loss: 3.0947\n",
      "Epoch 3073, Train Loss: 3.0207, Test Loss: 3.0941\n",
      "Epoch 3074, Train Loss: 3.0418, Test Loss: 3.0938\n",
      "Epoch 3075, Train Loss: 3.0313, Test Loss: 3.0934\n",
      "Epoch 3076, Train Loss: 3.0247, Test Loss: 3.0939\n",
      "Epoch 3077, Train Loss: 3.0113, Test Loss: 3.0945\n",
      "Epoch 3078, Train Loss: 3.0267, Test Loss: 3.0942\n",
      "Epoch 3079, Train Loss: 3.0248, Test Loss: 3.0955\n",
      "Epoch 3080, Train Loss: 3.0271, Test Loss: 3.0949\n",
      "Epoch 3081, Train Loss: 3.0154, Test Loss: 3.0925\n",
      "Epoch 3082, Train Loss: 3.0164, Test Loss: 3.0926\n",
      "Epoch 3083, Train Loss: 3.0269, Test Loss: 3.0932\n",
      "Epoch 3084, Train Loss: 3.0148, Test Loss: 3.0961\n",
      "Epoch 3085, Train Loss: 3.0117, Test Loss: 3.0945\n",
      "Epoch 3086, Train Loss: 3.0187, Test Loss: 3.0915\n",
      "Epoch 3087, Train Loss: 3.0246, Test Loss: 3.0920\n",
      "Epoch 3088, Train Loss: 3.0313, Test Loss: 3.0921\n",
      "Epoch 3089, Train Loss: 3.0179, Test Loss: 3.0938\n",
      "Epoch 3090, Train Loss: 3.0257, Test Loss: 3.0934\n",
      "Epoch 3091, Train Loss: 3.0300, Test Loss: 3.0915\n",
      "Epoch 3092, Train Loss: 3.0187, Test Loss: 3.0907\n",
      "Epoch 3093, Train Loss: 3.0191, Test Loss: 3.0896\n",
      "Epoch 3094, Train Loss: 3.0123, Test Loss: 3.0902\n",
      "Epoch 3095, Train Loss: 3.0278, Test Loss: 3.0926\n",
      "Epoch 3096, Train Loss: 3.0293, Test Loss: 3.0929\n",
      "Epoch 3097, Train Loss: 3.0190, Test Loss: 3.0910\n",
      "Epoch 3098, Train Loss: 3.0285, Test Loss: 3.0913\n",
      "Epoch 3099, Train Loss: 3.0173, Test Loss: 3.0921\n",
      "Epoch 3100, Train Loss: 3.0275, Test Loss: 3.0933\n",
      "Epoch 3101, Train Loss: 3.0185, Test Loss: 3.0914\n",
      "Epoch 3102, Train Loss: 3.0261, Test Loss: 3.0905\n",
      "Epoch 3103, Train Loss: 3.0207, Test Loss: 3.0911\n",
      "Epoch 3104, Train Loss: 3.0137, Test Loss: 3.0906\n",
      "Epoch 3105, Train Loss: 3.0251, Test Loss: 3.0908\n",
      "Epoch 3106, Train Loss: 3.0192, Test Loss: 3.0913\n",
      "Epoch 3107, Train Loss: 3.0188, Test Loss: 3.0923\n",
      "Epoch 3108, Train Loss: 3.0264, Test Loss: 3.0948\n",
      "Epoch 3109, Train Loss: 3.0179, Test Loss: 3.0943\n",
      "Epoch 3110, Train Loss: 3.0325, Test Loss: 3.0933\n",
      "Epoch 3111, Train Loss: 3.0220, Test Loss: 3.0932\n",
      "Epoch 3112, Train Loss: 3.0124, Test Loss: 3.0940\n",
      "Epoch 3113, Train Loss: 3.0211, Test Loss: 3.0917\n",
      "Epoch 3114, Train Loss: 3.0144, Test Loss: 3.0907\n",
      "Epoch 3115, Train Loss: 3.0115, Test Loss: 3.0904\n",
      "Epoch 3116, Train Loss: 3.0252, Test Loss: 3.0908\n",
      "Epoch 3117, Train Loss: 3.0270, Test Loss: 3.0903\n",
      "Epoch 3118, Train Loss: 3.0099, Test Loss: 3.0914\n",
      "Epoch 3119, Train Loss: 3.0344, Test Loss: 3.0904\n",
      "Epoch 3120, Train Loss: 3.0174, Test Loss: 3.0890\n",
      "Epoch 3121, Train Loss: 3.0229, Test Loss: 3.0886\n",
      "Epoch 3122, Train Loss: 3.0139, Test Loss: 3.0897\n",
      "Epoch 3123, Train Loss: 3.0137, Test Loss: 3.0927\n",
      "Epoch 3124, Train Loss: 3.0223, Test Loss: 3.0922\n",
      "Epoch 3125, Train Loss: 3.0299, Test Loss: 3.0931\n",
      "Epoch 3126, Train Loss: 3.0214, Test Loss: 3.0937\n",
      "Epoch 3127, Train Loss: 3.0130, Test Loss: 3.0988\n",
      "Epoch 3128, Train Loss: 3.0233, Test Loss: 3.0939\n",
      "Epoch 3129, Train Loss: 3.0261, Test Loss: 3.0916\n",
      "Epoch 3130, Train Loss: 3.0241, Test Loss: 3.0905\n",
      "Epoch 3131, Train Loss: 3.0238, Test Loss: 3.0916\n",
      "Epoch 3132, Train Loss: 3.0203, Test Loss: 3.0910\n",
      "Epoch 3133, Train Loss: 3.0218, Test Loss: 3.0886\n",
      "Epoch 3134, Train Loss: 3.0134, Test Loss: 3.0880\n",
      "Epoch 3135, Train Loss: 3.0324, Test Loss: 3.0884\n",
      "Epoch 3136, Train Loss: 3.0291, Test Loss: 3.0923\n",
      "Epoch 3137, Train Loss: 3.0218, Test Loss: 3.0910\n",
      "Epoch 3138, Train Loss: 3.0218, Test Loss: 3.0912\n",
      "Epoch 3139, Train Loss: 3.0275, Test Loss: 3.0917\n",
      "Epoch 3140, Train Loss: 3.0218, Test Loss: 3.0939\n",
      "Epoch 3141, Train Loss: 3.0314, Test Loss: 3.0927\n",
      "Epoch 3142, Train Loss: 3.0205, Test Loss: 3.0927\n",
      "Epoch 3143, Train Loss: 3.0196, Test Loss: 3.0917\n",
      "Epoch 3144, Train Loss: 3.0167, Test Loss: 3.0916\n",
      "Epoch 3145, Train Loss: 3.0085, Test Loss: 3.0901\n",
      "Epoch 3146, Train Loss: 3.0208, Test Loss: 3.0904\n",
      "Epoch 3147, Train Loss: 3.0163, Test Loss: 3.0907\n",
      "Epoch 3148, Train Loss: 3.0216, Test Loss: 3.0917\n",
      "Epoch 3149, Train Loss: 3.0252, Test Loss: 3.0915\n",
      "Epoch 3150, Train Loss: 3.0153, Test Loss: 3.0923\n",
      "Epoch 3151, Train Loss: 3.0177, Test Loss: 3.0927\n",
      "Epoch 3152, Train Loss: 3.0256, Test Loss: 3.0912\n",
      "Epoch 3153, Train Loss: 3.0141, Test Loss: 3.0904\n",
      "Epoch 3154, Train Loss: 3.0238, Test Loss: 3.0905\n",
      "Epoch 3155, Train Loss: 3.0147, Test Loss: 3.0917\n",
      "Epoch 3156, Train Loss: 3.0321, Test Loss: 3.0916\n",
      "Epoch 3157, Train Loss: 3.0125, Test Loss: 3.0904\n",
      "Epoch 3158, Train Loss: 3.0148, Test Loss: 3.0906\n",
      "Epoch 3159, Train Loss: 3.0181, Test Loss: 3.0914\n",
      "Epoch 3160, Train Loss: 3.0187, Test Loss: 3.0922\n",
      "Epoch 3161, Train Loss: 3.0151, Test Loss: 3.0920\n",
      "Epoch 3162, Train Loss: 3.0178, Test Loss: 3.0910\n",
      "Epoch 3163, Train Loss: 3.0192, Test Loss: 3.0914\n",
      "Epoch 3164, Train Loss: 3.0239, Test Loss: 3.0934\n",
      "Epoch 3165, Train Loss: 3.0307, Test Loss: 3.0908\n",
      "Epoch 3166, Train Loss: 3.0173, Test Loss: 3.0939\n",
      "Epoch 3167, Train Loss: 3.0323, Test Loss: 3.0925\n",
      "Epoch 3168, Train Loss: 3.0260, Test Loss: 3.0933\n",
      "Epoch 3169, Train Loss: 3.0154, Test Loss: 3.0926\n",
      "Epoch 3170, Train Loss: 3.0325, Test Loss: 3.0896\n",
      "Epoch 3171, Train Loss: 3.0275, Test Loss: 3.0932\n",
      "Epoch 3172, Train Loss: 3.0096, Test Loss: 3.0949\n",
      "Epoch 3173, Train Loss: 3.0283, Test Loss: 3.0967\n",
      "Epoch 3174, Train Loss: 3.0185, Test Loss: 3.0939\n",
      "Epoch 3175, Train Loss: 3.0144, Test Loss: 3.0935\n",
      "Epoch 3176, Train Loss: 3.0456, Test Loss: 3.0935\n",
      "Epoch 3177, Train Loss: 3.0214, Test Loss: 3.0948\n",
      "Epoch 3178, Train Loss: 3.0154, Test Loss: 3.0919\n",
      "Epoch 3179, Train Loss: 3.0140, Test Loss: 3.0909\n",
      "Epoch 3180, Train Loss: 3.0189, Test Loss: 3.0907\n",
      "Epoch 3181, Train Loss: 3.0186, Test Loss: 3.0916\n",
      "Epoch 3182, Train Loss: 3.0177, Test Loss: 3.0922\n",
      "Epoch 3183, Train Loss: 3.0118, Test Loss: 3.0910\n",
      "Epoch 3184, Train Loss: 3.0224, Test Loss: 3.0901\n",
      "Epoch 3185, Train Loss: 3.0325, Test Loss: 3.0914\n",
      "Epoch 3186, Train Loss: 3.0212, Test Loss: 3.0921\n",
      "Epoch 3187, Train Loss: 3.0334, Test Loss: 3.0913\n",
      "Epoch 3188, Train Loss: 3.0232, Test Loss: 3.0911\n",
      "Epoch 3189, Train Loss: 3.0196, Test Loss: 3.0902\n",
      "Epoch 3190, Train Loss: 3.0131, Test Loss: 3.0914\n",
      "Epoch 3191, Train Loss: 3.0256, Test Loss: 3.0901\n",
      "Epoch 3192, Train Loss: 3.0079, Test Loss: 3.0918\n",
      "Epoch 3193, Train Loss: 3.0106, Test Loss: 3.0962\n",
      "Epoch 3194, Train Loss: 3.0307, Test Loss: 3.0920\n",
      "Epoch 3195, Train Loss: 3.0161, Test Loss: 3.0903\n",
      "Epoch 3196, Train Loss: 3.0260, Test Loss: 3.0899\n",
      "Epoch 3197, Train Loss: 3.0202, Test Loss: 3.0922\n",
      "Epoch 3198, Train Loss: 3.0285, Test Loss: 3.0935\n",
      "Epoch 3199, Train Loss: 3.0356, Test Loss: 3.0917\n",
      "Epoch 3200, Train Loss: 3.0232, Test Loss: 3.0919\n",
      "Epoch 3201, Train Loss: 3.0233, Test Loss: 3.0941\n",
      "Epoch 3202, Train Loss: 3.0234, Test Loss: 3.0937\n",
      "Epoch 3203, Train Loss: 3.0185, Test Loss: 3.0976\n",
      "Epoch 3204, Train Loss: 3.0262, Test Loss: 3.0939\n",
      "Epoch 3205, Train Loss: 3.0287, Test Loss: 3.0912\n",
      "Epoch 3206, Train Loss: 3.0208, Test Loss: 3.0926\n",
      "Epoch 3207, Train Loss: 3.0261, Test Loss: 3.0896\n",
      "Epoch 3208, Train Loss: 3.0161, Test Loss: 3.0936\n",
      "Epoch 3209, Train Loss: 3.0272, Test Loss: 3.0944\n",
      "Epoch 3210, Train Loss: 3.0298, Test Loss: 3.0890\n",
      "Epoch 3211, Train Loss: 3.0304, Test Loss: 3.0901\n",
      "Epoch 3212, Train Loss: 3.0280, Test Loss: 3.0899\n",
      "Epoch 3213, Train Loss: 3.0299, Test Loss: 3.0974\n",
      "Epoch 3214, Train Loss: 3.0234, Test Loss: 3.0957\n",
      "Epoch 3215, Train Loss: 3.0198, Test Loss: 3.0904\n",
      "Epoch 3216, Train Loss: 3.0190, Test Loss: 3.0923\n",
      "Epoch 3217, Train Loss: 3.0247, Test Loss: 3.0902\n",
      "Epoch 3218, Train Loss: 3.0191, Test Loss: 3.0936\n",
      "Epoch 3219, Train Loss: 3.0185, Test Loss: 3.1006\n",
      "Epoch 3220, Train Loss: 3.0429, Test Loss: 3.0928\n",
      "Epoch 3221, Train Loss: 3.0281, Test Loss: 3.0895\n",
      "Epoch 3222, Train Loss: 3.0254, Test Loss: 3.0917\n",
      "Epoch 3223, Train Loss: 3.0197, Test Loss: 3.0925\n",
      "Epoch 3224, Train Loss: 3.0201, Test Loss: 3.0976\n",
      "Epoch 3225, Train Loss: 3.0157, Test Loss: 3.0989\n",
      "Epoch 3226, Train Loss: 3.0218, Test Loss: 3.0913\n",
      "Epoch 3227, Train Loss: 3.0181, Test Loss: 3.0922\n",
      "Epoch 3228, Train Loss: 3.0258, Test Loss: 3.0896\n",
      "Epoch 3229, Train Loss: 3.0188, Test Loss: 3.0933\n",
      "Epoch 3230, Train Loss: 3.0130, Test Loss: 3.0984\n",
      "Epoch 3231, Train Loss: 3.0264, Test Loss: 3.0909\n",
      "Epoch 3232, Train Loss: 3.0259, Test Loss: 3.0916\n",
      "Epoch 3233, Train Loss: 3.0348, Test Loss: 3.0910\n",
      "Epoch 3234, Train Loss: 3.0239, Test Loss: 3.0939\n",
      "Epoch 3235, Train Loss: 3.0213, Test Loss: 3.1003\n",
      "Epoch 3236, Train Loss: 3.0325, Test Loss: 3.0937\n",
      "Epoch 3237, Train Loss: 3.0249, Test Loss: 3.0986\n",
      "Epoch 3238, Train Loss: 3.0487, Test Loss: 3.1039\n",
      "Epoch 3239, Train Loss: 3.0466, Test Loss: 3.1036\n",
      "Epoch 3240, Train Loss: 3.0242, Test Loss: 3.1208\n",
      "Epoch 3241, Train Loss: 3.0513, Test Loss: 3.0986\n",
      "Epoch 3242, Train Loss: 3.0187, Test Loss: 3.1061\n",
      "Epoch 3243, Train Loss: 3.0443, Test Loss: 3.0973\n",
      "Epoch 3244, Train Loss: 3.0357, Test Loss: 3.1058\n",
      "Epoch 3245, Train Loss: 3.0422, Test Loss: 3.1061\n",
      "Epoch 3246, Train Loss: 3.0362, Test Loss: 3.0918\n",
      "Epoch 3247, Train Loss: 3.0239, Test Loss: 3.1021\n",
      "Epoch 3248, Train Loss: 3.0377, Test Loss: 3.0967\n",
      "Epoch 3249, Train Loss: 3.0268, Test Loss: 3.1073\n",
      "Epoch 3250, Train Loss: 3.0400, Test Loss: 3.0975\n",
      "Epoch 3251, Train Loss: 3.0216, Test Loss: 3.0920\n",
      "Epoch 3252, Train Loss: 3.0301, Test Loss: 3.0996\n",
      "Epoch 3253, Train Loss: 3.0298, Test Loss: 3.0916\n",
      "Epoch 3254, Train Loss: 3.0292, Test Loss: 3.0991\n",
      "Epoch 3255, Train Loss: 3.0309, Test Loss: 3.0949\n",
      "Epoch 3256, Train Loss: 3.0238, Test Loss: 3.0911\n",
      "Epoch 3257, Train Loss: 3.0120, Test Loss: 3.0920\n",
      "Epoch 3258, Train Loss: 3.0326, Test Loss: 3.0992\n",
      "Epoch 3259, Train Loss: 3.0289, Test Loss: 3.1090\n",
      "Epoch 3260, Train Loss: 3.0344, Test Loss: 3.1070\n",
      "Epoch 3261, Train Loss: 3.0346, Test Loss: 3.0941\n",
      "Epoch 3262, Train Loss: 3.0222, Test Loss: 3.1035\n",
      "Epoch 3263, Train Loss: 3.0342, Test Loss: 3.0957\n",
      "Epoch 3264, Train Loss: 3.0207, Test Loss: 3.0983\n",
      "Epoch 3265, Train Loss: 3.0272, Test Loss: 3.0970\n",
      "Epoch 3266, Train Loss: 3.0271, Test Loss: 3.0894\n",
      "Epoch 3267, Train Loss: 3.0349, Test Loss: 3.0947\n",
      "Epoch 3268, Train Loss: 3.0272, Test Loss: 3.0982\n",
      "Epoch 3269, Train Loss: 3.0437, Test Loss: 3.1025\n",
      "Epoch 3270, Train Loss: 3.0222, Test Loss: 3.0936\n",
      "Epoch 3271, Train Loss: 3.0304, Test Loss: 3.0923\n",
      "Epoch 3272, Train Loss: 3.0365, Test Loss: 3.0943\n",
      "Epoch 3273, Train Loss: 3.0260, Test Loss: 3.0939\n",
      "Epoch 3274, Train Loss: 3.0259, Test Loss: 3.0983\n",
      "Epoch 3275, Train Loss: 3.0082, Test Loss: 3.0939\n",
      "Epoch 3276, Train Loss: 3.0212, Test Loss: 3.0916\n",
      "Epoch 3277, Train Loss: 3.0274, Test Loss: 3.0911\n",
      "Epoch 3278, Train Loss: 3.0227, Test Loss: 3.0899\n",
      "Epoch 3279, Train Loss: 3.0119, Test Loss: 3.0909\n",
      "Epoch 3280, Train Loss: 3.0170, Test Loss: 3.0911\n",
      "Epoch 3281, Train Loss: 3.0112, Test Loss: 3.0884\n",
      "Epoch 3282, Train Loss: 3.0140, Test Loss: 3.0883\n",
      "Epoch 3283, Train Loss: 3.0199, Test Loss: 3.0914\n",
      "Epoch 3284, Train Loss: 3.0336, Test Loss: 3.0912\n",
      "Epoch 3285, Train Loss: 3.0125, Test Loss: 3.0909\n",
      "Epoch 3286, Train Loss: 3.0125, Test Loss: 3.0905\n",
      "Epoch 3287, Train Loss: 3.0249, Test Loss: 3.0914\n",
      "Epoch 3288, Train Loss: 3.0167, Test Loss: 3.0924\n",
      "Epoch 3289, Train Loss: 3.0279, Test Loss: 3.0904\n",
      "Epoch 3290, Train Loss: 3.0286, Test Loss: 3.0899\n",
      "Epoch 3291, Train Loss: 3.0169, Test Loss: 3.0885\n",
      "Epoch 3292, Train Loss: 3.0256, Test Loss: 3.0879\n",
      "Epoch 3293, Train Loss: 3.0392, Test Loss: 3.0885\n",
      "Epoch 3294, Train Loss: 3.0194, Test Loss: 3.0916\n",
      "Epoch 3295, Train Loss: 3.0070, Test Loss: 3.0891\n",
      "Epoch 3296, Train Loss: 3.0253, Test Loss: 3.0887\n",
      "Epoch 3297, Train Loss: 3.0205, Test Loss: 3.0881\n",
      "Epoch 3298, Train Loss: 3.0206, Test Loss: 3.0913\n",
      "Epoch 3299, Train Loss: 3.0134, Test Loss: 3.0902\n",
      "Epoch 3300, Train Loss: 3.0263, Test Loss: 3.0889\n",
      "Epoch 3301, Train Loss: 3.0320, Test Loss: 3.0907\n",
      "Epoch 3302, Train Loss: 3.0154, Test Loss: 3.0916\n",
      "Epoch 3303, Train Loss: 3.0198, Test Loss: 3.0901\n",
      "Epoch 3304, Train Loss: 3.0177, Test Loss: 3.0879\n",
      "Epoch 3305, Train Loss: 3.0230, Test Loss: 3.0889\n",
      "Epoch 3306, Train Loss: 3.0231, Test Loss: 3.0875\n",
      "Epoch 3307, Train Loss: 3.0094, Test Loss: 3.0886\n",
      "Epoch 3308, Train Loss: 3.0267, Test Loss: 3.0881\n",
      "Epoch 3309, Train Loss: 3.0198, Test Loss: 3.0884\n",
      "Epoch 3310, Train Loss: 3.0226, Test Loss: 3.0890\n",
      "Epoch 3311, Train Loss: 3.0242, Test Loss: 3.0902\n",
      "Epoch 3312, Train Loss: 3.0262, Test Loss: 3.0923\n",
      "Epoch 3313, Train Loss: 3.0120, Test Loss: 3.0926\n",
      "Epoch 3314, Train Loss: 3.0229, Test Loss: 3.0901\n",
      "Epoch 3315, Train Loss: 3.0212, Test Loss: 3.0889\n",
      "Epoch 3316, Train Loss: 3.0171, Test Loss: 3.0903\n",
      "Epoch 3317, Train Loss: 3.0220, Test Loss: 3.0917\n",
      "Epoch 3318, Train Loss: 3.0354, Test Loss: 3.0890\n",
      "Epoch 3319, Train Loss: 3.0314, Test Loss: 3.0875\n",
      "Epoch 3320, Train Loss: 3.0153, Test Loss: 3.0883\n",
      "Epoch 3321, Train Loss: 3.0135, Test Loss: 3.0904\n",
      "Epoch 3322, Train Loss: 3.0224, Test Loss: 3.0921\n",
      "Epoch 3323, Train Loss: 3.0165, Test Loss: 3.0914\n",
      "Epoch 3324, Train Loss: 3.0184, Test Loss: 3.0905\n",
      "Epoch 3325, Train Loss: 3.0115, Test Loss: 3.0902\n",
      "Epoch 3326, Train Loss: 3.0160, Test Loss: 3.0908\n",
      "Epoch 3327, Train Loss: 3.0239, Test Loss: 3.0906\n",
      "Epoch 3328, Train Loss: 3.0182, Test Loss: 3.0897\n",
      "Epoch 3329, Train Loss: 3.0169, Test Loss: 3.0883\n",
      "Epoch 3330, Train Loss: 3.0213, Test Loss: 3.0888\n",
      "Epoch 3331, Train Loss: 3.0243, Test Loss: 3.0890\n",
      "Epoch 3332, Train Loss: 3.0180, Test Loss: 3.0913\n",
      "Epoch 3333, Train Loss: 3.0106, Test Loss: 3.0926\n",
      "Epoch 3334, Train Loss: 3.0178, Test Loss: 3.0901\n",
      "Epoch 3335, Train Loss: 3.0125, Test Loss: 3.0914\n",
      "Epoch 3336, Train Loss: 3.0245, Test Loss: 3.0915\n",
      "Epoch 3337, Train Loss: 3.0219, Test Loss: 3.0922\n",
      "Epoch 3338, Train Loss: 3.0171, Test Loss: 3.0911\n",
      "Epoch 3339, Train Loss: 3.0170, Test Loss: 3.0911\n",
      "Epoch 3340, Train Loss: 3.0178, Test Loss: 3.0885\n",
      "Epoch 3341, Train Loss: 3.0142, Test Loss: 3.0900\n",
      "Epoch 3342, Train Loss: 3.0313, Test Loss: 3.0884\n",
      "Epoch 3343, Train Loss: 3.0168, Test Loss: 3.0953\n",
      "Epoch 3344, Train Loss: 3.0218, Test Loss: 3.0911\n",
      "Epoch 3345, Train Loss: 3.0209, Test Loss: 3.0897\n",
      "Epoch 3346, Train Loss: 3.0218, Test Loss: 3.0907\n",
      "Epoch 3347, Train Loss: 3.0149, Test Loss: 3.0935\n",
      "Epoch 3348, Train Loss: 3.0130, Test Loss: 3.0942\n",
      "Epoch 3349, Train Loss: 3.0185, Test Loss: 3.0899\n",
      "Epoch 3350, Train Loss: 3.0153, Test Loss: 3.0900\n",
      "Epoch 3351, Train Loss: 3.0269, Test Loss: 3.0924\n",
      "Epoch 3352, Train Loss: 3.0164, Test Loss: 3.0924\n",
      "Epoch 3353, Train Loss: 3.0301, Test Loss: 3.0899\n",
      "Epoch 3354, Train Loss: 3.0195, Test Loss: 3.0911\n",
      "Epoch 3355, Train Loss: 3.0219, Test Loss: 3.0945\n",
      "Epoch 3356, Train Loss: 3.0233, Test Loss: 3.0942\n",
      "Epoch 3357, Train Loss: 3.0237, Test Loss: 3.0891\n",
      "Epoch 3358, Train Loss: 3.0183, Test Loss: 3.0937\n",
      "Epoch 3359, Train Loss: 3.0268, Test Loss: 3.0934\n",
      "Epoch 3360, Train Loss: 3.0281, Test Loss: 3.1010\n",
      "Epoch 3361, Train Loss: 3.0150, Test Loss: 3.0938\n",
      "Epoch 3362, Train Loss: 3.0340, Test Loss: 3.0905\n",
      "Epoch 3363, Train Loss: 3.0176, Test Loss: 3.0887\n",
      "Epoch 3364, Train Loss: 3.0158, Test Loss: 3.0945\n",
      "Epoch 3365, Train Loss: 3.0373, Test Loss: 3.0926\n",
      "Epoch 3366, Train Loss: 3.0160, Test Loss: 3.0876\n",
      "Epoch 3367, Train Loss: 3.0255, Test Loss: 3.0883\n",
      "Epoch 3368, Train Loss: 3.0241, Test Loss: 3.0907\n",
      "Epoch 3369, Train Loss: 3.0220, Test Loss: 3.0945\n",
      "Epoch 3370, Train Loss: 3.0216, Test Loss: 3.0904\n",
      "Epoch 3371, Train Loss: 3.0211, Test Loss: 3.0904\n",
      "Epoch 3372, Train Loss: 3.0144, Test Loss: 3.0900\n",
      "Epoch 3373, Train Loss: 3.0152, Test Loss: 3.0924\n",
      "Epoch 3374, Train Loss: 3.0267, Test Loss: 3.0897\n",
      "Epoch 3375, Train Loss: 3.0238, Test Loss: 3.0883\n",
      "Epoch 3376, Train Loss: 3.0181, Test Loss: 3.0892\n",
      "Epoch 3377, Train Loss: 3.0127, Test Loss: 3.0929\n",
      "Epoch 3378, Train Loss: 3.0267, Test Loss: 3.0879\n",
      "Epoch 3379, Train Loss: 3.0177, Test Loss: 3.0901\n",
      "Epoch 3380, Train Loss: 3.0288, Test Loss: 3.0920\n",
      "Epoch 3381, Train Loss: 3.0216, Test Loss: 3.0945\n",
      "Epoch 3382, Train Loss: 3.0204, Test Loss: 3.0908\n",
      "Epoch 3383, Train Loss: 3.0158, Test Loss: 3.0922\n",
      "Epoch 3384, Train Loss: 3.0274, Test Loss: 3.0934\n",
      "Epoch 3385, Train Loss: 3.0227, Test Loss: 3.1001\n",
      "Epoch 3386, Train Loss: 3.0346, Test Loss: 3.0929\n",
      "Epoch 3387, Train Loss: 3.0240, Test Loss: 3.0948\n",
      "Epoch 3388, Train Loss: 3.0354, Test Loss: 3.0897\n",
      "Epoch 3389, Train Loss: 3.0333, Test Loss: 3.0909\n",
      "Epoch 3390, Train Loss: 3.0245, Test Loss: 3.0970\n",
      "Epoch 3391, Train Loss: 3.0321, Test Loss: 3.0899\n",
      "Epoch 3392, Train Loss: 3.0222, Test Loss: 3.0950\n",
      "Epoch 3393, Train Loss: 3.0325, Test Loss: 3.0926\n",
      "Epoch 3394, Train Loss: 3.0282, Test Loss: 3.0910\n",
      "Epoch 3395, Train Loss: 3.0176, Test Loss: 3.1029\n",
      "Epoch 3396, Train Loss: 3.0297, Test Loss: 3.0961\n",
      "Epoch 3397, Train Loss: 3.0236, Test Loss: 3.0901\n",
      "Epoch 3398, Train Loss: 3.0201, Test Loss: 3.0945\n",
      "Epoch 3399, Train Loss: 3.0282, Test Loss: 3.0901\n",
      "Epoch 3400, Train Loss: 3.0134, Test Loss: 3.1002\n",
      "Epoch 3401, Train Loss: 3.0290, Test Loss: 3.1000\n",
      "Epoch 3402, Train Loss: 3.0416, Test Loss: 3.0957\n",
      "Epoch 3403, Train Loss: 3.0215, Test Loss: 3.1014\n",
      "Epoch 3404, Train Loss: 3.0448, Test Loss: 3.0920\n",
      "Epoch 3405, Train Loss: 3.0158, Test Loss: 3.0962\n",
      "Epoch 3406, Train Loss: 3.0303, Test Loss: 3.0980\n",
      "Epoch 3407, Train Loss: 3.0312, Test Loss: 3.0934\n",
      "Epoch 3408, Train Loss: 3.0163, Test Loss: 3.0923\n",
      "Epoch 3409, Train Loss: 3.0351, Test Loss: 3.0923\n",
      "Epoch 3410, Train Loss: 3.0272, Test Loss: 3.0897\n",
      "Epoch 3411, Train Loss: 3.0184, Test Loss: 3.0985\n",
      "Epoch 3412, Train Loss: 3.0317, Test Loss: 3.0985\n",
      "Epoch 3413, Train Loss: 3.0378, Test Loss: 3.0943\n",
      "Epoch 3414, Train Loss: 3.0418, Test Loss: 3.1026\n",
      "Epoch 3415, Train Loss: 3.0456, Test Loss: 3.0971\n",
      "Epoch 3416, Train Loss: 3.0320, Test Loss: 3.0917\n",
      "Epoch 3417, Train Loss: 3.0390, Test Loss: 3.1154\n",
      "Epoch 3418, Train Loss: 3.0329, Test Loss: 3.1156\n",
      "Epoch 3419, Train Loss: 3.0494, Test Loss: 3.1007\n",
      "Epoch 3420, Train Loss: 3.0314, Test Loss: 3.0941\n",
      "Epoch 3421, Train Loss: 3.0487, Test Loss: 3.1023\n",
      "Epoch 3422, Train Loss: 3.0449, Test Loss: 3.0914\n",
      "Epoch 3423, Train Loss: 3.0203, Test Loss: 3.1092\n",
      "Epoch 3424, Train Loss: 3.0286, Test Loss: 3.1186\n",
      "Epoch 3425, Train Loss: 3.0295, Test Loss: 3.1048\n",
      "Epoch 3426, Train Loss: 3.0217, Test Loss: 3.0902\n",
      "Epoch 3427, Train Loss: 3.0155, Test Loss: 3.0947\n",
      "Epoch 3428, Train Loss: 3.0275, Test Loss: 3.0904\n",
      "Epoch 3429, Train Loss: 3.0261, Test Loss: 3.0923\n",
      "Epoch 3430, Train Loss: 3.0317, Test Loss: 3.1021\n",
      "Epoch 3431, Train Loss: 3.0305, Test Loss: 3.0969\n",
      "Epoch 3432, Train Loss: 3.0263, Test Loss: 3.0974\n",
      "Epoch 3433, Train Loss: 3.0299, Test Loss: 3.0960\n",
      "Epoch 3434, Train Loss: 3.0351, Test Loss: 3.0928\n",
      "Epoch 3435, Train Loss: 3.0220, Test Loss: 3.0938\n",
      "Epoch 3436, Train Loss: 3.0238, Test Loss: 3.1025\n",
      "Epoch 3437, Train Loss: 3.0174, Test Loss: 3.0941\n",
      "Epoch 3438, Train Loss: 3.0344, Test Loss: 3.0977\n",
      "Epoch 3439, Train Loss: 3.0375, Test Loss: 3.1012\n",
      "Epoch 3440, Train Loss: 3.0346, Test Loss: 3.0971\n",
      "Epoch 3441, Train Loss: 3.0318, Test Loss: 3.1034\n",
      "Epoch 3442, Train Loss: 3.0307, Test Loss: 3.1078\n",
      "Epoch 3443, Train Loss: 3.0503, Test Loss: 3.0960\n",
      "Epoch 3444, Train Loss: 3.0261, Test Loss: 3.0999\n",
      "Epoch 3445, Train Loss: 3.0430, Test Loss: 3.0890\n",
      "Epoch 3446, Train Loss: 3.0319, Test Loss: 3.0924\n",
      "Epoch 3447, Train Loss: 3.0367, Test Loss: 3.1152\n",
      "Epoch 3448, Train Loss: 3.0460, Test Loss: 3.1013\n",
      "Epoch 3449, Train Loss: 3.0458, Test Loss: 3.0916\n",
      "Epoch 3450, Train Loss: 3.0273, Test Loss: 3.1066\n",
      "Epoch 3451, Train Loss: 3.0653, Test Loss: 3.0988\n",
      "Epoch 3452, Train Loss: 3.0306, Test Loss: 3.0968\n",
      "Epoch 3453, Train Loss: 3.0302, Test Loss: 3.1083\n",
      "Epoch 3454, Train Loss: 3.0336, Test Loss: 3.1037\n",
      "Epoch 3455, Train Loss: 3.0295, Test Loss: 3.0988\n",
      "Epoch 3456, Train Loss: 3.0429, Test Loss: 3.1096\n",
      "Epoch 3457, Train Loss: 3.0373, Test Loss: 3.1320\n",
      "Epoch 3458, Train Loss: 3.0443, Test Loss: 3.1366\n",
      "Epoch 3459, Train Loss: 3.0711, Test Loss: 3.1133\n",
      "Epoch 3460, Train Loss: 3.0459, Test Loss: 3.0969\n",
      "Epoch 3461, Train Loss: 3.0314, Test Loss: 3.1194\n",
      "Epoch 3462, Train Loss: 3.0674, Test Loss: 3.0939\n",
      "Epoch 3463, Train Loss: 3.0316, Test Loss: 3.1098\n",
      "Epoch 3464, Train Loss: 3.0410, Test Loss: 3.1042\n",
      "Epoch 3465, Train Loss: 3.0326, Test Loss: 3.0927\n",
      "Epoch 3466, Train Loss: 3.0319, Test Loss: 3.0962\n",
      "Epoch 3467, Train Loss: 3.0268, Test Loss: 3.0953\n",
      "Epoch 3468, Train Loss: 3.0245, Test Loss: 3.0985\n",
      "Epoch 3469, Train Loss: 3.0237, Test Loss: 3.0975\n",
      "Epoch 3470, Train Loss: 3.0328, Test Loss: 3.0930\n",
      "Epoch 3471, Train Loss: 3.0119, Test Loss: 3.0946\n",
      "Epoch 3472, Train Loss: 3.0233, Test Loss: 3.1006\n",
      "Epoch 3473, Train Loss: 3.0202, Test Loss: 3.1012\n",
      "Epoch 3474, Train Loss: 3.0363, Test Loss: 3.0891\n",
      "Epoch 3475, Train Loss: 3.0244, Test Loss: 3.0978\n",
      "Epoch 3476, Train Loss: 3.0386, Test Loss: 3.1053\n",
      "Epoch 3477, Train Loss: 3.0171, Test Loss: 3.1225\n",
      "Epoch 3478, Train Loss: 3.0461, Test Loss: 3.1086\n",
      "Epoch 3479, Train Loss: 3.0482, Test Loss: 3.0975\n",
      "Epoch 3480, Train Loss: 3.0369, Test Loss: 3.1086\n",
      "Epoch 3481, Train Loss: 3.0472, Test Loss: 3.1033\n",
      "Epoch 3482, Train Loss: 3.0407, Test Loss: 3.1103\n",
      "Epoch 3483, Train Loss: 3.0382, Test Loss: 3.1023\n",
      "Epoch 3484, Train Loss: 3.0356, Test Loss: 3.0937\n",
      "Epoch 3485, Train Loss: 3.0249, Test Loss: 3.0993\n",
      "Epoch 3486, Train Loss: 3.0326, Test Loss: 3.0982\n",
      "Epoch 3487, Train Loss: 3.0292, Test Loss: 3.1026\n",
      "Epoch 3488, Train Loss: 3.0380, Test Loss: 3.0926\n",
      "Epoch 3489, Train Loss: 3.0310, Test Loss: 3.0932\n",
      "Epoch 3490, Train Loss: 3.0398, Test Loss: 3.0935\n",
      "Epoch 3491, Train Loss: 3.0281, Test Loss: 3.0950\n",
      "Epoch 3492, Train Loss: 3.0219, Test Loss: 3.1079\n",
      "Epoch 3493, Train Loss: 3.0416, Test Loss: 3.0898\n",
      "Epoch 3494, Train Loss: 3.0240, Test Loss: 3.0925\n",
      "Epoch 3495, Train Loss: 3.0347, Test Loss: 3.0930\n",
      "Epoch 3496, Train Loss: 3.0296, Test Loss: 3.0991\n",
      "Epoch 3497, Train Loss: 3.0252, Test Loss: 3.1091\n",
      "Epoch 3498, Train Loss: 3.0309, Test Loss: 3.1027\n",
      "Epoch 3499, Train Loss: 3.0246, Test Loss: 3.0959\n",
      "Epoch 3500, Train Loss: 3.0228, Test Loss: 3.0969\n",
      "Epoch 3501, Train Loss: 3.0340, Test Loss: 3.0931\n",
      "Epoch 3502, Train Loss: 3.0204, Test Loss: 3.0939\n",
      "Epoch 3503, Train Loss: 3.0202, Test Loss: 3.0893\n",
      "Epoch 3504, Train Loss: 3.0234, Test Loss: 3.0894\n",
      "Epoch 3505, Train Loss: 3.0266, Test Loss: 3.0878\n",
      "Epoch 3506, Train Loss: 3.0259, Test Loss: 3.0923\n",
      "Epoch 3507, Train Loss: 3.0170, Test Loss: 3.0891\n",
      "Epoch 3508, Train Loss: 3.0254, Test Loss: 3.0897\n",
      "Epoch 3509, Train Loss: 3.0247, Test Loss: 3.0909\n",
      "Epoch 3510, Train Loss: 3.0234, Test Loss: 3.0927\n",
      "Epoch 3511, Train Loss: 3.0214, Test Loss: 3.0909\n",
      "Epoch 3512, Train Loss: 3.0211, Test Loss: 3.0885\n",
      "Epoch 3513, Train Loss: 3.0210, Test Loss: 3.0889\n",
      "Epoch 3514, Train Loss: 3.0171, Test Loss: 3.0898\n",
      "Epoch 3515, Train Loss: 3.0225, Test Loss: 3.0884\n",
      "Epoch 3516, Train Loss: 3.0216, Test Loss: 3.0881\n",
      "Epoch 3517, Train Loss: 3.0177, Test Loss: 3.0915\n",
      "Epoch 3518, Train Loss: 3.0219, Test Loss: 3.0926\n",
      "Epoch 3519, Train Loss: 3.0158, Test Loss: 3.0905\n",
      "Epoch 3520, Train Loss: 3.0168, Test Loss: 3.0917\n",
      "Epoch 3521, Train Loss: 3.0248, Test Loss: 3.0930\n",
      "Epoch 3522, Train Loss: 3.0124, Test Loss: 3.0951\n",
      "Epoch 3523, Train Loss: 3.0250, Test Loss: 3.0891\n",
      "Epoch 3524, Train Loss: 3.0241, Test Loss: 3.0907\n",
      "Epoch 3525, Train Loss: 3.0257, Test Loss: 3.0897\n",
      "Epoch 3526, Train Loss: 3.0275, Test Loss: 3.0937\n",
      "Epoch 3527, Train Loss: 3.0250, Test Loss: 3.0950\n",
      "Epoch 3528, Train Loss: 3.0203, Test Loss: 3.0911\n",
      "Epoch 3529, Train Loss: 3.0152, Test Loss: 3.0986\n",
      "Epoch 3530, Train Loss: 3.0338, Test Loss: 3.0974\n",
      "Epoch 3531, Train Loss: 3.0231, Test Loss: 3.1001\n",
      "Epoch 3532, Train Loss: 3.0246, Test Loss: 3.0942\n",
      "Epoch 3533, Train Loss: 3.0196, Test Loss: 3.0913\n",
      "Epoch 3534, Train Loss: 3.0253, Test Loss: 3.0914\n",
      "Epoch 3535, Train Loss: 3.0117, Test Loss: 3.0916\n",
      "Epoch 3536, Train Loss: 3.0263, Test Loss: 3.0919\n",
      "Epoch 3537, Train Loss: 3.0279, Test Loss: 3.0872\n",
      "Epoch 3538, Train Loss: 3.0261, Test Loss: 3.0947\n",
      "Epoch 3539, Train Loss: 3.0250, Test Loss: 3.0994\n",
      "Epoch 3540, Train Loss: 3.0226, Test Loss: 3.1117\n",
      "Epoch 3541, Train Loss: 3.0275, Test Loss: 3.1087\n",
      "Epoch 3542, Train Loss: 3.0378, Test Loss: 3.0963\n",
      "Epoch 3543, Train Loss: 3.0224, Test Loss: 3.1021\n",
      "Epoch 3544, Train Loss: 3.0386, Test Loss: 3.1036\n",
      "Epoch 3545, Train Loss: 3.0411, Test Loss: 3.1051\n",
      "Epoch 3546, Train Loss: 3.0487, Test Loss: 3.0954\n",
      "Epoch 3547, Train Loss: 3.0322, Test Loss: 3.0938\n",
      "Epoch 3548, Train Loss: 3.0390, Test Loss: 3.0954\n",
      "Epoch 3549, Train Loss: 3.0412, Test Loss: 3.0992\n",
      "Epoch 3550, Train Loss: 3.0327, Test Loss: 3.1219\n",
      "Epoch 3551, Train Loss: 3.0520, Test Loss: 3.1013\n",
      "Epoch 3552, Train Loss: 3.0315, Test Loss: 3.0984\n",
      "Epoch 3553, Train Loss: 3.0226, Test Loss: 3.0960\n",
      "Epoch 3554, Train Loss: 3.0272, Test Loss: 3.1132\n",
      "Epoch 3555, Train Loss: 3.0417, Test Loss: 3.1238\n",
      "Epoch 3556, Train Loss: 3.0467, Test Loss: 3.1096\n",
      "Epoch 3557, Train Loss: 3.0416, Test Loss: 3.0927\n",
      "Epoch 3558, Train Loss: 3.0280, Test Loss: 3.0886\n",
      "Epoch 3559, Train Loss: 3.0123, Test Loss: 3.0961\n",
      "Epoch 3560, Train Loss: 3.0192, Test Loss: 3.0961\n",
      "Epoch 3561, Train Loss: 3.0315, Test Loss: 3.0889\n",
      "Epoch 3562, Train Loss: 3.0281, Test Loss: 3.0949\n",
      "Epoch 3563, Train Loss: 3.0248, Test Loss: 3.0885\n",
      "Epoch 3564, Train Loss: 3.0231, Test Loss: 3.0993\n",
      "Epoch 3565, Train Loss: 3.0167, Test Loss: 3.1010\n",
      "Epoch 3566, Train Loss: 3.0265, Test Loss: 3.0907\n",
      "Epoch 3567, Train Loss: 3.0237, Test Loss: 3.0909\n",
      "Epoch 3568, Train Loss: 3.0246, Test Loss: 3.0958\n",
      "Epoch 3569, Train Loss: 3.0307, Test Loss: 3.1035\n",
      "Epoch 3570, Train Loss: 3.0305, Test Loss: 3.0992\n",
      "Epoch 3571, Train Loss: 3.0235, Test Loss: 3.0892\n",
      "Epoch 3572, Train Loss: 3.0215, Test Loss: 3.0944\n",
      "Epoch 3573, Train Loss: 3.0403, Test Loss: 3.1054\n",
      "Epoch 3574, Train Loss: 3.0371, Test Loss: 3.1047\n",
      "Epoch 3575, Train Loss: 3.0325, Test Loss: 3.0929\n",
      "Epoch 3576, Train Loss: 3.0279, Test Loss: 3.1032\n",
      "Epoch 3577, Train Loss: 3.0545, Test Loss: 3.1044\n",
      "Epoch 3578, Train Loss: 3.0401, Test Loss: 3.1170\n",
      "Epoch 3579, Train Loss: 3.0584, Test Loss: 3.1113\n",
      "Epoch 3580, Train Loss: 3.0352, Test Loss: 3.1023\n",
      "Epoch 3581, Train Loss: 3.0332, Test Loss: 3.1065\n",
      "Epoch 3582, Train Loss: 3.0535, Test Loss: 3.1075\n",
      "Epoch 3583, Train Loss: 3.0500, Test Loss: 3.1162\n",
      "Epoch 3584, Train Loss: 3.0355, Test Loss: 3.1018\n",
      "Epoch 3585, Train Loss: 3.0358, Test Loss: 3.1049\n",
      "Epoch 3586, Train Loss: 3.0515, Test Loss: 3.0948\n",
      "Epoch 3587, Train Loss: 3.0231, Test Loss: 3.1053\n",
      "Epoch 3588, Train Loss: 3.0275, Test Loss: 3.1001\n",
      "Epoch 3589, Train Loss: 3.0313, Test Loss: 3.0917\n",
      "Epoch 3590, Train Loss: 3.0245, Test Loss: 3.0987\n",
      "Epoch 3591, Train Loss: 3.0323, Test Loss: 3.0963\n",
      "Epoch 3592, Train Loss: 3.0190, Test Loss: 3.0956\n",
      "Epoch 3593, Train Loss: 3.0227, Test Loss: 3.0913\n",
      "Epoch 3594, Train Loss: 3.0193, Test Loss: 3.0954\n",
      "Epoch 3595, Train Loss: 3.0222, Test Loss: 3.1059\n",
      "Epoch 3596, Train Loss: 3.0369, Test Loss: 3.0955\n",
      "Epoch 3597, Train Loss: 3.0278, Test Loss: 3.0915\n",
      "Epoch 3598, Train Loss: 3.0281, Test Loss: 3.0992\n",
      "Epoch 3599, Train Loss: 3.0287, Test Loss: 3.1020\n",
      "Epoch 3600, Train Loss: 3.0386, Test Loss: 3.1031\n",
      "Epoch 3601, Train Loss: 3.0242, Test Loss: 3.0951\n",
      "Epoch 3602, Train Loss: 3.0208, Test Loss: 3.0948\n",
      "Epoch 3603, Train Loss: 3.0214, Test Loss: 3.0938\n",
      "Epoch 3604, Train Loss: 3.0263, Test Loss: 3.0881\n",
      "Epoch 3605, Train Loss: 3.0238, Test Loss: 3.1038\n",
      "Epoch 3606, Train Loss: 3.0282, Test Loss: 3.0909\n",
      "Epoch 3607, Train Loss: 3.0247, Test Loss: 3.0953\n",
      "Epoch 3608, Train Loss: 3.0227, Test Loss: 3.0955\n",
      "Epoch 3609, Train Loss: 3.0225, Test Loss: 3.0931\n",
      "Epoch 3610, Train Loss: 3.0114, Test Loss: 3.1017\n",
      "Epoch 3611, Train Loss: 3.0273, Test Loss: 3.0983\n",
      "Epoch 3612, Train Loss: 3.0276, Test Loss: 3.0883\n",
      "Epoch 3613, Train Loss: 3.0167, Test Loss: 3.0949\n",
      "Epoch 3614, Train Loss: 3.0208, Test Loss: 3.0943\n",
      "Epoch 3615, Train Loss: 3.0240, Test Loss: 3.0918\n",
      "Epoch 3616, Train Loss: 3.0228, Test Loss: 3.0990\n",
      "Epoch 3617, Train Loss: 3.0273, Test Loss: 3.1009\n",
      "Epoch 3618, Train Loss: 3.0233, Test Loss: 3.1098\n",
      "Epoch 3619, Train Loss: 3.0369, Test Loss: 3.1079\n",
      "Epoch 3620, Train Loss: 3.0316, Test Loss: 3.0969\n",
      "Epoch 3621, Train Loss: 3.0153, Test Loss: 3.1089\n",
      "Epoch 3622, Train Loss: 3.0361, Test Loss: 3.0940\n",
      "Epoch 3623, Train Loss: 3.0153, Test Loss: 3.0985\n",
      "Epoch 3624, Train Loss: 3.0302, Test Loss: 3.0976\n",
      "Epoch 3625, Train Loss: 3.0285, Test Loss: 3.0882\n",
      "Epoch 3626, Train Loss: 3.0124, Test Loss: 3.0991\n",
      "Epoch 3627, Train Loss: 3.0218, Test Loss: 3.1027\n",
      "Epoch 3628, Train Loss: 3.0193, Test Loss: 3.1100\n",
      "Epoch 3629, Train Loss: 3.0360, Test Loss: 3.0933\n",
      "Epoch 3630, Train Loss: 3.0275, Test Loss: 3.0937\n",
      "Epoch 3631, Train Loss: 3.0256, Test Loss: 3.0910\n",
      "Epoch 3632, Train Loss: 3.0120, Test Loss: 3.0936\n",
      "Epoch 3633, Train Loss: 3.0253, Test Loss: 3.0979\n",
      "Epoch 3634, Train Loss: 3.0291, Test Loss: 3.0899\n",
      "Epoch 3635, Train Loss: 3.0216, Test Loss: 3.0938\n",
      "Epoch 3636, Train Loss: 3.0300, Test Loss: 3.0888\n",
      "Epoch 3637, Train Loss: 3.0215, Test Loss: 3.1036\n",
      "Epoch 3638, Train Loss: 3.0230, Test Loss: 3.0989\n",
      "Epoch 3639, Train Loss: 3.0258, Test Loss: 3.0912\n",
      "Epoch 3640, Train Loss: 3.0134, Test Loss: 3.0967\n",
      "Epoch 3641, Train Loss: 3.0356, Test Loss: 3.0938\n",
      "Epoch 3642, Train Loss: 3.0133, Test Loss: 3.0961\n",
      "Epoch 3643, Train Loss: 3.0149, Test Loss: 3.1046\n",
      "Epoch 3644, Train Loss: 3.0358, Test Loss: 3.0973\n",
      "Epoch 3645, Train Loss: 3.0200, Test Loss: 3.0944\n",
      "Epoch 3646, Train Loss: 3.0235, Test Loss: 3.0965\n",
      "Epoch 3647, Train Loss: 3.0301, Test Loss: 3.0914\n",
      "Epoch 3648, Train Loss: 3.0300, Test Loss: 3.0991\n",
      "Epoch 3649, Train Loss: 3.0185, Test Loss: 3.1033\n",
      "Epoch 3650, Train Loss: 3.0256, Test Loss: 3.0883\n",
      "Epoch 3651, Train Loss: 3.0233, Test Loss: 3.0939\n",
      "Epoch 3652, Train Loss: 3.0281, Test Loss: 3.0892\n",
      "Epoch 3653, Train Loss: 3.0317, Test Loss: 3.0948\n",
      "Epoch 3654, Train Loss: 3.0237, Test Loss: 3.1061\n",
      "Epoch 3655, Train Loss: 3.0332, Test Loss: 3.0879\n",
      "Epoch 3656, Train Loss: 3.0122, Test Loss: 3.0885\n",
      "Epoch 3657, Train Loss: 3.0212, Test Loss: 3.0929\n",
      "Epoch 3658, Train Loss: 3.0281, Test Loss: 3.0883\n",
      "Epoch 3659, Train Loss: 3.0214, Test Loss: 3.1011\n",
      "Epoch 3660, Train Loss: 3.0277, Test Loss: 3.0943\n",
      "Epoch 3661, Train Loss: 3.0231, Test Loss: 3.0918\n",
      "Epoch 3662, Train Loss: 3.0203, Test Loss: 3.0947\n",
      "Epoch 3663, Train Loss: 3.0254, Test Loss: 3.0903\n",
      "Epoch 3664, Train Loss: 3.0179, Test Loss: 3.0952\n",
      "Epoch 3665, Train Loss: 3.0272, Test Loss: 3.0974\n",
      "Epoch 3666, Train Loss: 3.0308, Test Loss: 3.0881\n",
      "Epoch 3667, Train Loss: 3.0120, Test Loss: 3.0909\n",
      "Epoch 3668, Train Loss: 3.0342, Test Loss: 3.0901\n",
      "Epoch 3669, Train Loss: 3.0348, Test Loss: 3.0899\n",
      "Epoch 3670, Train Loss: 3.0258, Test Loss: 3.1028\n",
      "Epoch 3671, Train Loss: 3.0246, Test Loss: 3.0913\n",
      "Epoch 3672, Train Loss: 3.0165, Test Loss: 3.0905\n",
      "Epoch 3673, Train Loss: 3.0273, Test Loss: 3.0951\n",
      "Epoch 3674, Train Loss: 3.0367, Test Loss: 3.0971\n",
      "Epoch 3675, Train Loss: 3.0192, Test Loss: 3.0882\n",
      "Epoch 3676, Train Loss: 3.0156, Test Loss: 3.0880\n",
      "Epoch 3677, Train Loss: 3.0255, Test Loss: 3.0899\n",
      "Epoch 3678, Train Loss: 3.0179, Test Loss: 3.0960\n",
      "Epoch 3679, Train Loss: 3.0213, Test Loss: 3.0885\n",
      "Epoch 3680, Train Loss: 3.0150, Test Loss: 3.0900\n",
      "Epoch 3681, Train Loss: 3.0317, Test Loss: 3.0898\n",
      "Epoch 3682, Train Loss: 3.0126, Test Loss: 3.0922\n",
      "Epoch 3683, Train Loss: 3.0132, Test Loss: 3.0887\n",
      "Epoch 3684, Train Loss: 3.0126, Test Loss: 3.0884\n",
      "Epoch 3685, Train Loss: 3.0161, Test Loss: 3.0907\n",
      "Epoch 3686, Train Loss: 3.0169, Test Loss: 3.0883\n",
      "Epoch 3687, Train Loss: 3.0217, Test Loss: 3.0895\n",
      "Epoch 3688, Train Loss: 3.0237, Test Loss: 3.0889\n",
      "Epoch 3689, Train Loss: 3.0140, Test Loss: 3.0898\n",
      "Epoch 3690, Train Loss: 3.0180, Test Loss: 3.0886\n",
      "Epoch 3691, Train Loss: 3.0106, Test Loss: 3.0885\n",
      "Epoch 3692, Train Loss: 3.0201, Test Loss: 3.0865\n",
      "Epoch 3693, Train Loss: 3.0177, Test Loss: 3.0871\n",
      "Epoch 3694, Train Loss: 3.0139, Test Loss: 3.0895\n",
      "Epoch 3695, Train Loss: 3.0170, Test Loss: 3.0885\n",
      "Epoch 3696, Train Loss: 3.0131, Test Loss: 3.0884\n",
      "Epoch 3697, Train Loss: 3.0136, Test Loss: 3.0890\n",
      "Epoch 3698, Train Loss: 3.0266, Test Loss: 3.0912\n",
      "Epoch 3699, Train Loss: 3.0220, Test Loss: 3.0901\n",
      "Epoch 3700, Train Loss: 3.0169, Test Loss: 3.0921\n",
      "Epoch 3701, Train Loss: 3.0301, Test Loss: 3.0923\n",
      "Epoch 3702, Train Loss: 3.0160, Test Loss: 3.0925\n",
      "Epoch 3703, Train Loss: 3.0251, Test Loss: 3.0889\n",
      "Epoch 3704, Train Loss: 3.0152, Test Loss: 3.0893\n",
      "Epoch 3705, Train Loss: 3.0312, Test Loss: 3.0962\n",
      "Epoch 3706, Train Loss: 3.0341, Test Loss: 3.0943\n",
      "Epoch 3707, Train Loss: 3.0152, Test Loss: 3.0874\n",
      "Epoch 3708, Train Loss: 3.0229, Test Loss: 3.0952\n",
      "Epoch 3709, Train Loss: 3.0269, Test Loss: 3.0981\n",
      "Epoch 3710, Train Loss: 3.0194, Test Loss: 3.0961\n",
      "Epoch 3711, Train Loss: 3.0325, Test Loss: 3.0880\n",
      "Epoch 3712, Train Loss: 3.0125, Test Loss: 3.0908\n",
      "Epoch 3713, Train Loss: 3.0256, Test Loss: 3.0902\n",
      "Epoch 3714, Train Loss: 3.0160, Test Loss: 3.1025\n",
      "Epoch 3715, Train Loss: 3.0356, Test Loss: 3.0877\n",
      "Epoch 3716, Train Loss: 3.0155, Test Loss: 3.0911\n",
      "Epoch 3717, Train Loss: 3.0272, Test Loss: 3.0904\n",
      "Epoch 3718, Train Loss: 3.0299, Test Loss: 3.0943\n",
      "Epoch 3719, Train Loss: 3.0205, Test Loss: 3.0914\n",
      "Epoch 3720, Train Loss: 3.0201, Test Loss: 3.0878\n",
      "Epoch 3721, Train Loss: 3.0144, Test Loss: 3.0868\n",
      "Epoch 3722, Train Loss: 3.0058, Test Loss: 3.0880\n",
      "Epoch 3723, Train Loss: 3.0260, Test Loss: 3.0885\n",
      "Epoch 3724, Train Loss: 3.0161, Test Loss: 3.0890\n",
      "Epoch 3725, Train Loss: 3.0108, Test Loss: 3.0891\n",
      "Epoch 3726, Train Loss: 3.0158, Test Loss: 3.0894\n",
      "Epoch 3727, Train Loss: 3.0171, Test Loss: 3.0869\n",
      "Epoch 3728, Train Loss: 3.0096, Test Loss: 3.0891\n",
      "Epoch 3729, Train Loss: 3.0309, Test Loss: 3.0909\n",
      "Epoch 3730, Train Loss: 3.0210, Test Loss: 3.0881\n",
      "Epoch 3731, Train Loss: 3.0249, Test Loss: 3.0877\n",
      "Epoch 3732, Train Loss: 3.0155, Test Loss: 3.0878\n",
      "Epoch 3733, Train Loss: 3.0177, Test Loss: 3.0907\n",
      "Epoch 3734, Train Loss: 3.0237, Test Loss: 3.0900\n",
      "Epoch 3735, Train Loss: 3.0096, Test Loss: 3.0889\n",
      "Epoch 3736, Train Loss: 3.0138, Test Loss: 3.0882\n",
      "Epoch 3737, Train Loss: 3.0204, Test Loss: 3.0886\n",
      "Epoch 3738, Train Loss: 3.0149, Test Loss: 3.0913\n",
      "Epoch 3739, Train Loss: 3.0239, Test Loss: 3.0881\n",
      "Epoch 3740, Train Loss: 3.0095, Test Loss: 3.0870\n",
      "Epoch 3741, Train Loss: 3.0101, Test Loss: 3.0913\n",
      "Epoch 3742, Train Loss: 3.0128, Test Loss: 3.0897\n",
      "Epoch 3743, Train Loss: 3.0082, Test Loss: 3.0861\n",
      "Epoch 3744, Train Loss: 3.0129, Test Loss: 3.0865\n",
      "Epoch 3745, Train Loss: 3.0165, Test Loss: 3.0890\n",
      "Epoch 3746, Train Loss: 3.0159, Test Loss: 3.0926\n",
      "Epoch 3747, Train Loss: 3.0191, Test Loss: 3.0884\n",
      "Epoch 3748, Train Loss: 3.0091, Test Loss: 3.0918\n",
      "Epoch 3749, Train Loss: 3.0199, Test Loss: 3.0886\n",
      "Epoch 3750, Train Loss: 3.0089, Test Loss: 3.0937\n",
      "Epoch 3751, Train Loss: 3.0148, Test Loss: 3.0875\n",
      "Epoch 3752, Train Loss: 3.0325, Test Loss: 3.0895\n",
      "Epoch 3753, Train Loss: 3.0251, Test Loss: 3.0900\n",
      "Epoch 3754, Train Loss: 3.0153, Test Loss: 3.0925\n",
      "Epoch 3755, Train Loss: 3.0162, Test Loss: 3.0896\n",
      "Epoch 3756, Train Loss: 3.0175, Test Loss: 3.0867\n",
      "Epoch 3757, Train Loss: 3.0265, Test Loss: 3.0886\n",
      "Epoch 3758, Train Loss: 3.0113, Test Loss: 3.0866\n",
      "Epoch 3759, Train Loss: 3.0095, Test Loss: 3.0875\n",
      "Epoch 3760, Train Loss: 3.0148, Test Loss: 3.0897\n",
      "Epoch 3761, Train Loss: 3.0170, Test Loss: 3.0905\n",
      "Epoch 3762, Train Loss: 3.0187, Test Loss: 3.0893\n",
      "Epoch 3763, Train Loss: 3.0195, Test Loss: 3.0906\n",
      "Epoch 3764, Train Loss: 3.0110, Test Loss: 3.0909\n",
      "Epoch 3765, Train Loss: 3.0089, Test Loss: 3.0929\n",
      "Epoch 3766, Train Loss: 3.0184, Test Loss: 3.0921\n",
      "Epoch 3767, Train Loss: 3.0117, Test Loss: 3.0894\n",
      "Epoch 3768, Train Loss: 3.0155, Test Loss: 3.0889\n",
      "Epoch 3769, Train Loss: 3.0194, Test Loss: 3.0912\n",
      "Epoch 3770, Train Loss: 3.0192, Test Loss: 3.0896\n",
      "Epoch 3771, Train Loss: 3.0123, Test Loss: 3.0877\n",
      "Epoch 3772, Train Loss: 3.0162, Test Loss: 3.0871\n",
      "Epoch 3773, Train Loss: 3.0110, Test Loss: 3.0911\n",
      "Epoch 3774, Train Loss: 3.0166, Test Loss: 3.0925\n",
      "Epoch 3775, Train Loss: 3.0254, Test Loss: 3.0903\n",
      "Epoch 3776, Train Loss: 3.0312, Test Loss: 3.0888\n",
      "Epoch 3777, Train Loss: 3.0167, Test Loss: 3.0936\n",
      "Epoch 3778, Train Loss: 3.0192, Test Loss: 3.0904\n",
      "Epoch 3779, Train Loss: 3.0181, Test Loss: 3.0951\n",
      "Epoch 3780, Train Loss: 3.0303, Test Loss: 3.0983\n",
      "Epoch 3781, Train Loss: 3.0344, Test Loss: 3.1101\n",
      "Epoch 3782, Train Loss: 3.0303, Test Loss: 3.1017\n",
      "Epoch 3783, Train Loss: 3.0163, Test Loss: 3.0958\n",
      "Epoch 3784, Train Loss: 3.0391, Test Loss: 3.0891\n",
      "Epoch 3785, Train Loss: 3.0087, Test Loss: 3.0979\n",
      "Epoch 3786, Train Loss: 3.0368, Test Loss: 3.0965\n",
      "Epoch 3787, Train Loss: 3.0203, Test Loss: 3.0899\n",
      "Epoch 3788, Train Loss: 3.0177, Test Loss: 3.0896\n",
      "Epoch 3789, Train Loss: 3.0114, Test Loss: 3.0920\n",
      "Epoch 3790, Train Loss: 3.0212, Test Loss: 3.0937\n",
      "Epoch 3791, Train Loss: 3.0183, Test Loss: 3.0912\n",
      "Epoch 3792, Train Loss: 3.0157, Test Loss: 3.0923\n",
      "Epoch 3793, Train Loss: 3.0224, Test Loss: 3.0899\n",
      "Epoch 3794, Train Loss: 3.0200, Test Loss: 3.0923\n",
      "Epoch 3795, Train Loss: 3.0255, Test Loss: 3.0882\n",
      "Epoch 3796, Train Loss: 3.0249, Test Loss: 3.0907\n",
      "Epoch 3797, Train Loss: 3.0220, Test Loss: 3.0894\n",
      "Epoch 3798, Train Loss: 3.0192, Test Loss: 3.0981\n",
      "Epoch 3799, Train Loss: 3.0273, Test Loss: 3.0870\n",
      "Epoch 3800, Train Loss: 3.0280, Test Loss: 3.0884\n",
      "Epoch 3801, Train Loss: 3.0195, Test Loss: 3.0881\n",
      "Epoch 3802, Train Loss: 3.0166, Test Loss: 3.0889\n",
      "Epoch 3803, Train Loss: 3.0108, Test Loss: 3.0863\n",
      "Epoch 3804, Train Loss: 3.0111, Test Loss: 3.0910\n",
      "Epoch 3805, Train Loss: 3.0218, Test Loss: 3.0876\n",
      "Epoch 3806, Train Loss: 3.0120, Test Loss: 3.0893\n",
      "Epoch 3807, Train Loss: 3.0183, Test Loss: 3.0846\n",
      "Epoch 3808, Train Loss: 3.0110, Test Loss: 3.0889\n",
      "Epoch 3809, Train Loss: 3.0218, Test Loss: 3.0961\n",
      "Epoch 3810, Train Loss: 3.0298, Test Loss: 3.0993\n",
      "Epoch 3811, Train Loss: 3.0266, Test Loss: 3.0873\n",
      "Epoch 3812, Train Loss: 3.0116, Test Loss: 3.0953\n",
      "Epoch 3813, Train Loss: 3.0349, Test Loss: 3.0941\n",
      "Epoch 3814, Train Loss: 3.0202, Test Loss: 3.1018\n",
      "Epoch 3815, Train Loss: 3.0325, Test Loss: 3.0954\n",
      "Epoch 3816, Train Loss: 3.0224, Test Loss: 3.0863\n",
      "Epoch 3817, Train Loss: 3.0157, Test Loss: 3.0931\n",
      "Epoch 3818, Train Loss: 3.0161, Test Loss: 3.1019\n",
      "Epoch 3819, Train Loss: 3.0274, Test Loss: 3.0953\n",
      "Epoch 3820, Train Loss: 3.0173, Test Loss: 3.0909\n",
      "Epoch 3821, Train Loss: 3.0359, Test Loss: 3.0932\n",
      "Epoch 3822, Train Loss: 3.0173, Test Loss: 3.0924\n",
      "Epoch 3823, Train Loss: 3.0240, Test Loss: 3.1046\n",
      "Epoch 3824, Train Loss: 3.0192, Test Loss: 3.1021\n",
      "Epoch 3825, Train Loss: 3.0494, Test Loss: 3.0894\n",
      "Epoch 3826, Train Loss: 3.0185, Test Loss: 3.1090\n",
      "Epoch 3827, Train Loss: 3.0552, Test Loss: 3.0995\n",
      "Epoch 3828, Train Loss: 3.0304, Test Loss: 3.1223\n",
      "Epoch 3829, Train Loss: 3.0335, Test Loss: 3.1277\n",
      "Epoch 3830, Train Loss: 3.0663, Test Loss: 3.1058\n",
      "Epoch 3831, Train Loss: 3.0347, Test Loss: 3.1065\n",
      "Epoch 3832, Train Loss: 3.0515, Test Loss: 3.0940\n",
      "Epoch 3833, Train Loss: 3.0371, Test Loss: 3.0965\n",
      "Epoch 3834, Train Loss: 3.0291, Test Loss: 3.1024\n",
      "Epoch 3835, Train Loss: 3.0406, Test Loss: 3.0867\n",
      "Epoch 3836, Train Loss: 3.0218, Test Loss: 3.1068\n",
      "Epoch 3837, Train Loss: 3.0496, Test Loss: 3.0918\n",
      "Epoch 3838, Train Loss: 3.0136, Test Loss: 3.1083\n",
      "Epoch 3839, Train Loss: 3.0372, Test Loss: 3.0965\n",
      "Epoch 3840, Train Loss: 3.0260, Test Loss: 3.0900\n",
      "Epoch 3841, Train Loss: 3.0293, Test Loss: 3.0876\n",
      "Epoch 3842, Train Loss: 3.0192, Test Loss: 3.0891\n",
      "Epoch 3843, Train Loss: 3.0197, Test Loss: 3.0854\n",
      "Epoch 3844, Train Loss: 3.0197, Test Loss: 3.0866\n",
      "Epoch 3845, Train Loss: 3.0179, Test Loss: 3.0869\n",
      "Epoch 3846, Train Loss: 3.0108, Test Loss: 3.0915\n",
      "Epoch 3847, Train Loss: 3.0200, Test Loss: 3.0897\n",
      "Epoch 3848, Train Loss: 3.0290, Test Loss: 3.0882\n",
      "Epoch 3849, Train Loss: 3.0249, Test Loss: 3.0860\n",
      "Epoch 3850, Train Loss: 3.0101, Test Loss: 3.0874\n",
      "Epoch 3851, Train Loss: 3.0197, Test Loss: 3.0871\n",
      "Epoch 3852, Train Loss: 3.0144, Test Loss: 3.0856\n",
      "Epoch 3853, Train Loss: 3.0092, Test Loss: 3.0848\n",
      "Epoch 3854, Train Loss: 3.0160, Test Loss: 3.0851\n",
      "Epoch 3855, Train Loss: 3.0060, Test Loss: 3.0875\n",
      "Epoch 3856, Train Loss: 3.0107, Test Loss: 3.0895\n",
      "Epoch 3857, Train Loss: 3.0202, Test Loss: 3.0884\n",
      "Epoch 3858, Train Loss: 3.0200, Test Loss: 3.0875\n",
      "Epoch 3859, Train Loss: 3.0085, Test Loss: 3.0894\n",
      "Epoch 3860, Train Loss: 3.0214, Test Loss: 3.0876\n",
      "Epoch 3861, Train Loss: 3.0104, Test Loss: 3.0861\n",
      "Epoch 3862, Train Loss: 3.0200, Test Loss: 3.0871\n",
      "Epoch 3863, Train Loss: 3.0222, Test Loss: 3.0884\n",
      "Epoch 3864, Train Loss: 3.0161, Test Loss: 3.0905\n",
      "Epoch 3865, Train Loss: 3.0141, Test Loss: 3.0853\n",
      "Epoch 3866, Train Loss: 3.0039, Test Loss: 3.0844\n",
      "Epoch 3867, Train Loss: 3.0130, Test Loss: 3.0843\n",
      "Epoch 3868, Train Loss: 3.0144, Test Loss: 3.0871\n",
      "Epoch 3869, Train Loss: 3.0079, Test Loss: 3.0882\n",
      "Epoch 3870, Train Loss: 3.0137, Test Loss: 3.0841\n",
      "Epoch 3871, Train Loss: 3.0151, Test Loss: 3.0832\n",
      "Epoch 3872, Train Loss: 3.0146, Test Loss: 3.0865\n",
      "Epoch 3873, Train Loss: 3.0125, Test Loss: 3.0868\n",
      "Epoch 3874, Train Loss: 3.0121, Test Loss: 3.0855\n",
      "Epoch 3875, Train Loss: 3.0225, Test Loss: 3.0859\n",
      "Epoch 3876, Train Loss: 3.0111, Test Loss: 3.0868\n",
      "Epoch 3877, Train Loss: 3.0175, Test Loss: 3.0875\n",
      "Epoch 3878, Train Loss: 3.0264, Test Loss: 3.0850\n",
      "Epoch 3879, Train Loss: 3.0128, Test Loss: 3.0861\n",
      "Epoch 3880, Train Loss: 3.0233, Test Loss: 3.0866\n",
      "Epoch 3881, Train Loss: 3.0188, Test Loss: 3.0865\n",
      "Epoch 3882, Train Loss: 3.0230, Test Loss: 3.0870\n",
      "Epoch 3883, Train Loss: 3.0082, Test Loss: 3.0852\n",
      "Epoch 3884, Train Loss: 3.0156, Test Loss: 3.0848\n",
      "Epoch 3885, Train Loss: 3.0226, Test Loss: 3.0875\n",
      "Epoch 3886, Train Loss: 3.0157, Test Loss: 3.0940\n",
      "Epoch 3887, Train Loss: 3.0184, Test Loss: 3.0887\n",
      "Epoch 3888, Train Loss: 3.0203, Test Loss: 3.0899\n",
      "Epoch 3889, Train Loss: 3.0118, Test Loss: 3.0910\n",
      "Epoch 3890, Train Loss: 3.0205, Test Loss: 3.0918\n",
      "Epoch 3891, Train Loss: 3.0158, Test Loss: 3.0851\n",
      "Epoch 3892, Train Loss: 3.0214, Test Loss: 3.0856\n",
      "Epoch 3893, Train Loss: 3.0059, Test Loss: 3.0855\n",
      "Epoch 3894, Train Loss: 3.0141, Test Loss: 3.0872\n",
      "Epoch 3895, Train Loss: 3.0153, Test Loss: 3.0843\n",
      "Epoch 3896, Train Loss: 3.0185, Test Loss: 3.0865\n",
      "Epoch 3897, Train Loss: 3.0165, Test Loss: 3.0930\n",
      "Epoch 3898, Train Loss: 3.0242, Test Loss: 3.0949\n",
      "Epoch 3899, Train Loss: 3.0170, Test Loss: 3.0863\n",
      "Epoch 3900, Train Loss: 3.0105, Test Loss: 3.0994\n",
      "Epoch 3901, Train Loss: 3.0300, Test Loss: 3.1050\n",
      "Epoch 3902, Train Loss: 3.0238, Test Loss: 3.1067\n",
      "Epoch 3903, Train Loss: 3.0250, Test Loss: 3.1037\n",
      "Epoch 3904, Train Loss: 3.0230, Test Loss: 3.0883\n",
      "Epoch 3905, Train Loss: 3.0194, Test Loss: 3.0957\n",
      "Epoch 3906, Train Loss: 3.0403, Test Loss: 3.1046\n",
      "Epoch 3907, Train Loss: 3.0187, Test Loss: 3.0991\n",
      "Epoch 3908, Train Loss: 3.0249, Test Loss: 3.0937\n",
      "Epoch 3909, Train Loss: 3.0310, Test Loss: 3.0959\n",
      "Epoch 3910, Train Loss: 3.0355, Test Loss: 3.0974\n",
      "Epoch 3911, Train Loss: 3.0148, Test Loss: 3.1070\n",
      "Epoch 3912, Train Loss: 3.0374, Test Loss: 3.0894\n",
      "Epoch 3913, Train Loss: 3.0118, Test Loss: 3.0918\n",
      "Epoch 3914, Train Loss: 3.0305, Test Loss: 3.0904\n",
      "Epoch 3915, Train Loss: 3.0209, Test Loss: 3.0987\n",
      "Epoch 3916, Train Loss: 3.0408, Test Loss: 3.0860\n",
      "Epoch 3917, Train Loss: 3.0199, Test Loss: 3.0846\n",
      "Epoch 3918, Train Loss: 3.0131, Test Loss: 3.0848\n",
      "Epoch 3919, Train Loss: 3.0164, Test Loss: 3.0837\n",
      "Epoch 3920, Train Loss: 3.0119, Test Loss: 3.0885\n",
      "Epoch 3921, Train Loss: 3.0175, Test Loss: 3.0865\n",
      "Epoch 3922, Train Loss: 3.0109, Test Loss: 3.0863\n",
      "Epoch 3923, Train Loss: 3.0245, Test Loss: 3.0866\n",
      "Epoch 3924, Train Loss: 3.0215, Test Loss: 3.0898\n",
      "Epoch 3925, Train Loss: 3.0115, Test Loss: 3.0909\n",
      "Epoch 3926, Train Loss: 3.0215, Test Loss: 3.0881\n",
      "Epoch 3927, Train Loss: 3.0210, Test Loss: 3.0857\n",
      "Epoch 3928, Train Loss: 3.0200, Test Loss: 3.0861\n",
      "Epoch 3929, Train Loss: 3.0167, Test Loss: 3.0882\n",
      "Epoch 3930, Train Loss: 3.0168, Test Loss: 3.0840\n",
      "Epoch 3931, Train Loss: 3.0069, Test Loss: 3.0861\n",
      "Epoch 3932, Train Loss: 3.0191, Test Loss: 3.0876\n",
      "Epoch 3933, Train Loss: 3.0217, Test Loss: 3.0911\n",
      "Epoch 3934, Train Loss: 3.0282, Test Loss: 3.0880\n",
      "Epoch 3935, Train Loss: 3.0166, Test Loss: 3.0896\n",
      "Epoch 3936, Train Loss: 3.0210, Test Loss: 3.0911\n",
      "Epoch 3937, Train Loss: 3.0161, Test Loss: 3.0911\n",
      "Epoch 3938, Train Loss: 3.0143, Test Loss: 3.0983\n",
      "Epoch 3939, Train Loss: 3.0315, Test Loss: 3.0954\n",
      "Epoch 3940, Train Loss: 3.0174, Test Loss: 3.0865\n",
      "Epoch 3941, Train Loss: 3.0292, Test Loss: 3.0882\n",
      "Epoch 3942, Train Loss: 3.0267, Test Loss: 3.0868\n",
      "Epoch 3943, Train Loss: 3.0217, Test Loss: 3.1013\n",
      "Epoch 3944, Train Loss: 3.0233, Test Loss: 3.1056\n",
      "Epoch 3945, Train Loss: 3.0252, Test Loss: 3.0981\n",
      "Epoch 3946, Train Loss: 3.0192, Test Loss: 3.0927\n",
      "Epoch 3947, Train Loss: 3.0265, Test Loss: 3.0969\n",
      "Epoch 3948, Train Loss: 3.0332, Test Loss: 3.1014\n",
      "Epoch 3949, Train Loss: 3.0274, Test Loss: 3.0909\n",
      "Epoch 3950, Train Loss: 3.0164, Test Loss: 3.0877\n",
      "Epoch 3951, Train Loss: 3.0171, Test Loss: 3.0873\n",
      "Epoch 3952, Train Loss: 3.0149, Test Loss: 3.0963\n",
      "Epoch 3953, Train Loss: 3.0243, Test Loss: 3.0868\n",
      "Epoch 3954, Train Loss: 3.0136, Test Loss: 3.0853\n",
      "Epoch 3955, Train Loss: 3.0213, Test Loss: 3.0841\n",
      "Epoch 3956, Train Loss: 3.0150, Test Loss: 3.0945\n",
      "Epoch 3957, Train Loss: 3.0211, Test Loss: 3.0889\n",
      "Epoch 3958, Train Loss: 3.0213, Test Loss: 3.0877\n",
      "Epoch 3959, Train Loss: 3.0320, Test Loss: 3.0913\n",
      "Epoch 3960, Train Loss: 3.0326, Test Loss: 3.1009\n",
      "Epoch 3961, Train Loss: 3.0306, Test Loss: 3.0942\n",
      "Epoch 3962, Train Loss: 3.0228, Test Loss: 3.1014\n",
      "Epoch 3963, Train Loss: 3.0400, Test Loss: 3.0905\n",
      "Epoch 3964, Train Loss: 3.0215, Test Loss: 3.0966\n",
      "Epoch 3965, Train Loss: 3.0244, Test Loss: 3.1004\n",
      "Epoch 3966, Train Loss: 3.0356, Test Loss: 3.1010\n",
      "Epoch 3967, Train Loss: 3.0409, Test Loss: 3.0891\n",
      "Epoch 3968, Train Loss: 3.0351, Test Loss: 3.0950\n",
      "Epoch 3969, Train Loss: 3.0228, Test Loss: 3.1025\n",
      "Epoch 3970, Train Loss: 3.0357, Test Loss: 3.0872\n",
      "Epoch 3971, Train Loss: 3.0228, Test Loss: 3.0886\n",
      "Epoch 3972, Train Loss: 3.0233, Test Loss: 3.0872\n",
      "Epoch 3973, Train Loss: 3.0171, Test Loss: 3.0942\n",
      "Epoch 3974, Train Loss: 3.0110, Test Loss: 3.0927\n",
      "Epoch 3975, Train Loss: 3.0146, Test Loss: 3.0855\n",
      "Epoch 3976, Train Loss: 3.0147, Test Loss: 3.0875\n",
      "Epoch 3977, Train Loss: 3.0238, Test Loss: 3.0889\n",
      "Epoch 3978, Train Loss: 3.0110, Test Loss: 3.0903\n",
      "Epoch 3979, Train Loss: 3.0139, Test Loss: 3.0980\n",
      "Epoch 3980, Train Loss: 3.0123, Test Loss: 3.0953\n",
      "Epoch 3981, Train Loss: 3.0152, Test Loss: 3.0903\n",
      "Epoch 3982, Train Loss: 3.0190, Test Loss: 3.0888\n",
      "Epoch 3983, Train Loss: 3.0197, Test Loss: 3.0884\n",
      "Epoch 3984, Train Loss: 3.0117, Test Loss: 3.0978\n",
      "Epoch 3985, Train Loss: 3.0244, Test Loss: 3.0875\n",
      "Epoch 3986, Train Loss: 3.0176, Test Loss: 3.0889\n",
      "Epoch 3987, Train Loss: 3.0190, Test Loss: 3.0910\n",
      "Epoch 3988, Train Loss: 3.0181, Test Loss: 3.0928\n",
      "Epoch 3989, Train Loss: 3.0184, Test Loss: 3.0901\n",
      "Epoch 3990, Train Loss: 3.0154, Test Loss: 3.0867\n",
      "Epoch 3991, Train Loss: 3.0140, Test Loss: 3.0883\n",
      "Epoch 3992, Train Loss: 3.0238, Test Loss: 3.0854\n",
      "Epoch 3993, Train Loss: 3.0282, Test Loss: 3.0898\n",
      "Epoch 3994, Train Loss: 3.0171, Test Loss: 3.0921\n",
      "Epoch 3995, Train Loss: 3.0212, Test Loss: 3.0928\n",
      "Epoch 3996, Train Loss: 3.0244, Test Loss: 3.0869\n",
      "Epoch 3997, Train Loss: 3.0106, Test Loss: 3.0914\n",
      "Epoch 3998, Train Loss: 3.0190, Test Loss: 3.0990\n",
      "Epoch 3999, Train Loss: 3.0191, Test Loss: 3.0934\n",
      "Epoch 4000, Train Loss: 3.0205, Test Loss: 3.0867\n",
      "Epoch 4001, Train Loss: 3.0159, Test Loss: 3.0904\n",
      "Epoch 4002, Train Loss: 3.0215, Test Loss: 3.0995\n",
      "Epoch 4003, Train Loss: 3.0223, Test Loss: 3.0885\n",
      "Epoch 4004, Train Loss: 3.0234, Test Loss: 3.0878\n",
      "Epoch 4005, Train Loss: 3.0204, Test Loss: 3.0934\n",
      "Epoch 4006, Train Loss: 3.0327, Test Loss: 3.0991\n",
      "Epoch 4007, Train Loss: 3.0171, Test Loss: 3.1051\n",
      "Epoch 4008, Train Loss: 3.0252, Test Loss: 3.0942\n",
      "Epoch 4009, Train Loss: 3.0267, Test Loss: 3.0904\n",
      "Epoch 4010, Train Loss: 3.0388, Test Loss: 3.0853\n",
      "Epoch 4011, Train Loss: 3.0106, Test Loss: 3.1004\n",
      "Epoch 4012, Train Loss: 3.0404, Test Loss: 3.0958\n",
      "Epoch 4013, Train Loss: 3.0274, Test Loss: 3.0870\n",
      "Epoch 4014, Train Loss: 3.0266, Test Loss: 3.0874\n",
      "Epoch 4015, Train Loss: 3.0166, Test Loss: 3.0906\n",
      "Epoch 4016, Train Loss: 3.0142, Test Loss: 3.0939\n",
      "Epoch 4017, Train Loss: 3.0154, Test Loss: 3.0848\n",
      "Epoch 4018, Train Loss: 3.0200, Test Loss: 3.0831\n",
      "Epoch 4019, Train Loss: 3.0174, Test Loss: 3.0901\n",
      "Epoch 4020, Train Loss: 3.0272, Test Loss: 3.0883\n",
      "Epoch 4021, Train Loss: 3.0179, Test Loss: 3.0860\n",
      "Epoch 4022, Train Loss: 3.0150, Test Loss: 3.0862\n",
      "Epoch 4023, Train Loss: 3.0126, Test Loss: 3.0900\n",
      "Epoch 4024, Train Loss: 3.0107, Test Loss: 3.0880\n",
      "Epoch 4025, Train Loss: 3.0140, Test Loss: 3.0861\n",
      "Epoch 4026, Train Loss: 3.0109, Test Loss: 3.0858\n",
      "Epoch 4027, Train Loss: 3.0193, Test Loss: 3.0856\n",
      "Epoch 4028, Train Loss: 3.0169, Test Loss: 3.0848\n",
      "Epoch 4029, Train Loss: 3.0148, Test Loss: 3.0905\n",
      "Epoch 4030, Train Loss: 3.0186, Test Loss: 3.0852\n",
      "Epoch 4031, Train Loss: 3.0126, Test Loss: 3.0878\n",
      "Epoch 4032, Train Loss: 3.0216, Test Loss: 3.0866\n",
      "Epoch 4033, Train Loss: 3.0142, Test Loss: 3.0912\n",
      "Epoch 4034, Train Loss: 3.0255, Test Loss: 3.0879\n",
      "Epoch 4035, Train Loss: 3.0201, Test Loss: 3.0873\n",
      "Epoch 4036, Train Loss: 3.0138, Test Loss: 3.0829\n",
      "Epoch 4037, Train Loss: 3.0183, Test Loss: 3.0862\n",
      "Epoch 4038, Train Loss: 3.0275, Test Loss: 3.0978\n",
      "Epoch 4039, Train Loss: 3.0217, Test Loss: 3.0967\n",
      "Epoch 4040, Train Loss: 3.0261, Test Loss: 3.0906\n",
      "Epoch 4041, Train Loss: 3.0297, Test Loss: 3.1002\n",
      "Epoch 4042, Train Loss: 3.0448, Test Loss: 3.1199\n",
      "Epoch 4043, Train Loss: 3.0521, Test Loss: 3.1135\n",
      "Epoch 4044, Train Loss: 3.0493, Test Loss: 3.0991\n",
      "Epoch 4045, Train Loss: 3.0369, Test Loss: 3.1031\n",
      "Epoch 4046, Train Loss: 3.0510, Test Loss: 3.0890\n",
      "Epoch 4047, Train Loss: 3.0138, Test Loss: 3.1203\n",
      "Epoch 4048, Train Loss: 3.0471, Test Loss: 3.1004\n",
      "Epoch 4049, Train Loss: 3.0415, Test Loss: 3.0835\n",
      "Epoch 4050, Train Loss: 3.0156, Test Loss: 3.1001\n",
      "Epoch 4051, Train Loss: 3.0544, Test Loss: 3.1139\n",
      "Epoch 4052, Train Loss: 3.0399, Test Loss: 3.1094\n",
      "Epoch 4053, Train Loss: 3.0340, Test Loss: 3.0987\n",
      "Epoch 4054, Train Loss: 3.0297, Test Loss: 3.0996\n",
      "Epoch 4055, Train Loss: 3.0447, Test Loss: 3.0947\n",
      "Epoch 4056, Train Loss: 3.0116, Test Loss: 3.1000\n",
      "Epoch 4057, Train Loss: 3.0224, Test Loss: 3.0904\n",
      "Epoch 4058, Train Loss: 3.0199, Test Loss: 3.0919\n",
      "Epoch 4059, Train Loss: 3.0240, Test Loss: 3.0928\n",
      "Epoch 4060, Train Loss: 3.0202, Test Loss: 3.0910\n",
      "Epoch 4061, Train Loss: 3.0137, Test Loss: 3.1097\n",
      "Epoch 4062, Train Loss: 3.0330, Test Loss: 3.0862\n",
      "Epoch 4063, Train Loss: 3.0182, Test Loss: 3.0930\n",
      "Epoch 4064, Train Loss: 3.0350, Test Loss: 3.0871\n",
      "Epoch 4065, Train Loss: 3.0177, Test Loss: 3.1087\n",
      "Epoch 4066, Train Loss: 3.0318, Test Loss: 3.0968\n",
      "Epoch 4067, Train Loss: 3.0199, Test Loss: 3.0910\n",
      "Epoch 4068, Train Loss: 3.0233, Test Loss: 3.0931\n",
      "Epoch 4069, Train Loss: 3.0204, Test Loss: 3.0949\n",
      "Epoch 4070, Train Loss: 3.0218, Test Loss: 3.1002\n",
      "Epoch 4071, Train Loss: 3.0264, Test Loss: 3.0971\n",
      "Epoch 4072, Train Loss: 3.0387, Test Loss: 3.1002\n",
      "Epoch 4073, Train Loss: 3.0298, Test Loss: 3.1071\n",
      "Epoch 4074, Train Loss: 3.0399, Test Loss: 3.0991\n",
      "Epoch 4075, Train Loss: 3.0267, Test Loss: 3.1047\n",
      "Epoch 4076, Train Loss: 3.0278, Test Loss: 3.1068\n",
      "Epoch 4077, Train Loss: 3.0385, Test Loss: 3.1091\n",
      "Epoch 4078, Train Loss: 3.0310, Test Loss: 3.1096\n",
      "Epoch 4079, Train Loss: 3.0359, Test Loss: 3.1015\n",
      "Epoch 4080, Train Loss: 3.0362, Test Loss: 3.1085\n",
      "Epoch 4081, Train Loss: 3.0488, Test Loss: 3.0853\n",
      "Epoch 4082, Train Loss: 3.0115, Test Loss: 3.1006\n",
      "Epoch 4083, Train Loss: 3.0318, Test Loss: 3.1027\n",
      "Epoch 4084, Train Loss: 3.0320, Test Loss: 3.0899\n",
      "Epoch 4085, Train Loss: 3.0091, Test Loss: 3.1067\n",
      "Epoch 4086, Train Loss: 3.0509, Test Loss: 3.0992\n",
      "Epoch 4087, Train Loss: 3.0173, Test Loss: 3.1194\n",
      "Epoch 4088, Train Loss: 3.0303, Test Loss: 3.1088\n",
      "Epoch 4089, Train Loss: 3.0239, Test Loss: 3.0878\n",
      "Epoch 4090, Train Loss: 3.0094, Test Loss: 3.1182\n",
      "Epoch 4091, Train Loss: 3.0648, Test Loss: 3.1117\n",
      "Epoch 4092, Train Loss: 3.0390, Test Loss: 3.1208\n",
      "Epoch 4093, Train Loss: 3.0458, Test Loss: 3.1190\n",
      "Epoch 4094, Train Loss: 3.0402, Test Loss: 3.0957\n",
      "Epoch 4095, Train Loss: 3.0338, Test Loss: 3.1013\n",
      "Epoch 4096, Train Loss: 3.0442, Test Loss: 3.1153\n",
      "Epoch 4097, Train Loss: 3.0526, Test Loss: 3.0875\n",
      "Epoch 4098, Train Loss: 3.0213, Test Loss: 3.0864\n",
      "Epoch 4099, Train Loss: 3.0206, Test Loss: 3.0881\n",
      "Epoch 4100, Train Loss: 3.0149, Test Loss: 3.1031\n",
      "Epoch 4101, Train Loss: 3.0345, Test Loss: 3.0870\n",
      "Epoch 4102, Train Loss: 3.0058, Test Loss: 3.0896\n",
      "Epoch 4103, Train Loss: 3.0196, Test Loss: 3.0840\n",
      "Epoch 4104, Train Loss: 3.0206, Test Loss: 3.0997\n",
      "Epoch 4105, Train Loss: 3.0296, Test Loss: 3.0905\n",
      "Epoch 4106, Train Loss: 3.0136, Test Loss: 3.0866\n",
      "Epoch 4107, Train Loss: 3.0168, Test Loss: 3.0844\n",
      "Epoch 4108, Train Loss: 3.0191, Test Loss: 3.0855\n",
      "Epoch 4109, Train Loss: 3.0141, Test Loss: 3.0857\n",
      "Epoch 4110, Train Loss: 3.0104, Test Loss: 3.0872\n",
      "Epoch 4111, Train Loss: 3.0170, Test Loss: 3.0834\n",
      "Epoch 4112, Train Loss: 3.0122, Test Loss: 3.0860\n",
      "Epoch 4113, Train Loss: 3.0198, Test Loss: 3.0834\n",
      "Epoch 4114, Train Loss: 3.0105, Test Loss: 3.0888\n",
      "Epoch 4115, Train Loss: 3.0221, Test Loss: 3.0898\n",
      "Epoch 4116, Train Loss: 3.0110, Test Loss: 3.0923\n",
      "Epoch 4117, Train Loss: 3.0174, Test Loss: 3.0888\n",
      "Epoch 4118, Train Loss: 3.0117, Test Loss: 3.0988\n",
      "Epoch 4119, Train Loss: 3.0159, Test Loss: 3.0935\n",
      "Epoch 4120, Train Loss: 3.0157, Test Loss: 3.0935\n",
      "Epoch 4121, Train Loss: 3.0235, Test Loss: 3.0869\n",
      "Epoch 4122, Train Loss: 3.0140, Test Loss: 3.0886\n",
      "Epoch 4123, Train Loss: 3.0209, Test Loss: 3.0860\n",
      "Epoch 4124, Train Loss: 3.0185, Test Loss: 3.0837\n",
      "Epoch 4125, Train Loss: 3.0129, Test Loss: 3.0822\n",
      "Epoch 4126, Train Loss: 3.0104, Test Loss: 3.0855\n",
      "Epoch 4127, Train Loss: 3.0152, Test Loss: 3.0901\n",
      "Epoch 4128, Train Loss: 3.0088, Test Loss: 3.0864\n",
      "Epoch 4129, Train Loss: 3.0218, Test Loss: 3.0849\n",
      "Epoch 4130, Train Loss: 3.0214, Test Loss: 3.0839\n",
      "Epoch 4131, Train Loss: 3.0123, Test Loss: 3.0868\n",
      "Epoch 4132, Train Loss: 3.0118, Test Loss: 3.0861\n",
      "Epoch 4133, Train Loss: 3.0210, Test Loss: 3.0853\n",
      "Epoch 4134, Train Loss: 3.0135, Test Loss: 3.0862\n",
      "Epoch 4135, Train Loss: 3.0097, Test Loss: 3.0872\n",
      "Epoch 4136, Train Loss: 3.0053, Test Loss: 3.0880\n",
      "Epoch 4137, Train Loss: 3.0167, Test Loss: 3.0885\n",
      "Epoch 4138, Train Loss: 3.0114, Test Loss: 3.0903\n",
      "Epoch 4139, Train Loss: 3.0116, Test Loss: 3.0844\n",
      "Epoch 4140, Train Loss: 3.0099, Test Loss: 3.0832\n",
      "Epoch 4141, Train Loss: 3.0134, Test Loss: 3.0868\n",
      "Epoch 4142, Train Loss: 3.0167, Test Loss: 3.0839\n",
      "Epoch 4143, Train Loss: 3.0012, Test Loss: 3.0878\n",
      "Epoch 4144, Train Loss: 3.0128, Test Loss: 3.0947\n",
      "Epoch 4145, Train Loss: 3.0141, Test Loss: 3.0909\n",
      "Epoch 4146, Train Loss: 3.0223, Test Loss: 3.0862\n",
      "Epoch 4147, Train Loss: 3.0121, Test Loss: 3.0860\n",
      "Epoch 4148, Train Loss: 3.0201, Test Loss: 3.1006\n",
      "Epoch 4149, Train Loss: 3.0305, Test Loss: 3.0972\n",
      "Epoch 4150, Train Loss: 3.0370, Test Loss: 3.1014\n",
      "Epoch 4151, Train Loss: 3.0466, Test Loss: 3.1040\n",
      "Epoch 4152, Train Loss: 3.0567, Test Loss: 3.0985\n",
      "Epoch 4153, Train Loss: 3.0282, Test Loss: 3.1153\n",
      "Epoch 4154, Train Loss: 3.0355, Test Loss: 3.1047\n",
      "Epoch 4155, Train Loss: 3.0294, Test Loss: 3.1123\n",
      "Epoch 4156, Train Loss: 3.0631, Test Loss: 3.1119\n",
      "Epoch 4157, Train Loss: 3.0383, Test Loss: 3.1288\n",
      "Epoch 4158, Train Loss: 3.0546, Test Loss: 3.1222\n",
      "Epoch 4159, Train Loss: 3.0584, Test Loss: 3.0915\n",
      "Epoch 4160, Train Loss: 3.0253, Test Loss: 3.1275\n",
      "Epoch 4161, Train Loss: 3.0795, Test Loss: 3.1293\n",
      "Epoch 4162, Train Loss: 3.0446, Test Loss: 3.1584\n",
      "Epoch 4163, Train Loss: 3.0859, Test Loss: 3.1379\n",
      "Epoch 4164, Train Loss: 3.0767, Test Loss: 3.1092\n",
      "Epoch 4165, Train Loss: 3.0373, Test Loss: 3.1079\n",
      "Epoch 4166, Train Loss: 3.0482, Test Loss: 3.1239\n",
      "Epoch 4167, Train Loss: 3.0565, Test Loss: 3.1205\n",
      "Epoch 4168, Train Loss: 3.0605, Test Loss: 3.0922\n",
      "Epoch 4169, Train Loss: 3.0148, Test Loss: 3.1188\n",
      "Epoch 4170, Train Loss: 3.0503, Test Loss: 3.1230\n",
      "Epoch 4171, Train Loss: 3.0505, Test Loss: 3.1189\n",
      "Epoch 4172, Train Loss: 3.0448, Test Loss: 3.0921\n",
      "Epoch 4173, Train Loss: 3.0217, Test Loss: 3.0916\n",
      "Epoch 4174, Train Loss: 3.0246, Test Loss: 3.0992\n",
      "Epoch 4175, Train Loss: 3.0488, Test Loss: 3.0954\n",
      "Epoch 4176, Train Loss: 3.0214, Test Loss: 3.1089\n",
      "Epoch 4177, Train Loss: 3.0235, Test Loss: 3.1151\n",
      "Epoch 4178, Train Loss: 3.0306, Test Loss: 3.1106\n",
      "Epoch 4179, Train Loss: 3.0498, Test Loss: 3.1040\n",
      "Epoch 4180, Train Loss: 3.0212, Test Loss: 3.0875\n",
      "Epoch 4181, Train Loss: 3.0330, Test Loss: 3.0915\n",
      "Epoch 4182, Train Loss: 3.0348, Test Loss: 3.0958\n",
      "Epoch 4183, Train Loss: 3.0293, Test Loss: 3.0907\n",
      "Epoch 4184, Train Loss: 3.0310, Test Loss: 3.1054\n",
      "Epoch 4185, Train Loss: 3.0388, Test Loss: 3.1131\n",
      "Epoch 4186, Train Loss: 3.0314, Test Loss: 3.0975\n",
      "Epoch 4187, Train Loss: 3.0195, Test Loss: 3.1140\n",
      "Epoch 4188, Train Loss: 3.0551, Test Loss: 3.1065\n",
      "Epoch 4189, Train Loss: 3.0248, Test Loss: 3.1146\n",
      "Epoch 4190, Train Loss: 3.0449, Test Loss: 3.0899\n",
      "Epoch 4191, Train Loss: 3.0128, Test Loss: 3.0960\n",
      "Epoch 4192, Train Loss: 3.0403, Test Loss: 3.0921\n",
      "Epoch 4193, Train Loss: 3.0288, Test Loss: 3.1009\n",
      "Epoch 4194, Train Loss: 3.0207, Test Loss: 3.0978\n",
      "Epoch 4195, Train Loss: 3.0348, Test Loss: 3.0958\n",
      "Epoch 4196, Train Loss: 3.0191, Test Loss: 3.0864\n",
      "Epoch 4197, Train Loss: 3.0179, Test Loss: 3.0921\n",
      "Epoch 4198, Train Loss: 3.0192, Test Loss: 3.1048\n",
      "Epoch 4199, Train Loss: 3.0327, Test Loss: 3.0970\n",
      "Epoch 4200, Train Loss: 3.0309, Test Loss: 3.0887\n",
      "Epoch 4201, Train Loss: 3.0221, Test Loss: 3.0964\n",
      "Epoch 4202, Train Loss: 3.0357, Test Loss: 3.0890\n",
      "Epoch 4203, Train Loss: 3.0134, Test Loss: 3.1004\n",
      "Epoch 4204, Train Loss: 3.0380, Test Loss: 3.0894\n",
      "Epoch 4205, Train Loss: 3.0178, Test Loss: 3.0942\n",
      "Epoch 4206, Train Loss: 3.0330, Test Loss: 3.0849\n",
      "Epoch 4207, Train Loss: 3.0130, Test Loss: 3.0993\n",
      "Epoch 4208, Train Loss: 3.0235, Test Loss: 3.1002\n",
      "Epoch 4209, Train Loss: 3.0253, Test Loss: 3.0988\n",
      "Epoch 4210, Train Loss: 3.0208, Test Loss: 3.0955\n",
      "Epoch 4211, Train Loss: 3.0410, Test Loss: 3.0963\n",
      "Epoch 4212, Train Loss: 3.0448, Test Loss: 3.0938\n",
      "Epoch 4213, Train Loss: 3.0315, Test Loss: 3.1062\n",
      "Epoch 4214, Train Loss: 3.0293, Test Loss: 3.0864\n",
      "Epoch 4215, Train Loss: 3.0178, Test Loss: 3.0936\n",
      "Epoch 4216, Train Loss: 3.0415, Test Loss: 3.0865\n",
      "Epoch 4217, Train Loss: 3.0158, Test Loss: 3.1002\n",
      "Epoch 4218, Train Loss: 3.0394, Test Loss: 3.0929\n",
      "Epoch 4219, Train Loss: 3.0231, Test Loss: 3.0857\n",
      "Epoch 4220, Train Loss: 3.0155, Test Loss: 3.0973\n",
      "Epoch 4221, Train Loss: 3.0239, Test Loss: 3.1039\n",
      "Epoch 4222, Train Loss: 3.0308, Test Loss: 3.0951\n",
      "Epoch 4223, Train Loss: 3.0273, Test Loss: 3.0901\n",
      "Epoch 4224, Train Loss: 3.0336, Test Loss: 3.0987\n",
      "Epoch 4225, Train Loss: 3.0224, Test Loss: 3.0928\n",
      "Epoch 4226, Train Loss: 3.0182, Test Loss: 3.0992\n",
      "Epoch 4227, Train Loss: 3.0410, Test Loss: 3.0969\n",
      "Epoch 4228, Train Loss: 3.0291, Test Loss: 3.1018\n",
      "Epoch 4229, Train Loss: 3.0213, Test Loss: 3.0934\n",
      "Epoch 4230, Train Loss: 3.0174, Test Loss: 3.0995\n",
      "Epoch 4231, Train Loss: 3.0460, Test Loss: 3.1146\n",
      "Epoch 4232, Train Loss: 3.0350, Test Loss: 3.1074\n",
      "Epoch 4233, Train Loss: 3.0570, Test Loss: 3.1005\n",
      "Epoch 4234, Train Loss: 3.0295, Test Loss: 3.1056\n",
      "Epoch 4235, Train Loss: 3.0369, Test Loss: 3.0996\n",
      "Epoch 4236, Train Loss: 3.0399, Test Loss: 3.0888\n",
      "Epoch 4237, Train Loss: 3.0195, Test Loss: 3.0831\n",
      "Epoch 4238, Train Loss: 3.0156, Test Loss: 3.0871\n",
      "Epoch 4239, Train Loss: 3.0251, Test Loss: 3.0847\n",
      "Epoch 4240, Train Loss: 3.0127, Test Loss: 3.0879\n",
      "Epoch 4241, Train Loss: 3.0173, Test Loss: 3.0887\n",
      "Epoch 4242, Train Loss: 3.0233, Test Loss: 3.0858\n",
      "Epoch 4243, Train Loss: 3.0090, Test Loss: 3.0976\n",
      "Epoch 4244, Train Loss: 3.0370, Test Loss: 3.0897\n",
      "Epoch 4245, Train Loss: 3.0141, Test Loss: 3.0834\n",
      "Epoch 4246, Train Loss: 3.0138, Test Loss: 3.0911\n",
      "Epoch 4247, Train Loss: 3.0210, Test Loss: 3.0984\n",
      "Epoch 4248, Train Loss: 3.0277, Test Loss: 3.0896\n",
      "Epoch 4249, Train Loss: 3.0243, Test Loss: 3.0813\n",
      "Epoch 4250, Train Loss: 3.0217, Test Loss: 3.0863\n",
      "Epoch 4251, Train Loss: 3.0189, Test Loss: 3.0836\n",
      "Epoch 4252, Train Loss: 3.0072, Test Loss: 3.0884\n",
      "Epoch 4253, Train Loss: 3.0172, Test Loss: 3.0885\n",
      "Epoch 4254, Train Loss: 3.0125, Test Loss: 3.0856\n",
      "Epoch 4255, Train Loss: 3.0273, Test Loss: 3.0832\n",
      "Epoch 4256, Train Loss: 3.0105, Test Loss: 3.0933\n",
      "Epoch 4257, Train Loss: 3.0185, Test Loss: 3.0876\n",
      "Epoch 4258, Train Loss: 3.0099, Test Loss: 3.0842\n",
      "Epoch 4259, Train Loss: 3.0113, Test Loss: 3.0862\n",
      "Epoch 4260, Train Loss: 3.0095, Test Loss: 3.0899\n",
      "Epoch 4261, Train Loss: 3.0284, Test Loss: 3.0814\n",
      "Epoch 4262, Train Loss: 3.0090, Test Loss: 3.0830\n",
      "Epoch 4263, Train Loss: 3.0191, Test Loss: 3.0832\n",
      "Epoch 4264, Train Loss: 3.0123, Test Loss: 3.0875\n",
      "Epoch 4265, Train Loss: 3.0205, Test Loss: 3.0869\n",
      "Epoch 4266, Train Loss: 3.0174, Test Loss: 3.0856\n",
      "Epoch 4267, Train Loss: 3.0174, Test Loss: 3.0958\n",
      "Epoch 4268, Train Loss: 3.0251, Test Loss: 3.0887\n",
      "Epoch 4269, Train Loss: 3.0133, Test Loss: 3.0872\n",
      "Epoch 4270, Train Loss: 3.0224, Test Loss: 3.0889\n",
      "Epoch 4271, Train Loss: 3.0126, Test Loss: 3.0928\n",
      "Epoch 4272, Train Loss: 3.0288, Test Loss: 3.0859\n",
      "Epoch 4273, Train Loss: 3.0302, Test Loss: 3.0926\n",
      "Epoch 4274, Train Loss: 3.0376, Test Loss: 3.0871\n",
      "Epoch 4275, Train Loss: 3.0144, Test Loss: 3.0994\n",
      "Epoch 4276, Train Loss: 3.0300, Test Loss: 3.1013\n",
      "Epoch 4277, Train Loss: 3.0294, Test Loss: 3.1054\n",
      "Epoch 4278, Train Loss: 3.0605, Test Loss: 3.1068\n",
      "Epoch 4279, Train Loss: 3.0350, Test Loss: 3.1129\n",
      "Epoch 4280, Train Loss: 3.0368, Test Loss: 3.1140\n",
      "Epoch 4281, Train Loss: 3.0397, Test Loss: 3.0953\n",
      "Epoch 4282, Train Loss: 3.0160, Test Loss: 3.0922\n",
      "Epoch 4283, Train Loss: 3.0223, Test Loss: 3.1040\n",
      "Epoch 4284, Train Loss: 3.0319, Test Loss: 3.0927\n",
      "Epoch 4285, Train Loss: 3.0154, Test Loss: 3.1056\n",
      "Epoch 4286, Train Loss: 3.0325, Test Loss: 3.1020\n",
      "Epoch 4287, Train Loss: 3.0225, Test Loss: 3.1072\n",
      "Epoch 4288, Train Loss: 3.0265, Test Loss: 3.1060\n",
      "Epoch 4289, Train Loss: 3.0432, Test Loss: 3.0864\n",
      "Epoch 4290, Train Loss: 3.0188, Test Loss: 3.0981\n",
      "Epoch 4291, Train Loss: 3.0232, Test Loss: 3.0930\n",
      "Epoch 4292, Train Loss: 3.0305, Test Loss: 3.0871\n",
      "Epoch 4293, Train Loss: 3.0347, Test Loss: 3.0965\n",
      "Epoch 4294, Train Loss: 3.0506, Test Loss: 3.0932\n",
      "Epoch 4295, Train Loss: 3.0195, Test Loss: 3.1012\n",
      "Epoch 4296, Train Loss: 3.0218, Test Loss: 3.0968\n",
      "Epoch 4297, Train Loss: 3.0206, Test Loss: 3.1025\n",
      "Epoch 4298, Train Loss: 3.0253, Test Loss: 3.0958\n",
      "Epoch 4299, Train Loss: 3.0156, Test Loss: 3.0899\n",
      "Epoch 4300, Train Loss: 3.0225, Test Loss: 3.0957\n",
      "Epoch 4301, Train Loss: 3.0269, Test Loss: 3.0853\n",
      "Epoch 4302, Train Loss: 3.0261, Test Loss: 3.0897\n",
      "Epoch 4303, Train Loss: 3.0125, Test Loss: 3.0987\n",
      "Epoch 4304, Train Loss: 3.0225, Test Loss: 3.0887\n",
      "Epoch 4305, Train Loss: 3.0149, Test Loss: 3.0862\n",
      "Epoch 4306, Train Loss: 3.0223, Test Loss: 3.0933\n",
      "Epoch 4307, Train Loss: 3.0252, Test Loss: 3.0934\n",
      "Epoch 4308, Train Loss: 3.0187, Test Loss: 3.0858\n",
      "Epoch 4309, Train Loss: 3.0186, Test Loss: 3.0865\n",
      "Epoch 4310, Train Loss: 3.0270, Test Loss: 3.0877\n",
      "Epoch 4311, Train Loss: 3.0245, Test Loss: 3.0842\n",
      "Epoch 4312, Train Loss: 3.0180, Test Loss: 3.0872\n",
      "Epoch 4313, Train Loss: 3.0137, Test Loss: 3.0825\n",
      "Epoch 4314, Train Loss: 3.0142, Test Loss: 3.0833\n",
      "Epoch 4315, Train Loss: 3.0104, Test Loss: 3.0864\n",
      "Epoch 4316, Train Loss: 3.0149, Test Loss: 3.0861\n",
      "Epoch 4317, Train Loss: 3.0200, Test Loss: 3.0818\n",
      "Epoch 4318, Train Loss: 3.0102, Test Loss: 3.0811\n",
      "Epoch 4319, Train Loss: 3.0124, Test Loss: 3.0813\n",
      "Epoch 4320, Train Loss: 3.0172, Test Loss: 3.0841\n",
      "Epoch 4321, Train Loss: 3.0122, Test Loss: 3.0828\n",
      "Epoch 4322, Train Loss: 3.0217, Test Loss: 3.0811\n",
      "Epoch 4323, Train Loss: 3.0088, Test Loss: 3.0878\n",
      "Epoch 4324, Train Loss: 3.0213, Test Loss: 3.0821\n",
      "Epoch 4325, Train Loss: 3.0216, Test Loss: 3.0831\n",
      "Epoch 4326, Train Loss: 3.0057, Test Loss: 3.0912\n",
      "Epoch 4327, Train Loss: 3.0195, Test Loss: 3.0807\n",
      "Epoch 4328, Train Loss: 3.0158, Test Loss: 3.0821\n",
      "Epoch 4329, Train Loss: 3.0234, Test Loss: 3.0852\n",
      "Epoch 4330, Train Loss: 3.0182, Test Loss: 3.0930\n",
      "Epoch 4331, Train Loss: 3.0181, Test Loss: 3.0832\n",
      "Epoch 4332, Train Loss: 3.0106, Test Loss: 3.0880\n",
      "Epoch 4333, Train Loss: 3.0105, Test Loss: 3.0862\n",
      "Epoch 4334, Train Loss: 3.0089, Test Loss: 3.0891\n",
      "Epoch 4335, Train Loss: 3.0148, Test Loss: 3.0863\n",
      "Epoch 4336, Train Loss: 3.0181, Test Loss: 3.0846\n",
      "Epoch 4337, Train Loss: 3.0239, Test Loss: 3.0849\n",
      "Epoch 4338, Train Loss: 3.0088, Test Loss: 3.0834\n",
      "Epoch 4339, Train Loss: 3.0125, Test Loss: 3.0832\n",
      "Epoch 4340, Train Loss: 3.0093, Test Loss: 3.0807\n",
      "Epoch 4341, Train Loss: 3.0165, Test Loss: 3.0855\n",
      "Epoch 4342, Train Loss: 3.0114, Test Loss: 3.0831\n",
      "Epoch 4343, Train Loss: 3.0088, Test Loss: 3.0879\n",
      "Epoch 4344, Train Loss: 3.0197, Test Loss: 3.0909\n",
      "Epoch 4345, Train Loss: 3.0157, Test Loss: 3.0919\n",
      "Epoch 4346, Train Loss: 3.0138, Test Loss: 3.0862\n",
      "Epoch 4347, Train Loss: 3.0091, Test Loss: 3.0888\n",
      "Epoch 4348, Train Loss: 3.0162, Test Loss: 3.0844\n",
      "Epoch 4349, Train Loss: 3.0142, Test Loss: 3.0863\n",
      "Epoch 4350, Train Loss: 3.0213, Test Loss: 3.0797\n",
      "Epoch 4351, Train Loss: 3.0151, Test Loss: 3.0847\n",
      "Epoch 4352, Train Loss: 3.0113, Test Loss: 3.1002\n",
      "Epoch 4353, Train Loss: 3.0314, Test Loss: 3.1047\n",
      "Epoch 4354, Train Loss: 3.0320, Test Loss: 3.0939\n",
      "Epoch 4355, Train Loss: 3.0256, Test Loss: 3.1054\n",
      "Epoch 4356, Train Loss: 3.0411, Test Loss: 3.1180\n",
      "Epoch 4357, Train Loss: 3.0562, Test Loss: 3.0991\n",
      "Epoch 4358, Train Loss: 3.0484, Test Loss: 3.1059\n",
      "Epoch 4359, Train Loss: 3.0470, Test Loss: 3.0937\n",
      "Epoch 4360, Train Loss: 3.0341, Test Loss: 3.0859\n",
      "Epoch 4361, Train Loss: 3.0176, Test Loss: 3.1018\n",
      "Epoch 4362, Train Loss: 3.0328, Test Loss: 3.1098\n",
      "Epoch 4363, Train Loss: 3.0610, Test Loss: 3.1111\n",
      "Epoch 4364, Train Loss: 3.0724, Test Loss: 3.0958\n",
      "Epoch 4365, Train Loss: 3.0325, Test Loss: 3.1030\n",
      "Epoch 4366, Train Loss: 3.0427, Test Loss: 3.1051\n",
      "Epoch 4367, Train Loss: 3.0482, Test Loss: 3.0938\n",
      "Epoch 4368, Train Loss: 3.0265, Test Loss: 3.0971\n",
      "Epoch 4369, Train Loss: 3.0328, Test Loss: 3.1009\n",
      "Epoch 4370, Train Loss: 3.0342, Test Loss: 3.1055\n",
      "Epoch 4371, Train Loss: 3.0429, Test Loss: 3.1030\n",
      "Epoch 4372, Train Loss: 3.0331, Test Loss: 3.1223\n",
      "Epoch 4373, Train Loss: 3.0622, Test Loss: 3.1136\n",
      "Epoch 4374, Train Loss: 3.0595, Test Loss: 3.1056\n",
      "Epoch 4375, Train Loss: 3.0380, Test Loss: 3.1183\n",
      "Epoch 4376, Train Loss: 3.0618, Test Loss: 3.1072\n",
      "Epoch 4377, Train Loss: 3.0346, Test Loss: 3.0986\n",
      "Epoch 4378, Train Loss: 3.0483, Test Loss: 3.0900\n",
      "Epoch 4379, Train Loss: 3.0190, Test Loss: 3.1107\n",
      "Epoch 4380, Train Loss: 3.0548, Test Loss: 3.1096\n",
      "Epoch 4381, Train Loss: 3.0508, Test Loss: 3.1230\n",
      "Epoch 4382, Train Loss: 3.0456, Test Loss: 3.1115\n",
      "Epoch 4383, Train Loss: 3.0448, Test Loss: 3.1067\n",
      "Epoch 4384, Train Loss: 3.0548, Test Loss: 3.1164\n",
      "Epoch 4385, Train Loss: 3.0569, Test Loss: 3.1368\n",
      "Epoch 4386, Train Loss: 3.0949, Test Loss: 3.1274\n",
      "Epoch 4387, Train Loss: 3.0561, Test Loss: 3.1141\n",
      "Epoch 4388, Train Loss: 3.0580, Test Loss: 3.0922\n",
      "Epoch 4389, Train Loss: 3.0317, Test Loss: 3.1168\n",
      "Epoch 4390, Train Loss: 3.0711, Test Loss: 3.1270\n",
      "Epoch 4391, Train Loss: 3.0435, Test Loss: 3.1403\n",
      "Epoch 4392, Train Loss: 3.0585, Test Loss: 3.1381\n",
      "Epoch 4393, Train Loss: 3.0957, Test Loss: 3.1404\n",
      "Epoch 4394, Train Loss: 3.0655, Test Loss: 3.1974\n",
      "Epoch 4395, Train Loss: 3.1486, Test Loss: 3.2004\n",
      "Epoch 4396, Train Loss: 3.1184, Test Loss: 3.1736\n",
      "Epoch 4397, Train Loss: 3.0968, Test Loss: 3.1281\n",
      "Epoch 4398, Train Loss: 3.0709, Test Loss: 3.1179\n",
      "Epoch 4399, Train Loss: 3.0820, Test Loss: 3.0894\n",
      "Epoch 4400, Train Loss: 3.0265, Test Loss: 3.1205\n",
      "Epoch 4401, Train Loss: 3.0567, Test Loss: 3.1350\n",
      "Epoch 4402, Train Loss: 3.0790, Test Loss: 3.1046\n",
      "Epoch 4403, Train Loss: 3.0528, Test Loss: 3.1003\n",
      "Epoch 4404, Train Loss: 3.0426, Test Loss: 3.0900\n",
      "Epoch 4405, Train Loss: 3.0201, Test Loss: 3.1065\n",
      "Epoch 4406, Train Loss: 3.0355, Test Loss: 3.0964\n",
      "Epoch 4407, Train Loss: 3.0345, Test Loss: 3.0878\n",
      "Epoch 4408, Train Loss: 3.0274, Test Loss: 3.0910\n",
      "Epoch 4409, Train Loss: 3.0202, Test Loss: 3.0971\n",
      "Epoch 4410, Train Loss: 3.0246, Test Loss: 3.0967\n",
      "Epoch 4411, Train Loss: 3.0246, Test Loss: 3.0903\n",
      "Epoch 4412, Train Loss: 3.0414, Test Loss: 3.0877\n",
      "Epoch 4413, Train Loss: 3.0233, Test Loss: 3.0828\n",
      "Epoch 4414, Train Loss: 3.0128, Test Loss: 3.0881\n",
      "Epoch 4415, Train Loss: 3.0219, Test Loss: 3.0916\n",
      "Epoch 4416, Train Loss: 3.0259, Test Loss: 3.0915\n",
      "Epoch 4417, Train Loss: 3.0227, Test Loss: 3.0882\n",
      "Epoch 4418, Train Loss: 3.0393, Test Loss: 3.0925\n",
      "Epoch 4419, Train Loss: 3.0317, Test Loss: 3.0947\n",
      "Epoch 4420, Train Loss: 3.0303, Test Loss: 3.0866\n",
      "Epoch 4421, Train Loss: 3.0153, Test Loss: 3.1056\n",
      "Epoch 4422, Train Loss: 3.0328, Test Loss: 3.0967\n",
      "Epoch 4423, Train Loss: 3.0307, Test Loss: 3.0873\n",
      "Epoch 4424, Train Loss: 3.0374, Test Loss: 3.0961\n",
      "Epoch 4425, Train Loss: 3.0316, Test Loss: 3.1113\n",
      "Epoch 4426, Train Loss: 3.0478, Test Loss: 3.0908\n",
      "Epoch 4427, Train Loss: 3.0224, Test Loss: 3.1049\n",
      "Epoch 4428, Train Loss: 3.0434, Test Loss: 3.1050\n",
      "Epoch 4429, Train Loss: 3.0376, Test Loss: 3.1216\n",
      "Epoch 4430, Train Loss: 3.0466, Test Loss: 3.0953\n",
      "Epoch 4431, Train Loss: 3.0269, Test Loss: 3.0993\n",
      "Epoch 4432, Train Loss: 3.0360, Test Loss: 3.1098\n",
      "Epoch 4433, Train Loss: 3.0648, Test Loss: 3.0976\n",
      "Epoch 4434, Train Loss: 3.0316, Test Loss: 3.0947\n",
      "Epoch 4435, Train Loss: 3.0292, Test Loss: 3.1161\n",
      "Epoch 4436, Train Loss: 3.0709, Test Loss: 3.1003\n",
      "Epoch 4437, Train Loss: 3.0278, Test Loss: 3.0920\n",
      "Epoch 4438, Train Loss: 3.0155, Test Loss: 3.0846\n",
      "Epoch 4439, Train Loss: 3.0139, Test Loss: 3.0961\n",
      "Epoch 4440, Train Loss: 3.0249, Test Loss: 3.0881\n",
      "Epoch 4441, Train Loss: 3.0162, Test Loss: 3.1012\n",
      "Epoch 4442, Train Loss: 3.0249, Test Loss: 3.1064\n",
      "Epoch 4443, Train Loss: 3.0444, Test Loss: 3.0934\n",
      "Epoch 4444, Train Loss: 3.0309, Test Loss: 3.0846\n",
      "Epoch 4445, Train Loss: 3.0224, Test Loss: 3.1000\n",
      "Epoch 4446, Train Loss: 3.0362, Test Loss: 3.0926\n",
      "Epoch 4447, Train Loss: 3.0269, Test Loss: 3.0898\n",
      "Epoch 4448, Train Loss: 3.0427, Test Loss: 3.0929\n",
      "Epoch 4449, Train Loss: 3.0316, Test Loss: 3.0984\n",
      "Epoch 4450, Train Loss: 3.0211, Test Loss: 3.1220\n",
      "Epoch 4451, Train Loss: 3.0480, Test Loss: 3.0960\n",
      "Epoch 4452, Train Loss: 3.0318, Test Loss: 3.1013\n",
      "Epoch 4453, Train Loss: 3.0342, Test Loss: 3.0876\n",
      "Epoch 4454, Train Loss: 3.0115, Test Loss: 3.1090\n",
      "Epoch 4455, Train Loss: 3.0359, Test Loss: 3.1073\n",
      "Epoch 4456, Train Loss: 3.0364, Test Loss: 3.0843\n",
      "Epoch 4457, Train Loss: 3.0153, Test Loss: 3.0993\n",
      "Epoch 4458, Train Loss: 3.0391, Test Loss: 3.1027\n",
      "Epoch 4459, Train Loss: 3.0328, Test Loss: 3.0918\n",
      "Epoch 4460, Train Loss: 3.0281, Test Loss: 3.0914\n",
      "Epoch 4461, Train Loss: 3.0210, Test Loss: 3.0983\n",
      "Epoch 4462, Train Loss: 3.0461, Test Loss: 3.1000\n",
      "Epoch 4463, Train Loss: 3.0302, Test Loss: 3.1082\n",
      "Epoch 4464, Train Loss: 3.0322, Test Loss: 3.0867\n",
      "Epoch 4465, Train Loss: 3.0087, Test Loss: 3.0946\n",
      "Epoch 4466, Train Loss: 3.0412, Test Loss: 3.0848\n",
      "Epoch 4467, Train Loss: 3.0205, Test Loss: 3.0877\n",
      "Epoch 4468, Train Loss: 3.0096, Test Loss: 3.0915\n",
      "Epoch 4469, Train Loss: 3.0345, Test Loss: 3.0814\n",
      "Epoch 4470, Train Loss: 3.0218, Test Loss: 3.0871\n",
      "Epoch 4471, Train Loss: 3.0360, Test Loss: 3.1034\n",
      "Epoch 4472, Train Loss: 3.0433, Test Loss: 3.1046\n",
      "Epoch 4473, Train Loss: 3.0254, Test Loss: 3.0979\n",
      "Epoch 4474, Train Loss: 3.0320, Test Loss: 3.1080\n",
      "Epoch 4475, Train Loss: 3.0673, Test Loss: 3.0930\n",
      "Epoch 4476, Train Loss: 3.0385, Test Loss: 3.1178\n",
      "Epoch 4477, Train Loss: 3.0558, Test Loss: 3.1108\n",
      "Epoch 4478, Train Loss: 3.0500, Test Loss: 3.1268\n",
      "Epoch 4479, Train Loss: 3.0458, Test Loss: 3.1094\n",
      "Epoch 4480, Train Loss: 3.0557, Test Loss: 3.1346\n",
      "Epoch 4481, Train Loss: 3.0599, Test Loss: 3.1898\n",
      "Epoch 4482, Train Loss: 3.1361, Test Loss: 3.1794\n",
      "Epoch 4483, Train Loss: 3.0873, Test Loss: 3.1674\n",
      "Epoch 4484, Train Loss: 3.1262, Test Loss: 3.1334\n",
      "Epoch 4485, Train Loss: 3.0860, Test Loss: 3.1168\n",
      "Epoch 4486, Train Loss: 3.0589, Test Loss: 3.1095\n",
      "Epoch 4487, Train Loss: 3.0340, Test Loss: 3.1399\n",
      "Epoch 4488, Train Loss: 3.0686, Test Loss: 3.1535\n",
      "Epoch 4489, Train Loss: 3.1002, Test Loss: 3.1522\n",
      "Epoch 4490, Train Loss: 3.0940, Test Loss: 3.1522\n",
      "Epoch 4491, Train Loss: 3.0865, Test Loss: 3.1185\n",
      "Epoch 4492, Train Loss: 3.0633, Test Loss: 3.1136\n",
      "Epoch 4493, Train Loss: 3.0764, Test Loss: 3.1536\n",
      "Epoch 4494, Train Loss: 3.0935, Test Loss: 3.1374\n",
      "Epoch 4495, Train Loss: 3.0846, Test Loss: 3.1186\n",
      "Epoch 4496, Train Loss: 3.0535, Test Loss: 3.1070\n",
      "Epoch 4497, Train Loss: 3.0495, Test Loss: 3.1146\n",
      "Epoch 4498, Train Loss: 3.0453, Test Loss: 3.1425\n",
      "Epoch 4499, Train Loss: 3.0684, Test Loss: 3.1097\n",
      "Epoch 4500, Train Loss: 3.0608, Test Loss: 3.0892\n",
      "Epoch 4501, Train Loss: 3.0215, Test Loss: 3.1212\n",
      "Epoch 4502, Train Loss: 3.0754, Test Loss: 3.1394\n",
      "Epoch 4503, Train Loss: 3.0890, Test Loss: 3.0998\n",
      "Epoch 4504, Train Loss: 3.0418, Test Loss: 3.1058\n",
      "Epoch 4505, Train Loss: 3.0418, Test Loss: 3.1221\n",
      "Epoch 4506, Train Loss: 3.0486, Test Loss: 3.1224\n",
      "Epoch 4507, Train Loss: 3.0447, Test Loss: 3.1190\n",
      "Epoch 4508, Train Loss: 3.0602, Test Loss: 3.1007\n",
      "Epoch 4509, Train Loss: 3.0155, Test Loss: 3.1308\n",
      "Epoch 4510, Train Loss: 3.0924, Test Loss: 3.0904\n",
      "Epoch 4511, Train Loss: 3.0202, Test Loss: 3.1015\n",
      "Epoch 4512, Train Loss: 3.0408, Test Loss: 3.1010\n",
      "Epoch 4513, Train Loss: 3.0364, Test Loss: 3.0953\n",
      "Epoch 4514, Train Loss: 3.0298, Test Loss: 3.0882\n",
      "Epoch 4515, Train Loss: 3.0293, Test Loss: 3.0914\n",
      "Epoch 4516, Train Loss: 3.0313, Test Loss: 3.0971\n",
      "Epoch 4517, Train Loss: 3.0346, Test Loss: 3.0967\n",
      "Epoch 4518, Train Loss: 3.0273, Test Loss: 3.0948\n",
      "Epoch 4519, Train Loss: 3.0227, Test Loss: 3.0934\n",
      "Epoch 4520, Train Loss: 3.0228, Test Loss: 3.0887\n",
      "Epoch 4521, Train Loss: 3.0210, Test Loss: 3.0821\n",
      "Epoch 4522, Train Loss: 3.0205, Test Loss: 3.1033\n",
      "Epoch 4523, Train Loss: 3.0590, Test Loss: 3.1031\n",
      "Epoch 4524, Train Loss: 3.0436, Test Loss: 3.1089\n",
      "Epoch 4525, Train Loss: 3.0502, Test Loss: 3.1106\n",
      "Epoch 4526, Train Loss: 3.0506, Test Loss: 3.1117\n",
      "Epoch 4527, Train Loss: 3.0539, Test Loss: 3.1110\n",
      "Epoch 4528, Train Loss: 3.0679, Test Loss: 3.1116\n",
      "Epoch 4529, Train Loss: 3.0430, Test Loss: 3.1462\n",
      "Epoch 4530, Train Loss: 3.0558, Test Loss: 3.1323\n",
      "Epoch 4531, Train Loss: 3.0760, Test Loss: 3.1021\n",
      "Epoch 4532, Train Loss: 3.0516, Test Loss: 3.1079\n",
      "Epoch 4533, Train Loss: 3.0555, Test Loss: 3.1060\n",
      "Epoch 4534, Train Loss: 3.0536, Test Loss: 3.1245\n",
      "Epoch 4535, Train Loss: 3.0538, Test Loss: 3.1681\n",
      "Epoch 4536, Train Loss: 3.1140, Test Loss: 3.1081\n",
      "Epoch 4537, Train Loss: 3.0441, Test Loss: 3.1107\n",
      "Epoch 4538, Train Loss: 3.0616, Test Loss: 3.1496\n",
      "Epoch 4539, Train Loss: 3.1166, Test Loss: 3.1456\n",
      "Epoch 4540, Train Loss: 3.1303, Test Loss: 3.1255\n",
      "Epoch 4541, Train Loss: 3.1026, Test Loss: 3.1031\n",
      "Epoch 4542, Train Loss: 3.0553, Test Loss: 3.1098\n",
      "Epoch 4543, Train Loss: 3.0521, Test Loss: 3.1086\n",
      "Epoch 4544, Train Loss: 3.0423, Test Loss: 3.1252\n",
      "Epoch 4545, Train Loss: 3.0714, Test Loss: 3.1310\n",
      "Epoch 4546, Train Loss: 3.0707, Test Loss: 3.1286\n",
      "Epoch 4547, Train Loss: 3.0556, Test Loss: 3.1327\n",
      "Epoch 4548, Train Loss: 3.0531, Test Loss: 3.1059\n",
      "Epoch 4549, Train Loss: 3.0286, Test Loss: 3.1126\n",
      "Epoch 4550, Train Loss: 3.0595, Test Loss: 3.1132\n",
      "Epoch 4551, Train Loss: 3.0544, Test Loss: 3.1302\n",
      "Epoch 4552, Train Loss: 3.0752, Test Loss: 3.1336\n",
      "Epoch 4553, Train Loss: 3.0427, Test Loss: 3.1386\n",
      "Epoch 4554, Train Loss: 3.0882, Test Loss: 3.1499\n",
      "Epoch 4555, Train Loss: 3.1149, Test Loss: 3.1299\n",
      "Epoch 4556, Train Loss: 3.1007, Test Loss: 3.1576\n",
      "Epoch 4557, Train Loss: 3.1333, Test Loss: 3.1198\n",
      "Epoch 4558, Train Loss: 3.0588, Test Loss: 3.1349\n",
      "Epoch 4559, Train Loss: 3.0651, Test Loss: 3.1352\n",
      "Epoch 4560, Train Loss: 3.0823, Test Loss: 3.1284\n",
      "Epoch 4561, Train Loss: 3.0726, Test Loss: 3.1048\n",
      "Epoch 4562, Train Loss: 3.0436, Test Loss: 3.1185\n",
      "Epoch 4563, Train Loss: 3.0725, Test Loss: 3.0912\n",
      "Epoch 4564, Train Loss: 3.0263, Test Loss: 3.1126\n",
      "Epoch 4565, Train Loss: 3.0442, Test Loss: 3.1716\n",
      "Epoch 4566, Train Loss: 3.1028, Test Loss: 3.1039\n",
      "Epoch 4567, Train Loss: 3.0393, Test Loss: 3.0962\n",
      "Epoch 4568, Train Loss: 3.0309, Test Loss: 3.1197\n",
      "Epoch 4569, Train Loss: 3.0786, Test Loss: 3.1108\n",
      "Epoch 4570, Train Loss: 3.0723, Test Loss: 3.0997\n",
      "Epoch 4571, Train Loss: 3.0414, Test Loss: 3.1307\n",
      "Epoch 4572, Train Loss: 3.0639, Test Loss: 3.0926\n",
      "Epoch 4573, Train Loss: 3.0365, Test Loss: 3.1103\n",
      "Epoch 4574, Train Loss: 3.0541, Test Loss: 3.0997\n",
      "Epoch 4575, Train Loss: 3.0489, Test Loss: 3.0969\n",
      "Epoch 4576, Train Loss: 3.0297, Test Loss: 3.0962\n",
      "Epoch 4577, Train Loss: 3.0282, Test Loss: 3.0862\n",
      "Epoch 4578, Train Loss: 3.0215, Test Loss: 3.0936\n",
      "Epoch 4579, Train Loss: 3.0317, Test Loss: 3.0781\n",
      "Epoch 4580, Train Loss: 3.0160, Test Loss: 3.0908\n",
      "Epoch 4581, Train Loss: 3.0230, Test Loss: 3.0860\n",
      "Epoch 4582, Train Loss: 3.0225, Test Loss: 3.0804\n",
      "Epoch 4583, Train Loss: 3.0075, Test Loss: 3.0884\n",
      "Epoch 4584, Train Loss: 3.0370, Test Loss: 3.0925\n",
      "Epoch 4585, Train Loss: 3.0278, Test Loss: 3.0940\n",
      "Epoch 4586, Train Loss: 3.0277, Test Loss: 3.0895\n",
      "Epoch 4587, Train Loss: 3.0258, Test Loss: 3.0999\n",
      "Epoch 4588, Train Loss: 3.0292, Test Loss: 3.0863\n",
      "Epoch 4589, Train Loss: 3.0101, Test Loss: 3.1052\n",
      "Epoch 4590, Train Loss: 3.0465, Test Loss: 3.0833\n",
      "Epoch 4591, Train Loss: 3.0128, Test Loss: 3.0972\n",
      "Epoch 4592, Train Loss: 3.0220, Test Loss: 3.0951\n",
      "Epoch 4593, Train Loss: 3.0235, Test Loss: 3.0810\n",
      "Epoch 4594, Train Loss: 3.0162, Test Loss: 3.0842\n",
      "Epoch 4595, Train Loss: 3.0200, Test Loss: 3.0813\n",
      "Epoch 4596, Train Loss: 3.0124, Test Loss: 3.0955\n",
      "Epoch 4597, Train Loss: 3.0259, Test Loss: 3.0856\n",
      "Epoch 4598, Train Loss: 3.0191, Test Loss: 3.0897\n",
      "Epoch 4599, Train Loss: 3.0309, Test Loss: 3.0806\n",
      "Epoch 4600, Train Loss: 3.0134, Test Loss: 3.0956\n",
      "Epoch 4601, Train Loss: 3.0281, Test Loss: 3.0836\n",
      "Epoch 4602, Train Loss: 3.0218, Test Loss: 3.0831\n",
      "Epoch 4603, Train Loss: 3.0188, Test Loss: 3.0907\n",
      "Epoch 4604, Train Loss: 3.0217, Test Loss: 3.0875\n",
      "Epoch 4605, Train Loss: 3.0156, Test Loss: 3.0952\n",
      "Epoch 4606, Train Loss: 3.0335, Test Loss: 3.0877\n",
      "Epoch 4607, Train Loss: 3.0131, Test Loss: 3.0929\n",
      "Epoch 4608, Train Loss: 3.0217, Test Loss: 3.0932\n",
      "Epoch 4609, Train Loss: 3.0288, Test Loss: 3.0907\n",
      "Epoch 4610, Train Loss: 3.0288, Test Loss: 3.1070\n",
      "Epoch 4611, Train Loss: 3.0548, Test Loss: 3.1093\n",
      "Epoch 4612, Train Loss: 3.0493, Test Loss: 3.1146\n",
      "Epoch 4613, Train Loss: 3.0363, Test Loss: 3.1162\n",
      "Epoch 4614, Train Loss: 3.0281, Test Loss: 3.1035\n",
      "Epoch 4615, Train Loss: 3.0408, Test Loss: 3.1161\n",
      "Epoch 4616, Train Loss: 3.0518, Test Loss: 3.1125\n",
      "Epoch 4617, Train Loss: 3.0302, Test Loss: 3.1087\n",
      "Epoch 4618, Train Loss: 3.0494, Test Loss: 3.0895\n",
      "Epoch 4619, Train Loss: 3.0368, Test Loss: 3.0907\n",
      "Epoch 4620, Train Loss: 3.0349, Test Loss: 3.1025\n",
      "Epoch 4621, Train Loss: 3.0308, Test Loss: 3.1003\n",
      "Epoch 4622, Train Loss: 3.0328, Test Loss: 3.0931\n",
      "Epoch 4623, Train Loss: 3.0251, Test Loss: 3.1012\n",
      "Epoch 4624, Train Loss: 3.0317, Test Loss: 3.1014\n",
      "Epoch 4625, Train Loss: 3.0300, Test Loss: 3.0927\n",
      "Epoch 4626, Train Loss: 3.0303, Test Loss: 3.0849\n",
      "Epoch 4627, Train Loss: 3.0303, Test Loss: 3.0865\n",
      "Epoch 4628, Train Loss: 3.0214, Test Loss: 3.0844\n",
      "Epoch 4629, Train Loss: 3.0167, Test Loss: 3.0828\n",
      "Epoch 4630, Train Loss: 3.0163, Test Loss: 3.0843\n",
      "Epoch 4631, Train Loss: 3.0104, Test Loss: 3.0842\n",
      "Epoch 4632, Train Loss: 3.0116, Test Loss: 3.0840\n",
      "Epoch 4633, Train Loss: 3.0173, Test Loss: 3.0853\n",
      "Epoch 4634, Train Loss: 3.0190, Test Loss: 3.0888\n",
      "Epoch 4635, Train Loss: 3.0185, Test Loss: 3.0858\n",
      "Epoch 4636, Train Loss: 3.0189, Test Loss: 3.0796\n",
      "Epoch 4637, Train Loss: 3.0137, Test Loss: 3.0909\n",
      "Epoch 4638, Train Loss: 3.0148, Test Loss: 3.1067\n",
      "Epoch 4639, Train Loss: 3.0348, Test Loss: 3.0874\n",
      "Epoch 4640, Train Loss: 3.0172, Test Loss: 3.0963\n",
      "Epoch 4641, Train Loss: 3.0452, Test Loss: 3.1017\n",
      "Epoch 4642, Train Loss: 3.0410, Test Loss: 3.1056\n",
      "Epoch 4643, Train Loss: 3.0225, Test Loss: 3.1417\n",
      "Epoch 4644, Train Loss: 3.0666, Test Loss: 3.0843\n",
      "Epoch 4645, Train Loss: 3.0206, Test Loss: 3.1181\n",
      "Epoch 4646, Train Loss: 3.0688, Test Loss: 3.1278\n",
      "Epoch 4647, Train Loss: 3.1151, Test Loss: 3.0932\n",
      "Epoch 4648, Train Loss: 3.0469, Test Loss: 3.1385\n",
      "Epoch 4649, Train Loss: 3.0796, Test Loss: 3.1570\n",
      "Epoch 4650, Train Loss: 3.1120, Test Loss: 3.0928\n",
      "Epoch 4651, Train Loss: 3.0274, Test Loss: 3.1299\n",
      "Epoch 4652, Train Loss: 3.1098, Test Loss: 3.1072\n",
      "Epoch 4653, Train Loss: 3.0444, Test Loss: 3.1006\n",
      "Epoch 4654, Train Loss: 3.0175, Test Loss: 3.1445\n",
      "Epoch 4655, Train Loss: 3.0615, Test Loss: 3.0969\n",
      "Epoch 4656, Train Loss: 3.0325, Test Loss: 3.0912\n",
      "Epoch 4657, Train Loss: 3.0306, Test Loss: 3.1120\n",
      "Epoch 4658, Train Loss: 3.0589, Test Loss: 3.0955\n",
      "Epoch 4659, Train Loss: 3.0298, Test Loss: 3.1168\n",
      "Epoch 4660, Train Loss: 3.0452, Test Loss: 3.0941\n",
      "Epoch 4661, Train Loss: 3.0271, Test Loss: 3.1064\n",
      "Epoch 4662, Train Loss: 3.0547, Test Loss: 3.1000\n",
      "Epoch 4663, Train Loss: 3.0389, Test Loss: 3.1119\n",
      "Epoch 4664, Train Loss: 3.0650, Test Loss: 3.0936\n",
      "Epoch 4665, Train Loss: 3.0258, Test Loss: 3.1112\n",
      "Epoch 4666, Train Loss: 3.0433, Test Loss: 3.1077\n",
      "Epoch 4667, Train Loss: 3.0375, Test Loss: 3.0834\n",
      "Epoch 4668, Train Loss: 3.0150, Test Loss: 3.0934\n",
      "Epoch 4669, Train Loss: 3.0302, Test Loss: 3.0900\n",
      "Epoch 4670, Train Loss: 3.0240, Test Loss: 3.0762\n",
      "Epoch 4671, Train Loss: 3.0172, Test Loss: 3.1032\n",
      "Epoch 4672, Train Loss: 3.0275, Test Loss: 3.0874\n",
      "Epoch 4673, Train Loss: 3.0139, Test Loss: 3.0781\n",
      "Epoch 4674, Train Loss: 3.0186, Test Loss: 3.0863\n",
      "Epoch 4675, Train Loss: 3.0225, Test Loss: 3.0816\n",
      "Epoch 4676, Train Loss: 3.0155, Test Loss: 3.0958\n",
      "Epoch 4677, Train Loss: 3.0306, Test Loss: 3.0809\n",
      "Epoch 4678, Train Loss: 3.0172, Test Loss: 3.0863\n",
      "Epoch 4679, Train Loss: 3.0247, Test Loss: 3.0829\n",
      "Epoch 4680, Train Loss: 3.0230, Test Loss: 3.0885\n",
      "Epoch 4681, Train Loss: 3.0130, Test Loss: 3.1054\n",
      "Epoch 4682, Train Loss: 3.0321, Test Loss: 3.0832\n",
      "Epoch 4683, Train Loss: 3.0077, Test Loss: 3.1015\n",
      "Epoch 4684, Train Loss: 3.0448, Test Loss: 3.0892\n",
      "Epoch 4685, Train Loss: 3.0250, Test Loss: 3.0875\n",
      "Epoch 4686, Train Loss: 3.0137, Test Loss: 3.0999\n",
      "Epoch 4687, Train Loss: 3.0233, Test Loss: 3.0787\n",
      "Epoch 4688, Train Loss: 3.0245, Test Loss: 3.0999\n",
      "Epoch 4689, Train Loss: 3.0370, Test Loss: 3.1033\n",
      "Epoch 4690, Train Loss: 3.0247, Test Loss: 3.1040\n",
      "Epoch 4691, Train Loss: 3.0239, Test Loss: 3.0896\n",
      "Epoch 4692, Train Loss: 3.0233, Test Loss: 3.0913\n",
      "Epoch 4693, Train Loss: 3.0188, Test Loss: 3.0907\n",
      "Epoch 4694, Train Loss: 3.0224, Test Loss: 3.0915\n",
      "Epoch 4695, Train Loss: 3.0224, Test Loss: 3.0922\n",
      "Epoch 4696, Train Loss: 3.0196, Test Loss: 3.0840\n",
      "Epoch 4697, Train Loss: 3.0120, Test Loss: 3.0886\n",
      "Epoch 4698, Train Loss: 3.0155, Test Loss: 3.0881\n",
      "Epoch 4699, Train Loss: 3.0124, Test Loss: 3.0822\n",
      "Epoch 4700, Train Loss: 3.0123, Test Loss: 3.0835\n",
      "Epoch 4701, Train Loss: 3.0166, Test Loss: 3.0795\n",
      "Epoch 4702, Train Loss: 3.0151, Test Loss: 3.0789\n",
      "Epoch 4703, Train Loss: 3.0132, Test Loss: 3.0760\n",
      "Epoch 4704, Train Loss: 3.0129, Test Loss: 3.0847\n",
      "Epoch 4705, Train Loss: 3.0152, Test Loss: 3.0921\n",
      "Epoch 4706, Train Loss: 3.0150, Test Loss: 3.0852\n",
      "Epoch 4707, Train Loss: 3.0081, Test Loss: 3.0820\n",
      "Epoch 4708, Train Loss: 3.0136, Test Loss: 3.0839\n",
      "Epoch 4709, Train Loss: 3.0169, Test Loss: 3.0864\n",
      "Epoch 4710, Train Loss: 3.0114, Test Loss: 3.0814\n",
      "Epoch 4711, Train Loss: 3.0266, Test Loss: 3.0845\n",
      "Epoch 4712, Train Loss: 3.0187, Test Loss: 3.0842\n",
      "Epoch 4713, Train Loss: 3.0228, Test Loss: 3.1019\n",
      "Epoch 4714, Train Loss: 3.0264, Test Loss: 3.0917\n",
      "Epoch 4715, Train Loss: 3.0185, Test Loss: 3.0949\n",
      "Epoch 4716, Train Loss: 3.0250, Test Loss: 3.0856\n",
      "Epoch 4717, Train Loss: 3.0217, Test Loss: 3.0938\n",
      "Epoch 4718, Train Loss: 3.0210, Test Loss: 3.0816\n",
      "Epoch 4719, Train Loss: 3.0159, Test Loss: 3.0876\n",
      "Epoch 4720, Train Loss: 3.0309, Test Loss: 3.0864\n",
      "Epoch 4721, Train Loss: 3.0241, Test Loss: 3.0798\n",
      "Epoch 4722, Train Loss: 3.0113, Test Loss: 3.1022\n",
      "Epoch 4723, Train Loss: 3.0249, Test Loss: 3.0888\n",
      "Epoch 4724, Train Loss: 3.0212, Test Loss: 3.1079\n",
      "Epoch 4725, Train Loss: 3.0472, Test Loss: 3.1174\n",
      "Epoch 4726, Train Loss: 3.0413, Test Loss: 3.1119\n",
      "Epoch 4727, Train Loss: 3.0291, Test Loss: 3.0913\n",
      "Epoch 4728, Train Loss: 3.0261, Test Loss: 3.0875\n",
      "Epoch 4729, Train Loss: 3.0244, Test Loss: 3.0850\n",
      "Epoch 4730, Train Loss: 3.0184, Test Loss: 3.0883\n",
      "Epoch 4731, Train Loss: 3.0206, Test Loss: 3.0861\n",
      "Epoch 4732, Train Loss: 3.0120, Test Loss: 3.0838\n",
      "Epoch 4733, Train Loss: 3.0108, Test Loss: 3.0786\n",
      "Epoch 4734, Train Loss: 3.0182, Test Loss: 3.0855\n",
      "Epoch 4735, Train Loss: 3.0145, Test Loss: 3.1022\n",
      "Epoch 4736, Train Loss: 3.0449, Test Loss: 3.0911\n",
      "Epoch 4737, Train Loss: 3.0504, Test Loss: 3.1110\n",
      "Epoch 4738, Train Loss: 3.0646, Test Loss: 3.0914\n",
      "Epoch 4739, Train Loss: 3.0182, Test Loss: 3.0959\n",
      "Epoch 4740, Train Loss: 3.0179, Test Loss: 3.1082\n",
      "Epoch 4741, Train Loss: 3.0391, Test Loss: 3.0843\n",
      "Epoch 4742, Train Loss: 3.0209, Test Loss: 3.1032\n",
      "Epoch 4743, Train Loss: 3.0462, Test Loss: 3.0888\n",
      "Epoch 4744, Train Loss: 3.0188, Test Loss: 3.0946\n",
      "Epoch 4745, Train Loss: 3.0213, Test Loss: 3.0943\n",
      "Epoch 4746, Train Loss: 3.0227, Test Loss: 3.0908\n",
      "Epoch 4747, Train Loss: 3.0313, Test Loss: 3.0916\n",
      "Epoch 4748, Train Loss: 3.0197, Test Loss: 3.0848\n",
      "Epoch 4749, Train Loss: 3.0245, Test Loss: 3.0927\n",
      "Epoch 4750, Train Loss: 3.0256, Test Loss: 3.0847\n",
      "Epoch 4751, Train Loss: 3.0122, Test Loss: 3.0881\n",
      "Epoch 4752, Train Loss: 3.0272, Test Loss: 3.0793\n",
      "Epoch 4753, Train Loss: 3.0084, Test Loss: 3.0901\n",
      "Epoch 4754, Train Loss: 3.0167, Test Loss: 3.0897\n",
      "Epoch 4755, Train Loss: 3.0155, Test Loss: 3.0848\n",
      "Epoch 4756, Train Loss: 3.0190, Test Loss: 3.0856\n",
      "Epoch 4757, Train Loss: 3.0271, Test Loss: 3.0878\n",
      "Epoch 4758, Train Loss: 3.0145, Test Loss: 3.1031\n",
      "Epoch 4759, Train Loss: 3.0270, Test Loss: 3.0864\n",
      "Epoch 4760, Train Loss: 3.0266, Test Loss: 3.0837\n",
      "Epoch 4761, Train Loss: 3.0137, Test Loss: 3.1052\n",
      "Epoch 4762, Train Loss: 3.0287, Test Loss: 3.1117\n",
      "Epoch 4763, Train Loss: 3.0524, Test Loss: 3.0875\n",
      "Epoch 4764, Train Loss: 3.0174, Test Loss: 3.0976\n",
      "Epoch 4765, Train Loss: 3.0281, Test Loss: 3.0848\n",
      "Epoch 4766, Train Loss: 3.0168, Test Loss: 3.1030\n",
      "Epoch 4767, Train Loss: 3.0314, Test Loss: 3.0887\n",
      "Epoch 4768, Train Loss: 3.0294, Test Loss: 3.0910\n",
      "Epoch 4769, Train Loss: 3.0248, Test Loss: 3.0918\n",
      "Epoch 4770, Train Loss: 3.0325, Test Loss: 3.0860\n",
      "Epoch 4771, Train Loss: 3.0200, Test Loss: 3.1090\n",
      "Epoch 4772, Train Loss: 3.0303, Test Loss: 3.0894\n",
      "Epoch 4773, Train Loss: 3.0229, Test Loss: 3.0844\n",
      "Epoch 4774, Train Loss: 3.0184, Test Loss: 3.0775\n",
      "Epoch 4775, Train Loss: 3.0204, Test Loss: 3.0833\n",
      "Epoch 4776, Train Loss: 3.0133, Test Loss: 3.0856\n",
      "Epoch 4777, Train Loss: 3.0244, Test Loss: 3.0801\n",
      "Epoch 4778, Train Loss: 3.0166, Test Loss: 3.0889\n",
      "Epoch 4779, Train Loss: 3.0251, Test Loss: 3.0933\n",
      "Epoch 4780, Train Loss: 3.0161, Test Loss: 3.0884\n",
      "Epoch 4781, Train Loss: 3.0234, Test Loss: 3.0854\n",
      "Epoch 4782, Train Loss: 3.0291, Test Loss: 3.0814\n",
      "Epoch 4783, Train Loss: 3.0148, Test Loss: 3.0951\n",
      "Epoch 4784, Train Loss: 3.0250, Test Loss: 3.0901\n",
      "Epoch 4785, Train Loss: 3.0154, Test Loss: 3.0891\n",
      "Epoch 4786, Train Loss: 3.0181, Test Loss: 3.0898\n",
      "Epoch 4787, Train Loss: 3.0255, Test Loss: 3.0895\n",
      "Epoch 4788, Train Loss: 3.0206, Test Loss: 3.0823\n",
      "Epoch 4789, Train Loss: 3.0084, Test Loss: 3.0887\n",
      "Epoch 4790, Train Loss: 3.0335, Test Loss: 3.0988\n",
      "Epoch 4791, Train Loss: 3.0258, Test Loss: 3.1046\n",
      "Epoch 4792, Train Loss: 3.0439, Test Loss: 3.0851\n",
      "Epoch 4793, Train Loss: 3.0202, Test Loss: 3.1179\n",
      "Epoch 4794, Train Loss: 3.0636, Test Loss: 3.0939\n",
      "Epoch 4795, Train Loss: 3.0298, Test Loss: 3.1284\n",
      "Epoch 4796, Train Loss: 3.0419, Test Loss: 3.1054\n",
      "Epoch 4797, Train Loss: 3.0310, Test Loss: 3.0891\n",
      "Epoch 4798, Train Loss: 3.0307, Test Loss: 3.1081\n",
      "Epoch 4799, Train Loss: 3.0443, Test Loss: 3.0884\n",
      "Epoch 4800, Train Loss: 3.0173, Test Loss: 3.1147\n",
      "Epoch 4801, Train Loss: 3.0427, Test Loss: 3.0841\n",
      "Epoch 4802, Train Loss: 3.0198, Test Loss: 3.0901\n",
      "Epoch 4803, Train Loss: 3.0313, Test Loss: 3.0783\n",
      "Epoch 4804, Train Loss: 3.0140, Test Loss: 3.0998\n",
      "Epoch 4805, Train Loss: 3.0313, Test Loss: 3.0868\n",
      "Epoch 4806, Train Loss: 3.0379, Test Loss: 3.0871\n",
      "Epoch 4807, Train Loss: 3.0321, Test Loss: 3.0868\n",
      "Epoch 4808, Train Loss: 3.0253, Test Loss: 3.1114\n",
      "Epoch 4809, Train Loss: 3.0350, Test Loss: 3.0836\n",
      "Epoch 4810, Train Loss: 3.0252, Test Loss: 3.0810\n",
      "Epoch 4811, Train Loss: 3.0276, Test Loss: 3.0839\n",
      "Epoch 4812, Train Loss: 3.0205, Test Loss: 3.0926\n",
      "Epoch 4813, Train Loss: 3.0176, Test Loss: 3.0808\n",
      "Epoch 4814, Train Loss: 3.0231, Test Loss: 3.0838\n",
      "Epoch 4815, Train Loss: 3.0238, Test Loss: 3.0795\n",
      "Epoch 4816, Train Loss: 3.0222, Test Loss: 3.0830\n",
      "Epoch 4817, Train Loss: 3.0213, Test Loss: 3.0926\n",
      "Epoch 4818, Train Loss: 3.0315, Test Loss: 3.0869\n",
      "Epoch 4819, Train Loss: 3.0222, Test Loss: 3.0850\n",
      "Epoch 4820, Train Loss: 3.0236, Test Loss: 3.0902\n",
      "Epoch 4821, Train Loss: 3.0167, Test Loss: 3.1088\n",
      "Epoch 4822, Train Loss: 3.0307, Test Loss: 3.0878\n",
      "Epoch 4823, Train Loss: 3.0265, Test Loss: 3.0975\n",
      "Epoch 4824, Train Loss: 3.0388, Test Loss: 3.0784\n",
      "Epoch 4825, Train Loss: 3.0035, Test Loss: 3.0936\n",
      "Epoch 4826, Train Loss: 3.0282, Test Loss: 3.0912\n",
      "Epoch 4827, Train Loss: 3.0210, Test Loss: 3.0829\n",
      "Epoch 4828, Train Loss: 3.0201, Test Loss: 3.1045\n",
      "Epoch 4829, Train Loss: 3.0382, Test Loss: 3.1069\n",
      "Epoch 4830, Train Loss: 3.0394, Test Loss: 3.0822\n",
      "Epoch 4831, Train Loss: 3.0221, Test Loss: 3.0955\n",
      "Epoch 4832, Train Loss: 3.0463, Test Loss: 3.1071\n",
      "Epoch 4833, Train Loss: 3.0361, Test Loss: 3.1471\n",
      "Epoch 4834, Train Loss: 3.0596, Test Loss: 3.1251\n",
      "Epoch 4835, Train Loss: 3.0439, Test Loss: 3.0966\n",
      "Epoch 4836, Train Loss: 3.0409, Test Loss: 3.1332\n",
      "Epoch 4837, Train Loss: 3.0818, Test Loss: 3.1129\n",
      "Epoch 4838, Train Loss: 3.0475, Test Loss: 3.1289\n",
      "Epoch 4839, Train Loss: 3.0606, Test Loss: 3.0917\n",
      "Epoch 4840, Train Loss: 3.0254, Test Loss: 3.0990\n",
      "Epoch 4841, Train Loss: 3.0401, Test Loss: 3.1148\n",
      "Epoch 4842, Train Loss: 3.0711, Test Loss: 3.1246\n",
      "Epoch 4843, Train Loss: 3.0520, Test Loss: 3.1167\n",
      "Epoch 4844, Train Loss: 3.0643, Test Loss: 3.1209\n",
      "Epoch 4845, Train Loss: 3.0686, Test Loss: 3.0999\n",
      "Epoch 4846, Train Loss: 3.0410, Test Loss: 3.1102\n",
      "Epoch 4847, Train Loss: 3.0739, Test Loss: 3.1196\n",
      "Epoch 4848, Train Loss: 3.0786, Test Loss: 3.1390\n",
      "Epoch 4849, Train Loss: 3.0568, Test Loss: 3.1248\n",
      "Epoch 4850, Train Loss: 3.0474, Test Loss: 3.1353\n",
      "Epoch 4851, Train Loss: 3.0997, Test Loss: 3.1199\n",
      "Epoch 4852, Train Loss: 3.0669, Test Loss: 3.1139\n",
      "Epoch 4853, Train Loss: 3.0601, Test Loss: 3.1837\n",
      "Epoch 4854, Train Loss: 3.0936, Test Loss: 3.1646\n",
      "Epoch 4855, Train Loss: 3.1314, Test Loss: 3.1437\n",
      "Epoch 4856, Train Loss: 3.1089, Test Loss: 3.1664\n",
      "Epoch 4857, Train Loss: 3.1983, Test Loss: 3.1697\n",
      "Epoch 4858, Train Loss: 3.1146, Test Loss: 3.1959\n",
      "Epoch 4859, Train Loss: 3.1660, Test Loss: 3.1776\n",
      "Epoch 4860, Train Loss: 3.1397, Test Loss: 3.1272\n",
      "Epoch 4861, Train Loss: 3.1055, Test Loss: 3.1349\n",
      "Epoch 4862, Train Loss: 3.0774, Test Loss: 3.1994\n",
      "Epoch 4863, Train Loss: 3.1890, Test Loss: 3.2285\n",
      "Epoch 4864, Train Loss: 3.1777, Test Loss: 3.1801\n",
      "Epoch 4865, Train Loss: 3.1342, Test Loss: 3.1306\n",
      "Epoch 4866, Train Loss: 3.0619, Test Loss: 3.1140\n",
      "Epoch 4867, Train Loss: 3.0741, Test Loss: 3.1582\n",
      "Epoch 4868, Train Loss: 3.0952, Test Loss: 3.1933\n",
      "Epoch 4869, Train Loss: 3.1162, Test Loss: 3.1484\n",
      "Epoch 4870, Train Loss: 3.0862, Test Loss: 3.1607\n",
      "Epoch 4871, Train Loss: 3.0769, Test Loss: 3.1461\n",
      "Epoch 4872, Train Loss: 3.0972, Test Loss: 3.1653\n",
      "Epoch 4873, Train Loss: 3.1074, Test Loss: 3.1242\n",
      "Epoch 4874, Train Loss: 3.0464, Test Loss: 3.1567\n",
      "Epoch 4875, Train Loss: 3.1017, Test Loss: 3.1260\n",
      "Epoch 4876, Train Loss: 3.0692, Test Loss: 3.0977\n",
      "Epoch 4877, Train Loss: 3.0369, Test Loss: 3.1173\n",
      "Epoch 4878, Train Loss: 3.0592, Test Loss: 3.1026\n",
      "Epoch 4879, Train Loss: 3.0501, Test Loss: 3.0982\n",
      "Epoch 4880, Train Loss: 3.0281, Test Loss: 3.1198\n",
      "Epoch 4881, Train Loss: 3.0459, Test Loss: 3.0976\n",
      "Epoch 4882, Train Loss: 3.0495, Test Loss: 3.1349\n",
      "Epoch 4883, Train Loss: 3.0939, Test Loss: 3.1014\n",
      "Epoch 4884, Train Loss: 3.0204, Test Loss: 3.1433\n",
      "Epoch 4885, Train Loss: 3.0550, Test Loss: 3.1328\n",
      "Epoch 4886, Train Loss: 3.0500, Test Loss: 3.1012\n",
      "Epoch 4887, Train Loss: 3.0262, Test Loss: 3.0951\n",
      "Epoch 4888, Train Loss: 3.0385, Test Loss: 3.0895\n",
      "Epoch 4889, Train Loss: 3.0204, Test Loss: 3.0928\n",
      "Epoch 4890, Train Loss: 3.0339, Test Loss: 3.0997\n",
      "Epoch 4891, Train Loss: 3.0347, Test Loss: 3.0950\n",
      "Epoch 4892, Train Loss: 3.0308, Test Loss: 3.1025\n",
      "Epoch 4893, Train Loss: 3.0458, Test Loss: 3.1169\n",
      "Epoch 4894, Train Loss: 3.0477, Test Loss: 3.1324\n",
      "Epoch 4895, Train Loss: 3.0535, Test Loss: 3.1327\n",
      "Epoch 4896, Train Loss: 3.0815, Test Loss: 3.1025\n",
      "Epoch 4897, Train Loss: 3.0504, Test Loss: 3.0840\n",
      "Epoch 4898, Train Loss: 3.0225, Test Loss: 3.0859\n",
      "Epoch 4899, Train Loss: 3.0259, Test Loss: 3.1040\n",
      "Epoch 4900, Train Loss: 3.0481, Test Loss: 3.0946\n",
      "Epoch 4901, Train Loss: 3.0175, Test Loss: 3.0895\n",
      "Epoch 4902, Train Loss: 3.0170, Test Loss: 3.0913\n",
      "Epoch 4903, Train Loss: 3.0111, Test Loss: 3.0986\n",
      "Epoch 4904, Train Loss: 3.0249, Test Loss: 3.0950\n",
      "Epoch 4905, Train Loss: 3.0253, Test Loss: 3.0881\n",
      "Epoch 4906, Train Loss: 3.0129, Test Loss: 3.0896\n",
      "Epoch 4907, Train Loss: 3.0172, Test Loss: 3.0899\n",
      "Epoch 4908, Train Loss: 3.0177, Test Loss: 3.0853\n",
      "Epoch 4909, Train Loss: 3.0243, Test Loss: 3.0867\n",
      "Epoch 4910, Train Loss: 3.0220, Test Loss: 3.1027\n",
      "Epoch 4911, Train Loss: 3.0382, Test Loss: 3.1007\n",
      "Epoch 4912, Train Loss: 3.0444, Test Loss: 3.1042\n",
      "Epoch 4913, Train Loss: 3.0371, Test Loss: 3.0922\n",
      "Epoch 4914, Train Loss: 3.0281, Test Loss: 3.0884\n",
      "Epoch 4915, Train Loss: 3.0120, Test Loss: 3.1266\n",
      "Epoch 4916, Train Loss: 3.0614, Test Loss: 3.0914\n",
      "Epoch 4917, Train Loss: 3.0339, Test Loss: 3.0936\n",
      "Epoch 4918, Train Loss: 3.0405, Test Loss: 3.0927\n",
      "Epoch 4919, Train Loss: 3.0330, Test Loss: 3.0940\n",
      "Epoch 4920, Train Loss: 3.0334, Test Loss: 3.1044\n",
      "Epoch 4921, Train Loss: 3.0358, Test Loss: 3.0882\n",
      "Epoch 4922, Train Loss: 3.0316, Test Loss: 3.1130\n",
      "Epoch 4923, Train Loss: 3.0594, Test Loss: 3.0909\n",
      "Epoch 4924, Train Loss: 3.0277, Test Loss: 3.0852\n",
      "Epoch 4925, Train Loss: 3.0204, Test Loss: 3.1068\n",
      "Epoch 4926, Train Loss: 3.0342, Test Loss: 3.0912\n",
      "Epoch 4927, Train Loss: 3.0348, Test Loss: 3.0908\n",
      "Epoch 4928, Train Loss: 3.0198, Test Loss: 3.0970\n",
      "Epoch 4929, Train Loss: 3.0428, Test Loss: 3.1001\n",
      "Epoch 4930, Train Loss: 3.0408, Test Loss: 3.1297\n",
      "Epoch 4931, Train Loss: 3.0770, Test Loss: 3.0840\n",
      "Epoch 4932, Train Loss: 3.0109, Test Loss: 3.1167\n",
      "Epoch 4933, Train Loss: 3.0480, Test Loss: 3.1366\n",
      "Epoch 4934, Train Loss: 3.0701, Test Loss: 3.1073\n",
      "Epoch 4935, Train Loss: 3.0352, Test Loss: 3.0915\n",
      "Epoch 4936, Train Loss: 3.0406, Test Loss: 3.1115\n",
      "Epoch 4937, Train Loss: 3.0624, Test Loss: 3.1067\n",
      "Epoch 4938, Train Loss: 3.0305, Test Loss: 3.1342\n",
      "Epoch 4939, Train Loss: 3.0596, Test Loss: 3.1075\n",
      "Epoch 4940, Train Loss: 3.0386, Test Loss: 3.0873\n",
      "Epoch 4941, Train Loss: 3.0187, Test Loss: 3.1178\n",
      "Epoch 4942, Train Loss: 3.0741, Test Loss: 3.1295\n",
      "Epoch 4943, Train Loss: 3.0599, Test Loss: 3.1309\n",
      "Epoch 4944, Train Loss: 3.0620, Test Loss: 3.1480\n",
      "Epoch 4945, Train Loss: 3.1102, Test Loss: 3.0908\n",
      "Epoch 4946, Train Loss: 3.0238, Test Loss: 3.1205\n",
      "Epoch 4947, Train Loss: 3.0789, Test Loss: 3.1442\n",
      "Epoch 4948, Train Loss: 3.0726, Test Loss: 3.1030\n",
      "Epoch 4949, Train Loss: 3.0523, Test Loss: 3.1051\n",
      "Epoch 4950, Train Loss: 3.0619, Test Loss: 3.1074\n",
      "Epoch 4951, Train Loss: 3.0384, Test Loss: 3.1120\n",
      "Epoch 4952, Train Loss: 3.0617, Test Loss: 3.1262\n",
      "Epoch 4953, Train Loss: 3.0596, Test Loss: 3.1307\n",
      "Epoch 4954, Train Loss: 3.0719, Test Loss: 3.1213\n",
      "Epoch 4955, Train Loss: 3.0717, Test Loss: 3.1074\n",
      "Epoch 4956, Train Loss: 3.0439, Test Loss: 3.1359\n",
      "Epoch 4957, Train Loss: 3.0835, Test Loss: 3.1022\n",
      "Epoch 4958, Train Loss: 3.0368, Test Loss: 3.1046\n",
      "Epoch 4959, Train Loss: 3.0274, Test Loss: 3.1269\n",
      "Epoch 4960, Train Loss: 3.0773, Test Loss: 3.1117\n",
      "Epoch 4961, Train Loss: 3.0689, Test Loss: 3.0914\n",
      "Epoch 4962, Train Loss: 3.0440, Test Loss: 3.0940\n",
      "Epoch 4963, Train Loss: 3.0266, Test Loss: 3.1051\n",
      "Epoch 4964, Train Loss: 3.0374, Test Loss: 3.0963\n",
      "Epoch 4965, Train Loss: 3.0395, Test Loss: 3.1089\n",
      "Epoch 4966, Train Loss: 3.0357, Test Loss: 3.1094\n",
      "Epoch 4967, Train Loss: 3.0353, Test Loss: 3.0968\n",
      "Epoch 4968, Train Loss: 3.0381, Test Loss: 3.0895\n",
      "Epoch 4969, Train Loss: 3.0264, Test Loss: 3.0931\n",
      "Epoch 4970, Train Loss: 3.0182, Test Loss: 3.0998\n",
      "Epoch 4971, Train Loss: 3.0404, Test Loss: 3.0995\n",
      "Epoch 4972, Train Loss: 3.0339, Test Loss: 3.0846\n",
      "Epoch 4973, Train Loss: 3.0289, Test Loss: 3.0859\n",
      "Epoch 4974, Train Loss: 3.0163, Test Loss: 3.1064\n",
      "Epoch 4975, Train Loss: 3.0351, Test Loss: 3.1075\n",
      "Epoch 4976, Train Loss: 3.0530, Test Loss: 3.1013\n",
      "Epoch 4977, Train Loss: 3.0420, Test Loss: 3.0875\n",
      "Epoch 4978, Train Loss: 3.0199, Test Loss: 3.1074\n",
      "Epoch 4979, Train Loss: 3.0423, Test Loss: 3.1078\n",
      "Epoch 4980, Train Loss: 3.0463, Test Loss: 3.0911\n",
      "Epoch 4981, Train Loss: 3.0268, Test Loss: 3.0900\n",
      "Epoch 4982, Train Loss: 3.0264, Test Loss: 3.0976\n",
      "Epoch 4983, Train Loss: 3.0415, Test Loss: 3.1002\n",
      "Epoch 4984, Train Loss: 3.0369, Test Loss: 3.0925\n",
      "Epoch 4985, Train Loss: 3.0367, Test Loss: 3.0964\n",
      "Epoch 4986, Train Loss: 3.0421, Test Loss: 3.1332\n",
      "Epoch 4987, Train Loss: 3.0887, Test Loss: 3.0959\n",
      "Epoch 4988, Train Loss: 3.0339, Test Loss: 3.1031\n",
      "Epoch 4989, Train Loss: 3.0424, Test Loss: 3.0903\n",
      "Epoch 4990, Train Loss: 3.0373, Test Loss: 3.0975\n",
      "Epoch 4991, Train Loss: 3.0463, Test Loss: 3.0925\n",
      "Epoch 4992, Train Loss: 3.0245, Test Loss: 3.0883\n",
      "Epoch 4993, Train Loss: 3.0133, Test Loss: 3.0825\n",
      "Epoch 4994, Train Loss: 3.0221, Test Loss: 3.0927\n",
      "Epoch 4995, Train Loss: 3.0329, Test Loss: 3.1025\n",
      "Epoch 4996, Train Loss: 3.0232, Test Loss: 3.1141\n",
      "Epoch 4997, Train Loss: 3.0543, Test Loss: 3.1089\n",
      "Epoch 4998, Train Loss: 3.0668, Test Loss: 3.0974\n",
      "Epoch 4999, Train Loss: 3.0437, Test Loss: 3.0923\n",
      "Epoch 5000, Train Loss: 3.0407, Test Loss: 3.0926\n",
      "Epoch 5001, Train Loss: 3.0337, Test Loss: 3.1121\n",
      "Epoch 5002, Train Loss: 3.0546, Test Loss: 3.0960\n",
      "Epoch 5003, Train Loss: 3.0286, Test Loss: 3.1500\n",
      "Epoch 5004, Train Loss: 3.0888, Test Loss: 3.0967\n",
      "Epoch 5005, Train Loss: 3.0296, Test Loss: 3.1116\n",
      "Epoch 5006, Train Loss: 3.0374, Test Loss: 3.0992\n",
      "Epoch 5007, Train Loss: 3.0475, Test Loss: 3.0744\n",
      "Epoch 5008, Train Loss: 3.0203, Test Loss: 3.0855\n",
      "Epoch 5009, Train Loss: 3.0403, Test Loss: 3.1061\n",
      "Epoch 5010, Train Loss: 3.0463, Test Loss: 3.1184\n",
      "Epoch 5011, Train Loss: 3.0620, Test Loss: 3.1118\n",
      "Epoch 5012, Train Loss: 3.0411, Test Loss: 3.0917\n",
      "Epoch 5013, Train Loss: 3.0163, Test Loss: 3.1147\n",
      "Epoch 5014, Train Loss: 3.0415, Test Loss: 3.1288\n",
      "Epoch 5015, Train Loss: 3.0392, Test Loss: 3.1166\n",
      "Epoch 5016, Train Loss: 3.0496, Test Loss: 3.0998\n",
      "Epoch 5017, Train Loss: 3.0590, Test Loss: 3.1194\n",
      "Epoch 5018, Train Loss: 3.0753, Test Loss: 3.1177\n",
      "Epoch 5019, Train Loss: 3.0782, Test Loss: 3.1105\n",
      "Epoch 5020, Train Loss: 3.0715, Test Loss: 3.0888\n",
      "Epoch 5021, Train Loss: 3.0356, Test Loss: 3.1249\n",
      "Epoch 5022, Train Loss: 3.0472, Test Loss: 3.1128\n",
      "Epoch 5023, Train Loss: 3.0502, Test Loss: 3.1446\n",
      "Epoch 5024, Train Loss: 3.1244, Test Loss: 3.1528\n",
      "Epoch 5025, Train Loss: 3.0716, Test Loss: 3.1230\n",
      "Epoch 5026, Train Loss: 3.0789, Test Loss: 3.1100\n",
      "Epoch 5027, Train Loss: 3.0590, Test Loss: 3.1643\n",
      "Epoch 5028, Train Loss: 3.1146, Test Loss: 3.1528\n",
      "Epoch 5029, Train Loss: 3.1029, Test Loss: 3.1033\n",
      "Epoch 5030, Train Loss: 3.0591, Test Loss: 3.1318\n",
      "Epoch 5031, Train Loss: 3.1202, Test Loss: 3.1035\n",
      "Epoch 5032, Train Loss: 3.0503, Test Loss: 3.1273\n",
      "Epoch 5033, Train Loss: 3.0812, Test Loss: 3.1674\n",
      "Epoch 5034, Train Loss: 3.1294, Test Loss: 3.1296\n",
      "Epoch 5035, Train Loss: 3.0545, Test Loss: 3.1257\n",
      "Epoch 5036, Train Loss: 3.0685, Test Loss: 3.0925\n",
      "Epoch 5037, Train Loss: 3.0319, Test Loss: 3.1239\n",
      "Epoch 5038, Train Loss: 3.0544, Test Loss: 3.1640\n",
      "Epoch 5039, Train Loss: 3.0930, Test Loss: 3.1176\n",
      "Epoch 5040, Train Loss: 3.0569, Test Loss: 3.1039\n",
      "Epoch 5041, Train Loss: 3.0881, Test Loss: 3.1454\n",
      "Epoch 5042, Train Loss: 3.1139, Test Loss: 3.1467\n",
      "Epoch 5043, Train Loss: 3.0825, Test Loss: 3.1382\n",
      "Epoch 5044, Train Loss: 3.0917, Test Loss: 3.1533\n",
      "Epoch 5045, Train Loss: 3.0612, Test Loss: 3.1775\n",
      "Epoch 5046, Train Loss: 3.1040, Test Loss: 3.1595\n",
      "Epoch 5047, Train Loss: 3.1113, Test Loss: 3.1624\n",
      "Epoch 5048, Train Loss: 3.1057, Test Loss: 3.1999\n",
      "Epoch 5049, Train Loss: 3.1449, Test Loss: 3.1752\n",
      "Epoch 5050, Train Loss: 3.1178, Test Loss: 3.1296\n",
      "Epoch 5051, Train Loss: 3.0941, Test Loss: 3.1708\n",
      "Epoch 5052, Train Loss: 3.1641, Test Loss: 3.1286\n",
      "Epoch 5053, Train Loss: 3.1319, Test Loss: 3.1306\n",
      "Epoch 5054, Train Loss: 3.1060, Test Loss: 3.0876\n",
      "Epoch 5055, Train Loss: 3.0430, Test Loss: 3.1497\n",
      "Epoch 5056, Train Loss: 3.0986, Test Loss: 3.1822\n",
      "Epoch 5057, Train Loss: 3.0991, Test Loss: 3.1767\n",
      "Epoch 5058, Train Loss: 3.1037, Test Loss: 3.1428\n",
      "Epoch 5059, Train Loss: 3.0711, Test Loss: 3.1175\n",
      "Epoch 5060, Train Loss: 3.0510, Test Loss: 3.1181\n",
      "Epoch 5061, Train Loss: 3.0814, Test Loss: 3.1143\n",
      "Epoch 5062, Train Loss: 3.1104, Test Loss: 3.1246\n",
      "Epoch 5063, Train Loss: 3.1194, Test Loss: 3.1703\n",
      "Epoch 5064, Train Loss: 3.0786, Test Loss: 3.2598\n",
      "Epoch 5065, Train Loss: 3.1886, Test Loss: 3.2830\n",
      "Epoch 5066, Train Loss: 3.2331, Test Loss: 3.2370\n",
      "Epoch 5067, Train Loss: 3.1599, Test Loss: 3.2075\n",
      "Epoch 5068, Train Loss: 3.1347, Test Loss: 3.1643\n",
      "Epoch 5069, Train Loss: 3.0719, Test Loss: 3.1858\n",
      "Epoch 5070, Train Loss: 3.1441, Test Loss: 3.1887\n",
      "Epoch 5071, Train Loss: 3.1117, Test Loss: 3.1572\n",
      "Epoch 5072, Train Loss: 3.0799, Test Loss: 3.1473\n",
      "Epoch 5073, Train Loss: 3.0877, Test Loss: 3.1272\n",
      "Epoch 5074, Train Loss: 3.0877, Test Loss: 3.1334\n",
      "Epoch 5075, Train Loss: 3.0822, Test Loss: 3.1233\n",
      "Epoch 5076, Train Loss: 3.0952, Test Loss: 3.1197\n",
      "Epoch 5077, Train Loss: 3.0732, Test Loss: 3.1190\n",
      "Epoch 5078, Train Loss: 3.0504, Test Loss: 3.1642\n",
      "Epoch 5079, Train Loss: 3.1309, Test Loss: 3.1597\n",
      "Epoch 5080, Train Loss: 3.1040, Test Loss: 3.1376\n",
      "Epoch 5081, Train Loss: 3.0884, Test Loss: 3.2064\n",
      "Epoch 5082, Train Loss: 3.1236, Test Loss: 3.1739\n",
      "Epoch 5083, Train Loss: 3.1158, Test Loss: 3.2131\n",
      "Epoch 5084, Train Loss: 3.1703, Test Loss: 3.1594\n",
      "Epoch 5085, Train Loss: 3.1180, Test Loss: 3.1936\n",
      "Epoch 5086, Train Loss: 3.1709, Test Loss: 3.1690\n",
      "Epoch 5087, Train Loss: 3.1243, Test Loss: 3.1831\n",
      "Epoch 5088, Train Loss: 3.1689, Test Loss: 3.1939\n",
      "Epoch 5089, Train Loss: 3.1774, Test Loss: 3.1335\n",
      "Epoch 5090, Train Loss: 3.1040, Test Loss: 3.1203\n",
      "Epoch 5091, Train Loss: 3.0755, Test Loss: 3.1822\n",
      "Epoch 5092, Train Loss: 3.1614, Test Loss: 3.1540\n",
      "Epoch 5093, Train Loss: 3.0692, Test Loss: 3.1857\n",
      "Epoch 5094, Train Loss: 3.1289, Test Loss: 3.1463\n",
      "Epoch 5095, Train Loss: 3.0727, Test Loss: 3.1515\n",
      "Epoch 5096, Train Loss: 3.0888, Test Loss: 3.1389\n",
      "Epoch 5097, Train Loss: 3.0663, Test Loss: 3.1280\n",
      "Epoch 5098, Train Loss: 3.0855, Test Loss: 3.0937\n",
      "Epoch 5099, Train Loss: 3.0335, Test Loss: 3.1418\n",
      "Epoch 5100, Train Loss: 3.1040, Test Loss: 3.1248\n",
      "Epoch 5101, Train Loss: 3.0570, Test Loss: 3.1334\n",
      "Epoch 5102, Train Loss: 3.0977, Test Loss: 3.1103\n",
      "Epoch 5103, Train Loss: 3.0380, Test Loss: 3.1440\n",
      "Epoch 5104, Train Loss: 3.1192, Test Loss: 3.1408\n",
      "Epoch 5105, Train Loss: 3.1004, Test Loss: 3.1283\n",
      "Epoch 5106, Train Loss: 3.0996, Test Loss: 3.1647\n",
      "Epoch 5107, Train Loss: 3.1015, Test Loss: 3.1348\n",
      "Epoch 5108, Train Loss: 3.0521, Test Loss: 3.1547\n",
      "Epoch 5109, Train Loss: 3.1010, Test Loss: 3.1195\n",
      "Epoch 5110, Train Loss: 3.0476, Test Loss: 3.1446\n",
      "Epoch 5111, Train Loss: 3.0999, Test Loss: 3.1441\n",
      "Epoch 5112, Train Loss: 3.0868, Test Loss: 3.1204\n",
      "Epoch 5113, Train Loss: 3.0777, Test Loss: 3.1399\n",
      "Epoch 5114, Train Loss: 3.1001, Test Loss: 3.0978\n",
      "Epoch 5115, Train Loss: 3.0454, Test Loss: 3.1061\n",
      "Epoch 5116, Train Loss: 3.0466, Test Loss: 3.1005\n",
      "Epoch 5117, Train Loss: 3.0491, Test Loss: 3.0806\n",
      "Epoch 5118, Train Loss: 3.0295, Test Loss: 3.1143\n",
      "Epoch 5119, Train Loss: 3.0562, Test Loss: 3.1217\n",
      "Epoch 5120, Train Loss: 3.0570, Test Loss: 3.1100\n",
      "Epoch 5121, Train Loss: 3.0394, Test Loss: 3.1007\n",
      "Epoch 5122, Train Loss: 3.0226, Test Loss: 3.0956\n",
      "Epoch 5123, Train Loss: 3.0318, Test Loss: 3.0938\n",
      "Epoch 5124, Train Loss: 3.0309, Test Loss: 3.0820\n",
      "Epoch 5125, Train Loss: 3.0153, Test Loss: 3.0809\n",
      "Epoch 5126, Train Loss: 3.0204, Test Loss: 3.0818\n",
      "Epoch 5127, Train Loss: 3.0284, Test Loss: 3.0930\n",
      "Epoch 5128, Train Loss: 3.0294, Test Loss: 3.0973\n",
      "Epoch 5129, Train Loss: 3.0310, Test Loss: 3.0993\n",
      "Epoch 5130, Train Loss: 3.0307, Test Loss: 3.1021\n",
      "Epoch 5131, Train Loss: 3.0379, Test Loss: 3.1102\n",
      "Epoch 5132, Train Loss: 3.0302, Test Loss: 3.0947\n",
      "Epoch 5133, Train Loss: 3.0290, Test Loss: 3.0798\n",
      "Epoch 5134, Train Loss: 3.0097, Test Loss: 3.0879\n",
      "Epoch 5135, Train Loss: 3.0363, Test Loss: 3.0985\n",
      "Epoch 5136, Train Loss: 3.0388, Test Loss: 3.1118\n",
      "Epoch 5137, Train Loss: 3.0566, Test Loss: 3.1042\n",
      "Epoch 5138, Train Loss: 3.0307, Test Loss: 3.0891\n",
      "Epoch 5139, Train Loss: 3.0211, Test Loss: 3.1228\n",
      "Epoch 5140, Train Loss: 3.0612, Test Loss: 3.1122\n",
      "Epoch 5141, Train Loss: 3.0503, Test Loss: 3.1044\n",
      "Epoch 5142, Train Loss: 3.0444, Test Loss: 3.0967\n",
      "Epoch 5143, Train Loss: 3.0533, Test Loss: 3.1005\n",
      "Epoch 5144, Train Loss: 3.0309, Test Loss: 3.1194\n",
      "Epoch 5145, Train Loss: 3.0511, Test Loss: 3.1434\n",
      "Epoch 5146, Train Loss: 3.1004, Test Loss: 3.0991\n",
      "Epoch 5147, Train Loss: 3.0312, Test Loss: 3.1316\n",
      "Epoch 5148, Train Loss: 3.0875, Test Loss: 3.1077\n",
      "Epoch 5149, Train Loss: 3.0535, Test Loss: 3.0903\n",
      "Epoch 5150, Train Loss: 3.0177, Test Loss: 3.1350\n",
      "Epoch 5151, Train Loss: 3.0469, Test Loss: 3.0941\n",
      "Epoch 5152, Train Loss: 3.0221, Test Loss: 3.0895\n",
      "Epoch 5153, Train Loss: 3.0301, Test Loss: 3.1154\n",
      "Epoch 5154, Train Loss: 3.0831, Test Loss: 3.1081\n",
      "Epoch 5155, Train Loss: 3.0485, Test Loss: 3.1131\n",
      "Epoch 5156, Train Loss: 3.0542, Test Loss: 3.1388\n",
      "Epoch 5157, Train Loss: 3.0846, Test Loss: 3.0914\n",
      "Epoch 5158, Train Loss: 3.0446, Test Loss: 3.1010\n",
      "Epoch 5159, Train Loss: 3.0463, Test Loss: 3.0873\n",
      "Epoch 5160, Train Loss: 3.0305, Test Loss: 3.1016\n",
      "Epoch 5161, Train Loss: 3.0335, Test Loss: 3.1208\n",
      "Epoch 5162, Train Loss: 3.0511, Test Loss: 3.1058\n",
      "Epoch 5163, Train Loss: 3.0472, Test Loss: 3.0771\n",
      "Epoch 5164, Train Loss: 3.0216, Test Loss: 3.0844\n",
      "Epoch 5165, Train Loss: 3.0253, Test Loss: 3.0957\n",
      "Epoch 5166, Train Loss: 3.0328, Test Loss: 3.0886\n",
      "Epoch 5167, Train Loss: 3.0253, Test Loss: 3.0877\n",
      "Epoch 5168, Train Loss: 3.0206, Test Loss: 3.0990\n",
      "Epoch 5169, Train Loss: 3.0286, Test Loss: 3.1100\n",
      "Epoch 5170, Train Loss: 3.0305, Test Loss: 3.0774\n",
      "Epoch 5171, Train Loss: 3.0119, Test Loss: 3.0786\n",
      "Epoch 5172, Train Loss: 3.0157, Test Loss: 3.0803\n",
      "Epoch 5173, Train Loss: 3.0280, Test Loss: 3.0793\n",
      "Epoch 5174, Train Loss: 3.0121, Test Loss: 3.0928\n",
      "Epoch 5175, Train Loss: 3.0243, Test Loss: 3.0821\n",
      "Epoch 5176, Train Loss: 3.0205, Test Loss: 3.0813\n",
      "Epoch 5177, Train Loss: 3.0211, Test Loss: 3.0854\n",
      "Epoch 5178, Train Loss: 3.0267, Test Loss: 3.0946\n",
      "Epoch 5179, Train Loss: 3.0329, Test Loss: 3.0957\n",
      "Epoch 5180, Train Loss: 3.0266, Test Loss: 3.0976\n",
      "Epoch 5181, Train Loss: 3.0581, Test Loss: 3.0806\n",
      "Epoch 5182, Train Loss: 3.0181, Test Loss: 3.0850\n",
      "Epoch 5183, Train Loss: 3.0202, Test Loss: 3.0934\n",
      "Epoch 5184, Train Loss: 3.0226, Test Loss: 3.0839\n",
      "Epoch 5185, Train Loss: 3.0136, Test Loss: 3.0959\n",
      "Epoch 5186, Train Loss: 3.0263, Test Loss: 3.0940\n",
      "Epoch 5187, Train Loss: 3.0222, Test Loss: 3.0921\n",
      "Epoch 5188, Train Loss: 3.0189, Test Loss: 3.0798\n",
      "Epoch 5189, Train Loss: 3.0127, Test Loss: 3.0794\n",
      "Epoch 5190, Train Loss: 3.0307, Test Loss: 3.0912\n",
      "Epoch 5191, Train Loss: 3.0259, Test Loss: 3.1040\n",
      "Epoch 5192, Train Loss: 3.0315, Test Loss: 3.1009\n",
      "Epoch 5193, Train Loss: 3.0395, Test Loss: 3.0892\n",
      "Epoch 5194, Train Loss: 3.0387, Test Loss: 3.1086\n",
      "Epoch 5195, Train Loss: 3.0539, Test Loss: 3.0907\n",
      "Epoch 5196, Train Loss: 3.0177, Test Loss: 3.1266\n",
      "Epoch 5197, Train Loss: 3.0593, Test Loss: 3.0895\n",
      "Epoch 5198, Train Loss: 3.0291, Test Loss: 3.0968\n",
      "Epoch 5199, Train Loss: 3.0449, Test Loss: 3.0812\n",
      "Epoch 5200, Train Loss: 3.0197, Test Loss: 3.0846\n",
      "Epoch 5201, Train Loss: 3.0161, Test Loss: 3.1145\n",
      "Epoch 5202, Train Loss: 3.0416, Test Loss: 3.0824\n",
      "Epoch 5203, Train Loss: 3.0099, Test Loss: 3.0871\n",
      "Epoch 5204, Train Loss: 3.0342, Test Loss: 3.0948\n",
      "Epoch 5205, Train Loss: 3.0355, Test Loss: 3.1099\n",
      "Epoch 5206, Train Loss: 3.0304, Test Loss: 3.1033\n",
      "Epoch 5207, Train Loss: 3.0286, Test Loss: 3.1153\n",
      "Epoch 5208, Train Loss: 3.0604, Test Loss: 3.0970\n",
      "Epoch 5209, Train Loss: 3.0385, Test Loss: 3.1153\n",
      "Epoch 5210, Train Loss: 3.0427, Test Loss: 3.1159\n",
      "Epoch 5211, Train Loss: 3.0681, Test Loss: 3.1498\n",
      "Epoch 5212, Train Loss: 3.0975, Test Loss: 3.1798\n",
      "Epoch 5213, Train Loss: 3.1437, Test Loss: 3.1019\n",
      "Epoch 5214, Train Loss: 3.0355, Test Loss: 3.1263\n",
      "Epoch 5215, Train Loss: 3.0607, Test Loss: 3.1360\n",
      "Epoch 5216, Train Loss: 3.0786, Test Loss: 3.1262\n",
      "Epoch 5217, Train Loss: 3.0615, Test Loss: 3.1108\n",
      "Epoch 5218, Train Loss: 3.0670, Test Loss: 3.0887\n",
      "Epoch 5219, Train Loss: 3.0256, Test Loss: 3.1079\n",
      "Epoch 5220, Train Loss: 3.0431, Test Loss: 3.1168\n",
      "Epoch 5221, Train Loss: 3.0344, Test Loss: 3.0837\n",
      "Epoch 5222, Train Loss: 3.0139, Test Loss: 3.1072\n",
      "Epoch 5223, Train Loss: 3.0529, Test Loss: 3.1006\n",
      "Epoch 5224, Train Loss: 3.0390, Test Loss: 3.1027\n",
      "Epoch 5225, Train Loss: 3.0497, Test Loss: 3.1085\n",
      "Epoch 5226, Train Loss: 3.0399, Test Loss: 3.0954\n",
      "Epoch 5227, Train Loss: 3.0296, Test Loss: 3.1076\n",
      "Epoch 5228, Train Loss: 3.0502, Test Loss: 3.0774\n",
      "Epoch 5229, Train Loss: 3.0077, Test Loss: 3.1094\n",
      "Epoch 5230, Train Loss: 3.0481, Test Loss: 3.1197\n",
      "Epoch 5231, Train Loss: 3.0491, Test Loss: 3.0765\n",
      "Epoch 5232, Train Loss: 3.0128, Test Loss: 3.1070\n",
      "Epoch 5233, Train Loss: 3.0704, Test Loss: 3.0943\n",
      "Epoch 5234, Train Loss: 3.0396, Test Loss: 3.1047\n",
      "Epoch 5235, Train Loss: 3.0377, Test Loss: 3.1102\n",
      "Epoch 5236, Train Loss: 3.0349, Test Loss: 3.0934\n",
      "Epoch 5237, Train Loss: 3.0270, Test Loss: 3.1060\n",
      "Epoch 5238, Train Loss: 3.0621, Test Loss: 3.0975\n",
      "Epoch 5239, Train Loss: 3.0413, Test Loss: 3.1217\n",
      "Epoch 5240, Train Loss: 3.0522, Test Loss: 3.1080\n",
      "Epoch 5241, Train Loss: 3.0413, Test Loss: 3.0760\n",
      "Epoch 5242, Train Loss: 3.0192, Test Loss: 3.1132\n",
      "Epoch 5243, Train Loss: 3.0820, Test Loss: 3.1140\n",
      "Epoch 5244, Train Loss: 3.0387, Test Loss: 3.1324\n",
      "Epoch 5245, Train Loss: 3.0658, Test Loss: 3.1305\n",
      "Epoch 5246, Train Loss: 3.0694, Test Loss: 3.1022\n",
      "Epoch 5247, Train Loss: 3.0367, Test Loss: 3.1037\n",
      "Epoch 5248, Train Loss: 3.0464, Test Loss: 3.0896\n",
      "Epoch 5249, Train Loss: 3.0232, Test Loss: 3.1298\n",
      "Epoch 5250, Train Loss: 3.0738, Test Loss: 3.1360\n",
      "Epoch 5251, Train Loss: 3.0585, Test Loss: 3.1246\n",
      "Epoch 5252, Train Loss: 3.0583, Test Loss: 3.0849\n",
      "Epoch 5253, Train Loss: 3.0376, Test Loss: 3.1042\n",
      "Epoch 5254, Train Loss: 3.0627, Test Loss: 3.1392\n",
      "Epoch 5255, Train Loss: 3.0763, Test Loss: 3.1608\n",
      "Epoch 5256, Train Loss: 3.1180, Test Loss: 3.1004\n",
      "Epoch 5257, Train Loss: 3.0245, Test Loss: 3.1025\n",
      "Epoch 5258, Train Loss: 3.0412, Test Loss: 3.1373\n",
      "Epoch 5259, Train Loss: 3.1135, Test Loss: 3.1207\n",
      "Epoch 5260, Train Loss: 3.0694, Test Loss: 3.1503\n",
      "Epoch 5261, Train Loss: 3.0874, Test Loss: 3.1155\n",
      "Epoch 5262, Train Loss: 3.0535, Test Loss: 3.1096\n",
      "Epoch 5263, Train Loss: 3.0709, Test Loss: 3.1204\n",
      "Epoch 5264, Train Loss: 3.0917, Test Loss: 3.1714\n",
      "Epoch 5265, Train Loss: 3.1609, Test Loss: 3.1974\n",
      "Epoch 5266, Train Loss: 3.1053, Test Loss: 3.1891\n",
      "Epoch 5267, Train Loss: 3.1457, Test Loss: 3.1804\n",
      "Epoch 5268, Train Loss: 3.1264, Test Loss: 3.1404\n",
      "Epoch 5269, Train Loss: 3.0832, Test Loss: 3.1471\n",
      "Epoch 5270, Train Loss: 3.0930, Test Loss: 3.1989\n",
      "Epoch 5271, Train Loss: 3.1463, Test Loss: 3.1154\n",
      "Epoch 5272, Train Loss: 3.0836, Test Loss: 3.1455\n",
      "Epoch 5273, Train Loss: 3.0905, Test Loss: 3.1569\n",
      "Epoch 5274, Train Loss: 3.1145, Test Loss: 3.1270\n",
      "Epoch 5275, Train Loss: 3.0839, Test Loss: 3.1382\n",
      "Epoch 5276, Train Loss: 3.0957, Test Loss: 3.1526\n",
      "Epoch 5277, Train Loss: 3.1109, Test Loss: 3.1474\n",
      "Epoch 5278, Train Loss: 3.0783, Test Loss: 3.1487\n",
      "Epoch 5279, Train Loss: 3.0750, Test Loss: 3.1450\n",
      "Epoch 5280, Train Loss: 3.1124, Test Loss: 3.0953\n",
      "Epoch 5281, Train Loss: 3.0404, Test Loss: 3.1380\n",
      "Epoch 5282, Train Loss: 3.0869, Test Loss: 3.1962\n",
      "Epoch 5283, Train Loss: 3.1142, Test Loss: 3.1974\n",
      "Epoch 5284, Train Loss: 3.1485, Test Loss: 3.0973\n",
      "Epoch 5285, Train Loss: 3.0478, Test Loss: 3.1146\n",
      "Epoch 5286, Train Loss: 3.0818, Test Loss: 3.1081\n",
      "Epoch 5287, Train Loss: 3.0541, Test Loss: 3.1201\n",
      "Epoch 5288, Train Loss: 3.0777, Test Loss: 3.0873\n",
      "Epoch 5289, Train Loss: 3.0282, Test Loss: 3.1289\n",
      "Epoch 5290, Train Loss: 3.0866, Test Loss: 3.1073\n",
      "Epoch 5291, Train Loss: 3.0581, Test Loss: 3.1028\n",
      "Epoch 5292, Train Loss: 3.0503, Test Loss: 3.1011\n",
      "Epoch 5293, Train Loss: 3.0348, Test Loss: 3.1148\n",
      "Epoch 5294, Train Loss: 3.0482, Test Loss: 3.0830\n",
      "Epoch 5295, Train Loss: 3.0222, Test Loss: 3.0836\n",
      "Epoch 5296, Train Loss: 3.0283, Test Loss: 3.1076\n",
      "Epoch 5297, Train Loss: 3.0529, Test Loss: 3.1027\n",
      "Epoch 5298, Train Loss: 3.0547, Test Loss: 3.0844\n",
      "Epoch 5299, Train Loss: 3.0252, Test Loss: 3.0915\n",
      "Epoch 5300, Train Loss: 3.0276, Test Loss: 3.0984\n",
      "Epoch 5301, Train Loss: 3.0444, Test Loss: 3.0936\n",
      "Epoch 5302, Train Loss: 3.0343, Test Loss: 3.1074\n",
      "Epoch 5303, Train Loss: 3.0560, Test Loss: 3.0919\n",
      "Epoch 5304, Train Loss: 3.0414, Test Loss: 3.0961\n",
      "Epoch 5305, Train Loss: 3.0300, Test Loss: 3.0976\n",
      "Epoch 5306, Train Loss: 3.0429, Test Loss: 3.1043\n",
      "Epoch 5307, Train Loss: 3.0589, Test Loss: 3.0919\n",
      "Epoch 5308, Train Loss: 3.0374, Test Loss: 3.0998\n",
      "Epoch 5309, Train Loss: 3.0483, Test Loss: 3.1008\n",
      "Epoch 5310, Train Loss: 3.0400, Test Loss: 3.1307\n",
      "Epoch 5311, Train Loss: 3.0509, Test Loss: 3.1053\n",
      "Epoch 5312, Train Loss: 3.0380, Test Loss: 3.1181\n",
      "Epoch 5313, Train Loss: 3.0578, Test Loss: 3.1139\n",
      "Epoch 5314, Train Loss: 3.0792, Test Loss: 3.0950\n",
      "Epoch 5315, Train Loss: 3.0432, Test Loss: 3.0996\n",
      "Epoch 5316, Train Loss: 3.0341, Test Loss: 3.1093\n",
      "Epoch 5317, Train Loss: 3.0420, Test Loss: 3.0881\n",
      "Epoch 5318, Train Loss: 3.0291, Test Loss: 3.0948\n",
      "Epoch 5319, Train Loss: 3.0594, Test Loss: 3.1108\n",
      "Epoch 5320, Train Loss: 3.0591, Test Loss: 3.1265\n",
      "Epoch 5321, Train Loss: 3.0533, Test Loss: 3.1240\n",
      "Epoch 5322, Train Loss: 3.0595, Test Loss: 3.1091\n",
      "Epoch 5323, Train Loss: 3.0370, Test Loss: 3.0778\n",
      "Epoch 5324, Train Loss: 3.0124, Test Loss: 3.0877\n",
      "Epoch 5325, Train Loss: 3.0256, Test Loss: 3.1023\n",
      "Epoch 5326, Train Loss: 3.0434, Test Loss: 3.0878\n",
      "Epoch 5327, Train Loss: 3.0165, Test Loss: 3.0797\n",
      "Epoch 5328, Train Loss: 3.0303, Test Loss: 3.0871\n",
      "Epoch 5329, Train Loss: 3.0296, Test Loss: 3.1073\n",
      "Epoch 5330, Train Loss: 3.0454, Test Loss: 3.0971\n",
      "Epoch 5331, Train Loss: 3.0457, Test Loss: 3.0939\n",
      "Epoch 5332, Train Loss: 3.0396, Test Loss: 3.0838\n",
      "Epoch 5333, Train Loss: 3.0223, Test Loss: 3.0937\n",
      "Epoch 5334, Train Loss: 3.0369, Test Loss: 3.0889\n",
      "Epoch 5335, Train Loss: 3.0187, Test Loss: 3.1043\n",
      "Epoch 5336, Train Loss: 3.0610, Test Loss: 3.0832\n",
      "Epoch 5337, Train Loss: 3.0210, Test Loss: 3.1127\n",
      "Epoch 5338, Train Loss: 3.0573, Test Loss: 3.1481\n",
      "Epoch 5339, Train Loss: 3.0734, Test Loss: 3.0991\n",
      "Epoch 5340, Train Loss: 3.0215, Test Loss: 3.1076\n",
      "Epoch 5341, Train Loss: 3.0519, Test Loss: 3.0870\n",
      "Epoch 5342, Train Loss: 3.0376, Test Loss: 3.0870\n",
      "Epoch 5343, Train Loss: 3.0311, Test Loss: 3.1109\n",
      "Epoch 5344, Train Loss: 3.0465, Test Loss: 3.0917\n",
      "Epoch 5345, Train Loss: 3.0371, Test Loss: 3.0919\n",
      "Epoch 5346, Train Loss: 3.0443, Test Loss: 3.0921\n",
      "Epoch 5347, Train Loss: 3.0335, Test Loss: 3.0892\n",
      "Epoch 5348, Train Loss: 3.0241, Test Loss: 3.0803\n",
      "Epoch 5349, Train Loss: 3.0093, Test Loss: 3.0774\n",
      "Epoch 5350, Train Loss: 3.0171, Test Loss: 3.0843\n",
      "Epoch 5351, Train Loss: 3.0221, Test Loss: 3.0879\n",
      "Epoch 5352, Train Loss: 3.0306, Test Loss: 3.0838\n",
      "Epoch 5353, Train Loss: 3.0108, Test Loss: 3.0794\n",
      "Epoch 5354, Train Loss: 3.0205, Test Loss: 3.0862\n",
      "Epoch 5355, Train Loss: 3.0249, Test Loss: 3.0932\n",
      "Epoch 5356, Train Loss: 3.0318, Test Loss: 3.0869\n",
      "Epoch 5357, Train Loss: 3.0338, Test Loss: 3.0743\n",
      "Epoch 5358, Train Loss: 3.0079, Test Loss: 3.0824\n",
      "Epoch 5359, Train Loss: 3.0284, Test Loss: 3.0897\n",
      "Epoch 5360, Train Loss: 3.0263, Test Loss: 3.0826\n",
      "Epoch 5361, Train Loss: 3.0208, Test Loss: 3.0786\n",
      "Epoch 5362, Train Loss: 3.0147, Test Loss: 3.0710\n",
      "Epoch 5363, Train Loss: 3.0079, Test Loss: 3.0776\n",
      "Epoch 5364, Train Loss: 3.0114, Test Loss: 3.0766\n",
      "Epoch 5365, Train Loss: 3.0146, Test Loss: 3.0761\n",
      "Epoch 5366, Train Loss: 3.0144, Test Loss: 3.0788\n",
      "Epoch 5367, Train Loss: 3.0174, Test Loss: 3.0777\n",
      "Epoch 5368, Train Loss: 3.0201, Test Loss: 3.0936\n",
      "Epoch 5369, Train Loss: 3.0258, Test Loss: 3.0806\n",
      "Epoch 5370, Train Loss: 3.0131, Test Loss: 3.0758\n",
      "Epoch 5371, Train Loss: 3.0159, Test Loss: 3.0736\n",
      "Epoch 5372, Train Loss: 3.0215, Test Loss: 3.0769\n",
      "Epoch 5373, Train Loss: 3.0084, Test Loss: 3.0872\n",
      "Epoch 5374, Train Loss: 3.0248, Test Loss: 3.0749\n",
      "Epoch 5375, Train Loss: 3.0153, Test Loss: 3.0898\n",
      "Epoch 5376, Train Loss: 3.0301, Test Loss: 3.0923\n",
      "Epoch 5377, Train Loss: 3.0159, Test Loss: 3.0949\n",
      "Epoch 5378, Train Loss: 3.0218, Test Loss: 3.0779\n",
      "Epoch 5379, Train Loss: 3.0167, Test Loss: 3.0753\n",
      "Epoch 5380, Train Loss: 3.0153, Test Loss: 3.0780\n",
      "Epoch 5381, Train Loss: 3.0207, Test Loss: 3.0906\n",
      "Epoch 5382, Train Loss: 3.0195, Test Loss: 3.1025\n",
      "Epoch 5383, Train Loss: 3.0413, Test Loss: 3.0956\n",
      "Epoch 5384, Train Loss: 3.0375, Test Loss: 3.0926\n",
      "Epoch 5385, Train Loss: 3.0352, Test Loss: 3.1086\n",
      "Epoch 5386, Train Loss: 3.0333, Test Loss: 3.1140\n",
      "Epoch 5387, Train Loss: 3.0504, Test Loss: 3.0998\n",
      "Epoch 5388, Train Loss: 3.0374, Test Loss: 3.0942\n",
      "Epoch 5389, Train Loss: 3.0340, Test Loss: 3.0983\n",
      "Epoch 5390, Train Loss: 3.0321, Test Loss: 3.1209\n",
      "Epoch 5391, Train Loss: 3.0713, Test Loss: 3.0981\n",
      "Epoch 5392, Train Loss: 3.0404, Test Loss: 3.0983\n",
      "Epoch 5393, Train Loss: 3.0617, Test Loss: 3.0806\n",
      "Epoch 5394, Train Loss: 3.0253, Test Loss: 3.0874\n",
      "Epoch 5395, Train Loss: 3.0230, Test Loss: 3.0953\n",
      "Epoch 5396, Train Loss: 3.0315, Test Loss: 3.0918\n",
      "Epoch 5397, Train Loss: 3.0420, Test Loss: 3.0816\n",
      "Epoch 5398, Train Loss: 3.0212, Test Loss: 3.0896\n",
      "Epoch 5399, Train Loss: 3.0361, Test Loss: 3.0777\n",
      "Epoch 5400, Train Loss: 3.0204, Test Loss: 3.0863\n",
      "Epoch 5401, Train Loss: 3.0261, Test Loss: 3.0779\n",
      "Epoch 5402, Train Loss: 3.0191, Test Loss: 3.0865\n",
      "Epoch 5403, Train Loss: 3.0208, Test Loss: 3.0737\n",
      "Epoch 5404, Train Loss: 3.0093, Test Loss: 3.0778\n",
      "Epoch 5405, Train Loss: 3.0198, Test Loss: 3.0742\n",
      "Epoch 5406, Train Loss: 3.0129, Test Loss: 3.0808\n",
      "Epoch 5407, Train Loss: 3.0169, Test Loss: 3.0758\n",
      "Epoch 5408, Train Loss: 3.0097, Test Loss: 3.0813\n",
      "Epoch 5409, Train Loss: 3.0144, Test Loss: 3.0833\n",
      "Epoch 5410, Train Loss: 3.0265, Test Loss: 3.1100\n",
      "Epoch 5411, Train Loss: 3.0506, Test Loss: 3.1275\n",
      "Epoch 5412, Train Loss: 3.0592, Test Loss: 3.1251\n",
      "Epoch 5413, Train Loss: 3.0682, Test Loss: 3.1045\n",
      "Epoch 5414, Train Loss: 3.0475, Test Loss: 3.1055\n",
      "Epoch 5415, Train Loss: 3.0422, Test Loss: 3.1077\n",
      "Epoch 5416, Train Loss: 3.0568, Test Loss: 3.1220\n",
      "Epoch 5417, Train Loss: 3.0547, Test Loss: 3.1244\n",
      "Epoch 5418, Train Loss: 3.0725, Test Loss: 3.1162\n",
      "Epoch 5419, Train Loss: 3.0466, Test Loss: 3.1143\n",
      "Epoch 5420, Train Loss: 3.0609, Test Loss: 3.1279\n",
      "Epoch 5421, Train Loss: 3.0720, Test Loss: 3.1156\n",
      "Epoch 5422, Train Loss: 3.0490, Test Loss: 3.1389\n",
      "Epoch 5423, Train Loss: 3.0739, Test Loss: 3.1364\n",
      "Epoch 5424, Train Loss: 3.0682, Test Loss: 3.0810\n",
      "Epoch 5425, Train Loss: 3.0209, Test Loss: 3.1099\n",
      "Epoch 5426, Train Loss: 3.0616, Test Loss: 3.1097\n",
      "Epoch 5427, Train Loss: 3.0583, Test Loss: 3.0925\n",
      "Epoch 5428, Train Loss: 3.0235, Test Loss: 3.0879\n",
      "Epoch 5429, Train Loss: 3.0363, Test Loss: 3.1001\n",
      "Epoch 5430, Train Loss: 3.0480, Test Loss: 3.0967\n",
      "Epoch 5431, Train Loss: 3.0288, Test Loss: 3.0953\n",
      "Epoch 5432, Train Loss: 3.0327, Test Loss: 3.0759\n",
      "Epoch 5433, Train Loss: 3.0134, Test Loss: 3.0951\n",
      "Epoch 5434, Train Loss: 3.0478, Test Loss: 3.0790\n",
      "Epoch 5435, Train Loss: 3.0286, Test Loss: 3.0848\n",
      "Epoch 5436, Train Loss: 3.0189, Test Loss: 3.0805\n",
      "Epoch 5437, Train Loss: 3.0160, Test Loss: 3.0836\n",
      "Epoch 5438, Train Loss: 3.0184, Test Loss: 3.0773\n",
      "Epoch 5439, Train Loss: 3.0063, Test Loss: 3.0822\n",
      "Epoch 5440, Train Loss: 3.0162, Test Loss: 3.0827\n",
      "Epoch 5441, Train Loss: 3.0211, Test Loss: 3.0778\n",
      "Epoch 5442, Train Loss: 3.0162, Test Loss: 3.0794\n",
      "Epoch 5443, Train Loss: 3.0102, Test Loss: 3.0784\n",
      "Epoch 5444, Train Loss: 3.0131, Test Loss: 3.0732\n",
      "Epoch 5445, Train Loss: 3.0243, Test Loss: 3.0711\n",
      "Epoch 5446, Train Loss: 3.0089, Test Loss: 3.0724\n",
      "Epoch 5447, Train Loss: 3.0104, Test Loss: 3.0787\n",
      "Epoch 5448, Train Loss: 3.0096, Test Loss: 3.0768\n",
      "Epoch 5449, Train Loss: 3.0040, Test Loss: 3.0771\n",
      "Epoch 5450, Train Loss: 3.0144, Test Loss: 3.0816\n",
      "Epoch 5451, Train Loss: 3.0184, Test Loss: 3.0793\n",
      "Epoch 5452, Train Loss: 3.0156, Test Loss: 3.0731\n",
      "Epoch 5453, Train Loss: 3.0061, Test Loss: 3.0706\n",
      "Epoch 5454, Train Loss: 3.0066, Test Loss: 3.0726\n",
      "Epoch 5455, Train Loss: 3.0119, Test Loss: 3.0792\n",
      "Epoch 5456, Train Loss: 3.0104, Test Loss: 3.0814\n",
      "Epoch 5457, Train Loss: 3.0209, Test Loss: 3.0906\n",
      "Epoch 5458, Train Loss: 3.0258, Test Loss: 3.1002\n",
      "Epoch 5459, Train Loss: 3.0173, Test Loss: 3.1009\n",
      "Epoch 5460, Train Loss: 3.0288, Test Loss: 3.0846\n",
      "Epoch 5461, Train Loss: 3.0208, Test Loss: 3.0842\n",
      "Epoch 5462, Train Loss: 3.0302, Test Loss: 3.0770\n",
      "Epoch 5463, Train Loss: 3.0191, Test Loss: 3.0865\n",
      "Epoch 5464, Train Loss: 3.0232, Test Loss: 3.0815\n",
      "Epoch 5465, Train Loss: 3.0185, Test Loss: 3.0839\n",
      "Epoch 5466, Train Loss: 3.0165, Test Loss: 3.0790\n",
      "Epoch 5467, Train Loss: 3.0136, Test Loss: 3.0913\n",
      "Epoch 5468, Train Loss: 3.0267, Test Loss: 3.0783\n",
      "Epoch 5469, Train Loss: 3.0071, Test Loss: 3.0706\n",
      "Epoch 5470, Train Loss: 3.0023, Test Loss: 3.0740\n",
      "Epoch 5471, Train Loss: 3.0164, Test Loss: 3.0854\n",
      "Epoch 5472, Train Loss: 3.0130, Test Loss: 3.0773\n",
      "Epoch 5473, Train Loss: 3.0108, Test Loss: 3.0761\n",
      "Epoch 5474, Train Loss: 3.0141, Test Loss: 3.0791\n",
      "Epoch 5475, Train Loss: 3.0217, Test Loss: 3.0727\n",
      "Epoch 5476, Train Loss: 3.0106, Test Loss: 3.0959\n",
      "Epoch 5477, Train Loss: 3.0295, Test Loss: 3.0888\n",
      "Epoch 5478, Train Loss: 3.0292, Test Loss: 3.0941\n",
      "Epoch 5479, Train Loss: 3.0285, Test Loss: 3.0988\n",
      "Epoch 5480, Train Loss: 3.0221, Test Loss: 3.1121\n",
      "Epoch 5481, Train Loss: 3.0440, Test Loss: 3.0880\n",
      "Epoch 5482, Train Loss: 3.0219, Test Loss: 3.0800\n",
      "Epoch 5483, Train Loss: 3.0247, Test Loss: 3.0857\n",
      "Epoch 5484, Train Loss: 3.0209, Test Loss: 3.0904\n",
      "Epoch 5485, Train Loss: 3.0269, Test Loss: 3.0825\n",
      "Epoch 5486, Train Loss: 3.0133, Test Loss: 3.0855\n",
      "Epoch 5487, Train Loss: 3.0256, Test Loss: 3.0841\n",
      "Epoch 5488, Train Loss: 3.0140, Test Loss: 3.0820\n",
      "Epoch 5489, Train Loss: 3.0100, Test Loss: 3.0870\n",
      "Epoch 5490, Train Loss: 3.0286, Test Loss: 3.0837\n",
      "Epoch 5491, Train Loss: 3.0163, Test Loss: 3.0771\n",
      "Epoch 5492, Train Loss: 3.0188, Test Loss: 3.1004\n",
      "Epoch 5493, Train Loss: 3.0358, Test Loss: 3.1077\n",
      "Epoch 5494, Train Loss: 3.0501, Test Loss: 3.0973\n",
      "Epoch 5495, Train Loss: 3.0254, Test Loss: 3.0901\n",
      "Epoch 5496, Train Loss: 3.0302, Test Loss: 3.0755\n",
      "Epoch 5497, Train Loss: 3.0101, Test Loss: 3.0807\n",
      "Epoch 5498, Train Loss: 3.0193, Test Loss: 3.0741\n",
      "Epoch 5499, Train Loss: 3.0125, Test Loss: 3.0746\n",
      "Epoch 5500, Train Loss: 3.0234, Test Loss: 3.0764\n",
      "Epoch 5501, Train Loss: 3.0065, Test Loss: 3.0912\n",
      "Epoch 5502, Train Loss: 3.0259, Test Loss: 3.0845\n",
      "Epoch 5503, Train Loss: 3.0195, Test Loss: 3.0800\n",
      "Epoch 5504, Train Loss: 3.0187, Test Loss: 3.0847\n",
      "Epoch 5505, Train Loss: 3.0271, Test Loss: 3.0917\n",
      "Epoch 5506, Train Loss: 3.0323, Test Loss: 3.0841\n",
      "Epoch 5507, Train Loss: 3.0402, Test Loss: 3.1043\n",
      "Epoch 5508, Train Loss: 3.0670, Test Loss: 3.1056\n",
      "Epoch 5509, Train Loss: 3.0544, Test Loss: 3.1269\n",
      "Epoch 5510, Train Loss: 3.0505, Test Loss: 3.1157\n",
      "Epoch 5511, Train Loss: 3.0508, Test Loss: 3.1088\n",
      "Epoch 5512, Train Loss: 3.0560, Test Loss: 3.1301\n",
      "Epoch 5513, Train Loss: 3.0779, Test Loss: 3.1102\n",
      "Epoch 5514, Train Loss: 3.0634, Test Loss: 3.1012\n",
      "Epoch 5515, Train Loss: 3.0517, Test Loss: 3.0987\n",
      "Epoch 5516, Train Loss: 3.0330, Test Loss: 3.1442\n",
      "Epoch 5517, Train Loss: 3.0909, Test Loss: 3.0939\n",
      "Epoch 5518, Train Loss: 3.0263, Test Loss: 3.1085\n",
      "Epoch 5519, Train Loss: 3.0449, Test Loss: 3.1139\n",
      "Epoch 5520, Train Loss: 3.0507, Test Loss: 3.1085\n",
      "Epoch 5521, Train Loss: 3.0489, Test Loss: 3.0987\n",
      "Epoch 5522, Train Loss: 3.0329, Test Loss: 3.1174\n",
      "Epoch 5523, Train Loss: 3.0562, Test Loss: 3.0999\n",
      "Epoch 5524, Train Loss: 3.0410, Test Loss: 3.1194\n",
      "Epoch 5525, Train Loss: 3.0769, Test Loss: 3.1050\n",
      "Epoch 5526, Train Loss: 3.0492, Test Loss: 3.1448\n",
      "Epoch 5527, Train Loss: 3.0730, Test Loss: 3.1426\n",
      "Epoch 5528, Train Loss: 3.0927, Test Loss: 3.0801\n",
      "Epoch 5529, Train Loss: 3.0218, Test Loss: 3.0996\n",
      "Epoch 5530, Train Loss: 3.0701, Test Loss: 3.1383\n",
      "Epoch 5531, Train Loss: 3.0962, Test Loss: 3.1156\n",
      "Epoch 5532, Train Loss: 3.0452, Test Loss: 3.0905\n",
      "Epoch 5533, Train Loss: 3.0342, Test Loss: 3.1020\n",
      "Epoch 5534, Train Loss: 3.0500, Test Loss: 3.0930\n",
      "Epoch 5535, Train Loss: 3.0368, Test Loss: 3.0856\n",
      "Epoch 5536, Train Loss: 3.0431, Test Loss: 3.0833\n",
      "Epoch 5537, Train Loss: 3.0354, Test Loss: 3.1053\n",
      "Epoch 5538, Train Loss: 3.0309, Test Loss: 3.0838\n",
      "Epoch 5539, Train Loss: 3.0193, Test Loss: 3.0957\n",
      "Epoch 5540, Train Loss: 3.0379, Test Loss: 3.0925\n",
      "Epoch 5541, Train Loss: 3.0286, Test Loss: 3.0821\n",
      "Epoch 5542, Train Loss: 3.0223, Test Loss: 3.0947\n",
      "Epoch 5543, Train Loss: 3.0440, Test Loss: 3.0886\n",
      "Epoch 5544, Train Loss: 3.0320, Test Loss: 3.0832\n",
      "Epoch 5545, Train Loss: 3.0221, Test Loss: 3.0975\n",
      "Epoch 5546, Train Loss: 3.0349, Test Loss: 3.1122\n",
      "Epoch 5547, Train Loss: 3.0572, Test Loss: 3.1044\n",
      "Epoch 5548, Train Loss: 3.0286, Test Loss: 3.0889\n",
      "Epoch 5549, Train Loss: 3.0265, Test Loss: 3.0913\n",
      "Epoch 5550, Train Loss: 3.0403, Test Loss: 3.0949\n",
      "Epoch 5551, Train Loss: 3.0398, Test Loss: 3.1034\n",
      "Epoch 5552, Train Loss: 3.0371, Test Loss: 3.0779\n",
      "Epoch 5553, Train Loss: 3.0180, Test Loss: 3.0817\n",
      "Epoch 5554, Train Loss: 3.0338, Test Loss: 3.0831\n",
      "Epoch 5555, Train Loss: 3.0336, Test Loss: 3.0930\n",
      "Epoch 5556, Train Loss: 3.0315, Test Loss: 3.0972\n",
      "Epoch 5557, Train Loss: 3.0360, Test Loss: 3.0747\n",
      "Epoch 5558, Train Loss: 3.0067, Test Loss: 3.0988\n",
      "Epoch 5559, Train Loss: 3.0370, Test Loss: 3.0848\n",
      "Epoch 5560, Train Loss: 3.0386, Test Loss: 3.0861\n",
      "Epoch 5561, Train Loss: 3.0130, Test Loss: 3.0932\n",
      "Epoch 5562, Train Loss: 3.0281, Test Loss: 3.0949\n",
      "Epoch 5563, Train Loss: 3.0399, Test Loss: 3.0918\n",
      "Epoch 5564, Train Loss: 3.0284, Test Loss: 3.0796\n",
      "Epoch 5565, Train Loss: 3.0142, Test Loss: 3.0806\n",
      "Epoch 5566, Train Loss: 3.0173, Test Loss: 3.0898\n",
      "Epoch 5567, Train Loss: 3.0427, Test Loss: 3.0876\n",
      "Epoch 5568, Train Loss: 3.0262, Test Loss: 3.0984\n",
      "Epoch 5569, Train Loss: 3.0238, Test Loss: 3.1005\n",
      "Epoch 5570, Train Loss: 3.0228, Test Loss: 3.0936\n",
      "Epoch 5571, Train Loss: 3.0252, Test Loss: 3.1003\n",
      "Epoch 5572, Train Loss: 3.0419, Test Loss: 3.0925\n",
      "Epoch 5573, Train Loss: 3.0176, Test Loss: 3.0973\n",
      "Epoch 5574, Train Loss: 3.0402, Test Loss: 3.0711\n",
      "Epoch 5575, Train Loss: 3.0141, Test Loss: 3.0869\n",
      "Epoch 5576, Train Loss: 3.0410, Test Loss: 3.0888\n",
      "Epoch 5577, Train Loss: 3.0240, Test Loss: 3.1000\n",
      "Epoch 5578, Train Loss: 3.0352, Test Loss: 3.0900\n",
      "Epoch 5579, Train Loss: 3.0179, Test Loss: 3.0775\n",
      "Epoch 5580, Train Loss: 3.0093, Test Loss: 3.0928\n",
      "Epoch 5581, Train Loss: 3.0399, Test Loss: 3.0922\n",
      "Epoch 5582, Train Loss: 3.0209, Test Loss: 3.1098\n",
      "Epoch 5583, Train Loss: 3.0514, Test Loss: 3.0936\n",
      "Epoch 5584, Train Loss: 3.0323, Test Loss: 3.0902\n",
      "Epoch 5585, Train Loss: 3.0325, Test Loss: 3.0893\n",
      "Epoch 5586, Train Loss: 3.0254, Test Loss: 3.0873\n",
      "Epoch 5587, Train Loss: 3.0158, Test Loss: 3.0976\n",
      "Epoch 5588, Train Loss: 3.0376, Test Loss: 3.0809\n",
      "Epoch 5589, Train Loss: 3.0163, Test Loss: 3.0970\n",
      "Epoch 5590, Train Loss: 3.0311, Test Loss: 3.0923\n",
      "Epoch 5591, Train Loss: 3.0530, Test Loss: 3.1122\n",
      "Epoch 5592, Train Loss: 3.0568, Test Loss: 3.1418\n",
      "Epoch 5593, Train Loss: 3.0857, Test Loss: 3.1344\n",
      "Epoch 5594, Train Loss: 3.1019, Test Loss: 3.0977\n",
      "Epoch 5595, Train Loss: 3.0355, Test Loss: 3.1144\n",
      "Epoch 5596, Train Loss: 3.0450, Test Loss: 3.1222\n",
      "Epoch 5597, Train Loss: 3.0559, Test Loss: 3.0988\n",
      "Epoch 5598, Train Loss: 3.0375, Test Loss: 3.1186\n",
      "Epoch 5599, Train Loss: 3.0822, Test Loss: 3.1020\n",
      "Epoch 5600, Train Loss: 3.0477, Test Loss: 3.1141\n",
      "Epoch 5601, Train Loss: 3.0554, Test Loss: 3.1167\n",
      "Epoch 5602, Train Loss: 3.0547, Test Loss: 3.0913\n",
      "Epoch 5603, Train Loss: 3.0318, Test Loss: 3.1032\n",
      "Epoch 5604, Train Loss: 3.0460, Test Loss: 3.0792\n",
      "Epoch 5605, Train Loss: 3.0367, Test Loss: 3.0861\n",
      "Epoch 5606, Train Loss: 3.0237, Test Loss: 3.1132\n",
      "Epoch 5607, Train Loss: 3.0491, Test Loss: 3.0972\n",
      "Epoch 5608, Train Loss: 3.0458, Test Loss: 3.0806\n",
      "Epoch 5609, Train Loss: 3.0320, Test Loss: 3.1017\n",
      "Epoch 5610, Train Loss: 3.0366, Test Loss: 3.1272\n",
      "Epoch 5611, Train Loss: 3.0670, Test Loss: 3.1100\n",
      "Epoch 5612, Train Loss: 3.0264, Test Loss: 3.0836\n",
      "Epoch 5613, Train Loss: 3.0277, Test Loss: 3.0839\n",
      "Epoch 5614, Train Loss: 3.0229, Test Loss: 3.0761\n",
      "Epoch 5615, Train Loss: 3.0174, Test Loss: 3.0858\n",
      "Epoch 5616, Train Loss: 3.0271, Test Loss: 3.0831\n",
      "Epoch 5617, Train Loss: 3.0391, Test Loss: 3.0860\n",
      "Epoch 5618, Train Loss: 3.0286, Test Loss: 3.0885\n",
      "Epoch 5619, Train Loss: 3.0190, Test Loss: 3.1011\n",
      "Epoch 5620, Train Loss: 3.0310, Test Loss: 3.0782\n",
      "Epoch 5621, Train Loss: 3.0207, Test Loss: 3.0771\n",
      "Epoch 5622, Train Loss: 3.0253, Test Loss: 3.0902\n",
      "Epoch 5623, Train Loss: 3.0309, Test Loss: 3.0788\n",
      "Epoch 5624, Train Loss: 3.0139, Test Loss: 3.0824\n",
      "Epoch 5625, Train Loss: 3.0157, Test Loss: 3.0854\n",
      "Epoch 5626, Train Loss: 3.0364, Test Loss: 3.0795\n",
      "Epoch 5627, Train Loss: 3.0140, Test Loss: 3.0874\n",
      "Epoch 5628, Train Loss: 3.0350, Test Loss: 3.0751\n",
      "Epoch 5629, Train Loss: 3.0134, Test Loss: 3.0873\n",
      "Epoch 5630, Train Loss: 3.0206, Test Loss: 3.0790\n",
      "Epoch 5631, Train Loss: 3.0220, Test Loss: 3.0839\n",
      "Epoch 5632, Train Loss: 3.0208, Test Loss: 3.0789\n",
      "Epoch 5633, Train Loss: 3.0150, Test Loss: 3.0854\n",
      "Epoch 5634, Train Loss: 3.0226, Test Loss: 3.0704\n",
      "Epoch 5635, Train Loss: 3.0136, Test Loss: 3.0738\n",
      "Epoch 5636, Train Loss: 3.0153, Test Loss: 3.0704\n",
      "Epoch 5637, Train Loss: 3.0143, Test Loss: 3.0766\n",
      "Epoch 5638, Train Loss: 3.0151, Test Loss: 3.0856\n",
      "Epoch 5639, Train Loss: 3.0113, Test Loss: 3.0878\n",
      "Epoch 5640, Train Loss: 3.0132, Test Loss: 3.0858\n",
      "Epoch 5641, Train Loss: 3.0199, Test Loss: 3.0856\n",
      "Epoch 5642, Train Loss: 3.0197, Test Loss: 3.0938\n",
      "Epoch 5643, Train Loss: 3.0349, Test Loss: 3.0828\n",
      "Epoch 5644, Train Loss: 3.0254, Test Loss: 3.0820\n",
      "Epoch 5645, Train Loss: 3.0323, Test Loss: 3.0871\n",
      "Epoch 5646, Train Loss: 3.0397, Test Loss: 3.0867\n",
      "Epoch 5647, Train Loss: 3.0226, Test Loss: 3.0941\n",
      "Epoch 5648, Train Loss: 3.0165, Test Loss: 3.0841\n",
      "Epoch 5649, Train Loss: 3.0152, Test Loss: 3.0952\n",
      "Epoch 5650, Train Loss: 3.0304, Test Loss: 3.0854\n",
      "Epoch 5651, Train Loss: 3.0163, Test Loss: 3.0815\n",
      "Epoch 5652, Train Loss: 3.0163, Test Loss: 3.0766\n",
      "Epoch 5653, Train Loss: 3.0069, Test Loss: 3.0811\n",
      "Epoch 5654, Train Loss: 3.0211, Test Loss: 3.0761\n",
      "Epoch 5655, Train Loss: 3.0058, Test Loss: 3.0801\n",
      "Epoch 5656, Train Loss: 3.0138, Test Loss: 3.0846\n",
      "Epoch 5657, Train Loss: 3.0105, Test Loss: 3.0851\n",
      "Epoch 5658, Train Loss: 3.0054, Test Loss: 3.0835\n",
      "Epoch 5659, Train Loss: 3.0082, Test Loss: 3.0731\n",
      "Epoch 5660, Train Loss: 3.0065, Test Loss: 3.0715\n",
      "Epoch 5661, Train Loss: 3.0056, Test Loss: 3.0770\n",
      "Epoch 5662, Train Loss: 3.0226, Test Loss: 3.0740\n",
      "Epoch 5663, Train Loss: 3.0165, Test Loss: 3.0761\n",
      "Epoch 5664, Train Loss: 3.0088, Test Loss: 3.0732\n",
      "Epoch 5665, Train Loss: 3.0042, Test Loss: 3.0746\n",
      "Epoch 5666, Train Loss: 3.0090, Test Loss: 3.0828\n",
      "Epoch 5667, Train Loss: 3.0180, Test Loss: 3.0855\n",
      "Epoch 5668, Train Loss: 3.0170, Test Loss: 3.0719\n",
      "Epoch 5669, Train Loss: 3.0017, Test Loss: 3.0766\n",
      "Epoch 5670, Train Loss: 3.0270, Test Loss: 3.0725\n",
      "Epoch 5671, Train Loss: 3.0171, Test Loss: 3.0809\n",
      "Epoch 5672, Train Loss: 3.0123, Test Loss: 3.0763\n",
      "Epoch 5673, Train Loss: 3.0154, Test Loss: 3.0803\n",
      "Epoch 5674, Train Loss: 3.0201, Test Loss: 3.0866\n",
      "Epoch 5675, Train Loss: 3.0145, Test Loss: 3.0868\n",
      "Epoch 5676, Train Loss: 3.0180, Test Loss: 3.0709\n",
      "Epoch 5677, Train Loss: 3.0090, Test Loss: 3.0732\n",
      "Epoch 5678, Train Loss: 3.0239, Test Loss: 3.0790\n",
      "Epoch 5679, Train Loss: 3.0186, Test Loss: 3.0787\n",
      "Epoch 5680, Train Loss: 3.0121, Test Loss: 3.0818\n",
      "Epoch 5681, Train Loss: 3.0153, Test Loss: 3.0864\n",
      "Epoch 5682, Train Loss: 3.0205, Test Loss: 3.0873\n",
      "Epoch 5683, Train Loss: 3.0173, Test Loss: 3.0718\n",
      "Epoch 5684, Train Loss: 3.0124, Test Loss: 3.0794\n",
      "Epoch 5685, Train Loss: 3.0226, Test Loss: 3.0746\n",
      "Epoch 5686, Train Loss: 3.0206, Test Loss: 3.0748\n",
      "Epoch 5687, Train Loss: 3.0122, Test Loss: 3.0770\n",
      "Epoch 5688, Train Loss: 3.0129, Test Loss: 3.0790\n",
      "Epoch 5689, Train Loss: 3.0231, Test Loss: 3.0926\n",
      "Epoch 5690, Train Loss: 3.0189, Test Loss: 3.1096\n",
      "Epoch 5691, Train Loss: 3.0461, Test Loss: 3.0968\n",
      "Epoch 5692, Train Loss: 3.0344, Test Loss: 3.0855\n",
      "Epoch 5693, Train Loss: 3.0327, Test Loss: 3.0958\n",
      "Epoch 5694, Train Loss: 3.0546, Test Loss: 3.1042\n",
      "Epoch 5695, Train Loss: 3.0434, Test Loss: 3.1230\n",
      "Epoch 5696, Train Loss: 3.0704, Test Loss: 3.1128\n",
      "Epoch 5697, Train Loss: 3.0662, Test Loss: 3.1232\n",
      "Epoch 5698, Train Loss: 3.0937, Test Loss: 3.0868\n",
      "Epoch 5699, Train Loss: 3.0476, Test Loss: 3.1186\n",
      "Epoch 5700, Train Loss: 3.0589, Test Loss: 3.1191\n",
      "Epoch 5701, Train Loss: 3.0802, Test Loss: 3.0867\n",
      "Epoch 5702, Train Loss: 3.0370, Test Loss: 3.0917\n",
      "Epoch 5703, Train Loss: 3.0465, Test Loss: 3.0856\n",
      "Epoch 5704, Train Loss: 3.0244, Test Loss: 3.0941\n",
      "Epoch 5705, Train Loss: 3.0334, Test Loss: 3.0873\n",
      "Epoch 5706, Train Loss: 3.0253, Test Loss: 3.0859\n",
      "Epoch 5707, Train Loss: 3.0232, Test Loss: 3.0856\n",
      "Epoch 5708, Train Loss: 3.0343, Test Loss: 3.1151\n",
      "Epoch 5709, Train Loss: 3.0618, Test Loss: 3.1057\n",
      "Epoch 5710, Train Loss: 3.0406, Test Loss: 3.0985\n",
      "Epoch 5711, Train Loss: 3.0435, Test Loss: 3.0824\n",
      "Epoch 5712, Train Loss: 3.0207, Test Loss: 3.1015\n",
      "Epoch 5713, Train Loss: 3.0435, Test Loss: 3.0902\n",
      "Epoch 5714, Train Loss: 3.0287, Test Loss: 3.0879\n",
      "Epoch 5715, Train Loss: 3.0450, Test Loss: 3.1009\n",
      "Epoch 5716, Train Loss: 3.0744, Test Loss: 3.0962\n",
      "Epoch 5717, Train Loss: 3.0556, Test Loss: 3.0880\n",
      "Epoch 5718, Train Loss: 3.0333, Test Loss: 3.1171\n",
      "Epoch 5719, Train Loss: 3.0400, Test Loss: 3.1074\n",
      "Epoch 5720, Train Loss: 3.0418, Test Loss: 3.0931\n",
      "Epoch 5721, Train Loss: 3.0462, Test Loss: 3.1049\n",
      "Epoch 5722, Train Loss: 3.0559, Test Loss: 3.1064\n",
      "Epoch 5723, Train Loss: 3.0388, Test Loss: 3.0763\n",
      "Epoch 5724, Train Loss: 3.0172, Test Loss: 3.1030\n",
      "Epoch 5725, Train Loss: 3.0611, Test Loss: 3.0941\n",
      "Epoch 5726, Train Loss: 3.0518, Test Loss: 3.1061\n",
      "Epoch 5727, Train Loss: 3.0374, Test Loss: 3.1263\n",
      "Epoch 5728, Train Loss: 3.0684, Test Loss: 3.0927\n",
      "Epoch 5729, Train Loss: 3.0375, Test Loss: 3.1135\n",
      "Epoch 5730, Train Loss: 3.0721, Test Loss: 3.1105\n",
      "Epoch 5731, Train Loss: 3.0685, Test Loss: 3.1094\n",
      "Epoch 5732, Train Loss: 3.0394, Test Loss: 3.0829\n",
      "Epoch 5733, Train Loss: 3.0125, Test Loss: 3.0907\n",
      "Epoch 5734, Train Loss: 3.0469, Test Loss: 3.0777\n",
      "Epoch 5735, Train Loss: 3.0248, Test Loss: 3.0787\n",
      "Epoch 5736, Train Loss: 3.0246, Test Loss: 3.0944\n",
      "Epoch 5737, Train Loss: 3.0413, Test Loss: 3.0764\n",
      "Epoch 5738, Train Loss: 3.0163, Test Loss: 3.0842\n",
      "Epoch 5739, Train Loss: 3.0191, Test Loss: 3.0903\n",
      "Epoch 5740, Train Loss: 3.0243, Test Loss: 3.0822\n",
      "Epoch 5741, Train Loss: 3.0141, Test Loss: 3.0795\n",
      "Epoch 5742, Train Loss: 3.0198, Test Loss: 3.0806\n",
      "Epoch 5743, Train Loss: 3.0276, Test Loss: 3.0890\n",
      "Epoch 5744, Train Loss: 3.0324, Test Loss: 3.0788\n",
      "Epoch 5745, Train Loss: 3.0186, Test Loss: 3.0804\n",
      "Epoch 5746, Train Loss: 3.0252, Test Loss: 3.0810\n",
      "Epoch 5747, Train Loss: 3.0232, Test Loss: 3.0952\n",
      "Epoch 5748, Train Loss: 3.0394, Test Loss: 3.0917\n",
      "Epoch 5749, Train Loss: 3.0325, Test Loss: 3.0834\n",
      "Epoch 5750, Train Loss: 3.0274, Test Loss: 3.0831\n",
      "Epoch 5751, Train Loss: 3.0264, Test Loss: 3.0857\n",
      "Epoch 5752, Train Loss: 3.0326, Test Loss: 3.0921\n",
      "Epoch 5753, Train Loss: 3.0470, Test Loss: 3.1093\n",
      "Epoch 5754, Train Loss: 3.0695, Test Loss: 3.0943\n",
      "Epoch 5755, Train Loss: 3.0312, Test Loss: 3.0932\n",
      "Epoch 5756, Train Loss: 3.0289, Test Loss: 3.1033\n",
      "Epoch 5757, Train Loss: 3.0480, Test Loss: 3.0733\n",
      "Epoch 5758, Train Loss: 3.0188, Test Loss: 3.0870\n",
      "Epoch 5759, Train Loss: 3.0235, Test Loss: 3.0834\n",
      "Epoch 5760, Train Loss: 3.0206, Test Loss: 3.0684\n",
      "Epoch 5761, Train Loss: 3.0086, Test Loss: 3.0901\n",
      "Epoch 5762, Train Loss: 3.0452, Test Loss: 3.0870\n",
      "Epoch 5763, Train Loss: 3.0230, Test Loss: 3.1127\n",
      "Epoch 5764, Train Loss: 3.0443, Test Loss: 3.1103\n",
      "Epoch 5765, Train Loss: 3.0357, Test Loss: 3.0834\n",
      "Epoch 5766, Train Loss: 3.0178, Test Loss: 3.1003\n",
      "Epoch 5767, Train Loss: 3.0471, Test Loss: 3.0742\n",
      "Epoch 5768, Train Loss: 3.0158, Test Loss: 3.0983\n",
      "Epoch 5769, Train Loss: 3.0375, Test Loss: 3.1021\n",
      "Epoch 5770, Train Loss: 3.0390, Test Loss: 3.0768\n",
      "Epoch 5771, Train Loss: 3.0120, Test Loss: 3.0826\n",
      "Epoch 5772, Train Loss: 3.0319, Test Loss: 3.0872\n",
      "Epoch 5773, Train Loss: 3.0238, Test Loss: 3.1014\n",
      "Epoch 5774, Train Loss: 3.0408, Test Loss: 3.0970\n",
      "Epoch 5775, Train Loss: 3.0382, Test Loss: 3.0796\n",
      "Epoch 5776, Train Loss: 3.0180, Test Loss: 3.0935\n",
      "Epoch 5777, Train Loss: 3.0388, Test Loss: 3.0827\n",
      "Epoch 5778, Train Loss: 3.0116, Test Loss: 3.0883\n",
      "Epoch 5779, Train Loss: 3.0287, Test Loss: 3.0788\n",
      "Epoch 5780, Train Loss: 3.0218, Test Loss: 3.0765\n",
      "Epoch 5781, Train Loss: 3.0121, Test Loss: 3.0854\n",
      "Epoch 5782, Train Loss: 3.0162, Test Loss: 3.0812\n",
      "Epoch 5783, Train Loss: 3.0279, Test Loss: 3.0744\n",
      "Epoch 5784, Train Loss: 3.0110, Test Loss: 3.0694\n",
      "Epoch 5785, Train Loss: 3.0067, Test Loss: 3.0826\n",
      "Epoch 5786, Train Loss: 3.0193, Test Loss: 3.0769\n",
      "Epoch 5787, Train Loss: 3.0081, Test Loss: 3.0757\n",
      "Epoch 5788, Train Loss: 3.0161, Test Loss: 3.0670\n",
      "Epoch 5789, Train Loss: 3.0093, Test Loss: 3.0789\n",
      "Epoch 5790, Train Loss: 3.0140, Test Loss: 3.0814\n",
      "Epoch 5791, Train Loss: 3.0293, Test Loss: 3.0694\n",
      "Epoch 5792, Train Loss: 3.0117, Test Loss: 3.0695\n",
      "Epoch 5793, Train Loss: 3.0207, Test Loss: 3.0696\n",
      "Epoch 5794, Train Loss: 3.0032, Test Loss: 3.0871\n",
      "Epoch 5795, Train Loss: 3.0265, Test Loss: 3.0750\n",
      "Epoch 5796, Train Loss: 3.0141, Test Loss: 3.0835\n",
      "Epoch 5797, Train Loss: 3.0321, Test Loss: 3.0773\n",
      "Epoch 5798, Train Loss: 3.0266, Test Loss: 3.0707\n",
      "Epoch 5799, Train Loss: 3.0191, Test Loss: 3.0992\n",
      "Epoch 5800, Train Loss: 3.0481, Test Loss: 3.0775\n",
      "Epoch 5801, Train Loss: 3.0288, Test Loss: 3.0942\n",
      "Epoch 5802, Train Loss: 3.0413, Test Loss: 3.0841\n",
      "Epoch 5803, Train Loss: 3.0168, Test Loss: 3.1017\n",
      "Epoch 5804, Train Loss: 3.0316, Test Loss: 3.0800\n",
      "Epoch 5805, Train Loss: 3.0164, Test Loss: 3.0817\n",
      "Epoch 5806, Train Loss: 3.0449, Test Loss: 3.0869\n",
      "Epoch 5807, Train Loss: 3.0411, Test Loss: 3.0927\n",
      "Epoch 5808, Train Loss: 3.0586, Test Loss: 3.0811\n",
      "Epoch 5809, Train Loss: 3.0210, Test Loss: 3.1221\n",
      "Epoch 5810, Train Loss: 3.0419, Test Loss: 3.1255\n",
      "Epoch 5811, Train Loss: 3.0485, Test Loss: 3.1318\n",
      "Epoch 5812, Train Loss: 3.0845, Test Loss: 3.1056\n",
      "Epoch 5813, Train Loss: 3.0353, Test Loss: 3.1171\n",
      "Epoch 5814, Train Loss: 3.0456, Test Loss: 3.1318\n",
      "Epoch 5815, Train Loss: 3.0820, Test Loss: 3.0972\n",
      "Epoch 5816, Train Loss: 3.0290, Test Loss: 3.0814\n",
      "Epoch 5817, Train Loss: 3.0334, Test Loss: 3.1014\n",
      "Epoch 5818, Train Loss: 3.0704, Test Loss: 3.1201\n",
      "Epoch 5819, Train Loss: 3.0650, Test Loss: 3.1116\n",
      "Epoch 5820, Train Loss: 3.0399, Test Loss: 3.1397\n",
      "Epoch 5821, Train Loss: 3.0755, Test Loss: 3.0967\n",
      "Epoch 5822, Train Loss: 3.0326, Test Loss: 3.0826\n",
      "Epoch 5823, Train Loss: 3.0259, Test Loss: 3.1008\n",
      "Epoch 5824, Train Loss: 3.0495, Test Loss: 3.1126\n",
      "Epoch 5825, Train Loss: 3.0487, Test Loss: 3.0977\n",
      "Epoch 5826, Train Loss: 3.0277, Test Loss: 3.0983\n",
      "Epoch 5827, Train Loss: 3.0335, Test Loss: 3.1111\n",
      "Epoch 5828, Train Loss: 3.0418, Test Loss: 3.1044\n",
      "Epoch 5829, Train Loss: 3.0333, Test Loss: 3.1122\n",
      "Epoch 5830, Train Loss: 3.0514, Test Loss: 3.0963\n",
      "Epoch 5831, Train Loss: 3.0496, Test Loss: 3.0793\n",
      "Epoch 5832, Train Loss: 3.0278, Test Loss: 3.0838\n",
      "Epoch 5833, Train Loss: 3.0254, Test Loss: 3.0796\n",
      "Epoch 5834, Train Loss: 3.0227, Test Loss: 3.0914\n",
      "Epoch 5835, Train Loss: 3.0415, Test Loss: 3.0755\n",
      "Epoch 5836, Train Loss: 3.0294, Test Loss: 3.0714\n",
      "Epoch 5837, Train Loss: 3.0308, Test Loss: 3.0748\n",
      "Epoch 5838, Train Loss: 3.0212, Test Loss: 3.0850\n",
      "Epoch 5839, Train Loss: 3.0256, Test Loss: 3.0898\n",
      "Epoch 5840, Train Loss: 3.0256, Test Loss: 3.0767\n",
      "Epoch 5841, Train Loss: 3.0191, Test Loss: 3.0819\n",
      "Epoch 5842, Train Loss: 3.0218, Test Loss: 3.0785\n",
      "Epoch 5843, Train Loss: 3.0151, Test Loss: 3.0755\n",
      "Epoch 5844, Train Loss: 3.0118, Test Loss: 3.0679\n",
      "Epoch 5845, Train Loss: 3.0143, Test Loss: 3.0764\n",
      "Epoch 5846, Train Loss: 3.0307, Test Loss: 3.0709\n",
      "Epoch 5847, Train Loss: 3.0084, Test Loss: 3.0828\n",
      "Epoch 5848, Train Loss: 3.0271, Test Loss: 3.0741\n",
      "Epoch 5849, Train Loss: 3.0166, Test Loss: 3.0835\n",
      "Epoch 5850, Train Loss: 3.0278, Test Loss: 3.0924\n",
      "Epoch 5851, Train Loss: 3.0380, Test Loss: 3.0742\n",
      "Epoch 5852, Train Loss: 3.0214, Test Loss: 3.0814\n",
      "Epoch 5853, Train Loss: 3.0140, Test Loss: 3.0973\n",
      "Epoch 5854, Train Loss: 3.0476, Test Loss: 3.0863\n",
      "Epoch 5855, Train Loss: 3.0309, Test Loss: 3.1056\n",
      "Epoch 5856, Train Loss: 3.0488, Test Loss: 3.0970\n",
      "Epoch 5857, Train Loss: 3.0456, Test Loss: 3.0911\n",
      "Epoch 5858, Train Loss: 3.0459, Test Loss: 3.0781\n",
      "Epoch 5859, Train Loss: 3.0265, Test Loss: 3.0793\n",
      "Epoch 5860, Train Loss: 3.0245, Test Loss: 3.0863\n",
      "Epoch 5861, Train Loss: 3.0186, Test Loss: 3.0810\n",
      "Epoch 5862, Train Loss: 3.0230, Test Loss: 3.0825\n",
      "Epoch 5863, Train Loss: 3.0178, Test Loss: 3.0821\n",
      "Epoch 5864, Train Loss: 3.0197, Test Loss: 3.0741\n",
      "Epoch 5865, Train Loss: 3.0110, Test Loss: 3.0731\n",
      "Epoch 5866, Train Loss: 3.0154, Test Loss: 3.0668\n",
      "Epoch 5867, Train Loss: 3.0123, Test Loss: 3.0723\n",
      "Epoch 5868, Train Loss: 3.0095, Test Loss: 3.0751\n",
      "Epoch 5869, Train Loss: 3.0156, Test Loss: 3.0705\n",
      "Epoch 5870, Train Loss: 3.0119, Test Loss: 3.0734\n",
      "Epoch 5871, Train Loss: 3.0084, Test Loss: 3.0733\n",
      "Epoch 5872, Train Loss: 3.0066, Test Loss: 3.0723\n",
      "Epoch 5873, Train Loss: 3.0186, Test Loss: 3.0698\n",
      "Epoch 5874, Train Loss: 3.0138, Test Loss: 3.0708\n",
      "Epoch 5875, Train Loss: 3.0115, Test Loss: 3.0739\n",
      "Epoch 5876, Train Loss: 3.0086, Test Loss: 3.0736\n",
      "Epoch 5877, Train Loss: 3.0193, Test Loss: 3.0731\n",
      "Epoch 5878, Train Loss: 3.0153, Test Loss: 3.0715\n",
      "Epoch 5879, Train Loss: 3.0117, Test Loss: 3.0710\n",
      "Epoch 5880, Train Loss: 3.0100, Test Loss: 3.0758\n",
      "Epoch 5881, Train Loss: 3.0050, Test Loss: 3.0735\n",
      "Epoch 5882, Train Loss: 3.0058, Test Loss: 3.0703\n",
      "Epoch 5883, Train Loss: 3.0032, Test Loss: 3.0698\n",
      "Epoch 5884, Train Loss: 3.0079, Test Loss: 3.0661\n",
      "Epoch 5885, Train Loss: 3.0075, Test Loss: 3.0690\n",
      "Epoch 5886, Train Loss: 3.0012, Test Loss: 3.0751\n",
      "Epoch 5887, Train Loss: 3.0118, Test Loss: 3.0754\n",
      "Epoch 5888, Train Loss: 3.0052, Test Loss: 3.0745\n",
      "Epoch 5889, Train Loss: 3.0142, Test Loss: 3.0697\n",
      "Epoch 5890, Train Loss: 3.0001, Test Loss: 3.0671\n",
      "Epoch 5891, Train Loss: 3.0047, Test Loss: 3.0636\n",
      "Epoch 5892, Train Loss: 3.0038, Test Loss: 3.0645\n",
      "Epoch 5893, Train Loss: 3.0057, Test Loss: 3.0678\n",
      "Epoch 5894, Train Loss: 3.0116, Test Loss: 3.0742\n",
      "Epoch 5895, Train Loss: 3.0087, Test Loss: 3.0714\n",
      "Epoch 5896, Train Loss: 3.0045, Test Loss: 3.0730\n",
      "Epoch 5897, Train Loss: 3.0167, Test Loss: 3.0747\n",
      "Epoch 5898, Train Loss: 3.0048, Test Loss: 3.0748\n",
      "Epoch 5899, Train Loss: 3.0040, Test Loss: 3.0688\n",
      "Epoch 5900, Train Loss: 3.0116, Test Loss: 3.0708\n",
      "Epoch 5901, Train Loss: 3.0117, Test Loss: 3.0760\n",
      "Epoch 5902, Train Loss: 3.0083, Test Loss: 3.0768\n",
      "Epoch 5903, Train Loss: 3.0127, Test Loss: 3.0753\n",
      "Epoch 5904, Train Loss: 3.0115, Test Loss: 3.0746\n",
      "Epoch 5905, Train Loss: 3.0121, Test Loss: 3.0731\n",
      "Epoch 5906, Train Loss: 3.0144, Test Loss: 3.0746\n",
      "Epoch 5907, Train Loss: 3.0114, Test Loss: 3.0733\n",
      "Epoch 5908, Train Loss: 3.0040, Test Loss: 3.0724\n",
      "Epoch 5909, Train Loss: 3.0096, Test Loss: 3.0725\n",
      "Epoch 5910, Train Loss: 3.0139, Test Loss: 3.0778\n",
      "Epoch 5911, Train Loss: 3.0156, Test Loss: 3.0758\n",
      "Epoch 5912, Train Loss: 3.0179, Test Loss: 3.0812\n",
      "Epoch 5913, Train Loss: 3.0264, Test Loss: 3.0853\n",
      "Epoch 5914, Train Loss: 3.0208, Test Loss: 3.0773\n",
      "Epoch 5915, Train Loss: 3.0134, Test Loss: 3.0882\n",
      "Epoch 5916, Train Loss: 3.0357, Test Loss: 3.0848\n",
      "Epoch 5917, Train Loss: 3.0256, Test Loss: 3.1054\n",
      "Epoch 5918, Train Loss: 3.0470, Test Loss: 3.0852\n",
      "Epoch 5919, Train Loss: 3.0187, Test Loss: 3.0826\n",
      "Epoch 5920, Train Loss: 3.0344, Test Loss: 3.0945\n",
      "Epoch 5921, Train Loss: 3.0400, Test Loss: 3.0854\n",
      "Epoch 5922, Train Loss: 3.0255, Test Loss: 3.0823\n",
      "Epoch 5923, Train Loss: 3.0342, Test Loss: 3.0808\n",
      "Epoch 5924, Train Loss: 3.0244, Test Loss: 3.0856\n",
      "Epoch 5925, Train Loss: 3.0223, Test Loss: 3.0732\n",
      "Epoch 5926, Train Loss: 3.0126, Test Loss: 3.0685\n",
      "Epoch 5927, Train Loss: 3.0110, Test Loss: 3.0683\n",
      "Epoch 5928, Train Loss: 3.0171, Test Loss: 3.0703\n",
      "Epoch 5929, Train Loss: 3.0021, Test Loss: 3.0829\n",
      "Epoch 5930, Train Loss: 3.0160, Test Loss: 3.0719\n",
      "Epoch 5931, Train Loss: 3.0097, Test Loss: 3.0734\n",
      "Epoch 5932, Train Loss: 3.0146, Test Loss: 3.0726\n",
      "Epoch 5933, Train Loss: 3.0152, Test Loss: 3.0795\n",
      "Epoch 5934, Train Loss: 3.0190, Test Loss: 3.0783\n",
      "Epoch 5935, Train Loss: 3.0133, Test Loss: 3.0825\n",
      "Epoch 5936, Train Loss: 3.0296, Test Loss: 3.0773\n",
      "Epoch 5937, Train Loss: 3.0131, Test Loss: 3.0861\n",
      "Epoch 5938, Train Loss: 3.0207, Test Loss: 3.0817\n",
      "Epoch 5939, Train Loss: 3.0257, Test Loss: 3.0844\n",
      "Epoch 5940, Train Loss: 3.0368, Test Loss: 3.0735\n",
      "Epoch 5941, Train Loss: 3.0133, Test Loss: 3.0810\n",
      "Epoch 5942, Train Loss: 3.0101, Test Loss: 3.1011\n",
      "Epoch 5943, Train Loss: 3.0308, Test Loss: 3.0819\n",
      "Epoch 5944, Train Loss: 3.0198, Test Loss: 3.0853\n",
      "Epoch 5945, Train Loss: 3.0373, Test Loss: 3.0847\n",
      "Epoch 5946, Train Loss: 3.0190, Test Loss: 3.0790\n",
      "Epoch 5947, Train Loss: 3.0127, Test Loss: 3.0671\n",
      "Epoch 5948, Train Loss: 3.0153, Test Loss: 3.0722\n",
      "Epoch 5949, Train Loss: 3.0197, Test Loss: 3.0793\n",
      "Epoch 5950, Train Loss: 3.0227, Test Loss: 3.0892\n",
      "Epoch 5951, Train Loss: 3.0296, Test Loss: 3.0826\n",
      "Epoch 5952, Train Loss: 3.0161, Test Loss: 3.0777\n",
      "Epoch 5953, Train Loss: 3.0237, Test Loss: 3.0839\n",
      "Epoch 5954, Train Loss: 3.0240, Test Loss: 3.1123\n",
      "Epoch 5955, Train Loss: 3.0533, Test Loss: 3.0912\n",
      "Epoch 5956, Train Loss: 3.0349, Test Loss: 3.0987\n",
      "Epoch 5957, Train Loss: 3.0520, Test Loss: 3.0750\n",
      "Epoch 5958, Train Loss: 3.0316, Test Loss: 3.0748\n",
      "Epoch 5959, Train Loss: 3.0265, Test Loss: 3.1270\n",
      "Epoch 5960, Train Loss: 3.0563, Test Loss: 3.1227\n",
      "Epoch 5961, Train Loss: 3.0602, Test Loss: 3.0824\n",
      "Epoch 5962, Train Loss: 3.0193, Test Loss: 3.0976\n",
      "Epoch 5963, Train Loss: 3.0462, Test Loss: 3.0994\n",
      "Epoch 5964, Train Loss: 3.0461, Test Loss: 3.0856\n",
      "Epoch 5965, Train Loss: 3.0258, Test Loss: 3.0856\n",
      "Epoch 5966, Train Loss: 3.0471, Test Loss: 3.0883\n",
      "Epoch 5967, Train Loss: 3.0473, Test Loss: 3.0749\n",
      "Epoch 5968, Train Loss: 3.0118, Test Loss: 3.0826\n",
      "Epoch 5969, Train Loss: 3.0104, Test Loss: 3.0854\n",
      "Epoch 5970, Train Loss: 3.0337, Test Loss: 3.0709\n",
      "Epoch 5971, Train Loss: 3.0047, Test Loss: 3.0712\n",
      "Epoch 5972, Train Loss: 3.0119, Test Loss: 3.0741\n",
      "Epoch 5973, Train Loss: 3.0138, Test Loss: 3.0713\n",
      "Epoch 5974, Train Loss: 3.0126, Test Loss: 3.0724\n",
      "Epoch 5975, Train Loss: 3.0075, Test Loss: 3.0748\n",
      "Epoch 5976, Train Loss: 3.0093, Test Loss: 3.0793\n",
      "Epoch 5977, Train Loss: 3.0060, Test Loss: 3.0811\n",
      "Epoch 5978, Train Loss: 3.0164, Test Loss: 3.0685\n",
      "Epoch 5979, Train Loss: 3.0075, Test Loss: 3.0776\n",
      "Epoch 5980, Train Loss: 3.0097, Test Loss: 3.0768\n",
      "Epoch 5981, Train Loss: 3.0081, Test Loss: 3.0705\n",
      "Epoch 5982, Train Loss: 3.0048, Test Loss: 3.0718\n",
      "Epoch 5983, Train Loss: 3.0122, Test Loss: 3.0787\n",
      "Epoch 5984, Train Loss: 3.0160, Test Loss: 3.0769\n",
      "Epoch 5985, Train Loss: 3.0098, Test Loss: 3.0688\n",
      "Epoch 5986, Train Loss: 3.0095, Test Loss: 3.0676\n",
      "Epoch 5987, Train Loss: 3.0120, Test Loss: 3.0656\n",
      "Epoch 5988, Train Loss: 3.0014, Test Loss: 3.0830\n",
      "Epoch 5989, Train Loss: 3.0206, Test Loss: 3.0688\n",
      "Epoch 5990, Train Loss: 2.9999, Test Loss: 3.0723\n",
      "Epoch 5991, Train Loss: 3.0122, Test Loss: 3.0695\n",
      "Epoch 5992, Train Loss: 3.0079, Test Loss: 3.0692\n",
      "Epoch 5993, Train Loss: 3.0042, Test Loss: 3.0831\n",
      "Epoch 5994, Train Loss: 3.0230, Test Loss: 3.0785\n",
      "Epoch 5995, Train Loss: 3.0279, Test Loss: 3.0759\n",
      "Epoch 5996, Train Loss: 3.0151, Test Loss: 3.0788\n",
      "Epoch 5997, Train Loss: 3.0271, Test Loss: 3.0932\n",
      "Epoch 5998, Train Loss: 3.0327, Test Loss: 3.0831\n",
      "Epoch 5999, Train Loss: 3.0183, Test Loss: 3.1007\n",
      "Epoch 6000, Train Loss: 3.0410, Test Loss: 3.0875\n",
      "Epoch 6001, Train Loss: 3.0381, Test Loss: 3.0819\n",
      "Epoch 6002, Train Loss: 3.0266, Test Loss: 3.0971\n",
      "Epoch 6003, Train Loss: 3.0342, Test Loss: 3.0831\n",
      "Epoch 6004, Train Loss: 3.0260, Test Loss: 3.0952\n",
      "Epoch 6005, Train Loss: 3.0521, Test Loss: 3.0824\n",
      "Epoch 6006, Train Loss: 3.0199, Test Loss: 3.1083\n",
      "Epoch 6007, Train Loss: 3.0405, Test Loss: 3.1049\n",
      "Epoch 6008, Train Loss: 3.0374, Test Loss: 3.0808\n",
      "Epoch 6009, Train Loss: 3.0230, Test Loss: 3.1038\n",
      "Epoch 6010, Train Loss: 3.0534, Test Loss: 3.0766\n",
      "Epoch 6011, Train Loss: 3.0170, Test Loss: 3.0848\n",
      "Epoch 6012, Train Loss: 3.0181, Test Loss: 3.0896\n",
      "Epoch 6013, Train Loss: 3.0360, Test Loss: 3.0770\n",
      "Epoch 6014, Train Loss: 3.0147, Test Loss: 3.0939\n",
      "Epoch 6015, Train Loss: 3.0374, Test Loss: 3.0858\n",
      "Epoch 6016, Train Loss: 3.0174, Test Loss: 3.0997\n",
      "Epoch 6017, Train Loss: 3.0351, Test Loss: 3.0944\n",
      "Epoch 6018, Train Loss: 3.0371, Test Loss: 3.0880\n",
      "Epoch 6019, Train Loss: 3.0118, Test Loss: 3.1027\n",
      "Epoch 6020, Train Loss: 3.0530, Test Loss: 3.0833\n",
      "Epoch 6021, Train Loss: 3.0197, Test Loss: 3.1061\n",
      "Epoch 6022, Train Loss: 3.0582, Test Loss: 3.0819\n",
      "Epoch 6023, Train Loss: 3.0161, Test Loss: 3.0959\n",
      "Epoch 6024, Train Loss: 3.0337, Test Loss: 3.0930\n",
      "Epoch 6025, Train Loss: 3.0341, Test Loss: 3.0821\n",
      "Epoch 6026, Train Loss: 3.0190, Test Loss: 3.0937\n",
      "Epoch 6027, Train Loss: 3.0626, Test Loss: 3.0777\n",
      "Epoch 6028, Train Loss: 3.0240, Test Loss: 3.0921\n",
      "Epoch 6029, Train Loss: 3.0387, Test Loss: 3.1175\n",
      "Epoch 6030, Train Loss: 3.0338, Test Loss: 3.1049\n",
      "Epoch 6031, Train Loss: 3.0320, Test Loss: 3.0952\n",
      "Epoch 6032, Train Loss: 3.0368, Test Loss: 3.0910\n",
      "Epoch 6033, Train Loss: 3.0449, Test Loss: 3.0879\n",
      "Epoch 6034, Train Loss: 3.0292, Test Loss: 3.1095\n",
      "Epoch 6035, Train Loss: 3.0672, Test Loss: 3.0767\n",
      "Epoch 6036, Train Loss: 3.0201, Test Loss: 3.0923\n",
      "Epoch 6037, Train Loss: 3.0522, Test Loss: 3.0826\n",
      "Epoch 6038, Train Loss: 3.0367, Test Loss: 3.0942\n",
      "Epoch 6039, Train Loss: 3.0230, Test Loss: 3.1085\n",
      "Epoch 6040, Train Loss: 3.0494, Test Loss: 3.0824\n",
      "Epoch 6041, Train Loss: 3.0315, Test Loss: 3.1152\n",
      "Epoch 6042, Train Loss: 3.0889, Test Loss: 3.1082\n",
      "Epoch 6043, Train Loss: 3.0537, Test Loss: 3.0919\n",
      "Epoch 6044, Train Loss: 3.0320, Test Loss: 3.1033\n",
      "Epoch 6045, Train Loss: 3.0438, Test Loss: 3.0813\n",
      "Epoch 6046, Train Loss: 3.0338, Test Loss: 3.0913\n",
      "Epoch 6047, Train Loss: 3.0444, Test Loss: 3.0883\n",
      "Epoch 6048, Train Loss: 3.0410, Test Loss: 3.0903\n",
      "Epoch 6049, Train Loss: 3.0236, Test Loss: 3.0963\n",
      "Epoch 6050, Train Loss: 3.0392, Test Loss: 3.0822\n",
      "Epoch 6051, Train Loss: 3.0286, Test Loss: 3.0805\n",
      "Epoch 6052, Train Loss: 3.0271, Test Loss: 3.0765\n",
      "Epoch 6053, Train Loss: 3.0156, Test Loss: 3.0841\n",
      "Epoch 6054, Train Loss: 3.0246, Test Loss: 3.0708\n",
      "Epoch 6055, Train Loss: 3.0167, Test Loss: 3.0753\n",
      "Epoch 6056, Train Loss: 3.0292, Test Loss: 3.0736\n",
      "Epoch 6057, Train Loss: 3.0095, Test Loss: 3.0815\n",
      "Epoch 6058, Train Loss: 3.0114, Test Loss: 3.0711\n",
      "Epoch 6059, Train Loss: 3.0066, Test Loss: 3.0773\n",
      "Epoch 6060, Train Loss: 3.0198, Test Loss: 3.0733\n",
      "Epoch 6061, Train Loss: 3.0135, Test Loss: 3.0685\n",
      "Epoch 6062, Train Loss: 3.0105, Test Loss: 3.0742\n",
      "Epoch 6063, Train Loss: 3.0099, Test Loss: 3.0756\n",
      "Epoch 6064, Train Loss: 3.0142, Test Loss: 3.0738\n",
      "Epoch 6065, Train Loss: 3.0184, Test Loss: 3.0654\n",
      "Epoch 6066, Train Loss: 3.0040, Test Loss: 3.0679\n",
      "Epoch 6067, Train Loss: 3.0103, Test Loss: 3.0705\n",
      "Epoch 6068, Train Loss: 3.0216, Test Loss: 3.0680\n",
      "Epoch 6069, Train Loss: 3.0064, Test Loss: 3.0668\n",
      "Epoch 6070, Train Loss: 3.0031, Test Loss: 3.0756\n",
      "Epoch 6071, Train Loss: 3.0180, Test Loss: 3.0770\n",
      "Epoch 6072, Train Loss: 3.0099, Test Loss: 3.0754\n",
      "Epoch 6073, Train Loss: 3.0177, Test Loss: 3.0720\n",
      "Epoch 6074, Train Loss: 3.0052, Test Loss: 3.0701\n",
      "Epoch 6075, Train Loss: 3.0137, Test Loss: 3.0728\n",
      "Epoch 6076, Train Loss: 3.0072, Test Loss: 3.0730\n",
      "Epoch 6077, Train Loss: 3.0106, Test Loss: 3.0727\n",
      "Epoch 6078, Train Loss: 3.0121, Test Loss: 3.0714\n",
      "Epoch 6079, Train Loss: 3.0045, Test Loss: 3.0752\n",
      "Epoch 6080, Train Loss: 3.0118, Test Loss: 3.0760\n",
      "Epoch 6081, Train Loss: 3.0226, Test Loss: 3.0668\n",
      "Epoch 6082, Train Loss: 3.0153, Test Loss: 3.0695\n",
      "Epoch 6083, Train Loss: 3.0156, Test Loss: 3.0694\n",
      "Epoch 6084, Train Loss: 3.0137, Test Loss: 3.0657\n",
      "Epoch 6085, Train Loss: 3.0062, Test Loss: 3.0706\n",
      "Epoch 6086, Train Loss: 3.0085, Test Loss: 3.0760\n",
      "Epoch 6087, Train Loss: 3.0119, Test Loss: 3.0725\n",
      "Epoch 6088, Train Loss: 3.0088, Test Loss: 3.0652\n",
      "Epoch 6089, Train Loss: 3.0045, Test Loss: 3.0730\n",
      "Epoch 6090, Train Loss: 3.0153, Test Loss: 3.0738\n",
      "Epoch 6091, Train Loss: 3.0086, Test Loss: 3.0760\n",
      "Epoch 6092, Train Loss: 3.0146, Test Loss: 3.0717\n",
      "Epoch 6093, Train Loss: 3.0102, Test Loss: 3.0778\n",
      "Epoch 6094, Train Loss: 3.0161, Test Loss: 3.0756\n",
      "Epoch 6095, Train Loss: 3.0034, Test Loss: 3.0824\n",
      "Epoch 6096, Train Loss: 3.0184, Test Loss: 3.0702\n",
      "Epoch 6097, Train Loss: 3.0124, Test Loss: 3.0848\n",
      "Epoch 6098, Train Loss: 3.0308, Test Loss: 3.0939\n",
      "Epoch 6099, Train Loss: 3.0176, Test Loss: 3.1000\n",
      "Epoch 6100, Train Loss: 3.0413, Test Loss: 3.0917\n",
      "Epoch 6101, Train Loss: 3.0287, Test Loss: 3.0836\n",
      "Epoch 6102, Train Loss: 3.0311, Test Loss: 3.0913\n",
      "Epoch 6103, Train Loss: 3.0225, Test Loss: 3.1040\n",
      "Epoch 6104, Train Loss: 3.0495, Test Loss: 3.0827\n",
      "Epoch 6105, Train Loss: 3.0301, Test Loss: 3.0747\n",
      "Epoch 6106, Train Loss: 3.0243, Test Loss: 3.0729\n",
      "Epoch 6107, Train Loss: 3.0180, Test Loss: 3.0825\n",
      "Epoch 6108, Train Loss: 3.0224, Test Loss: 3.0807\n",
      "Epoch 6109, Train Loss: 3.0144, Test Loss: 3.0779\n",
      "Epoch 6110, Train Loss: 3.0265, Test Loss: 3.0880\n",
      "Epoch 6111, Train Loss: 3.0395, Test Loss: 3.0756\n",
      "Epoch 6112, Train Loss: 3.0328, Test Loss: 3.0974\n",
      "Epoch 6113, Train Loss: 3.0402, Test Loss: 3.1040\n",
      "Epoch 6114, Train Loss: 3.0492, Test Loss: 3.1057\n",
      "Epoch 6115, Train Loss: 3.0579, Test Loss: 3.0831\n",
      "Epoch 6116, Train Loss: 3.0268, Test Loss: 3.0982\n",
      "Epoch 6117, Train Loss: 3.0265, Test Loss: 3.1039\n",
      "Epoch 6118, Train Loss: 3.0483, Test Loss: 3.0737\n",
      "Epoch 6119, Train Loss: 3.0163, Test Loss: 3.0760\n",
      "Epoch 6120, Train Loss: 3.0245, Test Loss: 3.0803\n",
      "Epoch 6121, Train Loss: 3.0181, Test Loss: 3.0745\n",
      "Epoch 6122, Train Loss: 3.0195, Test Loss: 3.0927\n",
      "Epoch 6123, Train Loss: 3.0349, Test Loss: 3.0799\n",
      "Epoch 6124, Train Loss: 3.0033, Test Loss: 3.0864\n",
      "Epoch 6125, Train Loss: 3.0221, Test Loss: 3.0792\n",
      "Epoch 6126, Train Loss: 3.0179, Test Loss: 3.0765\n",
      "Epoch 6127, Train Loss: 3.0179, Test Loss: 3.0863\n",
      "Epoch 6128, Train Loss: 3.0174, Test Loss: 3.0924\n",
      "Epoch 6129, Train Loss: 3.0239, Test Loss: 3.0790\n",
      "Epoch 6130, Train Loss: 3.0149, Test Loss: 3.0764\n",
      "Epoch 6131, Train Loss: 3.0198, Test Loss: 3.0764\n",
      "Epoch 6132, Train Loss: 3.0181, Test Loss: 3.0705\n",
      "Epoch 6133, Train Loss: 3.0126, Test Loss: 3.0702\n",
      "Epoch 6134, Train Loss: 3.0126, Test Loss: 3.0680\n",
      "Epoch 6135, Train Loss: 3.0089, Test Loss: 3.0738\n",
      "Epoch 6136, Train Loss: 3.0159, Test Loss: 3.0716\n",
      "Epoch 6137, Train Loss: 3.0100, Test Loss: 3.0667\n",
      "Epoch 6138, Train Loss: 3.0090, Test Loss: 3.0702\n",
      "Epoch 6139, Train Loss: 3.0094, Test Loss: 3.0719\n",
      "Epoch 6140, Train Loss: 3.0107, Test Loss: 3.0727\n",
      "Epoch 6141, Train Loss: 3.0126, Test Loss: 3.0774\n",
      "Epoch 6142, Train Loss: 3.0171, Test Loss: 3.0752\n",
      "Epoch 6143, Train Loss: 3.0116, Test Loss: 3.0764\n",
      "Epoch 6144, Train Loss: 3.0113, Test Loss: 3.0698\n",
      "Epoch 6145, Train Loss: 3.0121, Test Loss: 3.0725\n",
      "Epoch 6146, Train Loss: 3.0103, Test Loss: 3.0693\n",
      "Epoch 6147, Train Loss: 3.0132, Test Loss: 3.0727\n",
      "Epoch 6148, Train Loss: 3.0114, Test Loss: 3.0752\n",
      "Epoch 6149, Train Loss: 3.0104, Test Loss: 3.0703\n",
      "Epoch 6150, Train Loss: 3.0078, Test Loss: 3.0681\n",
      "Epoch 6151, Train Loss: 3.0102, Test Loss: 3.0729\n",
      "Epoch 6152, Train Loss: 3.0126, Test Loss: 3.0687\n",
      "Epoch 6153, Train Loss: 3.0021, Test Loss: 3.0724\n",
      "Epoch 6154, Train Loss: 3.0196, Test Loss: 3.0713\n",
      "Epoch 6155, Train Loss: 3.0136, Test Loss: 3.0806\n",
      "Epoch 6156, Train Loss: 3.0186, Test Loss: 3.0693\n",
      "Epoch 6157, Train Loss: 3.0180, Test Loss: 3.0896\n",
      "Epoch 6158, Train Loss: 3.0349, Test Loss: 3.0922\n",
      "Epoch 6159, Train Loss: 3.0350, Test Loss: 3.0953\n",
      "Epoch 6160, Train Loss: 3.0348, Test Loss: 3.0767\n",
      "Epoch 6161, Train Loss: 3.0239, Test Loss: 3.0736\n",
      "Epoch 6162, Train Loss: 3.0299, Test Loss: 3.0752\n",
      "Epoch 6163, Train Loss: 3.0313, Test Loss: 3.0898\n",
      "Epoch 6164, Train Loss: 3.0300, Test Loss: 3.0733\n",
      "Epoch 6165, Train Loss: 3.0151, Test Loss: 3.0767\n",
      "Epoch 6166, Train Loss: 3.0246, Test Loss: 3.0787\n",
      "Epoch 6167, Train Loss: 3.0199, Test Loss: 3.0906\n",
      "Epoch 6168, Train Loss: 3.0290, Test Loss: 3.0697\n",
      "Epoch 6169, Train Loss: 3.0055, Test Loss: 3.0703\n",
      "Epoch 6170, Train Loss: 3.0223, Test Loss: 3.0819\n",
      "Epoch 6171, Train Loss: 3.0185, Test Loss: 3.1007\n",
      "Epoch 6172, Train Loss: 3.0354, Test Loss: 3.0730\n",
      "Epoch 6173, Train Loss: 3.0182, Test Loss: 3.0809\n",
      "Epoch 6174, Train Loss: 3.0314, Test Loss: 3.0871\n",
      "Epoch 6175, Train Loss: 3.0288, Test Loss: 3.1091\n",
      "Epoch 6176, Train Loss: 3.0295, Test Loss: 3.0876\n",
      "Epoch 6177, Train Loss: 3.0217, Test Loss: 3.0746\n",
      "Epoch 6178, Train Loss: 3.0232, Test Loss: 3.0829\n",
      "Epoch 6179, Train Loss: 3.0320, Test Loss: 3.0901\n",
      "Epoch 6180, Train Loss: 3.0417, Test Loss: 3.0947\n",
      "Epoch 6181, Train Loss: 3.0329, Test Loss: 3.0689\n",
      "Epoch 6182, Train Loss: 3.0078, Test Loss: 3.1068\n",
      "Epoch 6183, Train Loss: 3.0569, Test Loss: 3.0782\n",
      "Epoch 6184, Train Loss: 3.0135, Test Loss: 3.0951\n",
      "Epoch 6185, Train Loss: 3.0334, Test Loss: 3.0914\n",
      "Epoch 6186, Train Loss: 3.0450, Test Loss: 3.0740\n",
      "Epoch 6187, Train Loss: 3.0322, Test Loss: 3.0923\n",
      "Epoch 6188, Train Loss: 3.0638, Test Loss: 3.0787\n",
      "Epoch 6189, Train Loss: 3.0236, Test Loss: 3.0951\n",
      "Epoch 6190, Train Loss: 3.0435, Test Loss: 3.0906\n",
      "Epoch 6191, Train Loss: 3.0103, Test Loss: 3.1015\n",
      "Epoch 6192, Train Loss: 3.0354, Test Loss: 3.0927\n",
      "Epoch 6193, Train Loss: 3.0259, Test Loss: 3.0846\n",
      "Epoch 6194, Train Loss: 3.0207, Test Loss: 3.0783\n",
      "Epoch 6195, Train Loss: 3.0226, Test Loss: 3.0787\n",
      "Epoch 6196, Train Loss: 3.0227, Test Loss: 3.0766\n",
      "Epoch 6197, Train Loss: 3.0316, Test Loss: 3.0643\n",
      "Epoch 6198, Train Loss: 3.0259, Test Loss: 3.0768\n",
      "Epoch 6199, Train Loss: 3.0218, Test Loss: 3.0767\n",
      "Epoch 6200, Train Loss: 3.0176, Test Loss: 3.0808\n",
      "Epoch 6201, Train Loss: 3.0223, Test Loss: 3.0814\n",
      "Epoch 6202, Train Loss: 3.0330, Test Loss: 3.0724\n",
      "Epoch 6203, Train Loss: 3.0099, Test Loss: 3.0856\n",
      "Epoch 6204, Train Loss: 3.0330, Test Loss: 3.0752\n",
      "Epoch 6205, Train Loss: 3.0205, Test Loss: 3.0706\n",
      "Epoch 6206, Train Loss: 3.0246, Test Loss: 3.0805\n",
      "Epoch 6207, Train Loss: 3.0245, Test Loss: 3.0924\n",
      "Epoch 6208, Train Loss: 3.0338, Test Loss: 3.0779\n",
      "Epoch 6209, Train Loss: 3.0259, Test Loss: 3.0771\n",
      "Epoch 6210, Train Loss: 3.0285, Test Loss: 3.0852\n",
      "Epoch 6211, Train Loss: 3.0246, Test Loss: 3.1216\n",
      "Epoch 6212, Train Loss: 3.0602, Test Loss: 3.0836\n",
      "Epoch 6213, Train Loss: 3.0302, Test Loss: 3.0769\n",
      "Epoch 6214, Train Loss: 3.0334, Test Loss: 3.0753\n",
      "Epoch 6215, Train Loss: 3.0326, Test Loss: 3.0890\n",
      "Epoch 6216, Train Loss: 3.0420, Test Loss: 3.0922\n",
      "Epoch 6217, Train Loss: 3.0416, Test Loss: 3.0907\n",
      "Epoch 6218, Train Loss: 3.0372, Test Loss: 3.0957\n",
      "Epoch 6219, Train Loss: 3.0521, Test Loss: 3.0820\n",
      "Epoch 6220, Train Loss: 3.0240, Test Loss: 3.1152\n",
      "Epoch 6221, Train Loss: 3.0726, Test Loss: 3.0938\n",
      "Epoch 6222, Train Loss: 3.0453, Test Loss: 3.0902\n",
      "Epoch 6223, Train Loss: 3.0602, Test Loss: 3.0757\n",
      "Epoch 6224, Train Loss: 3.0205, Test Loss: 3.1240\n",
      "Epoch 6225, Train Loss: 3.0633, Test Loss: 3.0943\n",
      "Epoch 6226, Train Loss: 3.0483, Test Loss: 3.0912\n",
      "Epoch 6227, Train Loss: 3.0449, Test Loss: 3.0805\n",
      "Epoch 6228, Train Loss: 3.0212, Test Loss: 3.0876\n",
      "Epoch 6229, Train Loss: 3.0384, Test Loss: 3.0669\n",
      "Epoch 6230, Train Loss: 3.0288, Test Loss: 3.0704\n",
      "Epoch 6231, Train Loss: 3.0378, Test Loss: 3.0758\n",
      "Epoch 6232, Train Loss: 3.0210, Test Loss: 3.0914\n",
      "Epoch 6233, Train Loss: 3.0395, Test Loss: 3.0690\n",
      "Epoch 6234, Train Loss: 3.0177, Test Loss: 3.0753\n",
      "Epoch 6235, Train Loss: 3.0289, Test Loss: 3.0740\n",
      "Epoch 6236, Train Loss: 3.0164, Test Loss: 3.0848\n",
      "Epoch 6237, Train Loss: 3.0146, Test Loss: 3.0756\n",
      "Epoch 6238, Train Loss: 3.0137, Test Loss: 3.0707\n",
      "Epoch 6239, Train Loss: 3.0179, Test Loss: 3.0719\n",
      "Epoch 6240, Train Loss: 3.0178, Test Loss: 3.0724\n",
      "Epoch 6241, Train Loss: 3.0172, Test Loss: 3.0860\n",
      "Epoch 6242, Train Loss: 3.0332, Test Loss: 3.0592\n",
      "Epoch 6243, Train Loss: 3.0046, Test Loss: 3.0633\n",
      "Epoch 6244, Train Loss: 3.0173, Test Loss: 3.0670\n",
      "Epoch 6245, Train Loss: 3.0142, Test Loss: 3.0739\n",
      "Epoch 6246, Train Loss: 3.0120, Test Loss: 3.0648\n",
      "Epoch 6247, Train Loss: 3.0140, Test Loss: 3.0684\n",
      "Epoch 6248, Train Loss: 3.0173, Test Loss: 3.0829\n",
      "Epoch 6249, Train Loss: 3.0227, Test Loss: 3.0787\n",
      "Epoch 6250, Train Loss: 3.0237, Test Loss: 3.0698\n",
      "Epoch 6251, Train Loss: 3.0153, Test Loss: 3.0622\n",
      "Epoch 6252, Train Loss: 3.0086, Test Loss: 3.0693\n",
      "Epoch 6253, Train Loss: 3.0080, Test Loss: 3.0688\n",
      "Epoch 6254, Train Loss: 3.0125, Test Loss: 3.0665\n",
      "Epoch 6255, Train Loss: 3.0137, Test Loss: 3.0695\n",
      "Epoch 6256, Train Loss: 3.0120, Test Loss: 3.0773\n",
      "Epoch 6257, Train Loss: 3.0164, Test Loss: 3.0764\n",
      "Epoch 6258, Train Loss: 3.0132, Test Loss: 3.0657\n",
      "Epoch 6259, Train Loss: 3.0119, Test Loss: 3.0757\n",
      "Epoch 6260, Train Loss: 3.0238, Test Loss: 3.0704\n",
      "Epoch 6261, Train Loss: 3.0107, Test Loss: 3.0776\n",
      "Epoch 6262, Train Loss: 3.0161, Test Loss: 3.0677\n",
      "Epoch 6263, Train Loss: 3.0124, Test Loss: 3.0860\n",
      "Epoch 6264, Train Loss: 3.0440, Test Loss: 3.0665\n",
      "Epoch 6265, Train Loss: 3.0031, Test Loss: 3.0919\n",
      "Epoch 6266, Train Loss: 3.0330, Test Loss: 3.0642\n",
      "Epoch 6267, Train Loss: 3.0105, Test Loss: 3.0725\n",
      "Epoch 6268, Train Loss: 3.0216, Test Loss: 3.0686\n",
      "Epoch 6269, Train Loss: 3.0192, Test Loss: 3.0879\n",
      "Epoch 6270, Train Loss: 3.0174, Test Loss: 3.0778\n",
      "Epoch 6271, Train Loss: 3.0179, Test Loss: 3.0702\n",
      "Epoch 6272, Train Loss: 3.0156, Test Loss: 3.0683\n",
      "Epoch 6273, Train Loss: 3.0100, Test Loss: 3.0684\n",
      "Epoch 6274, Train Loss: 3.0124, Test Loss: 3.0841\n",
      "Epoch 6275, Train Loss: 3.0305, Test Loss: 3.0664\n",
      "Epoch 6276, Train Loss: 3.0143, Test Loss: 3.0756\n",
      "Epoch 6277, Train Loss: 3.0240, Test Loss: 3.0736\n",
      "Epoch 6278, Train Loss: 3.0180, Test Loss: 3.0807\n",
      "Epoch 6279, Train Loss: 3.0204, Test Loss: 3.0717\n",
      "Epoch 6280, Train Loss: 3.0149, Test Loss: 3.0754\n",
      "Epoch 6281, Train Loss: 3.0273, Test Loss: 3.0629\n",
      "Epoch 6282, Train Loss: 3.0060, Test Loss: 3.0682\n",
      "Epoch 6283, Train Loss: 3.0120, Test Loss: 3.0638\n",
      "Epoch 6284, Train Loss: 3.0132, Test Loss: 3.0699\n",
      "Epoch 6285, Train Loss: 3.0151, Test Loss: 3.0726\n",
      "Epoch 6286, Train Loss: 3.0100, Test Loss: 3.0712\n",
      "Epoch 6287, Train Loss: 3.0121, Test Loss: 3.0724\n",
      "Epoch 6288, Train Loss: 3.0215, Test Loss: 3.0754\n",
      "Epoch 6289, Train Loss: 3.0150, Test Loss: 3.0758\n",
      "Epoch 6290, Train Loss: 3.0076, Test Loss: 3.0690\n",
      "Epoch 6291, Train Loss: 3.0102, Test Loss: 3.0719\n",
      "Epoch 6292, Train Loss: 3.0185, Test Loss: 3.0700\n",
      "Epoch 6293, Train Loss: 3.0142, Test Loss: 3.0839\n",
      "Epoch 6294, Train Loss: 3.0263, Test Loss: 3.0623\n",
      "Epoch 6295, Train Loss: 3.0048, Test Loss: 3.0821\n",
      "Epoch 6296, Train Loss: 3.0251, Test Loss: 3.0747\n",
      "Epoch 6297, Train Loss: 3.0243, Test Loss: 3.0804\n",
      "Epoch 6298, Train Loss: 3.0116, Test Loss: 3.0819\n",
      "Epoch 6299, Train Loss: 3.0209, Test Loss: 3.0769\n",
      "Epoch 6300, Train Loss: 3.0196, Test Loss: 3.0723\n",
      "Epoch 6301, Train Loss: 3.0171, Test Loss: 3.0870\n",
      "Epoch 6302, Train Loss: 3.0392, Test Loss: 3.0756\n",
      "Epoch 6303, Train Loss: 3.0237, Test Loss: 3.0808\n",
      "Epoch 6304, Train Loss: 3.0283, Test Loss: 3.0899\n",
      "Epoch 6305, Train Loss: 3.0353, Test Loss: 3.0766\n",
      "Epoch 6306, Train Loss: 3.0124, Test Loss: 3.0798\n",
      "Epoch 6307, Train Loss: 3.0278, Test Loss: 3.0733\n",
      "Epoch 6308, Train Loss: 3.0220, Test Loss: 3.0716\n",
      "Epoch 6309, Train Loss: 3.0149, Test Loss: 3.0863\n",
      "Epoch 6310, Train Loss: 3.0210, Test Loss: 3.0828\n",
      "Epoch 6311, Train Loss: 3.0204, Test Loss: 3.0735\n",
      "Epoch 6312, Train Loss: 3.0223, Test Loss: 3.0801\n",
      "Epoch 6313, Train Loss: 3.0263, Test Loss: 3.0824\n",
      "Epoch 6314, Train Loss: 3.0272, Test Loss: 3.0723\n",
      "Epoch 6315, Train Loss: 3.0210, Test Loss: 3.0796\n",
      "Epoch 6316, Train Loss: 3.0348, Test Loss: 3.0682\n",
      "Epoch 6317, Train Loss: 3.0233, Test Loss: 3.0823\n",
      "Epoch 6318, Train Loss: 3.0219, Test Loss: 3.0806\n",
      "Epoch 6319, Train Loss: 3.0229, Test Loss: 3.0701\n",
      "Epoch 6320, Train Loss: 3.0128, Test Loss: 3.0801\n",
      "Epoch 6321, Train Loss: 3.0171, Test Loss: 3.0800\n",
      "Epoch 6322, Train Loss: 3.0198, Test Loss: 3.0679\n",
      "Epoch 6323, Train Loss: 3.0103, Test Loss: 3.0617\n",
      "Epoch 6324, Train Loss: 3.0110, Test Loss: 3.0687\n",
      "Epoch 6325, Train Loss: 3.0080, Test Loss: 3.0747\n",
      "Epoch 6326, Train Loss: 3.0119, Test Loss: 3.0669\n",
      "Epoch 6327, Train Loss: 3.0099, Test Loss: 3.0675\n",
      "Epoch 6328, Train Loss: 3.0177, Test Loss: 3.0649\n",
      "Epoch 6329, Train Loss: 3.0062, Test Loss: 3.0832\n",
      "Epoch 6330, Train Loss: 3.0317, Test Loss: 3.0676\n",
      "Epoch 6331, Train Loss: 3.0255, Test Loss: 3.0755\n",
      "Epoch 6332, Train Loss: 3.0232, Test Loss: 3.0872\n",
      "Epoch 6333, Train Loss: 3.0325, Test Loss: 3.0802\n",
      "Epoch 6334, Train Loss: 3.0134, Test Loss: 3.0739\n",
      "Epoch 6335, Train Loss: 3.0158, Test Loss: 3.0709\n",
      "Epoch 6336, Train Loss: 3.0143, Test Loss: 3.0767\n",
      "Epoch 6337, Train Loss: 3.0090, Test Loss: 3.0794\n",
      "Epoch 6338, Train Loss: 3.0242, Test Loss: 3.0685\n",
      "Epoch 6339, Train Loss: 3.0123, Test Loss: 3.0631\n",
      "Epoch 6340, Train Loss: 3.0117, Test Loss: 3.0797\n",
      "Epoch 6341, Train Loss: 3.0252, Test Loss: 3.0702\n",
      "Epoch 6342, Train Loss: 3.0119, Test Loss: 3.0769\n",
      "Epoch 6343, Train Loss: 3.0315, Test Loss: 3.0834\n",
      "Epoch 6344, Train Loss: 3.0200, Test Loss: 3.0861\n",
      "Epoch 6345, Train Loss: 3.0268, Test Loss: 3.0821\n",
      "Epoch 6346, Train Loss: 3.0184, Test Loss: 3.0797\n",
      "Epoch 6347, Train Loss: 3.0226, Test Loss: 3.0816\n",
      "Epoch 6348, Train Loss: 3.0211, Test Loss: 3.0938\n",
      "Epoch 6349, Train Loss: 3.0402, Test Loss: 3.0723\n",
      "Epoch 6350, Train Loss: 3.0108, Test Loss: 3.0911\n",
      "Epoch 6351, Train Loss: 3.0397, Test Loss: 3.0861\n",
      "Epoch 6352, Train Loss: 3.0240, Test Loss: 3.0823\n",
      "Epoch 6353, Train Loss: 3.0251, Test Loss: 3.0704\n",
      "Epoch 6354, Train Loss: 3.0217, Test Loss: 3.0705\n",
      "Epoch 6355, Train Loss: 3.0220, Test Loss: 3.0790\n",
      "Epoch 6356, Train Loss: 3.0262, Test Loss: 3.0729\n",
      "Epoch 6357, Train Loss: 3.0228, Test Loss: 3.0791\n",
      "Epoch 6358, Train Loss: 3.0331, Test Loss: 3.0797\n",
      "Epoch 6359, Train Loss: 3.0179, Test Loss: 3.1072\n",
      "Epoch 6360, Train Loss: 3.0526, Test Loss: 3.0792\n",
      "Epoch 6361, Train Loss: 3.0273, Test Loss: 3.0790\n",
      "Epoch 6362, Train Loss: 3.0268, Test Loss: 3.0771\n",
      "Epoch 6363, Train Loss: 3.0394, Test Loss: 3.0733\n",
      "Epoch 6364, Train Loss: 3.0317, Test Loss: 3.0686\n",
      "Epoch 6365, Train Loss: 3.0127, Test Loss: 3.0902\n",
      "Epoch 6366, Train Loss: 3.0309, Test Loss: 3.0825\n",
      "Epoch 6367, Train Loss: 3.0215, Test Loss: 3.0766\n",
      "Epoch 6368, Train Loss: 3.0240, Test Loss: 3.0760\n",
      "Epoch 6369, Train Loss: 3.0154, Test Loss: 3.0732\n",
      "Epoch 6370, Train Loss: 3.0209, Test Loss: 3.0660\n",
      "Epoch 6371, Train Loss: 3.0165, Test Loss: 3.0725\n",
      "Epoch 6372, Train Loss: 3.0178, Test Loss: 3.0743\n",
      "Epoch 6373, Train Loss: 3.0190, Test Loss: 3.0907\n",
      "Epoch 6374, Train Loss: 3.0254, Test Loss: 3.0955\n",
      "Epoch 6375, Train Loss: 3.0289, Test Loss: 3.0812\n",
      "Epoch 6376, Train Loss: 3.0217, Test Loss: 3.0690\n",
      "Epoch 6377, Train Loss: 2.9989, Test Loss: 3.0740\n",
      "Epoch 6378, Train Loss: 3.0252, Test Loss: 3.0697\n",
      "Epoch 6379, Train Loss: 3.0017, Test Loss: 3.0757\n",
      "Epoch 6380, Train Loss: 3.0077, Test Loss: 3.0795\n",
      "Epoch 6381, Train Loss: 3.0077, Test Loss: 3.0757\n",
      "Epoch 6382, Train Loss: 3.0185, Test Loss: 3.0662\n",
      "Epoch 6383, Train Loss: 3.0112, Test Loss: 3.0716\n",
      "Epoch 6384, Train Loss: 3.0184, Test Loss: 3.0757\n",
      "Epoch 6385, Train Loss: 3.0117, Test Loss: 3.0756\n",
      "Epoch 6386, Train Loss: 3.0204, Test Loss: 3.0753\n",
      "Epoch 6387, Train Loss: 3.0137, Test Loss: 3.0666\n",
      "Epoch 6388, Train Loss: 3.0131, Test Loss: 3.0707\n",
      "Epoch 6389, Train Loss: 3.0232, Test Loss: 3.0715\n",
      "Epoch 6390, Train Loss: 3.0189, Test Loss: 3.0639\n",
      "Epoch 6391, Train Loss: 3.0179, Test Loss: 3.0658\n",
      "Epoch 6392, Train Loss: 3.0168, Test Loss: 3.0638\n",
      "Epoch 6393, Train Loss: 3.0106, Test Loss: 3.0638\n",
      "Epoch 6394, Train Loss: 3.0012, Test Loss: 3.0662\n",
      "Epoch 6395, Train Loss: 3.0151, Test Loss: 3.0666\n",
      "Epoch 6396, Train Loss: 3.0093, Test Loss: 3.0658\n",
      "Epoch 6397, Train Loss: 3.0035, Test Loss: 3.0639\n",
      "Epoch 6398, Train Loss: 3.0093, Test Loss: 3.0604\n",
      "Epoch 6399, Train Loss: 3.0062, Test Loss: 3.0618\n",
      "Epoch 6400, Train Loss: 3.0058, Test Loss: 3.0709\n",
      "Epoch 6401, Train Loss: 3.0124, Test Loss: 3.0628\n",
      "Epoch 6402, Train Loss: 3.0078, Test Loss: 3.0639\n",
      "Epoch 6403, Train Loss: 3.0033, Test Loss: 3.0688\n",
      "Epoch 6404, Train Loss: 3.0050, Test Loss: 3.0637\n",
      "Epoch 6405, Train Loss: 3.0014, Test Loss: 3.0621\n",
      "Epoch 6406, Train Loss: 3.0049, Test Loss: 3.0628\n",
      "Epoch 6407, Train Loss: 3.0062, Test Loss: 3.0623\n",
      "Epoch 6408, Train Loss: 3.0015, Test Loss: 3.0686\n",
      "Epoch 6409, Train Loss: 3.0043, Test Loss: 3.0655\n",
      "Epoch 6410, Train Loss: 3.0091, Test Loss: 3.0661\n",
      "Epoch 6411, Train Loss: 3.0101, Test Loss: 3.0755\n",
      "Epoch 6412, Train Loss: 3.0114, Test Loss: 3.0597\n",
      "Epoch 6413, Train Loss: 3.0072, Test Loss: 3.0629\n",
      "Epoch 6414, Train Loss: 3.0207, Test Loss: 3.0682\n",
      "Epoch 6415, Train Loss: 3.0075, Test Loss: 3.0790\n",
      "Epoch 6416, Train Loss: 3.0141, Test Loss: 3.0744\n",
      "Epoch 6417, Train Loss: 3.0143, Test Loss: 3.0711\n",
      "Epoch 6418, Train Loss: 3.0158, Test Loss: 3.0712\n",
      "Epoch 6419, Train Loss: 3.0112, Test Loss: 3.0685\n",
      "Epoch 6420, Train Loss: 3.0165, Test Loss: 3.0612\n",
      "Epoch 6421, Train Loss: 3.0145, Test Loss: 3.0621\n",
      "Epoch 6422, Train Loss: 3.0123, Test Loss: 3.0652\n",
      "Epoch 6423, Train Loss: 3.0052, Test Loss: 3.0832\n",
      "Epoch 6424, Train Loss: 3.0176, Test Loss: 3.0795\n",
      "Epoch 6425, Train Loss: 3.0210, Test Loss: 3.0895\n",
      "Epoch 6426, Train Loss: 3.0360, Test Loss: 3.0882\n",
      "Epoch 6427, Train Loss: 3.0162, Test Loss: 3.0903\n",
      "Epoch 6428, Train Loss: 3.0266, Test Loss: 3.0669\n",
      "Epoch 6429, Train Loss: 3.0224, Test Loss: 3.0627\n",
      "Epoch 6430, Train Loss: 3.0141, Test Loss: 3.0751\n",
      "Epoch 6431, Train Loss: 3.0153, Test Loss: 3.0876\n",
      "Epoch 6432, Train Loss: 3.0271, Test Loss: 3.0839\n",
      "Epoch 6433, Train Loss: 3.0236, Test Loss: 3.0778\n",
      "Epoch 6434, Train Loss: 3.0195, Test Loss: 3.0781\n",
      "Epoch 6435, Train Loss: 3.0360, Test Loss: 3.0764\n",
      "Epoch 6436, Train Loss: 3.0246, Test Loss: 3.0901\n",
      "Epoch 6437, Train Loss: 3.0403, Test Loss: 3.0696\n",
      "Epoch 6438, Train Loss: 3.0096, Test Loss: 3.0891\n",
      "Epoch 6439, Train Loss: 3.0322, Test Loss: 3.0895\n",
      "Epoch 6440, Train Loss: 3.0338, Test Loss: 3.0912\n",
      "Epoch 6441, Train Loss: 3.0329, Test Loss: 3.0785\n",
      "Epoch 6442, Train Loss: 3.0190, Test Loss: 3.0783\n",
      "Epoch 6443, Train Loss: 3.0183, Test Loss: 3.0783\n",
      "Epoch 6444, Train Loss: 3.0208, Test Loss: 3.0723\n",
      "Epoch 6445, Train Loss: 3.0182, Test Loss: 3.0849\n",
      "Epoch 6446, Train Loss: 3.0381, Test Loss: 3.0668\n",
      "Epoch 6447, Train Loss: 3.0188, Test Loss: 3.0890\n",
      "Epoch 6448, Train Loss: 3.0233, Test Loss: 3.0792\n",
      "Epoch 6449, Train Loss: 3.0166, Test Loss: 3.0855\n",
      "Epoch 6450, Train Loss: 3.0229, Test Loss: 3.0687\n",
      "Epoch 6451, Train Loss: 3.0152, Test Loss: 3.0750\n",
      "Epoch 6452, Train Loss: 3.0231, Test Loss: 3.0710\n",
      "Epoch 6453, Train Loss: 3.0100, Test Loss: 3.0743\n",
      "Epoch 6454, Train Loss: 3.0367, Test Loss: 3.0883\n",
      "Epoch 6455, Train Loss: 3.0280, Test Loss: 3.0978\n",
      "Epoch 6456, Train Loss: 3.0462, Test Loss: 3.0911\n",
      "Epoch 6457, Train Loss: 3.0358, Test Loss: 3.0847\n",
      "Epoch 6458, Train Loss: 3.0394, Test Loss: 3.0683\n",
      "Epoch 6459, Train Loss: 3.0197, Test Loss: 3.0854\n",
      "Epoch 6460, Train Loss: 3.0325, Test Loss: 3.1027\n",
      "Epoch 6461, Train Loss: 3.0541, Test Loss: 3.0711\n",
      "Epoch 6462, Train Loss: 3.0286, Test Loss: 3.1042\n",
      "Epoch 6463, Train Loss: 3.0707, Test Loss: 3.0889\n",
      "Epoch 6464, Train Loss: 3.0431, Test Loss: 3.1264\n",
      "Epoch 6465, Train Loss: 3.0493, Test Loss: 3.1014\n",
      "Epoch 6466, Train Loss: 3.0424, Test Loss: 3.0898\n",
      "Epoch 6467, Train Loss: 3.0500, Test Loss: 3.1202\n",
      "Epoch 6468, Train Loss: 3.1080, Test Loss: 3.1095\n",
      "Epoch 6469, Train Loss: 3.0515, Test Loss: 3.1467\n",
      "Epoch 6470, Train Loss: 3.0746, Test Loss: 3.1393\n",
      "Epoch 6471, Train Loss: 3.0754, Test Loss: 3.0803\n",
      "Epoch 6472, Train Loss: 3.0248, Test Loss: 3.1338\n",
      "Epoch 6473, Train Loss: 3.1062, Test Loss: 3.1004\n",
      "Epoch 6474, Train Loss: 3.0653, Test Loss: 3.1015\n",
      "Epoch 6475, Train Loss: 3.0592, Test Loss: 3.1053\n",
      "Epoch 6476, Train Loss: 3.0500, Test Loss: 3.1045\n",
      "Epoch 6477, Train Loss: 3.0676, Test Loss: 3.1219\n",
      "Epoch 6478, Train Loss: 3.0619, Test Loss: 3.0786\n",
      "Epoch 6479, Train Loss: 3.0261, Test Loss: 3.1209\n",
      "Epoch 6480, Train Loss: 3.0591, Test Loss: 3.1328\n",
      "Epoch 6481, Train Loss: 3.0660, Test Loss: 3.1157\n",
      "Epoch 6482, Train Loss: 3.0474, Test Loss: 3.1000\n",
      "Epoch 6483, Train Loss: 3.0543, Test Loss: 3.0916\n",
      "Epoch 6484, Train Loss: 3.0495, Test Loss: 3.0975\n",
      "Epoch 6485, Train Loss: 3.0366, Test Loss: 3.0969\n",
      "Epoch 6486, Train Loss: 3.0482, Test Loss: 3.0874\n",
      "Epoch 6487, Train Loss: 3.0345, Test Loss: 3.0797\n",
      "Epoch 6488, Train Loss: 3.0256, Test Loss: 3.0775\n",
      "Epoch 6489, Train Loss: 3.0262, Test Loss: 3.0726\n",
      "Epoch 6490, Train Loss: 3.0279, Test Loss: 3.0749\n",
      "Epoch 6491, Train Loss: 3.0186, Test Loss: 3.0890\n",
      "Epoch 6492, Train Loss: 3.0367, Test Loss: 3.0710\n",
      "Epoch 6493, Train Loss: 3.0152, Test Loss: 3.0782\n",
      "Epoch 6494, Train Loss: 3.0443, Test Loss: 3.0802\n",
      "Epoch 6495, Train Loss: 3.0185, Test Loss: 3.0770\n",
      "Epoch 6496, Train Loss: 3.0221, Test Loss: 3.0984\n",
      "Epoch 6497, Train Loss: 3.0576, Test Loss: 3.0853\n",
      "Epoch 6498, Train Loss: 3.0207, Test Loss: 3.1069\n",
      "Epoch 6499, Train Loss: 3.0412, Test Loss: 3.1088\n",
      "Epoch 6500, Train Loss: 3.0270, Test Loss: 3.1036\n",
      "Epoch 6501, Train Loss: 3.0404, Test Loss: 3.1101\n",
      "Epoch 6502, Train Loss: 3.0528, Test Loss: 3.0903\n",
      "Epoch 6503, Train Loss: 3.0442, Test Loss: 3.0842\n",
      "Epoch 6504, Train Loss: 3.0371, Test Loss: 3.1073\n",
      "Epoch 6505, Train Loss: 3.0658, Test Loss: 3.0890\n",
      "Epoch 6506, Train Loss: 3.0285, Test Loss: 3.0969\n",
      "Epoch 6507, Train Loss: 3.0452, Test Loss: 3.0856\n",
      "Epoch 6508, Train Loss: 3.0434, Test Loss: 3.0967\n",
      "Epoch 6509, Train Loss: 3.0372, Test Loss: 3.0915\n",
      "Epoch 6510, Train Loss: 3.0380, Test Loss: 3.0749\n",
      "Epoch 6511, Train Loss: 3.0231, Test Loss: 3.0768\n",
      "Epoch 6512, Train Loss: 3.0273, Test Loss: 3.0791\n",
      "Epoch 6513, Train Loss: 3.0247, Test Loss: 3.0711\n",
      "Epoch 6514, Train Loss: 3.0067, Test Loss: 3.0888\n",
      "Epoch 6515, Train Loss: 3.0244, Test Loss: 3.0926\n",
      "Epoch 6516, Train Loss: 3.0315, Test Loss: 3.0719\n",
      "Epoch 6517, Train Loss: 3.0109, Test Loss: 3.0713\n",
      "Epoch 6518, Train Loss: 3.0111, Test Loss: 3.0707\n",
      "Epoch 6519, Train Loss: 3.0133, Test Loss: 3.0682\n",
      "Epoch 6520, Train Loss: 3.0171, Test Loss: 3.0617\n",
      "Epoch 6521, Train Loss: 3.0061, Test Loss: 3.0703\n",
      "Epoch 6522, Train Loss: 3.0170, Test Loss: 3.0737\n",
      "Epoch 6523, Train Loss: 3.0281, Test Loss: 3.0737\n",
      "Epoch 6524, Train Loss: 3.0160, Test Loss: 3.0733\n",
      "Epoch 6525, Train Loss: 3.0184, Test Loss: 3.0701\n",
      "Epoch 6526, Train Loss: 3.0188, Test Loss: 3.0746\n",
      "Epoch 6527, Train Loss: 3.0167, Test Loss: 3.0629\n",
      "Epoch 6528, Train Loss: 3.0035, Test Loss: 3.0701\n",
      "Epoch 6529, Train Loss: 3.0293, Test Loss: 3.0715\n",
      "Epoch 6530, Train Loss: 3.0184, Test Loss: 3.0646\n",
      "Epoch 6531, Train Loss: 3.0039, Test Loss: 3.0898\n",
      "Epoch 6532, Train Loss: 3.0320, Test Loss: 3.0733\n",
      "Epoch 6533, Train Loss: 3.0176, Test Loss: 3.0643\n",
      "Epoch 6534, Train Loss: 3.0114, Test Loss: 3.0600\n",
      "Epoch 6535, Train Loss: 3.0043, Test Loss: 3.0669\n",
      "Epoch 6536, Train Loss: 3.0052, Test Loss: 3.0647\n",
      "Epoch 6537, Train Loss: 3.0091, Test Loss: 3.0643\n",
      "Epoch 6538, Train Loss: 3.0136, Test Loss: 3.0674\n",
      "Epoch 6539, Train Loss: 3.0016, Test Loss: 3.0721\n",
      "Epoch 6540, Train Loss: 3.0112, Test Loss: 3.0656\n",
      "Epoch 6541, Train Loss: 3.0010, Test Loss: 3.0643\n",
      "Epoch 6542, Train Loss: 3.0063, Test Loss: 3.0660\n",
      "Epoch 6543, Train Loss: 3.0074, Test Loss: 3.0629\n",
      "Epoch 6544, Train Loss: 3.0057, Test Loss: 3.0614\n",
      "Epoch 6545, Train Loss: 3.0052, Test Loss: 3.0628\n",
      "Epoch 6546, Train Loss: 3.0017, Test Loss: 3.0616\n",
      "Epoch 6547, Train Loss: 3.0102, Test Loss: 3.0655\n",
      "Epoch 6548, Train Loss: 3.0105, Test Loss: 3.0646\n",
      "Epoch 6549, Train Loss: 3.0016, Test Loss: 3.0682\n",
      "Epoch 6550, Train Loss: 3.0105, Test Loss: 3.0648\n",
      "Epoch 6551, Train Loss: 3.0074, Test Loss: 3.0618\n",
      "Epoch 6552, Train Loss: 2.9995, Test Loss: 3.0631\n",
      "Epoch 6553, Train Loss: 3.0121, Test Loss: 3.0651\n",
      "Epoch 6554, Train Loss: 3.0054, Test Loss: 3.0667\n",
      "Epoch 6555, Train Loss: 3.0113, Test Loss: 3.0644\n",
      "Epoch 6556, Train Loss: 3.0037, Test Loss: 3.0659\n",
      "Epoch 6557, Train Loss: 3.0149, Test Loss: 3.0653\n",
      "Epoch 6558, Train Loss: 3.0044, Test Loss: 3.0660\n",
      "Epoch 6559, Train Loss: 3.0106, Test Loss: 3.0594\n",
      "Epoch 6560, Train Loss: 3.0094, Test Loss: 3.0602\n",
      "Epoch 6561, Train Loss: 3.0038, Test Loss: 3.0747\n",
      "Epoch 6562, Train Loss: 3.0166, Test Loss: 3.0754\n",
      "Epoch 6563, Train Loss: 3.0104, Test Loss: 3.0691\n",
      "Epoch 6564, Train Loss: 3.0125, Test Loss: 3.0683\n",
      "Epoch 6565, Train Loss: 3.0099, Test Loss: 3.0610\n",
      "Epoch 6566, Train Loss: 3.0054, Test Loss: 3.0662\n",
      "Epoch 6567, Train Loss: 3.0056, Test Loss: 3.0631\n",
      "Epoch 6568, Train Loss: 3.0072, Test Loss: 3.0685\n",
      "Epoch 6569, Train Loss: 3.0144, Test Loss: 3.0769\n",
      "Epoch 6570, Train Loss: 3.0141, Test Loss: 3.0741\n",
      "Epoch 6571, Train Loss: 3.0077, Test Loss: 3.0616\n",
      "Epoch 6572, Train Loss: 3.0096, Test Loss: 3.0849\n",
      "Epoch 6573, Train Loss: 3.0349, Test Loss: 3.0861\n",
      "Epoch 6574, Train Loss: 3.0323, Test Loss: 3.0746\n",
      "Epoch 6575, Train Loss: 3.0160, Test Loss: 3.0799\n",
      "Epoch 6576, Train Loss: 3.0326, Test Loss: 3.1033\n",
      "Epoch 6577, Train Loss: 3.0623, Test Loss: 3.1376\n",
      "Epoch 6578, Train Loss: 3.0771, Test Loss: 3.1342\n",
      "Epoch 6579, Train Loss: 3.0856, Test Loss: 3.0952\n",
      "Epoch 6580, Train Loss: 3.0277, Test Loss: 3.1076\n",
      "Epoch 6581, Train Loss: 3.0609, Test Loss: 3.1030\n",
      "Epoch 6582, Train Loss: 3.0385, Test Loss: 3.1170\n",
      "Epoch 6583, Train Loss: 3.0784, Test Loss: 3.0966\n",
      "Epoch 6584, Train Loss: 3.0685, Test Loss: 3.1109\n",
      "Epoch 6585, Train Loss: 3.0464, Test Loss: 3.1011\n",
      "Epoch 6586, Train Loss: 3.0576, Test Loss: 3.1143\n",
      "Epoch 6587, Train Loss: 3.0469, Test Loss: 3.1076\n",
      "Epoch 6588, Train Loss: 3.0475, Test Loss: 3.1308\n",
      "Epoch 6589, Train Loss: 3.0937, Test Loss: 3.1143\n",
      "Epoch 6590, Train Loss: 3.0889, Test Loss: 3.0902\n",
      "Epoch 6591, Train Loss: 3.0444, Test Loss: 3.0755\n",
      "Epoch 6592, Train Loss: 3.0370, Test Loss: 3.0917\n",
      "Epoch 6593, Train Loss: 3.0360, Test Loss: 3.1113\n",
      "Epoch 6594, Train Loss: 3.0575, Test Loss: 3.1074\n",
      "Epoch 6595, Train Loss: 3.0572, Test Loss: 3.1026\n",
      "Epoch 6596, Train Loss: 3.0500, Test Loss: 3.0711\n",
      "Epoch 6597, Train Loss: 3.0307, Test Loss: 3.0779\n",
      "Epoch 6598, Train Loss: 3.0340, Test Loss: 3.0846\n",
      "Epoch 6599, Train Loss: 3.0295, Test Loss: 3.0723\n",
      "Epoch 6600, Train Loss: 3.0088, Test Loss: 3.0966\n",
      "Epoch 6601, Train Loss: 3.0472, Test Loss: 3.0813\n",
      "Epoch 6602, Train Loss: 3.0219, Test Loss: 3.0722\n",
      "Epoch 6603, Train Loss: 3.0264, Test Loss: 3.0787\n",
      "Epoch 6604, Train Loss: 3.0258, Test Loss: 3.0749\n",
      "Epoch 6605, Train Loss: 3.0138, Test Loss: 3.0815\n",
      "Epoch 6606, Train Loss: 3.0185, Test Loss: 3.0759\n",
      "Epoch 6607, Train Loss: 3.0161, Test Loss: 3.0641\n",
      "Epoch 6608, Train Loss: 3.0130, Test Loss: 3.0716\n",
      "Epoch 6609, Train Loss: 3.0244, Test Loss: 3.0688\n",
      "Epoch 6610, Train Loss: 3.0087, Test Loss: 3.0791\n",
      "Epoch 6611, Train Loss: 3.0149, Test Loss: 3.0865\n",
      "Epoch 6612, Train Loss: 3.0134, Test Loss: 3.0722\n",
      "Epoch 6613, Train Loss: 3.0074, Test Loss: 3.0694\n",
      "Epoch 6614, Train Loss: 3.0093, Test Loss: 3.0729\n",
      "Epoch 6615, Train Loss: 3.0177, Test Loss: 3.0618\n",
      "Epoch 6616, Train Loss: 3.0067, Test Loss: 3.0687\n",
      "Epoch 6617, Train Loss: 3.0071, Test Loss: 3.0780\n",
      "Epoch 6618, Train Loss: 3.0126, Test Loss: 3.0660\n",
      "Epoch 6619, Train Loss: 3.0095, Test Loss: 3.0601\n",
      "Epoch 6620, Train Loss: 3.0059, Test Loss: 3.0688\n",
      "Epoch 6621, Train Loss: 3.0132, Test Loss: 3.0597\n",
      "Epoch 6622, Train Loss: 3.0067, Test Loss: 3.0600\n",
      "Epoch 6623, Train Loss: 3.0114, Test Loss: 3.0753\n",
      "Epoch 6624, Train Loss: 3.0150, Test Loss: 3.0668\n",
      "Epoch 6625, Train Loss: 3.0018, Test Loss: 3.0714\n",
      "Epoch 6626, Train Loss: 3.0120, Test Loss: 3.0695\n",
      "Epoch 6627, Train Loss: 3.0123, Test Loss: 3.0679\n",
      "Epoch 6628, Train Loss: 3.0099, Test Loss: 3.0597\n",
      "Epoch 6629, Train Loss: 3.0049, Test Loss: 3.0580\n",
      "Epoch 6630, Train Loss: 3.0031, Test Loss: 3.0605\n",
      "Epoch 6631, Train Loss: 3.0135, Test Loss: 3.0639\n",
      "Epoch 6632, Train Loss: 3.0077, Test Loss: 3.0679\n",
      "Epoch 6633, Train Loss: 3.0238, Test Loss: 3.0648\n",
      "Epoch 6634, Train Loss: 3.0082, Test Loss: 3.0708\n",
      "Epoch 6635, Train Loss: 3.0092, Test Loss: 3.0757\n",
      "Epoch 6636, Train Loss: 3.0136, Test Loss: 3.0647\n",
      "Epoch 6637, Train Loss: 3.0008, Test Loss: 3.0735\n",
      "Epoch 6638, Train Loss: 3.0214, Test Loss: 3.0683\n",
      "Epoch 6639, Train Loss: 3.0095, Test Loss: 3.0622\n",
      "Epoch 6640, Train Loss: 3.0059, Test Loss: 3.0694\n",
      "Epoch 6641, Train Loss: 3.0303, Test Loss: 3.0871\n",
      "Epoch 6642, Train Loss: 3.0337, Test Loss: 3.0943\n",
      "Epoch 6643, Train Loss: 3.0305, Test Loss: 3.0742\n",
      "Epoch 6644, Train Loss: 3.0157, Test Loss: 3.0649\n",
      "Epoch 6645, Train Loss: 3.0233, Test Loss: 3.0591\n",
      "Epoch 6646, Train Loss: 3.0072, Test Loss: 3.0827\n",
      "Epoch 6647, Train Loss: 3.0305, Test Loss: 3.0679\n",
      "Epoch 6648, Train Loss: 3.0144, Test Loss: 3.0786\n",
      "Epoch 6649, Train Loss: 3.0318, Test Loss: 3.0777\n",
      "Epoch 6650, Train Loss: 3.0100, Test Loss: 3.0999\n",
      "Epoch 6651, Train Loss: 3.0412, Test Loss: 3.1014\n",
      "Epoch 6652, Train Loss: 3.0355, Test Loss: 3.0871\n",
      "Epoch 6653, Train Loss: 3.0296, Test Loss: 3.0843\n",
      "Epoch 6654, Train Loss: 3.0531, Test Loss: 3.0809\n",
      "Epoch 6655, Train Loss: 3.0225, Test Loss: 3.0698\n",
      "Epoch 6656, Train Loss: 3.0178, Test Loss: 3.0809\n",
      "Epoch 6657, Train Loss: 3.0327, Test Loss: 3.0901\n",
      "Epoch 6658, Train Loss: 3.0404, Test Loss: 3.0773\n",
      "Epoch 6659, Train Loss: 3.0156, Test Loss: 3.0926\n",
      "Epoch 6660, Train Loss: 3.0304, Test Loss: 3.0916\n",
      "Epoch 6661, Train Loss: 3.0309, Test Loss: 3.0980\n",
      "Epoch 6662, Train Loss: 3.0613, Test Loss: 3.0719\n",
      "Epoch 6663, Train Loss: 3.0324, Test Loss: 3.0754\n",
      "Epoch 6664, Train Loss: 3.0298, Test Loss: 3.1163\n",
      "Epoch 6665, Train Loss: 3.0361, Test Loss: 3.1375\n",
      "Epoch 6666, Train Loss: 3.0637, Test Loss: 3.1198\n",
      "Epoch 6667, Train Loss: 3.0467, Test Loss: 3.1114\n",
      "Epoch 6668, Train Loss: 3.0732, Test Loss: 3.0888\n",
      "Epoch 6669, Train Loss: 3.0286, Test Loss: 3.1059\n",
      "Epoch 6670, Train Loss: 3.0438, Test Loss: 3.1208\n",
      "Epoch 6671, Train Loss: 3.0783, Test Loss: 3.0949\n",
      "Epoch 6672, Train Loss: 3.0520, Test Loss: 3.0899\n",
      "Epoch 6673, Train Loss: 3.0328, Test Loss: 3.1070\n",
      "Epoch 6674, Train Loss: 3.0553, Test Loss: 3.0847\n",
      "Epoch 6675, Train Loss: 3.0210, Test Loss: 3.0810\n",
      "Epoch 6676, Train Loss: 3.0297, Test Loss: 3.0862\n",
      "Epoch 6677, Train Loss: 3.0376, Test Loss: 3.1072\n",
      "Epoch 6678, Train Loss: 3.0506, Test Loss: 3.1112\n",
      "Epoch 6679, Train Loss: 3.0602, Test Loss: 3.0889\n",
      "Epoch 6680, Train Loss: 3.0468, Test Loss: 3.1009\n",
      "Epoch 6681, Train Loss: 3.0606, Test Loss: 3.0648\n",
      "Epoch 6682, Train Loss: 3.0324, Test Loss: 3.1297\n",
      "Epoch 6683, Train Loss: 3.0598, Test Loss: 3.1482\n",
      "Epoch 6684, Train Loss: 3.0736, Test Loss: 3.1393\n",
      "Epoch 6685, Train Loss: 3.0857, Test Loss: 3.1190\n",
      "Epoch 6686, Train Loss: 3.0851, Test Loss: 3.0776\n",
      "Epoch 6687, Train Loss: 3.0221, Test Loss: 3.0752\n",
      "Epoch 6688, Train Loss: 3.0208, Test Loss: 3.0999\n",
      "Epoch 6689, Train Loss: 3.0732, Test Loss: 3.0903\n",
      "Epoch 6690, Train Loss: 3.0531, Test Loss: 3.0870\n",
      "Epoch 6691, Train Loss: 3.0469, Test Loss: 3.0854\n",
      "Epoch 6692, Train Loss: 3.0405, Test Loss: 3.0594\n",
      "Epoch 6693, Train Loss: 3.0106, Test Loss: 3.0834\n",
      "Epoch 6694, Train Loss: 3.0322, Test Loss: 3.0850\n",
      "Epoch 6695, Train Loss: 3.0288, Test Loss: 3.0858\n",
      "Epoch 6696, Train Loss: 3.0340, Test Loss: 3.0819\n",
      "Epoch 6697, Train Loss: 3.0459, Test Loss: 3.0908\n",
      "Epoch 6698, Train Loss: 3.0414, Test Loss: 3.0768\n",
      "Epoch 6699, Train Loss: 3.0254, Test Loss: 3.0889\n",
      "Epoch 6700, Train Loss: 3.0360, Test Loss: 3.0874\n",
      "Epoch 6701, Train Loss: 3.0310, Test Loss: 3.0713\n",
      "Epoch 6702, Train Loss: 3.0113, Test Loss: 3.0772\n",
      "Epoch 6703, Train Loss: 3.0459, Test Loss: 3.0986\n",
      "Epoch 6704, Train Loss: 3.0463, Test Loss: 3.1127\n",
      "Epoch 6705, Train Loss: 3.0591, Test Loss: 3.0740\n",
      "Epoch 6706, Train Loss: 3.0191, Test Loss: 3.0840\n",
      "Epoch 6707, Train Loss: 3.0414, Test Loss: 3.0778\n",
      "Epoch 6708, Train Loss: 3.0318, Test Loss: 3.0872\n",
      "Epoch 6709, Train Loss: 3.0367, Test Loss: 3.1157\n",
      "Epoch 6710, Train Loss: 3.0706, Test Loss: 3.0799\n",
      "Epoch 6711, Train Loss: 3.0354, Test Loss: 3.1131\n",
      "Epoch 6712, Train Loss: 3.0813, Test Loss: 3.0693\n",
      "Epoch 6713, Train Loss: 3.0289, Test Loss: 3.0931\n",
      "Epoch 6714, Train Loss: 3.0453, Test Loss: 3.0807\n",
      "Epoch 6715, Train Loss: 3.0258, Test Loss: 3.0706\n",
      "Epoch 6716, Train Loss: 3.0169, Test Loss: 3.0775\n",
      "Epoch 6717, Train Loss: 3.0388, Test Loss: 3.1097\n",
      "Epoch 6718, Train Loss: 3.0422, Test Loss: 3.1386\n",
      "Epoch 6719, Train Loss: 3.0732, Test Loss: 3.1268\n",
      "Epoch 6720, Train Loss: 3.0638, Test Loss: 3.1218\n",
      "Epoch 6721, Train Loss: 3.0718, Test Loss: 3.0885\n",
      "Epoch 6722, Train Loss: 3.0358, Test Loss: 3.0913\n",
      "Epoch 6723, Train Loss: 3.0325, Test Loss: 3.1126\n",
      "Epoch 6724, Train Loss: 3.0567, Test Loss: 3.1506\n",
      "Epoch 6725, Train Loss: 3.1045, Test Loss: 3.1526\n",
      "Epoch 6726, Train Loss: 3.1055, Test Loss: 3.1008\n",
      "Epoch 6727, Train Loss: 3.0408, Test Loss: 3.0920\n",
      "Epoch 6728, Train Loss: 3.0598, Test Loss: 3.0851\n",
      "Epoch 6729, Train Loss: 3.0636, Test Loss: 3.0904\n",
      "Epoch 6730, Train Loss: 3.0385, Test Loss: 3.1299\n",
      "Epoch 6731, Train Loss: 3.0755, Test Loss: 3.1180\n",
      "Epoch 6732, Train Loss: 3.0640, Test Loss: 3.0852\n",
      "Epoch 6733, Train Loss: 3.0523, Test Loss: 3.1044\n",
      "Epoch 6734, Train Loss: 3.0548, Test Loss: 3.1013\n",
      "Epoch 6735, Train Loss: 3.0527, Test Loss: 3.1173\n",
      "Epoch 6736, Train Loss: 3.0785, Test Loss: 3.1037\n",
      "Epoch 6737, Train Loss: 3.0489, Test Loss: 3.1013\n",
      "Epoch 6738, Train Loss: 3.0497, Test Loss: 3.0895\n",
      "Epoch 6739, Train Loss: 3.0485, Test Loss: 3.0963\n",
      "Epoch 6740, Train Loss: 3.0484, Test Loss: 3.0864\n",
      "Epoch 6741, Train Loss: 3.0433, Test Loss: 3.0922\n",
      "Epoch 6742, Train Loss: 3.0305, Test Loss: 3.1027\n",
      "Epoch 6743, Train Loss: 3.0375, Test Loss: 3.1040\n",
      "Epoch 6744, Train Loss: 3.0629, Test Loss: 3.0830\n",
      "Epoch 6745, Train Loss: 3.0296, Test Loss: 3.0751\n",
      "Epoch 6746, Train Loss: 3.0389, Test Loss: 3.0720\n",
      "Epoch 6747, Train Loss: 3.0377, Test Loss: 3.0869\n",
      "Epoch 6748, Train Loss: 3.0451, Test Loss: 3.0853\n",
      "Epoch 6749, Train Loss: 3.0315, Test Loss: 3.0860\n",
      "Epoch 6750, Train Loss: 3.0459, Test Loss: 3.0794\n",
      "Epoch 6751, Train Loss: 3.0206, Test Loss: 3.1083\n",
      "Epoch 6752, Train Loss: 3.0487, Test Loss: 3.0671\n",
      "Epoch 6753, Train Loss: 3.0173, Test Loss: 3.0739\n",
      "Epoch 6754, Train Loss: 3.0429, Test Loss: 3.0722\n",
      "Epoch 6755, Train Loss: 3.0350, Test Loss: 3.0900\n",
      "Epoch 6756, Train Loss: 3.0419, Test Loss: 3.0706\n",
      "Epoch 6757, Train Loss: 3.0172, Test Loss: 3.0762\n",
      "Epoch 6758, Train Loss: 3.0241, Test Loss: 3.0987\n",
      "Epoch 6759, Train Loss: 3.0597, Test Loss: 3.0932\n",
      "Epoch 6760, Train Loss: 3.0363, Test Loss: 3.0774\n",
      "Epoch 6761, Train Loss: 3.0355, Test Loss: 3.0571\n",
      "Epoch 6762, Train Loss: 3.0044, Test Loss: 3.0840\n",
      "Epoch 6763, Train Loss: 3.0404, Test Loss: 3.0876\n",
      "Epoch 6764, Train Loss: 3.0318, Test Loss: 3.0665\n",
      "Epoch 6765, Train Loss: 3.0124, Test Loss: 3.0752\n",
      "Epoch 6766, Train Loss: 3.0210, Test Loss: 3.0801\n",
      "Epoch 6767, Train Loss: 3.0252, Test Loss: 3.0729\n",
      "Epoch 6768, Train Loss: 3.0314, Test Loss: 3.0625\n",
      "Epoch 6769, Train Loss: 3.0230, Test Loss: 3.0617\n",
      "Epoch 6770, Train Loss: 3.0152, Test Loss: 3.0722\n",
      "Epoch 6771, Train Loss: 3.0265, Test Loss: 3.0650\n",
      "Epoch 6772, Train Loss: 3.0110, Test Loss: 3.0710\n",
      "Epoch 6773, Train Loss: 3.0123, Test Loss: 3.0772\n",
      "Epoch 6774, Train Loss: 3.0225, Test Loss: 3.0685\n",
      "Epoch 6775, Train Loss: 3.0167, Test Loss: 3.0625\n",
      "Epoch 6776, Train Loss: 3.0120, Test Loss: 3.0638\n",
      "Epoch 6777, Train Loss: 3.0113, Test Loss: 3.0588\n",
      "Epoch 6778, Train Loss: 3.0105, Test Loss: 3.0623\n",
      "Epoch 6779, Train Loss: 3.0058, Test Loss: 3.0712\n",
      "Epoch 6780, Train Loss: 3.0092, Test Loss: 3.0671\n",
      "Epoch 6781, Train Loss: 3.0072, Test Loss: 3.0621\n",
      "Epoch 6782, Train Loss: 3.0020, Test Loss: 3.0653\n",
      "Epoch 6783, Train Loss: 3.0178, Test Loss: 3.0618\n",
      "Epoch 6784, Train Loss: 3.0078, Test Loss: 3.0593\n",
      "Epoch 6785, Train Loss: 3.0030, Test Loss: 3.0655\n",
      "Epoch 6786, Train Loss: 3.0113, Test Loss: 3.0656\n",
      "Epoch 6787, Train Loss: 3.0134, Test Loss: 3.0608\n",
      "Epoch 6788, Train Loss: 3.0021, Test Loss: 3.0628\n",
      "Epoch 6789, Train Loss: 3.0035, Test Loss: 3.0654\n",
      "Epoch 6790, Train Loss: 3.0104, Test Loss: 3.0606\n",
      "Epoch 6791, Train Loss: 3.0054, Test Loss: 3.0595\n",
      "Epoch 6792, Train Loss: 3.0029, Test Loss: 3.0657\n",
      "Epoch 6793, Train Loss: 3.0059, Test Loss: 3.0652\n",
      "Epoch 6794, Train Loss: 3.0032, Test Loss: 3.0595\n",
      "Epoch 6795, Train Loss: 3.0048, Test Loss: 3.0567\n",
      "Epoch 6796, Train Loss: 3.0079, Test Loss: 3.0589\n",
      "Epoch 6797, Train Loss: 3.0016, Test Loss: 3.0604\n",
      "Epoch 6798, Train Loss: 3.0011, Test Loss: 3.0644\n",
      "Epoch 6799, Train Loss: 3.0070, Test Loss: 3.0662\n",
      "Epoch 6800, Train Loss: 3.0029, Test Loss: 3.0604\n",
      "Epoch 6801, Train Loss: 3.0065, Test Loss: 3.0596\n",
      "Epoch 6802, Train Loss: 3.0048, Test Loss: 3.0652\n",
      "Epoch 6803, Train Loss: 3.0103, Test Loss: 3.0620\n",
      "Epoch 6804, Train Loss: 3.0003, Test Loss: 3.0714\n",
      "Epoch 6805, Train Loss: 3.0117, Test Loss: 3.0680\n",
      "Epoch 6806, Train Loss: 3.0067, Test Loss: 3.0616\n",
      "Epoch 6807, Train Loss: 3.0016, Test Loss: 3.0614\n",
      "Epoch 6808, Train Loss: 3.0014, Test Loss: 3.0625\n",
      "Epoch 6809, Train Loss: 3.0138, Test Loss: 3.0606\n",
      "Epoch 6810, Train Loss: 3.0069, Test Loss: 3.0590\n",
      "Epoch 6811, Train Loss: 3.0028, Test Loss: 3.0637\n",
      "Epoch 6812, Train Loss: 3.0115, Test Loss: 3.0639\n",
      "Epoch 6813, Train Loss: 3.0099, Test Loss: 3.0615\n",
      "Epoch 6814, Train Loss: 3.0065, Test Loss: 3.0624\n",
      "Epoch 6815, Train Loss: 3.0067, Test Loss: 3.0566\n",
      "Epoch 6816, Train Loss: 3.0033, Test Loss: 3.0566\n",
      "Epoch 6817, Train Loss: 3.0047, Test Loss: 3.0595\n",
      "Epoch 6818, Train Loss: 3.0041, Test Loss: 3.0632\n",
      "Epoch 6819, Train Loss: 3.0054, Test Loss: 3.0614\n",
      "Epoch 6820, Train Loss: 2.9996, Test Loss: 3.0588\n",
      "Epoch 6821, Train Loss: 3.0091, Test Loss: 3.0598\n",
      "Epoch 6822, Train Loss: 3.0075, Test Loss: 3.0573\n",
      "Epoch 6823, Train Loss: 2.9959, Test Loss: 3.0597\n",
      "Epoch 6824, Train Loss: 3.0041, Test Loss: 3.0615\n",
      "Epoch 6825, Train Loss: 3.0071, Test Loss: 3.0637\n",
      "Epoch 6826, Train Loss: 3.0158, Test Loss: 3.0597\n",
      "Epoch 6827, Train Loss: 3.0042, Test Loss: 3.0638\n",
      "Epoch 6828, Train Loss: 3.0024, Test Loss: 3.0639\n",
      "Epoch 6829, Train Loss: 3.0047, Test Loss: 3.0691\n",
      "Epoch 6830, Train Loss: 3.0045, Test Loss: 3.0604\n",
      "Epoch 6831, Train Loss: 3.0005, Test Loss: 3.0616\n",
      "Epoch 6832, Train Loss: 3.0048, Test Loss: 3.0614\n",
      "Epoch 6833, Train Loss: 3.0102, Test Loss: 3.0611\n",
      "Epoch 6834, Train Loss: 3.0050, Test Loss: 3.0658\n",
      "Epoch 6835, Train Loss: 3.0074, Test Loss: 3.0671\n",
      "Epoch 6836, Train Loss: 3.0100, Test Loss: 3.0636\n",
      "Epoch 6837, Train Loss: 3.0050, Test Loss: 3.0676\n",
      "Epoch 6838, Train Loss: 3.0042, Test Loss: 3.0630\n",
      "Epoch 6839, Train Loss: 3.0044, Test Loss: 3.0615\n",
      "Epoch 6840, Train Loss: 3.0019, Test Loss: 3.0624\n",
      "Epoch 6841, Train Loss: 3.0091, Test Loss: 3.0626\n",
      "Epoch 6842, Train Loss: 3.0027, Test Loss: 3.0599\n",
      "Epoch 6843, Train Loss: 3.0008, Test Loss: 3.0583\n",
      "Epoch 6844, Train Loss: 3.0073, Test Loss: 3.0624\n",
      "Epoch 6845, Train Loss: 3.0130, Test Loss: 3.0620\n",
      "Epoch 6846, Train Loss: 3.0164, Test Loss: 3.0592\n",
      "Epoch 6847, Train Loss: 3.0038, Test Loss: 3.0617\n",
      "Epoch 6848, Train Loss: 3.0065, Test Loss: 3.0630\n",
      "Epoch 6849, Train Loss: 3.0056, Test Loss: 3.0616\n",
      "Epoch 6850, Train Loss: 3.0015, Test Loss: 3.0617\n",
      "Epoch 6851, Train Loss: 2.9997, Test Loss: 3.0576\n",
      "Epoch 6852, Train Loss: 2.9977, Test Loss: 3.0628\n",
      "Epoch 6853, Train Loss: 3.0056, Test Loss: 3.0571\n",
      "Epoch 6854, Train Loss: 3.0011, Test Loss: 3.0623\n",
      "Epoch 6855, Train Loss: 3.0088, Test Loss: 3.0631\n",
      "Epoch 6856, Train Loss: 3.0098, Test Loss: 3.0644\n",
      "Epoch 6857, Train Loss: 3.0057, Test Loss: 3.0623\n",
      "Epoch 6858, Train Loss: 3.0116, Test Loss: 3.0674\n",
      "Epoch 6859, Train Loss: 3.0089, Test Loss: 3.0596\n",
      "Epoch 6860, Train Loss: 3.0019, Test Loss: 3.0634\n",
      "Epoch 6861, Train Loss: 3.0152, Test Loss: 3.0592\n",
      "Epoch 6862, Train Loss: 3.0054, Test Loss: 3.0619\n",
      "Epoch 6863, Train Loss: 3.0025, Test Loss: 3.0683\n",
      "Epoch 6864, Train Loss: 3.0047, Test Loss: 3.0646\n",
      "Epoch 6865, Train Loss: 3.0003, Test Loss: 3.0604\n",
      "Epoch 6866, Train Loss: 3.0008, Test Loss: 3.0582\n",
      "Epoch 6867, Train Loss: 3.0070, Test Loss: 3.0573\n",
      "Epoch 6868, Train Loss: 3.0025, Test Loss: 3.0630\n",
      "Epoch 6869, Train Loss: 3.0001, Test Loss: 3.0653\n",
      "Epoch 6870, Train Loss: 3.0058, Test Loss: 3.0659\n",
      "Epoch 6871, Train Loss: 3.0042, Test Loss: 3.0663\n",
      "Epoch 6872, Train Loss: 3.0042, Test Loss: 3.0664\n",
      "Epoch 6873, Train Loss: 3.0053, Test Loss: 3.0633\n",
      "Epoch 6874, Train Loss: 3.0094, Test Loss: 3.0593\n",
      "Epoch 6875, Train Loss: 3.0073, Test Loss: 3.0665\n",
      "Epoch 6876, Train Loss: 3.0115, Test Loss: 3.0647\n",
      "Epoch 6877, Train Loss: 3.0065, Test Loss: 3.0731\n",
      "Epoch 6878, Train Loss: 3.0046, Test Loss: 3.0835\n",
      "Epoch 6879, Train Loss: 3.0218, Test Loss: 3.0723\n",
      "Epoch 6880, Train Loss: 3.0142, Test Loss: 3.0583\n",
      "Epoch 6881, Train Loss: 3.0043, Test Loss: 3.0731\n",
      "Epoch 6882, Train Loss: 3.0254, Test Loss: 3.0818\n",
      "Epoch 6883, Train Loss: 3.0202, Test Loss: 3.0879\n",
      "Epoch 6884, Train Loss: 3.0235, Test Loss: 3.0743\n",
      "Epoch 6885, Train Loss: 3.0243, Test Loss: 3.0821\n",
      "Epoch 6886, Train Loss: 3.0398, Test Loss: 3.0633\n",
      "Epoch 6887, Train Loss: 3.0069, Test Loss: 3.0871\n",
      "Epoch 6888, Train Loss: 3.0313, Test Loss: 3.0898\n",
      "Epoch 6889, Train Loss: 3.0397, Test Loss: 3.0717\n",
      "Epoch 6890, Train Loss: 3.0120, Test Loss: 3.0911\n",
      "Epoch 6891, Train Loss: 3.0457, Test Loss: 3.0843\n",
      "Epoch 6892, Train Loss: 3.0169, Test Loss: 3.0750\n",
      "Epoch 6893, Train Loss: 3.0209, Test Loss: 3.0795\n",
      "Epoch 6894, Train Loss: 3.0233, Test Loss: 3.0642\n",
      "Epoch 6895, Train Loss: 3.0121, Test Loss: 3.0722\n",
      "Epoch 6896, Train Loss: 3.0220, Test Loss: 3.0687\n",
      "Epoch 6897, Train Loss: 3.0109, Test Loss: 3.0846\n",
      "Epoch 6898, Train Loss: 3.0305, Test Loss: 3.0806\n",
      "Epoch 6899, Train Loss: 3.0199, Test Loss: 3.0871\n",
      "Epoch 6900, Train Loss: 3.0347, Test Loss: 3.0803\n",
      "Epoch 6901, Train Loss: 3.0252, Test Loss: 3.0709\n",
      "Epoch 6902, Train Loss: 3.0102, Test Loss: 3.0860\n",
      "Epoch 6903, Train Loss: 3.0366, Test Loss: 3.0764\n",
      "Epoch 6904, Train Loss: 3.0153, Test Loss: 3.0693\n",
      "Epoch 6905, Train Loss: 3.0085, Test Loss: 3.0801\n",
      "Epoch 6906, Train Loss: 3.0261, Test Loss: 3.0670\n",
      "Epoch 6907, Train Loss: 3.0231, Test Loss: 3.0706\n",
      "Epoch 6908, Train Loss: 3.0190, Test Loss: 3.0737\n",
      "Epoch 6909, Train Loss: 3.0219, Test Loss: 3.0834\n",
      "Epoch 6910, Train Loss: 3.0288, Test Loss: 3.0808\n",
      "Epoch 6911, Train Loss: 3.0309, Test Loss: 3.0733\n",
      "Epoch 6912, Train Loss: 3.0199, Test Loss: 3.0755\n",
      "Epoch 6913, Train Loss: 3.0168, Test Loss: 3.0782\n",
      "Epoch 6914, Train Loss: 3.0173, Test Loss: 3.0783\n",
      "Epoch 6915, Train Loss: 3.0351, Test Loss: 3.0581\n",
      "Epoch 6916, Train Loss: 3.0040, Test Loss: 3.0725\n",
      "Epoch 6917, Train Loss: 3.0168, Test Loss: 3.0618\n",
      "Epoch 6918, Train Loss: 3.0073, Test Loss: 3.0676\n",
      "Epoch 6919, Train Loss: 3.0150, Test Loss: 3.0629\n",
      "Epoch 6920, Train Loss: 3.0049, Test Loss: 3.0726\n",
      "Epoch 6921, Train Loss: 3.0137, Test Loss: 3.0754\n",
      "Epoch 6922, Train Loss: 3.0080, Test Loss: 3.0645\n",
      "Epoch 6923, Train Loss: 2.9995, Test Loss: 3.0597\n",
      "Epoch 6924, Train Loss: 3.0077, Test Loss: 3.0645\n",
      "Epoch 6925, Train Loss: 3.0092, Test Loss: 3.0594\n",
      "Epoch 6926, Train Loss: 2.9990, Test Loss: 3.0685\n",
      "Epoch 6927, Train Loss: 3.0005, Test Loss: 3.0711\n",
      "Epoch 6928, Train Loss: 3.0061, Test Loss: 3.0653\n",
      "Epoch 6929, Train Loss: 3.0017, Test Loss: 3.0686\n",
      "Epoch 6930, Train Loss: 3.0081, Test Loss: 3.0660\n",
      "Epoch 6931, Train Loss: 3.0027, Test Loss: 3.0617\n",
      "Epoch 6932, Train Loss: 2.9985, Test Loss: 3.0625\n",
      "Epoch 6933, Train Loss: 3.0149, Test Loss: 3.0617\n",
      "Epoch 6934, Train Loss: 3.0133, Test Loss: 3.0693\n",
      "Epoch 6935, Train Loss: 3.0050, Test Loss: 3.0788\n",
      "Epoch 6936, Train Loss: 3.0246, Test Loss: 3.0697\n",
      "Epoch 6937, Train Loss: 3.0130, Test Loss: 3.0629\n",
      "Epoch 6938, Train Loss: 3.0159, Test Loss: 3.0670\n",
      "Epoch 6939, Train Loss: 3.0043, Test Loss: 3.0667\n",
      "Epoch 6940, Train Loss: 3.0060, Test Loss: 3.0604\n",
      "Epoch 6941, Train Loss: 3.0187, Test Loss: 3.0635\n",
      "Epoch 6942, Train Loss: 3.0081, Test Loss: 3.0758\n",
      "Epoch 6943, Train Loss: 3.0157, Test Loss: 3.0804\n",
      "Epoch 6944, Train Loss: 3.0109, Test Loss: 3.0733\n",
      "Epoch 6945, Train Loss: 3.0126, Test Loss: 3.0602\n",
      "Epoch 6946, Train Loss: 3.0072, Test Loss: 3.0661\n",
      "Epoch 6947, Train Loss: 3.0116, Test Loss: 3.0600\n",
      "Epoch 6948, Train Loss: 3.0104, Test Loss: 3.0586\n",
      "Epoch 6949, Train Loss: 3.0114, Test Loss: 3.0541\n",
      "Epoch 6950, Train Loss: 3.0021, Test Loss: 3.0719\n",
      "Epoch 6951, Train Loss: 3.0105, Test Loss: 3.0644\n",
      "Epoch 6952, Train Loss: 3.0040, Test Loss: 3.0722\n",
      "Epoch 6953, Train Loss: 3.0099, Test Loss: 3.0655\n",
      "Epoch 6954, Train Loss: 3.0070, Test Loss: 3.0724\n",
      "Epoch 6955, Train Loss: 3.0143, Test Loss: 3.0629\n",
      "Epoch 6956, Train Loss: 3.0174, Test Loss: 3.0570\n",
      "Epoch 6957, Train Loss: 3.0102, Test Loss: 3.0611\n",
      "Epoch 6958, Train Loss: 3.0035, Test Loss: 3.0716\n",
      "Epoch 6959, Train Loss: 3.0192, Test Loss: 3.0713\n",
      "Epoch 6960, Train Loss: 3.0141, Test Loss: 3.0700\n",
      "Epoch 6961, Train Loss: 3.0193, Test Loss: 3.0689\n",
      "Epoch 6962, Train Loss: 3.0121, Test Loss: 3.0788\n",
      "Epoch 6963, Train Loss: 3.0098, Test Loss: 3.0966\n",
      "Epoch 6964, Train Loss: 3.0427, Test Loss: 3.0715\n",
      "Epoch 6965, Train Loss: 3.0258, Test Loss: 3.0721\n",
      "Epoch 6966, Train Loss: 3.0145, Test Loss: 3.0869\n",
      "Epoch 6967, Train Loss: 3.0378, Test Loss: 3.0766\n",
      "Epoch 6968, Train Loss: 3.0254, Test Loss: 3.0675\n",
      "Epoch 6969, Train Loss: 3.0164, Test Loss: 3.0598\n",
      "Epoch 6970, Train Loss: 3.0084, Test Loss: 3.0687\n",
      "Epoch 6971, Train Loss: 3.0254, Test Loss: 3.0653\n",
      "Epoch 6972, Train Loss: 3.0173, Test Loss: 3.0648\n",
      "Epoch 6973, Train Loss: 3.0050, Test Loss: 3.0638\n",
      "Epoch 6974, Train Loss: 3.0111, Test Loss: 3.0634\n",
      "Epoch 6975, Train Loss: 3.0063, Test Loss: 3.0682\n",
      "Epoch 6976, Train Loss: 3.0131, Test Loss: 3.0586\n",
      "Epoch 6977, Train Loss: 3.0039, Test Loss: 3.0573\n",
      "Epoch 6978, Train Loss: 3.0089, Test Loss: 3.0660\n",
      "Epoch 6979, Train Loss: 3.0139, Test Loss: 3.0644\n",
      "Epoch 6980, Train Loss: 3.0048, Test Loss: 3.0664\n",
      "Epoch 6981, Train Loss: 3.0064, Test Loss: 3.0610\n",
      "Epoch 6982, Train Loss: 3.0030, Test Loss: 3.0598\n",
      "Epoch 6983, Train Loss: 3.0073, Test Loss: 3.0731\n",
      "Epoch 6984, Train Loss: 3.0221, Test Loss: 3.0637\n",
      "Epoch 6985, Train Loss: 3.0047, Test Loss: 3.0594\n",
      "Epoch 6986, Train Loss: 3.0043, Test Loss: 3.0618\n",
      "Epoch 6987, Train Loss: 3.0031, Test Loss: 3.0656\n",
      "Epoch 6988, Train Loss: 3.0083, Test Loss: 3.0634\n",
      "Epoch 6989, Train Loss: 3.0095, Test Loss: 3.0615\n",
      "Epoch 6990, Train Loss: 3.0021, Test Loss: 3.0688\n",
      "Epoch 6991, Train Loss: 3.0086, Test Loss: 3.0735\n",
      "Epoch 6992, Train Loss: 3.0123, Test Loss: 3.0665\n",
      "Epoch 6993, Train Loss: 3.0062, Test Loss: 3.0584\n",
      "Epoch 6994, Train Loss: 3.0062, Test Loss: 3.0580\n",
      "Epoch 6995, Train Loss: 3.0041, Test Loss: 3.0618\n",
      "Epoch 6996, Train Loss: 3.0042, Test Loss: 3.0631\n",
      "Epoch 6997, Train Loss: 3.0020, Test Loss: 3.0585\n",
      "Epoch 6998, Train Loss: 3.0028, Test Loss: 3.0572\n",
      "Epoch 6999, Train Loss: 3.0088, Test Loss: 3.0614\n",
      "Epoch 7000, Train Loss: 2.9983, Test Loss: 3.0713\n",
      "Epoch 7001, Train Loss: 3.0069, Test Loss: 3.0670\n",
      "Epoch 7002, Train Loss: 3.0032, Test Loss: 3.0628\n",
      "Epoch 7003, Train Loss: 3.0021, Test Loss: 3.0654\n",
      "Epoch 7004, Train Loss: 3.0079, Test Loss: 3.0671\n",
      "Epoch 7005, Train Loss: 3.0040, Test Loss: 3.0601\n",
      "Epoch 7006, Train Loss: 3.0089, Test Loss: 3.0572\n",
      "Epoch 7007, Train Loss: 3.0079, Test Loss: 3.0679\n",
      "Epoch 7008, Train Loss: 3.0109, Test Loss: 3.0729\n",
      "Epoch 7009, Train Loss: 3.0039, Test Loss: 3.0692\n",
      "Epoch 7010, Train Loss: 3.0112, Test Loss: 3.0673\n",
      "Epoch 7011, Train Loss: 3.0085, Test Loss: 3.0601\n",
      "Epoch 7012, Train Loss: 3.0072, Test Loss: 3.0605\n",
      "Epoch 7013, Train Loss: 3.0018, Test Loss: 3.0573\n",
      "Epoch 7014, Train Loss: 3.0018, Test Loss: 3.0606\n",
      "Epoch 7015, Train Loss: 3.0069, Test Loss: 3.0636\n",
      "Epoch 7016, Train Loss: 3.0075, Test Loss: 3.0688\n",
      "Epoch 7017, Train Loss: 3.0134, Test Loss: 3.0627\n",
      "Epoch 7018, Train Loss: 3.0078, Test Loss: 3.0664\n",
      "Epoch 7019, Train Loss: 3.0183, Test Loss: 3.0698\n",
      "Epoch 7020, Train Loss: 3.0172, Test Loss: 3.0760\n",
      "Epoch 7021, Train Loss: 3.0175, Test Loss: 3.0737\n",
      "Epoch 7022, Train Loss: 3.0144, Test Loss: 3.0615\n",
      "Epoch 7023, Train Loss: 3.0044, Test Loss: 3.0605\n",
      "Epoch 7024, Train Loss: 3.0041, Test Loss: 3.0624\n",
      "Epoch 7025, Train Loss: 3.0078, Test Loss: 3.0626\n",
      "Epoch 7026, Train Loss: 3.0042, Test Loss: 3.0614\n",
      "Epoch 7027, Train Loss: 3.0038, Test Loss: 3.0627\n",
      "Epoch 7028, Train Loss: 3.0158, Test Loss: 3.0618\n",
      "Epoch 7029, Train Loss: 3.0055, Test Loss: 3.0611\n",
      "Epoch 7030, Train Loss: 3.0041, Test Loss: 3.0604\n",
      "Epoch 7031, Train Loss: 3.0114, Test Loss: 3.0577\n",
      "Epoch 7032, Train Loss: 3.0060, Test Loss: 3.0662\n",
      "Epoch 7033, Train Loss: 3.0108, Test Loss: 3.0705\n",
      "Epoch 7034, Train Loss: 3.0141, Test Loss: 3.0589\n",
      "Epoch 7035, Train Loss: 2.9988, Test Loss: 3.0589\n",
      "Epoch 7036, Train Loss: 3.0060, Test Loss: 3.0595\n",
      "Epoch 7037, Train Loss: 3.0030, Test Loss: 3.0587\n",
      "Epoch 7038, Train Loss: 2.9990, Test Loss: 3.0580\n",
      "Epoch 7039, Train Loss: 3.0081, Test Loss: 3.0608\n",
      "Epoch 7040, Train Loss: 2.9975, Test Loss: 3.0681\n",
      "Epoch 7041, Train Loss: 3.0067, Test Loss: 3.0610\n",
      "Epoch 7042, Train Loss: 3.0024, Test Loss: 3.0663\n",
      "Epoch 7043, Train Loss: 3.0142, Test Loss: 3.0660\n",
      "Epoch 7044, Train Loss: 3.0047, Test Loss: 3.0688\n",
      "Epoch 7045, Train Loss: 3.0107, Test Loss: 3.0600\n",
      "Epoch 7046, Train Loss: 3.0103, Test Loss: 3.0577\n",
      "Epoch 7047, Train Loss: 3.0165, Test Loss: 3.0585\n",
      "Epoch 7048, Train Loss: 3.0107, Test Loss: 3.0735\n",
      "Epoch 7049, Train Loss: 3.0210, Test Loss: 3.0779\n",
      "Epoch 7050, Train Loss: 3.0231, Test Loss: 3.0718\n",
      "Epoch 7051, Train Loss: 3.0198, Test Loss: 3.0674\n",
      "Epoch 7052, Train Loss: 3.0086, Test Loss: 3.0663\n",
      "Epoch 7053, Train Loss: 3.0174, Test Loss: 3.0665\n",
      "Epoch 7054, Train Loss: 3.0218, Test Loss: 3.0679\n",
      "Epoch 7055, Train Loss: 3.0101, Test Loss: 3.0867\n",
      "Epoch 7056, Train Loss: 3.0285, Test Loss: 3.0844\n",
      "Epoch 7057, Train Loss: 3.0182, Test Loss: 3.0737\n",
      "Epoch 7058, Train Loss: 3.0200, Test Loss: 3.0657\n",
      "Epoch 7059, Train Loss: 3.0209, Test Loss: 3.0724\n",
      "Epoch 7060, Train Loss: 3.0164, Test Loss: 3.0726\n",
      "Epoch 7061, Train Loss: 3.0178, Test Loss: 3.0618\n",
      "Epoch 7062, Train Loss: 3.0126, Test Loss: 3.0629\n",
      "Epoch 7063, Train Loss: 3.0135, Test Loss: 3.0688\n",
      "Epoch 7064, Train Loss: 3.0091, Test Loss: 3.0682\n",
      "Epoch 7065, Train Loss: 3.0129, Test Loss: 3.0573\n",
      "Epoch 7066, Train Loss: 3.0122, Test Loss: 3.0569\n",
      "Epoch 7067, Train Loss: 3.0077, Test Loss: 3.0683\n",
      "Epoch 7068, Train Loss: 3.0171, Test Loss: 3.0600\n",
      "Epoch 7069, Train Loss: 3.0089, Test Loss: 3.0729\n",
      "Epoch 7070, Train Loss: 3.0260, Test Loss: 3.0631\n",
      "Epoch 7071, Train Loss: 3.0128, Test Loss: 3.0638\n",
      "Epoch 7072, Train Loss: 3.0103, Test Loss: 3.0562\n",
      "Epoch 7073, Train Loss: 3.0022, Test Loss: 3.0616\n",
      "Epoch 7074, Train Loss: 3.0095, Test Loss: 3.0743\n",
      "Epoch 7075, Train Loss: 3.0134, Test Loss: 3.0732\n",
      "Epoch 7076, Train Loss: 3.0094, Test Loss: 3.0702\n",
      "Epoch 7077, Train Loss: 3.0159, Test Loss: 3.0560\n",
      "Epoch 7078, Train Loss: 3.0001, Test Loss: 3.0585\n",
      "Epoch 7079, Train Loss: 3.0078, Test Loss: 3.0619\n",
      "Epoch 7080, Train Loss: 3.0049, Test Loss: 3.0579\n",
      "Epoch 7081, Train Loss: 3.0021, Test Loss: 3.0573\n",
      "Epoch 7082, Train Loss: 3.0009, Test Loss: 3.0571\n",
      "Epoch 7083, Train Loss: 3.0017, Test Loss: 3.0592\n",
      "Epoch 7084, Train Loss: 3.0013, Test Loss: 3.0580\n",
      "Epoch 7085, Train Loss: 3.0033, Test Loss: 3.0575\n",
      "Epoch 7086, Train Loss: 3.0001, Test Loss: 3.0574\n",
      "Epoch 7087, Train Loss: 3.0026, Test Loss: 3.0603\n",
      "Epoch 7088, Train Loss: 3.0054, Test Loss: 3.0571\n",
      "Epoch 7089, Train Loss: 2.9999, Test Loss: 3.0623\n",
      "Epoch 7090, Train Loss: 3.0061, Test Loss: 3.0613\n",
      "Epoch 7091, Train Loss: 2.9987, Test Loss: 3.0676\n",
      "Epoch 7092, Train Loss: 3.0068, Test Loss: 3.0591\n",
      "Epoch 7093, Train Loss: 2.9987, Test Loss: 3.0620\n",
      "Epoch 7094, Train Loss: 3.0141, Test Loss: 3.0676\n",
      "Epoch 7095, Train Loss: 3.0138, Test Loss: 3.0778\n",
      "Epoch 7096, Train Loss: 3.0106, Test Loss: 3.0796\n",
      "Epoch 7097, Train Loss: 3.0223, Test Loss: 3.0750\n",
      "Epoch 7098, Train Loss: 3.0149, Test Loss: 3.0637\n",
      "Epoch 7099, Train Loss: 3.0124, Test Loss: 3.0702\n",
      "Epoch 7100, Train Loss: 3.0096, Test Loss: 3.0850\n",
      "Epoch 7101, Train Loss: 3.0390, Test Loss: 3.0676\n",
      "Epoch 7102, Train Loss: 3.0172, Test Loss: 3.0702\n",
      "Epoch 7103, Train Loss: 3.0175, Test Loss: 3.0803\n",
      "Epoch 7104, Train Loss: 3.0200, Test Loss: 3.0932\n",
      "Epoch 7105, Train Loss: 3.0213, Test Loss: 3.0826\n",
      "Epoch 7106, Train Loss: 3.0164, Test Loss: 3.0772\n",
      "Epoch 7107, Train Loss: 3.0242, Test Loss: 3.0709\n",
      "Epoch 7108, Train Loss: 3.0183, Test Loss: 3.0773\n",
      "Epoch 7109, Train Loss: 3.0298, Test Loss: 3.0756\n",
      "Epoch 7110, Train Loss: 3.0264, Test Loss: 3.0557\n",
      "Epoch 7111, Train Loss: 3.0075, Test Loss: 3.0754\n",
      "Epoch 7112, Train Loss: 3.0212, Test Loss: 3.0728\n",
      "Epoch 7113, Train Loss: 3.0088, Test Loss: 3.0840\n",
      "Epoch 7114, Train Loss: 3.0313, Test Loss: 3.0741\n",
      "Epoch 7115, Train Loss: 3.0168, Test Loss: 3.0718\n",
      "Epoch 7116, Train Loss: 3.0262, Test Loss: 3.0676\n",
      "Epoch 7117, Train Loss: 3.0153, Test Loss: 3.0740\n",
      "Epoch 7118, Train Loss: 3.0183, Test Loss: 3.0721\n",
      "Epoch 7119, Train Loss: 3.0204, Test Loss: 3.0814\n",
      "Epoch 7120, Train Loss: 3.0234, Test Loss: 3.0699\n",
      "Epoch 7121, Train Loss: 3.0152, Test Loss: 3.0839\n",
      "Epoch 7122, Train Loss: 3.0210, Test Loss: 3.1056\n",
      "Epoch 7123, Train Loss: 3.0509, Test Loss: 3.0815\n",
      "Epoch 7124, Train Loss: 3.0268, Test Loss: 3.0691\n",
      "Epoch 7125, Train Loss: 3.0256, Test Loss: 3.0745\n",
      "Epoch 7126, Train Loss: 3.0235, Test Loss: 3.0795\n",
      "Epoch 7127, Train Loss: 3.0392, Test Loss: 3.0813\n",
      "Epoch 7128, Train Loss: 3.0280, Test Loss: 3.0746\n",
      "Epoch 7129, Train Loss: 3.0368, Test Loss: 3.0936\n",
      "Epoch 7130, Train Loss: 3.0360, Test Loss: 3.0893\n",
      "Epoch 7131, Train Loss: 3.0413, Test Loss: 3.0963\n",
      "Epoch 7132, Train Loss: 3.0322, Test Loss: 3.1038\n",
      "Epoch 7133, Train Loss: 3.0619, Test Loss: 3.0945\n",
      "Epoch 7134, Train Loss: 3.0285, Test Loss: 3.0989\n",
      "Epoch 7135, Train Loss: 3.0312, Test Loss: 3.0866\n",
      "Epoch 7136, Train Loss: 3.0413, Test Loss: 3.0714\n",
      "Epoch 7137, Train Loss: 3.0315, Test Loss: 3.0617\n",
      "Epoch 7138, Train Loss: 3.0306, Test Loss: 3.0850\n",
      "Epoch 7139, Train Loss: 3.0317, Test Loss: 3.0894\n",
      "Epoch 7140, Train Loss: 3.0292, Test Loss: 3.0924\n",
      "Epoch 7141, Train Loss: 3.0296, Test Loss: 3.0903\n",
      "Epoch 7142, Train Loss: 3.0443, Test Loss: 3.0877\n",
      "Epoch 7143, Train Loss: 3.0430, Test Loss: 3.0909\n",
      "Epoch 7144, Train Loss: 3.0436, Test Loss: 3.0919\n",
      "Epoch 7145, Train Loss: 3.0524, Test Loss: 3.0781\n",
      "Epoch 7146, Train Loss: 3.0460, Test Loss: 3.0665\n",
      "Epoch 7147, Train Loss: 3.0208, Test Loss: 3.0723\n",
      "Epoch 7148, Train Loss: 3.0256, Test Loss: 3.0822\n",
      "Epoch 7149, Train Loss: 3.0464, Test Loss: 3.0701\n",
      "Epoch 7150, Train Loss: 3.0070, Test Loss: 3.0926\n",
      "Epoch 7151, Train Loss: 3.0408, Test Loss: 3.0713\n",
      "Epoch 7152, Train Loss: 3.0205, Test Loss: 3.0648\n",
      "Epoch 7153, Train Loss: 3.0178, Test Loss: 3.0665\n",
      "Epoch 7154, Train Loss: 3.0169, Test Loss: 3.0660\n",
      "Epoch 7155, Train Loss: 3.0123, Test Loss: 3.0685\n",
      "Epoch 7156, Train Loss: 3.0156, Test Loss: 3.0790\n",
      "Epoch 7157, Train Loss: 3.0201, Test Loss: 3.0932\n",
      "Epoch 7158, Train Loss: 3.0340, Test Loss: 3.0797\n",
      "Epoch 7159, Train Loss: 3.0258, Test Loss: 3.0704\n",
      "Epoch 7160, Train Loss: 3.0163, Test Loss: 3.0670\n",
      "Epoch 7161, Train Loss: 3.0163, Test Loss: 3.0673\n",
      "Epoch 7162, Train Loss: 3.0324, Test Loss: 3.0745\n",
      "Epoch 7163, Train Loss: 3.0238, Test Loss: 3.0720\n",
      "Epoch 7164, Train Loss: 3.0071, Test Loss: 3.0888\n",
      "Epoch 7165, Train Loss: 3.0311, Test Loss: 3.0657\n",
      "Epoch 7166, Train Loss: 3.0126, Test Loss: 3.0708\n",
      "Epoch 7167, Train Loss: 3.0189, Test Loss: 3.0723\n",
      "Epoch 7168, Train Loss: 3.0356, Test Loss: 3.0723\n",
      "Epoch 7169, Train Loss: 3.0186, Test Loss: 3.0630\n",
      "Epoch 7170, Train Loss: 3.0048, Test Loss: 3.0746\n",
      "Epoch 7171, Train Loss: 3.0235, Test Loss: 3.0687\n",
      "Epoch 7172, Train Loss: 3.0134, Test Loss: 3.0619\n",
      "Epoch 7173, Train Loss: 3.0092, Test Loss: 3.0655\n",
      "Epoch 7174, Train Loss: 3.0178, Test Loss: 3.0671\n",
      "Epoch 7175, Train Loss: 3.0131, Test Loss: 3.0887\n",
      "Epoch 7176, Train Loss: 3.0194, Test Loss: 3.0852\n",
      "Epoch 7177, Train Loss: 3.0237, Test Loss: 3.0625\n",
      "Epoch 7178, Train Loss: 3.0080, Test Loss: 3.0661\n",
      "Epoch 7179, Train Loss: 3.0148, Test Loss: 3.0636\n",
      "Epoch 7180, Train Loss: 3.0077, Test Loss: 3.0752\n",
      "Epoch 7181, Train Loss: 3.0154, Test Loss: 3.0727\n",
      "Epoch 7182, Train Loss: 3.0139, Test Loss: 3.0631\n",
      "Epoch 7183, Train Loss: 3.0111, Test Loss: 3.0686\n",
      "Epoch 7184, Train Loss: 3.0260, Test Loss: 3.0790\n",
      "Epoch 7185, Train Loss: 3.0130, Test Loss: 3.0658\n",
      "Epoch 7186, Train Loss: 3.0096, Test Loss: 3.0646\n",
      "Epoch 7187, Train Loss: 3.0161, Test Loss: 3.0592\n",
      "Epoch 7188, Train Loss: 3.0094, Test Loss: 3.0677\n",
      "Epoch 7189, Train Loss: 3.0182, Test Loss: 3.0793\n",
      "Epoch 7190, Train Loss: 3.0256, Test Loss: 3.0693\n",
      "Epoch 7191, Train Loss: 3.0136, Test Loss: 3.0765\n",
      "Epoch 7192, Train Loss: 3.0257, Test Loss: 3.0909\n",
      "Epoch 7193, Train Loss: 3.0317, Test Loss: 3.0764\n",
      "Epoch 7194, Train Loss: 3.0204, Test Loss: 3.0666\n",
      "Epoch 7195, Train Loss: 3.0137, Test Loss: 3.0746\n",
      "Epoch 7196, Train Loss: 3.0320, Test Loss: 3.0757\n",
      "Epoch 7197, Train Loss: 3.0288, Test Loss: 3.0683\n",
      "Epoch 7198, Train Loss: 3.0111, Test Loss: 3.0675\n",
      "Epoch 7199, Train Loss: 3.0108, Test Loss: 3.0776\n",
      "Epoch 7200, Train Loss: 3.0222, Test Loss: 3.0666\n",
      "Epoch 7201, Train Loss: 3.0134, Test Loss: 3.0669\n",
      "Epoch 7202, Train Loss: 3.0155, Test Loss: 3.0631\n",
      "Epoch 7203, Train Loss: 3.0191, Test Loss: 3.0754\n",
      "Epoch 7204, Train Loss: 3.0233, Test Loss: 3.0750\n",
      "Epoch 7205, Train Loss: 3.0193, Test Loss: 3.0808\n",
      "Epoch 7206, Train Loss: 3.0308, Test Loss: 3.0742\n",
      "Epoch 7207, Train Loss: 3.0043, Test Loss: 3.0813\n",
      "Epoch 7208, Train Loss: 3.0237, Test Loss: 3.0804\n",
      "Epoch 7209, Train Loss: 3.0245, Test Loss: 3.0781\n",
      "Epoch 7210, Train Loss: 3.0339, Test Loss: 3.0611\n",
      "Epoch 7211, Train Loss: 3.0055, Test Loss: 3.0697\n",
      "Epoch 7212, Train Loss: 3.0259, Test Loss: 3.0734\n",
      "Epoch 7213, Train Loss: 3.0240, Test Loss: 3.0722\n",
      "Epoch 7214, Train Loss: 3.0345, Test Loss: 3.0831\n",
      "Epoch 7215, Train Loss: 3.0273, Test Loss: 3.0824\n",
      "Epoch 7216, Train Loss: 3.0340, Test Loss: 3.0742\n",
      "Epoch 7217, Train Loss: 3.0255, Test Loss: 3.0884\n",
      "Epoch 7218, Train Loss: 3.0436, Test Loss: 3.0825\n",
      "Epoch 7219, Train Loss: 3.0307, Test Loss: 3.0751\n",
      "Epoch 7220, Train Loss: 3.0237, Test Loss: 3.0745\n",
      "Epoch 7221, Train Loss: 3.0209, Test Loss: 3.0841\n",
      "Epoch 7222, Train Loss: 3.0306, Test Loss: 3.0701\n",
      "Epoch 7223, Train Loss: 3.0107, Test Loss: 3.0763\n",
      "Epoch 7224, Train Loss: 3.0143, Test Loss: 3.0712\n",
      "Epoch 7225, Train Loss: 3.0184, Test Loss: 3.0651\n",
      "Epoch 7226, Train Loss: 3.0183, Test Loss: 3.0577\n",
      "Epoch 7227, Train Loss: 3.0168, Test Loss: 3.0626\n",
      "Epoch 7228, Train Loss: 3.0146, Test Loss: 3.0740\n",
      "Epoch 7229, Train Loss: 3.0120, Test Loss: 3.0723\n",
      "Epoch 7230, Train Loss: 3.0117, Test Loss: 3.0692\n",
      "Epoch 7231, Train Loss: 3.0150, Test Loss: 3.0669\n",
      "Epoch 7232, Train Loss: 3.0138, Test Loss: 3.0597\n",
      "Epoch 7233, Train Loss: 3.0038, Test Loss: 3.0695\n",
      "Epoch 7234, Train Loss: 3.0229, Test Loss: 3.0626\n",
      "Epoch 7235, Train Loss: 3.0134, Test Loss: 3.0703\n",
      "Epoch 7236, Train Loss: 3.0180, Test Loss: 3.0693\n",
      "Epoch 7237, Train Loss: 3.0100, Test Loss: 3.0668\n",
      "Epoch 7238, Train Loss: 3.0093, Test Loss: 3.0617\n",
      "Epoch 7239, Train Loss: 3.0043, Test Loss: 3.0643\n",
      "Epoch 7240, Train Loss: 3.0149, Test Loss: 3.0714\n",
      "Epoch 7241, Train Loss: 3.0107, Test Loss: 3.0796\n",
      "Epoch 7242, Train Loss: 3.0160, Test Loss: 3.0837\n",
      "Epoch 7243, Train Loss: 3.0260, Test Loss: 3.0708\n",
      "Epoch 7244, Train Loss: 3.0189, Test Loss: 3.0592\n",
      "Epoch 7245, Train Loss: 3.0183, Test Loss: 3.0720\n",
      "Epoch 7246, Train Loss: 3.0207, Test Loss: 3.0690\n",
      "Epoch 7247, Train Loss: 3.0136, Test Loss: 3.0610\n",
      "Epoch 7248, Train Loss: 3.0050, Test Loss: 3.0690\n",
      "Epoch 7249, Train Loss: 3.0183, Test Loss: 3.0636\n",
      "Epoch 7250, Train Loss: 3.0114, Test Loss: 3.0712\n",
      "Epoch 7251, Train Loss: 3.0068, Test Loss: 3.0700\n",
      "Epoch 7252, Train Loss: 3.0223, Test Loss: 3.0623\n",
      "Epoch 7253, Train Loss: 3.0168, Test Loss: 3.0580\n",
      "Epoch 7254, Train Loss: 3.0017, Test Loss: 3.0725\n",
      "Epoch 7255, Train Loss: 3.0096, Test Loss: 3.0687\n",
      "Epoch 7256, Train Loss: 3.0036, Test Loss: 3.0607\n",
      "Epoch 7257, Train Loss: 3.0013, Test Loss: 3.0608\n",
      "Epoch 7258, Train Loss: 3.0063, Test Loss: 3.0577\n",
      "Epoch 7259, Train Loss: 2.9991, Test Loss: 3.0609\n",
      "Epoch 7260, Train Loss: 3.0033, Test Loss: 3.0590\n",
      "Epoch 7261, Train Loss: 3.0112, Test Loss: 3.0582\n",
      "Epoch 7262, Train Loss: 3.0064, Test Loss: 3.0602\n",
      "Epoch 7263, Train Loss: 3.0022, Test Loss: 3.0590\n",
      "Epoch 7264, Train Loss: 3.0050, Test Loss: 3.0579\n",
      "Epoch 7265, Train Loss: 3.0013, Test Loss: 3.0553\n",
      "Epoch 7266, Train Loss: 3.0035, Test Loss: 3.0601\n",
      "Epoch 7267, Train Loss: 3.0078, Test Loss: 3.0631\n",
      "Epoch 7268, Train Loss: 3.0019, Test Loss: 3.0597\n",
      "Epoch 7269, Train Loss: 3.0023, Test Loss: 3.0593\n",
      "Epoch 7270, Train Loss: 3.0120, Test Loss: 3.0661\n",
      "Epoch 7271, Train Loss: 3.0101, Test Loss: 3.0625\n",
      "Epoch 7272, Train Loss: 3.0135, Test Loss: 3.0586\n",
      "Epoch 7273, Train Loss: 3.0129, Test Loss: 3.0567\n",
      "Epoch 7274, Train Loss: 2.9992, Test Loss: 3.0663\n",
      "Epoch 7275, Train Loss: 3.0074, Test Loss: 3.0698\n",
      "Epoch 7276, Train Loss: 3.0123, Test Loss: 3.0649\n",
      "Epoch 7277, Train Loss: 3.0048, Test Loss: 3.0607\n",
      "Epoch 7278, Train Loss: 3.0065, Test Loss: 3.0631\n",
      "Epoch 7279, Train Loss: 3.0110, Test Loss: 3.0662\n",
      "Epoch 7280, Train Loss: 3.0094, Test Loss: 3.0615\n",
      "Epoch 7281, Train Loss: 3.0128, Test Loss: 3.0601\n",
      "Epoch 7282, Train Loss: 3.0093, Test Loss: 3.0547\n",
      "Epoch 7283, Train Loss: 2.9982, Test Loss: 3.0646\n",
      "Epoch 7284, Train Loss: 3.0147, Test Loss: 3.0587\n",
      "Epoch 7285, Train Loss: 3.0123, Test Loss: 3.0574\n",
      "Epoch 7286, Train Loss: 3.0079, Test Loss: 3.0643\n",
      "Epoch 7287, Train Loss: 3.0060, Test Loss: 3.0724\n",
      "Epoch 7288, Train Loss: 3.0191, Test Loss: 3.0776\n",
      "Epoch 7289, Train Loss: 3.0241, Test Loss: 3.0750\n",
      "Epoch 7290, Train Loss: 3.0310, Test Loss: 3.0727\n",
      "Epoch 7291, Train Loss: 3.0171, Test Loss: 3.0763\n",
      "Epoch 7292, Train Loss: 3.0187, Test Loss: 3.0670\n",
      "Epoch 7293, Train Loss: 3.0132, Test Loss: 3.0758\n",
      "Epoch 7294, Train Loss: 3.0208, Test Loss: 3.0827\n",
      "Epoch 7295, Train Loss: 3.0272, Test Loss: 3.0778\n",
      "Epoch 7296, Train Loss: 3.0253, Test Loss: 3.0647\n",
      "Epoch 7297, Train Loss: 3.0129, Test Loss: 3.0726\n",
      "Epoch 7298, Train Loss: 3.0296, Test Loss: 3.0598\n",
      "Epoch 7299, Train Loss: 3.0110, Test Loss: 3.0644\n",
      "Epoch 7300, Train Loss: 3.0229, Test Loss: 3.0662\n",
      "Epoch 7301, Train Loss: 3.0126, Test Loss: 3.0736\n",
      "Epoch 7302, Train Loss: 3.0195, Test Loss: 3.0588\n",
      "Epoch 7303, Train Loss: 3.0079, Test Loss: 3.0717\n",
      "Epoch 7304, Train Loss: 3.0201, Test Loss: 3.0746\n",
      "Epoch 7305, Train Loss: 3.0223, Test Loss: 3.0693\n",
      "Epoch 7306, Train Loss: 3.0197, Test Loss: 3.0681\n",
      "Epoch 7307, Train Loss: 3.0209, Test Loss: 3.0678\n",
      "Epoch 7308, Train Loss: 3.0066, Test Loss: 3.0740\n",
      "Epoch 7309, Train Loss: 3.0222, Test Loss: 3.0793\n",
      "Epoch 7310, Train Loss: 3.0303, Test Loss: 3.0691\n",
      "Epoch 7311, Train Loss: 3.0237, Test Loss: 3.0792\n",
      "Epoch 7312, Train Loss: 3.0211, Test Loss: 3.0800\n",
      "Epoch 7313, Train Loss: 3.0201, Test Loss: 3.0684\n",
      "Epoch 7314, Train Loss: 3.0072, Test Loss: 3.0718\n",
      "Epoch 7315, Train Loss: 3.0224, Test Loss: 3.0736\n",
      "Epoch 7316, Train Loss: 3.0235, Test Loss: 3.0689\n",
      "Epoch 7317, Train Loss: 3.0401, Test Loss: 3.0769\n",
      "Epoch 7318, Train Loss: 3.0260, Test Loss: 3.0803\n",
      "Epoch 7319, Train Loss: 3.0276, Test Loss: 3.0700\n",
      "Epoch 7320, Train Loss: 3.0137, Test Loss: 3.0751\n",
      "Epoch 7321, Train Loss: 3.0263, Test Loss: 3.1006\n",
      "Epoch 7322, Train Loss: 3.0477, Test Loss: 3.0932\n",
      "Epoch 7323, Train Loss: 3.0490, Test Loss: 3.0713\n",
      "Epoch 7324, Train Loss: 3.0267, Test Loss: 3.0763\n",
      "Epoch 7325, Train Loss: 3.0398, Test Loss: 3.0679\n",
      "Epoch 7326, Train Loss: 3.0170, Test Loss: 3.0844\n",
      "Epoch 7327, Train Loss: 3.0428, Test Loss: 3.0915\n",
      "Epoch 7328, Train Loss: 3.0292, Test Loss: 3.0915\n",
      "Epoch 7329, Train Loss: 3.0182, Test Loss: 3.0953\n",
      "Epoch 7330, Train Loss: 3.0319, Test Loss: 3.0886\n",
      "Epoch 7331, Train Loss: 3.0212, Test Loss: 3.0818\n",
      "Epoch 7332, Train Loss: 3.0266, Test Loss: 3.0746\n",
      "Epoch 7333, Train Loss: 3.0335, Test Loss: 3.0816\n",
      "Epoch 7334, Train Loss: 3.0424, Test Loss: 3.0793\n",
      "Epoch 7335, Train Loss: 3.0138, Test Loss: 3.0853\n",
      "Epoch 7336, Train Loss: 3.0245, Test Loss: 3.0871\n",
      "Epoch 7337, Train Loss: 3.0448, Test Loss: 3.0673\n",
      "Epoch 7338, Train Loss: 3.0238, Test Loss: 3.0703\n",
      "Epoch 7339, Train Loss: 3.0169, Test Loss: 3.0912\n",
      "Epoch 7340, Train Loss: 3.0341, Test Loss: 3.0924\n",
      "Epoch 7341, Train Loss: 3.0409, Test Loss: 3.0754\n",
      "Epoch 7342, Train Loss: 3.0211, Test Loss: 3.0742\n",
      "Epoch 7343, Train Loss: 3.0173, Test Loss: 3.0862\n",
      "Epoch 7344, Train Loss: 3.0341, Test Loss: 3.0713\n",
      "Epoch 7345, Train Loss: 3.0218, Test Loss: 3.0816\n",
      "Epoch 7346, Train Loss: 3.0161, Test Loss: 3.0995\n",
      "Epoch 7347, Train Loss: 3.0417, Test Loss: 3.1033\n",
      "Epoch 7348, Train Loss: 3.0468, Test Loss: 3.0815\n",
      "Epoch 7349, Train Loss: 3.0268, Test Loss: 3.0795\n",
      "Epoch 7350, Train Loss: 3.0282, Test Loss: 3.0709\n",
      "Epoch 7351, Train Loss: 3.0105, Test Loss: 3.0751\n",
      "Epoch 7352, Train Loss: 3.0168, Test Loss: 3.0698\n",
      "Epoch 7353, Train Loss: 3.0286, Test Loss: 3.0615\n",
      "Epoch 7354, Train Loss: 3.0175, Test Loss: 3.0598\n",
      "Epoch 7355, Train Loss: 3.0095, Test Loss: 3.0703\n",
      "Epoch 7356, Train Loss: 3.0183, Test Loss: 3.0656\n",
      "Epoch 7357, Train Loss: 3.0343, Test Loss: 3.0622\n",
      "Epoch 7358, Train Loss: 3.0132, Test Loss: 3.0638\n",
      "Epoch 7359, Train Loss: 3.0131, Test Loss: 3.0662\n",
      "Epoch 7360, Train Loss: 3.0140, Test Loss: 3.0724\n",
      "Epoch 7361, Train Loss: 3.0171, Test Loss: 3.0762\n",
      "Epoch 7362, Train Loss: 3.0170, Test Loss: 3.0687\n",
      "Epoch 7363, Train Loss: 3.0006, Test Loss: 3.0675\n",
      "Epoch 7364, Train Loss: 3.0103, Test Loss: 3.0617\n",
      "Epoch 7365, Train Loss: 3.0103, Test Loss: 3.0551\n",
      "Epoch 7366, Train Loss: 3.0091, Test Loss: 3.0637\n",
      "Epoch 7367, Train Loss: 3.0144, Test Loss: 3.0736\n",
      "Epoch 7368, Train Loss: 3.0151, Test Loss: 3.0657\n",
      "Epoch 7369, Train Loss: 3.0084, Test Loss: 3.0732\n",
      "Epoch 7370, Train Loss: 3.0171, Test Loss: 3.0735\n",
      "Epoch 7371, Train Loss: 3.0218, Test Loss: 3.0670\n",
      "Epoch 7372, Train Loss: 3.0158, Test Loss: 3.0603\n",
      "Epoch 7373, Train Loss: 3.0093, Test Loss: 3.0714\n",
      "Epoch 7374, Train Loss: 3.0087, Test Loss: 3.0707\n",
      "Epoch 7375, Train Loss: 3.0219, Test Loss: 3.0623\n",
      "Epoch 7376, Train Loss: 3.0102, Test Loss: 3.0600\n",
      "Epoch 7377, Train Loss: 3.0070, Test Loss: 3.0678\n",
      "Epoch 7378, Train Loss: 3.0224, Test Loss: 3.0783\n",
      "Epoch 7379, Train Loss: 3.0200, Test Loss: 3.0757\n",
      "Epoch 7380, Train Loss: 3.0199, Test Loss: 3.0813\n",
      "Epoch 7381, Train Loss: 3.0355, Test Loss: 3.0780\n",
      "Epoch 7382, Train Loss: 3.0246, Test Loss: 3.0635\n",
      "Epoch 7383, Train Loss: 3.0039, Test Loss: 3.0620\n",
      "Epoch 7384, Train Loss: 3.0146, Test Loss: 3.0677\n",
      "Epoch 7385, Train Loss: 3.0135, Test Loss: 3.0649\n",
      "Epoch 7386, Train Loss: 3.0184, Test Loss: 3.0610\n",
      "Epoch 7387, Train Loss: 3.0107, Test Loss: 3.0744\n",
      "Epoch 7388, Train Loss: 3.0201, Test Loss: 3.0752\n",
      "Epoch 7389, Train Loss: 3.0133, Test Loss: 3.0686\n",
      "Epoch 7390, Train Loss: 3.0059, Test Loss: 3.0545\n",
      "Epoch 7391, Train Loss: 3.0096, Test Loss: 3.0671\n",
      "Epoch 7392, Train Loss: 3.0193, Test Loss: 3.0563\n",
      "Epoch 7393, Train Loss: 3.0066, Test Loss: 3.0710\n",
      "Epoch 7394, Train Loss: 3.0088, Test Loss: 3.0816\n",
      "Epoch 7395, Train Loss: 3.0099, Test Loss: 3.1007\n",
      "Epoch 7396, Train Loss: 3.0373, Test Loss: 3.0849\n",
      "Epoch 7397, Train Loss: 3.0219, Test Loss: 3.0649\n",
      "Epoch 7398, Train Loss: 3.0139, Test Loss: 3.0638\n",
      "Epoch 7399, Train Loss: 3.0121, Test Loss: 3.0662\n",
      "Epoch 7400, Train Loss: 3.0139, Test Loss: 3.0810\n",
      "Epoch 7401, Train Loss: 3.0231, Test Loss: 3.0778\n",
      "Epoch 7402, Train Loss: 3.0285, Test Loss: 3.0763\n",
      "Epoch 7403, Train Loss: 3.0369, Test Loss: 3.0624\n",
      "Epoch 7404, Train Loss: 3.0096, Test Loss: 3.0599\n",
      "Epoch 7405, Train Loss: 3.0056, Test Loss: 3.0718\n",
      "Epoch 7406, Train Loss: 3.0226, Test Loss: 3.0670\n",
      "Epoch 7407, Train Loss: 3.0135, Test Loss: 3.0594\n",
      "Epoch 7408, Train Loss: 3.0049, Test Loss: 3.0598\n",
      "Epoch 7409, Train Loss: 3.0129, Test Loss: 3.0602\n",
      "Epoch 7410, Train Loss: 3.0138, Test Loss: 3.0565\n",
      "Epoch 7411, Train Loss: 3.0104, Test Loss: 3.0701\n",
      "Epoch 7412, Train Loss: 3.0144, Test Loss: 3.0670\n",
      "Epoch 7413, Train Loss: 3.0155, Test Loss: 3.0615\n",
      "Epoch 7414, Train Loss: 3.0067, Test Loss: 3.0552\n",
      "Epoch 7415, Train Loss: 3.0014, Test Loss: 3.0601\n",
      "Epoch 7416, Train Loss: 3.0107, Test Loss: 3.0613\n",
      "Epoch 7417, Train Loss: 3.0099, Test Loss: 3.0655\n",
      "Epoch 7418, Train Loss: 3.0090, Test Loss: 3.0680\n",
      "Epoch 7419, Train Loss: 3.0140, Test Loss: 3.0690\n",
      "Epoch 7420, Train Loss: 3.0051, Test Loss: 3.0725\n",
      "Epoch 7421, Train Loss: 3.0137, Test Loss: 3.0676\n",
      "Epoch 7422, Train Loss: 3.0206, Test Loss: 3.0530\n",
      "Epoch 7423, Train Loss: 3.0027, Test Loss: 3.0644\n",
      "Epoch 7424, Train Loss: 3.0114, Test Loss: 3.0598\n",
      "Epoch 7425, Train Loss: 3.0115, Test Loss: 3.0590\n",
      "Epoch 7426, Train Loss: 3.0085, Test Loss: 3.0654\n",
      "Epoch 7427, Train Loss: 3.0083, Test Loss: 3.0692\n",
      "Epoch 7428, Train Loss: 3.0256, Test Loss: 3.0669\n",
      "Epoch 7429, Train Loss: 3.0152, Test Loss: 3.0637\n",
      "Epoch 7430, Train Loss: 3.0105, Test Loss: 3.0729\n",
      "Epoch 7431, Train Loss: 3.0210, Test Loss: 3.0561\n",
      "Epoch 7432, Train Loss: 3.0158, Test Loss: 3.0768\n",
      "Epoch 7433, Train Loss: 3.0231, Test Loss: 3.0768\n",
      "Epoch 7434, Train Loss: 3.0201, Test Loss: 3.0798\n",
      "Epoch 7435, Train Loss: 3.0316, Test Loss: 3.0591\n",
      "Epoch 7436, Train Loss: 3.0015, Test Loss: 3.0636\n",
      "Epoch 7437, Train Loss: 3.0183, Test Loss: 3.0608\n",
      "Epoch 7438, Train Loss: 3.0172, Test Loss: 3.0560\n",
      "Epoch 7439, Train Loss: 2.9953, Test Loss: 3.0791\n",
      "Epoch 7440, Train Loss: 3.0094, Test Loss: 3.0700\n",
      "Epoch 7441, Train Loss: 3.0095, Test Loss: 3.0690\n",
      "Epoch 7442, Train Loss: 3.0104, Test Loss: 3.0590\n",
      "Epoch 7443, Train Loss: 3.0084, Test Loss: 3.0638\n",
      "Epoch 7444, Train Loss: 3.0135, Test Loss: 3.0589\n",
      "Epoch 7445, Train Loss: 3.0064, Test Loss: 3.0567\n",
      "Epoch 7446, Train Loss: 3.0032, Test Loss: 3.0673\n",
      "Epoch 7447, Train Loss: 3.0124, Test Loss: 3.0756\n",
      "Epoch 7448, Train Loss: 3.0092, Test Loss: 3.0668\n",
      "Epoch 7449, Train Loss: 3.0053, Test Loss: 3.0617\n",
      "Epoch 7450, Train Loss: 3.0155, Test Loss: 3.0628\n",
      "Epoch 7451, Train Loss: 3.0109, Test Loss: 3.0688\n",
      "Epoch 7452, Train Loss: 3.0150, Test Loss: 3.0620\n",
      "Epoch 7453, Train Loss: 3.0060, Test Loss: 3.0703\n",
      "Epoch 7454, Train Loss: 3.0098, Test Loss: 3.0612\n",
      "Epoch 7455, Train Loss: 3.0069, Test Loss: 3.0605\n",
      "Epoch 7456, Train Loss: 3.0074, Test Loss: 3.0642\n",
      "Epoch 7457, Train Loss: 3.0138, Test Loss: 3.0612\n",
      "Epoch 7458, Train Loss: 3.0122, Test Loss: 3.0624\n",
      "Epoch 7459, Train Loss: 3.0144, Test Loss: 3.0582\n",
      "Epoch 7460, Train Loss: 3.0055, Test Loss: 3.0636\n",
      "Epoch 7461, Train Loss: 3.0127, Test Loss: 3.0552\n",
      "Epoch 7462, Train Loss: 3.0018, Test Loss: 3.0578\n",
      "Epoch 7463, Train Loss: 3.0040, Test Loss: 3.0585\n",
      "Epoch 7464, Train Loss: 3.0135, Test Loss: 3.0537\n",
      "Epoch 7465, Train Loss: 2.9998, Test Loss: 3.0570\n",
      "Epoch 7466, Train Loss: 3.0067, Test Loss: 3.0565\n",
      "Epoch 7467, Train Loss: 3.0027, Test Loss: 3.0540\n",
      "Epoch 7468, Train Loss: 2.9992, Test Loss: 3.0619\n",
      "Epoch 7469, Train Loss: 3.0033, Test Loss: 3.0691\n",
      "Epoch 7470, Train Loss: 3.0201, Test Loss: 3.0548\n",
      "Epoch 7471, Train Loss: 3.0045, Test Loss: 3.0593\n",
      "Epoch 7472, Train Loss: 3.0111, Test Loss: 3.0607\n",
      "Epoch 7473, Train Loss: 3.0065, Test Loss: 3.0653\n",
      "Epoch 7474, Train Loss: 3.0129, Test Loss: 3.0604\n",
      "Epoch 7475, Train Loss: 3.0022, Test Loss: 3.0668\n",
      "Epoch 7476, Train Loss: 3.0018, Test Loss: 3.0613\n",
      "Epoch 7477, Train Loss: 3.0033, Test Loss: 3.0626\n",
      "Epoch 7478, Train Loss: 3.0134, Test Loss: 3.0608\n",
      "Epoch 7479, Train Loss: 3.0159, Test Loss: 3.0714\n",
      "Epoch 7480, Train Loss: 3.0204, Test Loss: 3.0690\n",
      "Epoch 7481, Train Loss: 3.0173, Test Loss: 3.0555\n",
      "Epoch 7482, Train Loss: 3.0051, Test Loss: 3.0650\n",
      "Epoch 7483, Train Loss: 3.0096, Test Loss: 3.0649\n",
      "Epoch 7484, Train Loss: 3.0134, Test Loss: 3.0641\n",
      "Epoch 7485, Train Loss: 3.0148, Test Loss: 3.0657\n",
      "Epoch 7486, Train Loss: 3.0189, Test Loss: 3.0715\n",
      "Epoch 7487, Train Loss: 3.0117, Test Loss: 3.0658\n",
      "Epoch 7488, Train Loss: 3.0078, Test Loss: 3.0564\n",
      "Epoch 7489, Train Loss: 2.9996, Test Loss: 3.0698\n",
      "Epoch 7490, Train Loss: 3.0163, Test Loss: 3.0870\n",
      "Epoch 7491, Train Loss: 3.0363, Test Loss: 3.0610\n",
      "Epoch 7492, Train Loss: 3.0094, Test Loss: 3.0777\n",
      "Epoch 7493, Train Loss: 3.0144, Test Loss: 3.0896\n",
      "Epoch 7494, Train Loss: 3.0277, Test Loss: 3.0674\n",
      "Epoch 7495, Train Loss: 3.0158, Test Loss: 3.0570\n",
      "Epoch 7496, Train Loss: 3.0069, Test Loss: 3.0708\n",
      "Epoch 7497, Train Loss: 3.0102, Test Loss: 3.0729\n",
      "Epoch 7498, Train Loss: 3.0174, Test Loss: 3.0678\n",
      "Epoch 7499, Train Loss: 3.0199, Test Loss: 3.0595\n",
      "Epoch 7500, Train Loss: 3.0034, Test Loss: 3.0598\n",
      "Epoch 7501, Train Loss: 3.0162, Test Loss: 3.0639\n",
      "Epoch 7502, Train Loss: 3.0158, Test Loss: 3.0664\n",
      "Epoch 7503, Train Loss: 3.0108, Test Loss: 3.0732\n",
      "Epoch 7504, Train Loss: 3.0188, Test Loss: 3.0669\n",
      "Epoch 7505, Train Loss: 3.0006, Test Loss: 3.0692\n",
      "Epoch 7506, Train Loss: 3.0127, Test Loss: 3.0564\n",
      "Epoch 7507, Train Loss: 3.0098, Test Loss: 3.0534\n",
      "Epoch 7508, Train Loss: 3.0026, Test Loss: 3.0569\n",
      "Epoch 7509, Train Loss: 3.0019, Test Loss: 3.0588\n",
      "Epoch 7510, Train Loss: 3.0059, Test Loss: 3.0602\n",
      "Epoch 7511, Train Loss: 3.0035, Test Loss: 3.0615\n",
      "Epoch 7512, Train Loss: 3.0034, Test Loss: 3.0645\n",
      "Epoch 7513, Train Loss: 3.0093, Test Loss: 3.0600\n",
      "Epoch 7514, Train Loss: 2.9982, Test Loss: 3.0579\n",
      "Epoch 7515, Train Loss: 3.0104, Test Loss: 3.0527\n",
      "Epoch 7516, Train Loss: 3.0025, Test Loss: 3.0565\n",
      "Epoch 7517, Train Loss: 3.0122, Test Loss: 3.0609\n",
      "Epoch 7518, Train Loss: 3.0041, Test Loss: 3.0617\n",
      "Epoch 7519, Train Loss: 2.9969, Test Loss: 3.0656\n",
      "Epoch 7520, Train Loss: 3.0138, Test Loss: 3.0561\n",
      "Epoch 7521, Train Loss: 3.0011, Test Loss: 3.0567\n",
      "Epoch 7522, Train Loss: 3.0101, Test Loss: 3.0586\n",
      "Epoch 7523, Train Loss: 3.0093, Test Loss: 3.0531\n",
      "Epoch 7524, Train Loss: 3.0079, Test Loss: 3.0547\n",
      "Epoch 7525, Train Loss: 3.0001, Test Loss: 3.0597\n",
      "Epoch 7526, Train Loss: 2.9982, Test Loss: 3.0629\n",
      "Epoch 7527, Train Loss: 3.0084, Test Loss: 3.0591\n",
      "Epoch 7528, Train Loss: 3.0002, Test Loss: 3.0554\n",
      "Epoch 7529, Train Loss: 3.0072, Test Loss: 3.0500\n",
      "Epoch 7530, Train Loss: 3.0030, Test Loss: 3.0524\n",
      "Epoch 7531, Train Loss: 3.0086, Test Loss: 3.0513\n",
      "Epoch 7532, Train Loss: 3.0021, Test Loss: 3.0606\n",
      "Epoch 7533, Train Loss: 3.0027, Test Loss: 3.0657\n",
      "Epoch 7534, Train Loss: 3.0018, Test Loss: 3.0587\n",
      "Epoch 7535, Train Loss: 3.0012, Test Loss: 3.0521\n",
      "Epoch 7536, Train Loss: 3.0071, Test Loss: 3.0543\n",
      "Epoch 7537, Train Loss: 3.0011, Test Loss: 3.0562\n",
      "Epoch 7538, Train Loss: 3.0037, Test Loss: 3.0563\n",
      "Epoch 7539, Train Loss: 2.9980, Test Loss: 3.0616\n",
      "Epoch 7540, Train Loss: 3.0031, Test Loss: 3.0621\n",
      "Epoch 7541, Train Loss: 2.9977, Test Loss: 3.0584\n",
      "Epoch 7542, Train Loss: 3.0049, Test Loss: 3.0508\n",
      "Epoch 7543, Train Loss: 3.0059, Test Loss: 3.0521\n",
      "Epoch 7544, Train Loss: 3.0033, Test Loss: 3.0589\n",
      "Epoch 7545, Train Loss: 3.0015, Test Loss: 3.0698\n",
      "Epoch 7546, Train Loss: 3.0124, Test Loss: 3.0592\n",
      "Epoch 7547, Train Loss: 2.9991, Test Loss: 3.0649\n",
      "Epoch 7548, Train Loss: 3.0141, Test Loss: 3.0587\n",
      "Epoch 7549, Train Loss: 3.0073, Test Loss: 3.0553\n",
      "Epoch 7550, Train Loss: 3.0053, Test Loss: 3.0594\n",
      "Epoch 7551, Train Loss: 3.0102, Test Loss: 3.0621\n",
      "Epoch 7552, Train Loss: 3.0145, Test Loss: 3.0633\n",
      "Epoch 7553, Train Loss: 3.0101, Test Loss: 3.0658\n",
      "Epoch 7554, Train Loss: 3.0089, Test Loss: 3.0605\n",
      "Epoch 7555, Train Loss: 3.0078, Test Loss: 3.0606\n",
      "Epoch 7556, Train Loss: 3.0167, Test Loss: 3.0656\n",
      "Epoch 7557, Train Loss: 3.0148, Test Loss: 3.0645\n",
      "Epoch 7558, Train Loss: 3.0127, Test Loss: 3.0682\n",
      "Epoch 7559, Train Loss: 3.0150, Test Loss: 3.0638\n",
      "Epoch 7560, Train Loss: 3.0070, Test Loss: 3.0695\n",
      "Epoch 7561, Train Loss: 3.0228, Test Loss: 3.0604\n",
      "Epoch 7562, Train Loss: 3.0111, Test Loss: 3.0599\n",
      "Epoch 7563, Train Loss: 3.0097, Test Loss: 3.0685\n",
      "Epoch 7564, Train Loss: 3.0188, Test Loss: 3.0765\n",
      "Epoch 7565, Train Loss: 3.0234, Test Loss: 3.0814\n",
      "Epoch 7566, Train Loss: 3.0400, Test Loss: 3.0728\n",
      "Epoch 7567, Train Loss: 3.0166, Test Loss: 3.0814\n",
      "Epoch 7568, Train Loss: 3.0319, Test Loss: 3.0702\n",
      "Epoch 7569, Train Loss: 3.0269, Test Loss: 3.0617\n",
      "Epoch 7570, Train Loss: 3.0338, Test Loss: 3.0758\n",
      "Epoch 7571, Train Loss: 3.0206, Test Loss: 3.0794\n",
      "Epoch 7572, Train Loss: 3.0266, Test Loss: 3.0811\n",
      "Epoch 7573, Train Loss: 3.0243, Test Loss: 3.0688\n",
      "Epoch 7574, Train Loss: 3.0116, Test Loss: 3.0696\n",
      "Epoch 7575, Train Loss: 3.0196, Test Loss: 3.0653\n",
      "Epoch 7576, Train Loss: 3.0178, Test Loss: 3.0597\n",
      "Epoch 7577, Train Loss: 3.0098, Test Loss: 3.0637\n",
      "Epoch 7578, Train Loss: 3.0245, Test Loss: 3.0685\n",
      "Epoch 7579, Train Loss: 3.0167, Test Loss: 3.0697\n",
      "Epoch 7580, Train Loss: 3.0304, Test Loss: 3.0720\n",
      "Epoch 7581, Train Loss: 3.0158, Test Loss: 3.0888\n",
      "Epoch 7582, Train Loss: 3.0427, Test Loss: 3.0876\n",
      "Epoch 7583, Train Loss: 3.0389, Test Loss: 3.0640\n",
      "Epoch 7584, Train Loss: 3.0118, Test Loss: 3.0640\n",
      "Epoch 7585, Train Loss: 3.0185, Test Loss: 3.0631\n",
      "Epoch 7586, Train Loss: 3.0217, Test Loss: 3.0705\n",
      "Epoch 7587, Train Loss: 3.0210, Test Loss: 3.0613\n",
      "Epoch 7588, Train Loss: 3.0161, Test Loss: 3.0607\n",
      "Epoch 7589, Train Loss: 3.0178, Test Loss: 3.0616\n",
      "Epoch 7590, Train Loss: 3.0254, Test Loss: 3.0786\n",
      "Epoch 7591, Train Loss: 3.0201, Test Loss: 3.0726\n",
      "Epoch 7592, Train Loss: 3.0112, Test Loss: 3.0763\n",
      "Epoch 7593, Train Loss: 3.0274, Test Loss: 3.0734\n",
      "Epoch 7594, Train Loss: 3.0130, Test Loss: 3.0609\n",
      "Epoch 7595, Train Loss: 3.0033, Test Loss: 3.0612\n",
      "Epoch 7596, Train Loss: 3.0166, Test Loss: 3.0557\n",
      "Epoch 7597, Train Loss: 3.0130, Test Loss: 3.0647\n",
      "Epoch 7598, Train Loss: 3.0181, Test Loss: 3.0670\n",
      "Epoch 7599, Train Loss: 3.0068, Test Loss: 3.0682\n",
      "Epoch 7600, Train Loss: 3.0049, Test Loss: 3.0712\n",
      "Epoch 7601, Train Loss: 3.0074, Test Loss: 3.0557\n",
      "Epoch 7602, Train Loss: 2.9978, Test Loss: 3.0540\n",
      "Epoch 7603, Train Loss: 3.0041, Test Loss: 3.0549\n",
      "Epoch 7604, Train Loss: 3.0102, Test Loss: 3.0529\n",
      "Epoch 7605, Train Loss: 2.9999, Test Loss: 3.0594\n",
      "Epoch 7606, Train Loss: 3.0017, Test Loss: 3.0631\n",
      "Epoch 7607, Train Loss: 3.0070, Test Loss: 3.0590\n",
      "Epoch 7608, Train Loss: 3.0016, Test Loss: 3.0586\n",
      "Epoch 7609, Train Loss: 3.0007, Test Loss: 3.0577\n",
      "Epoch 7610, Train Loss: 3.0027, Test Loss: 3.0578\n",
      "Epoch 7611, Train Loss: 3.0061, Test Loss: 3.0586\n",
      "Epoch 7612, Train Loss: 3.0047, Test Loss: 3.0586\n",
      "Epoch 7613, Train Loss: 3.0057, Test Loss: 3.0553\n",
      "Epoch 7614, Train Loss: 2.9986, Test Loss: 3.0569\n",
      "Epoch 7615, Train Loss: 3.0006, Test Loss: 3.0527\n",
      "Epoch 7616, Train Loss: 3.0097, Test Loss: 3.0564\n",
      "Epoch 7617, Train Loss: 3.0063, Test Loss: 3.0591\n",
      "Epoch 7618, Train Loss: 3.0013, Test Loss: 3.0567\n",
      "Epoch 7619, Train Loss: 3.0021, Test Loss: 3.0561\n",
      "Epoch 7620, Train Loss: 3.0008, Test Loss: 3.0557\n",
      "Epoch 7621, Train Loss: 3.0030, Test Loss: 3.0559\n",
      "Epoch 7622, Train Loss: 3.0000, Test Loss: 3.0499\n",
      "Epoch 7623, Train Loss: 2.9978, Test Loss: 3.0547\n",
      "Epoch 7624, Train Loss: 3.0038, Test Loss: 3.0565\n",
      "Epoch 7625, Train Loss: 3.0040, Test Loss: 3.0554\n",
      "Epoch 7626, Train Loss: 2.9986, Test Loss: 3.0542\n",
      "Epoch 7627, Train Loss: 3.0033, Test Loss: 3.0549\n",
      "Epoch 7628, Train Loss: 2.9975, Test Loss: 3.0557\n",
      "Epoch 7629, Train Loss: 2.9993, Test Loss: 3.0523\n",
      "Epoch 7630, Train Loss: 3.0009, Test Loss: 3.0508\n",
      "Epoch 7631, Train Loss: 3.0026, Test Loss: 3.0523\n",
      "Epoch 7632, Train Loss: 3.0027, Test Loss: 3.0532\n",
      "Epoch 7633, Train Loss: 3.0011, Test Loss: 3.0554\n",
      "Epoch 7634, Train Loss: 3.0009, Test Loss: 3.0554\n",
      "Epoch 7635, Train Loss: 3.0010, Test Loss: 3.0546\n",
      "Epoch 7636, Train Loss: 3.0011, Test Loss: 3.0574\n",
      "Epoch 7637, Train Loss: 3.0027, Test Loss: 3.0590\n",
      "Epoch 7638, Train Loss: 3.0060, Test Loss: 3.0620\n",
      "Epoch 7639, Train Loss: 3.0073, Test Loss: 3.0580\n",
      "Epoch 7640, Train Loss: 2.9983, Test Loss: 3.0571\n",
      "Epoch 7641, Train Loss: 3.0041, Test Loss: 3.0555\n",
      "Epoch 7642, Train Loss: 3.0061, Test Loss: 3.0552\n",
      "Epoch 7643, Train Loss: 3.0069, Test Loss: 3.0503\n",
      "Epoch 7644, Train Loss: 2.9959, Test Loss: 3.0549\n",
      "Epoch 7645, Train Loss: 3.0047, Test Loss: 3.0650\n",
      "Epoch 7646, Train Loss: 3.0128, Test Loss: 3.0639\n",
      "Epoch 7647, Train Loss: 3.0026, Test Loss: 3.0619\n",
      "Epoch 7648, Train Loss: 3.0037, Test Loss: 3.0597\n",
      "Epoch 7649, Train Loss: 3.0084, Test Loss: 3.0525\n",
      "Epoch 7650, Train Loss: 3.0000, Test Loss: 3.0583\n",
      "Epoch 7651, Train Loss: 3.0081, Test Loss: 3.0571\n",
      "Epoch 7652, Train Loss: 3.0077, Test Loss: 3.0552\n",
      "Epoch 7653, Train Loss: 3.0008, Test Loss: 3.0571\n",
      "Epoch 7654, Train Loss: 3.0068, Test Loss: 3.0571\n",
      "Epoch 7655, Train Loss: 3.0017, Test Loss: 3.0633\n",
      "Epoch 7656, Train Loss: 3.0063, Test Loss: 3.0592\n",
      "Epoch 7657, Train Loss: 3.0089, Test Loss: 3.0635\n",
      "Epoch 7658, Train Loss: 3.0063, Test Loss: 3.0614\n",
      "Epoch 7659, Train Loss: 3.0114, Test Loss: 3.0653\n",
      "Epoch 7660, Train Loss: 3.0084, Test Loss: 3.0731\n",
      "Epoch 7661, Train Loss: 3.0156, Test Loss: 3.0581\n",
      "Epoch 7662, Train Loss: 3.0082, Test Loss: 3.0617\n",
      "Epoch 7663, Train Loss: 3.0122, Test Loss: 3.0603\n",
      "Epoch 7664, Train Loss: 3.0064, Test Loss: 3.0614\n",
      "Epoch 7665, Train Loss: 3.0043, Test Loss: 3.0556\n",
      "Epoch 7666, Train Loss: 3.0028, Test Loss: 3.0602\n",
      "Epoch 7667, Train Loss: 3.0146, Test Loss: 3.0524\n",
      "Epoch 7668, Train Loss: 2.9960, Test Loss: 3.0700\n",
      "Epoch 7669, Train Loss: 3.0108, Test Loss: 3.0615\n",
      "Epoch 7670, Train Loss: 3.0021, Test Loss: 3.0566\n",
      "Epoch 7671, Train Loss: 3.0042, Test Loss: 3.0520\n",
      "Epoch 7672, Train Loss: 2.9951, Test Loss: 3.0596\n",
      "Epoch 7673, Train Loss: 3.0070, Test Loss: 3.0591\n",
      "Epoch 7674, Train Loss: 3.0045, Test Loss: 3.0606\n",
      "Epoch 7675, Train Loss: 3.0099, Test Loss: 3.0514\n",
      "Epoch 7676, Train Loss: 3.0055, Test Loss: 3.0665\n",
      "Epoch 7677, Train Loss: 3.0165, Test Loss: 3.0545\n",
      "Epoch 7678, Train Loss: 3.0004, Test Loss: 3.0628\n",
      "Epoch 7679, Train Loss: 3.0044, Test Loss: 3.0648\n",
      "Epoch 7680, Train Loss: 2.9987, Test Loss: 3.0651\n",
      "Epoch 7681, Train Loss: 3.0017, Test Loss: 3.0588\n",
      "Epoch 7682, Train Loss: 3.0087, Test Loss: 3.0555\n",
      "Epoch 7683, Train Loss: 3.0023, Test Loss: 3.0597\n",
      "Epoch 7684, Train Loss: 3.0070, Test Loss: 3.0575\n",
      "Epoch 7685, Train Loss: 2.9996, Test Loss: 3.0591\n",
      "Epoch 7686, Train Loss: 3.0019, Test Loss: 3.0589\n",
      "Epoch 7687, Train Loss: 3.0089, Test Loss: 3.0551\n",
      "Epoch 7688, Train Loss: 2.9998, Test Loss: 3.0570\n",
      "Epoch 7689, Train Loss: 3.0006, Test Loss: 3.0497\n",
      "Epoch 7690, Train Loss: 3.0030, Test Loss: 3.0523\n",
      "Epoch 7691, Train Loss: 3.0040, Test Loss: 3.0515\n",
      "Epoch 7692, Train Loss: 2.9967, Test Loss: 3.0609\n",
      "Epoch 7693, Train Loss: 3.0061, Test Loss: 3.0594\n",
      "Epoch 7694, Train Loss: 2.9987, Test Loss: 3.0591\n",
      "Epoch 7695, Train Loss: 3.0121, Test Loss: 3.0554\n",
      "Epoch 7696, Train Loss: 3.0058, Test Loss: 3.0623\n",
      "Epoch 7697, Train Loss: 3.0082, Test Loss: 3.0582\n",
      "Epoch 7698, Train Loss: 3.0073, Test Loss: 3.0562\n",
      "Epoch 7699, Train Loss: 3.0001, Test Loss: 3.0573\n",
      "Epoch 7700, Train Loss: 3.0052, Test Loss: 3.0612\n",
      "Epoch 7701, Train Loss: 3.0044, Test Loss: 3.0568\n",
      "Epoch 7702, Train Loss: 3.0058, Test Loss: 3.0600\n",
      "Epoch 7703, Train Loss: 3.0092, Test Loss: 3.0604\n",
      "Epoch 7704, Train Loss: 3.0122, Test Loss: 3.0595\n",
      "Epoch 7705, Train Loss: 3.0093, Test Loss: 3.0624\n",
      "Epoch 7706, Train Loss: 3.0090, Test Loss: 3.0604\n",
      "Epoch 7707, Train Loss: 3.0093, Test Loss: 3.0526\n",
      "Epoch 7708, Train Loss: 3.0020, Test Loss: 3.0599\n",
      "Epoch 7709, Train Loss: 3.0164, Test Loss: 3.0543\n",
      "Epoch 7710, Train Loss: 3.0013, Test Loss: 3.0618\n",
      "Epoch 7711, Train Loss: 3.0121, Test Loss: 3.0634\n",
      "Epoch 7712, Train Loss: 3.0090, Test Loss: 3.0648\n",
      "Epoch 7713, Train Loss: 3.0105, Test Loss: 3.0562\n",
      "Epoch 7714, Train Loss: 3.0060, Test Loss: 3.0584\n",
      "Epoch 7715, Train Loss: 3.0091, Test Loss: 3.0541\n",
      "Epoch 7716, Train Loss: 3.0000, Test Loss: 3.0596\n",
      "Epoch 7717, Train Loss: 3.0125, Test Loss: 3.0552\n",
      "Epoch 7718, Train Loss: 3.0112, Test Loss: 3.0575\n",
      "Epoch 7719, Train Loss: 3.0062, Test Loss: 3.0579\n",
      "Epoch 7720, Train Loss: 3.0040, Test Loss: 3.0587\n",
      "Epoch 7721, Train Loss: 3.0071, Test Loss: 3.0513\n",
      "Epoch 7722, Train Loss: 3.0000, Test Loss: 3.0561\n",
      "Epoch 7723, Train Loss: 3.0049, Test Loss: 3.0528\n",
      "Epoch 7724, Train Loss: 3.0020, Test Loss: 3.0557\n",
      "Epoch 7725, Train Loss: 3.0051, Test Loss: 3.0560\n",
      "Epoch 7726, Train Loss: 3.0013, Test Loss: 3.0566\n",
      "Epoch 7727, Train Loss: 3.0024, Test Loss: 3.0540\n",
      "Epoch 7728, Train Loss: 2.9991, Test Loss: 3.0558\n",
      "Epoch 7729, Train Loss: 3.0057, Test Loss: 3.0548\n",
      "Epoch 7730, Train Loss: 3.0002, Test Loss: 3.0547\n",
      "Epoch 7731, Train Loss: 3.0010, Test Loss: 3.0516\n",
      "Epoch 7732, Train Loss: 3.0067, Test Loss: 3.0526\n",
      "Epoch 7733, Train Loss: 2.9994, Test Loss: 3.0621\n",
      "Epoch 7734, Train Loss: 3.0036, Test Loss: 3.0635\n",
      "Epoch 7735, Train Loss: 3.0004, Test Loss: 3.0626\n",
      "Epoch 7736, Train Loss: 3.0018, Test Loss: 3.0581\n",
      "Epoch 7737, Train Loss: 3.0021, Test Loss: 3.0529\n",
      "Epoch 7738, Train Loss: 2.9988, Test Loss: 3.0546\n",
      "Epoch 7739, Train Loss: 2.9976, Test Loss: 3.0551\n",
      "Epoch 7740, Train Loss: 3.0009, Test Loss: 3.0624\n",
      "Epoch 7741, Train Loss: 3.0092, Test Loss: 3.0637\n",
      "Epoch 7742, Train Loss: 3.0132, Test Loss: 3.0598\n",
      "Epoch 7743, Train Loss: 3.0098, Test Loss: 3.0619\n",
      "Epoch 7744, Train Loss: 3.0050, Test Loss: 3.0634\n",
      "Epoch 7745, Train Loss: 3.0064, Test Loss: 3.0586\n",
      "Epoch 7746, Train Loss: 3.0062, Test Loss: 3.0619\n",
      "Epoch 7747, Train Loss: 3.0084, Test Loss: 3.0539\n",
      "Epoch 7748, Train Loss: 3.0135, Test Loss: 3.0597\n",
      "Epoch 7749, Train Loss: 3.0057, Test Loss: 3.0672\n",
      "Epoch 7750, Train Loss: 3.0070, Test Loss: 3.0615\n",
      "Epoch 7751, Train Loss: 3.0048, Test Loss: 3.0573\n",
      "Epoch 7752, Train Loss: 3.0100, Test Loss: 3.0604\n",
      "Epoch 7753, Train Loss: 3.0098, Test Loss: 3.0611\n",
      "Epoch 7754, Train Loss: 3.0114, Test Loss: 3.0517\n",
      "Epoch 7755, Train Loss: 3.0018, Test Loss: 3.0554\n",
      "Epoch 7756, Train Loss: 3.0043, Test Loss: 3.0557\n",
      "Epoch 7757, Train Loss: 2.9991, Test Loss: 3.0536\n",
      "Epoch 7758, Train Loss: 2.9966, Test Loss: 3.0553\n",
      "Epoch 7759, Train Loss: 3.0013, Test Loss: 3.0587\n",
      "Epoch 7760, Train Loss: 3.0083, Test Loss: 3.0621\n",
      "Epoch 7761, Train Loss: 3.0053, Test Loss: 3.0726\n",
      "Epoch 7762, Train Loss: 3.0115, Test Loss: 3.0562\n",
      "Epoch 7763, Train Loss: 3.0045, Test Loss: 3.0537\n",
      "Epoch 7764, Train Loss: 3.0018, Test Loss: 3.0538\n",
      "Epoch 7765, Train Loss: 3.0037, Test Loss: 3.0553\n",
      "Epoch 7766, Train Loss: 3.0039, Test Loss: 3.0560\n",
      "Epoch 7767, Train Loss: 3.0074, Test Loss: 3.0555\n",
      "Epoch 7768, Train Loss: 3.0020, Test Loss: 3.0523\n",
      "Epoch 7769, Train Loss: 3.0001, Test Loss: 3.0647\n",
      "Epoch 7770, Train Loss: 3.0100, Test Loss: 3.0664\n",
      "Epoch 7771, Train Loss: 3.0069, Test Loss: 3.0613\n",
      "Epoch 7772, Train Loss: 3.0005, Test Loss: 3.0571\n",
      "Epoch 7773, Train Loss: 3.0025, Test Loss: 3.0585\n",
      "Epoch 7774, Train Loss: 3.0063, Test Loss: 3.0581\n",
      "Epoch 7775, Train Loss: 3.0116, Test Loss: 3.0576\n",
      "Epoch 7776, Train Loss: 3.0095, Test Loss: 3.0575\n",
      "Epoch 7777, Train Loss: 3.0065, Test Loss: 3.0575\n",
      "Epoch 7778, Train Loss: 3.0011, Test Loss: 3.0724\n",
      "Epoch 7779, Train Loss: 3.0162, Test Loss: 3.0660\n",
      "Epoch 7780, Train Loss: 3.0043, Test Loss: 3.0548\n",
      "Epoch 7781, Train Loss: 3.0044, Test Loss: 3.0557\n",
      "Epoch 7782, Train Loss: 3.0084, Test Loss: 3.0589\n",
      "Epoch 7783, Train Loss: 3.0104, Test Loss: 3.0583\n",
      "Epoch 7784, Train Loss: 3.0062, Test Loss: 3.0669\n",
      "Epoch 7785, Train Loss: 3.0022, Test Loss: 3.0697\n",
      "Epoch 7786, Train Loss: 3.0115, Test Loss: 3.0567\n",
      "Epoch 7787, Train Loss: 3.0024, Test Loss: 3.0565\n",
      "Epoch 7788, Train Loss: 3.0083, Test Loss: 3.0519\n",
      "Epoch 7789, Train Loss: 3.0066, Test Loss: 3.0538\n",
      "Epoch 7790, Train Loss: 3.0066, Test Loss: 3.0560\n",
      "Epoch 7791, Train Loss: 3.0001, Test Loss: 3.0548\n",
      "Epoch 7792, Train Loss: 2.9990, Test Loss: 3.0566\n",
      "Epoch 7793, Train Loss: 3.0003, Test Loss: 3.0583\n",
      "Epoch 7794, Train Loss: 3.0056, Test Loss: 3.0582\n",
      "Epoch 7795, Train Loss: 3.0028, Test Loss: 3.0514\n",
      "Epoch 7796, Train Loss: 2.9988, Test Loss: 3.0510\n",
      "Epoch 7797, Train Loss: 3.0012, Test Loss: 3.0488\n",
      "Epoch 7798, Train Loss: 2.9989, Test Loss: 3.0495\n",
      "Epoch 7799, Train Loss: 2.9982, Test Loss: 3.0496\n",
      "Epoch 7800, Train Loss: 2.9979, Test Loss: 3.0530\n",
      "Epoch 7801, Train Loss: 3.0030, Test Loss: 3.0535\n",
      "Epoch 7802, Train Loss: 2.9975, Test Loss: 3.0547\n",
      "Epoch 7803, Train Loss: 3.0063, Test Loss: 3.0545\n",
      "Epoch 7804, Train Loss: 2.9982, Test Loss: 3.0556\n",
      "Epoch 7805, Train Loss: 3.0039, Test Loss: 3.0525\n",
      "Epoch 7806, Train Loss: 3.0038, Test Loss: 3.0511\n",
      "Epoch 7807, Train Loss: 3.0045, Test Loss: 3.0555\n",
      "Epoch 7808, Train Loss: 2.9985, Test Loss: 3.0556\n",
      "Epoch 7809, Train Loss: 3.0006, Test Loss: 3.0548\n",
      "Epoch 7810, Train Loss: 3.0032, Test Loss: 3.0550\n",
      "Epoch 7811, Train Loss: 3.0022, Test Loss: 3.0578\n",
      "Epoch 7812, Train Loss: 3.0031, Test Loss: 3.0528\n",
      "Epoch 7813, Train Loss: 3.0036, Test Loss: 3.0483\n",
      "Epoch 7814, Train Loss: 2.9988, Test Loss: 3.0524\n",
      "Epoch 7815, Train Loss: 3.0051, Test Loss: 3.0537\n",
      "Epoch 7816, Train Loss: 2.9979, Test Loss: 3.0567\n",
      "Epoch 7817, Train Loss: 3.0034, Test Loss: 3.0606\n",
      "Epoch 7818, Train Loss: 3.0036, Test Loss: 3.0648\n",
      "Epoch 7819, Train Loss: 3.0045, Test Loss: 3.0531\n",
      "Epoch 7820, Train Loss: 2.9968, Test Loss: 3.0526\n",
      "Epoch 7821, Train Loss: 3.0028, Test Loss: 3.0533\n",
      "Epoch 7822, Train Loss: 3.0046, Test Loss: 3.0572\n",
      "Epoch 7823, Train Loss: 3.0161, Test Loss: 3.0547\n",
      "Epoch 7824, Train Loss: 3.0037, Test Loss: 3.0510\n",
      "Epoch 7825, Train Loss: 3.0038, Test Loss: 3.0504\n",
      "Epoch 7826, Train Loss: 3.0005, Test Loss: 3.0558\n",
      "Epoch 7827, Train Loss: 3.0089, Test Loss: 3.0531\n",
      "Epoch 7828, Train Loss: 3.0026, Test Loss: 3.0579\n",
      "Epoch 7829, Train Loss: 3.0071, Test Loss: 3.0614\n",
      "Epoch 7830, Train Loss: 2.9969, Test Loss: 3.0616\n",
      "Epoch 7831, Train Loss: 3.0069, Test Loss: 3.0570\n",
      "Epoch 7832, Train Loss: 3.0051, Test Loss: 3.0561\n",
      "Epoch 7833, Train Loss: 3.0072, Test Loss: 3.0495\n",
      "Epoch 7834, Train Loss: 2.9998, Test Loss: 3.0550\n",
      "Epoch 7835, Train Loss: 3.0062, Test Loss: 3.0506\n",
      "Epoch 7836, Train Loss: 2.9989, Test Loss: 3.0602\n",
      "Epoch 7837, Train Loss: 3.0093, Test Loss: 3.0613\n",
      "Epoch 7838, Train Loss: 3.0065, Test Loss: 3.0685\n",
      "Epoch 7839, Train Loss: 3.0115, Test Loss: 3.0608\n",
      "Epoch 7840, Train Loss: 3.0080, Test Loss: 3.0505\n",
      "Epoch 7841, Train Loss: 3.0047, Test Loss: 3.0551\n",
      "Epoch 7842, Train Loss: 3.0020, Test Loss: 3.0627\n",
      "Epoch 7843, Train Loss: 3.0075, Test Loss: 3.0571\n",
      "Epoch 7844, Train Loss: 3.0106, Test Loss: 3.0602\n",
      "Epoch 7845, Train Loss: 2.9994, Test Loss: 3.0641\n",
      "Epoch 7846, Train Loss: 3.0033, Test Loss: 3.0579\n",
      "Epoch 7847, Train Loss: 3.0029, Test Loss: 3.0535\n",
      "Epoch 7848, Train Loss: 2.9980, Test Loss: 3.0616\n",
      "Epoch 7849, Train Loss: 3.0092, Test Loss: 3.0584\n",
      "Epoch 7850, Train Loss: 2.9985, Test Loss: 3.0605\n",
      "Epoch 7851, Train Loss: 3.0082, Test Loss: 3.0534\n",
      "Epoch 7852, Train Loss: 3.0058, Test Loss: 3.0517\n",
      "Epoch 7853, Train Loss: 2.9985, Test Loss: 3.0508\n",
      "Epoch 7854, Train Loss: 3.0036, Test Loss: 3.0531\n",
      "Epoch 7855, Train Loss: 2.9986, Test Loss: 3.0561\n",
      "Epoch 7856, Train Loss: 2.9987, Test Loss: 3.0605\n",
      "Epoch 7857, Train Loss: 3.0055, Test Loss: 3.0590\n",
      "Epoch 7858, Train Loss: 3.0144, Test Loss: 3.0569\n",
      "Epoch 7859, Train Loss: 3.0048, Test Loss: 3.0567\n",
      "Epoch 7860, Train Loss: 3.0102, Test Loss: 3.0542\n",
      "Epoch 7861, Train Loss: 3.0081, Test Loss: 3.0586\n",
      "Epoch 7862, Train Loss: 3.0006, Test Loss: 3.0613\n",
      "Epoch 7863, Train Loss: 3.0093, Test Loss: 3.0567\n",
      "Epoch 7864, Train Loss: 2.9995, Test Loss: 3.0632\n",
      "Epoch 7865, Train Loss: 3.0169, Test Loss: 3.0566\n",
      "Epoch 7866, Train Loss: 3.0001, Test Loss: 3.0551\n",
      "Epoch 7867, Train Loss: 3.0062, Test Loss: 3.0548\n",
      "Epoch 7868, Train Loss: 2.9990, Test Loss: 3.0534\n",
      "Epoch 7869, Train Loss: 3.0084, Test Loss: 3.0511\n",
      "Epoch 7870, Train Loss: 2.9973, Test Loss: 3.0500\n",
      "Epoch 7871, Train Loss: 3.0047, Test Loss: 3.0493\n",
      "Epoch 7872, Train Loss: 2.9962, Test Loss: 3.0549\n",
      "Epoch 7873, Train Loss: 3.0011, Test Loss: 3.0549\n",
      "Epoch 7874, Train Loss: 3.0009, Test Loss: 3.0537\n",
      "Epoch 7875, Train Loss: 3.0034, Test Loss: 3.0518\n",
      "Epoch 7876, Train Loss: 3.0032, Test Loss: 3.0536\n",
      "Epoch 7877, Train Loss: 3.0032, Test Loss: 3.0541\n",
      "Epoch 7878, Train Loss: 3.0036, Test Loss: 3.0559\n",
      "Epoch 7879, Train Loss: 3.0000, Test Loss: 3.0657\n",
      "Epoch 7880, Train Loss: 3.0060, Test Loss: 3.0625\n",
      "Epoch 7881, Train Loss: 3.0047, Test Loss: 3.0563\n",
      "Epoch 7882, Train Loss: 3.0011, Test Loss: 3.0555\n",
      "Epoch 7883, Train Loss: 3.0021, Test Loss: 3.0522\n",
      "Epoch 7884, Train Loss: 3.0029, Test Loss: 3.0537\n",
      "Epoch 7885, Train Loss: 2.9996, Test Loss: 3.0555\n",
      "Epoch 7886, Train Loss: 2.9982, Test Loss: 3.0560\n",
      "Epoch 7887, Train Loss: 2.9992, Test Loss: 3.0521\n",
      "Epoch 7888, Train Loss: 3.0011, Test Loss: 3.0492\n",
      "Epoch 7889, Train Loss: 3.0046, Test Loss: 3.0590\n",
      "Epoch 7890, Train Loss: 3.0104, Test Loss: 3.0582\n",
      "Epoch 7891, Train Loss: 3.0037, Test Loss: 3.0678\n",
      "Epoch 7892, Train Loss: 3.0122, Test Loss: 3.0559\n",
      "Epoch 7893, Train Loss: 3.0004, Test Loss: 3.0604\n",
      "Epoch 7894, Train Loss: 3.0108, Test Loss: 3.0535\n",
      "Epoch 7895, Train Loss: 3.0017, Test Loss: 3.0730\n",
      "Epoch 7896, Train Loss: 3.0135, Test Loss: 3.0706\n",
      "Epoch 7897, Train Loss: 3.0096, Test Loss: 3.0768\n",
      "Epoch 7898, Train Loss: 3.0155, Test Loss: 3.0736\n",
      "Epoch 7899, Train Loss: 3.0219, Test Loss: 3.0784\n",
      "Epoch 7900, Train Loss: 3.0171, Test Loss: 3.0759\n",
      "Epoch 7901, Train Loss: 3.0193, Test Loss: 3.0707\n",
      "Epoch 7902, Train Loss: 3.0205, Test Loss: 3.0631\n",
      "Epoch 7903, Train Loss: 3.0105, Test Loss: 3.0724\n",
      "Epoch 7904, Train Loss: 3.0273, Test Loss: 3.0694\n",
      "Epoch 7905, Train Loss: 3.0080, Test Loss: 3.0691\n",
      "Epoch 7906, Train Loss: 3.0134, Test Loss: 3.0657\n",
      "Epoch 7907, Train Loss: 3.0251, Test Loss: 3.0574\n",
      "Epoch 7908, Train Loss: 3.0068, Test Loss: 3.0662\n",
      "Epoch 7909, Train Loss: 3.0086, Test Loss: 3.0622\n",
      "Epoch 7910, Train Loss: 3.0108, Test Loss: 3.0607\n",
      "Epoch 7911, Train Loss: 3.0065, Test Loss: 3.0548\n",
      "Epoch 7912, Train Loss: 3.0012, Test Loss: 3.0643\n",
      "Epoch 7913, Train Loss: 3.0157, Test Loss: 3.0555\n",
      "Epoch 7914, Train Loss: 3.0021, Test Loss: 3.0621\n",
      "Epoch 7915, Train Loss: 3.0110, Test Loss: 3.0622\n",
      "Epoch 7916, Train Loss: 3.0036, Test Loss: 3.0582\n",
      "Epoch 7917, Train Loss: 3.0048, Test Loss: 3.0528\n",
      "Epoch 7918, Train Loss: 3.0032, Test Loss: 3.0559\n",
      "Epoch 7919, Train Loss: 3.0094, Test Loss: 3.0533\n",
      "Epoch 7920, Train Loss: 3.0043, Test Loss: 3.0622\n",
      "Epoch 7921, Train Loss: 3.0015, Test Loss: 3.0659\n",
      "Epoch 7922, Train Loss: 3.0022, Test Loss: 3.0536\n",
      "Epoch 7923, Train Loss: 2.9994, Test Loss: 3.0541\n",
      "Epoch 7924, Train Loss: 3.0042, Test Loss: 3.0544\n",
      "Epoch 7925, Train Loss: 3.0055, Test Loss: 3.0552\n",
      "Epoch 7926, Train Loss: 3.0042, Test Loss: 3.0610\n",
      "Epoch 7927, Train Loss: 3.0059, Test Loss: 3.0567\n",
      "Epoch 7928, Train Loss: 3.0023, Test Loss: 3.0647\n",
      "Epoch 7929, Train Loss: 3.0091, Test Loss: 3.0569\n",
      "Epoch 7930, Train Loss: 2.9999, Test Loss: 3.0589\n",
      "Epoch 7931, Train Loss: 3.0055, Test Loss: 3.0549\n",
      "Epoch 7932, Train Loss: 2.9995, Test Loss: 3.0602\n",
      "Epoch 7933, Train Loss: 3.0091, Test Loss: 3.0527\n",
      "Epoch 7934, Train Loss: 2.9959, Test Loss: 3.0574\n",
      "Epoch 7935, Train Loss: 3.0063, Test Loss: 3.0508\n",
      "Epoch 7936, Train Loss: 3.0041, Test Loss: 3.0554\n",
      "Epoch 7937, Train Loss: 3.0030, Test Loss: 3.0616\n",
      "Epoch 7938, Train Loss: 3.0060, Test Loss: 3.0628\n",
      "Epoch 7939, Train Loss: 3.0098, Test Loss: 3.0617\n",
      "Epoch 7940, Train Loss: 2.9995, Test Loss: 3.0745\n",
      "Epoch 7941, Train Loss: 3.0236, Test Loss: 3.0685\n",
      "Epoch 7942, Train Loss: 3.0215, Test Loss: 3.0566\n",
      "Epoch 7943, Train Loss: 3.0119, Test Loss: 3.0649\n",
      "Epoch 7944, Train Loss: 3.0228, Test Loss: 3.0578\n",
      "Epoch 7945, Train Loss: 3.0045, Test Loss: 3.0682\n",
      "Epoch 7946, Train Loss: 3.0053, Test Loss: 3.0724\n",
      "Epoch 7947, Train Loss: 3.0162, Test Loss: 3.0628\n",
      "Epoch 7948, Train Loss: 3.0103, Test Loss: 3.0546\n",
      "Epoch 7949, Train Loss: 3.0018, Test Loss: 3.0568\n",
      "Epoch 7950, Train Loss: 3.0117, Test Loss: 3.0587\n",
      "Epoch 7951, Train Loss: 3.0118, Test Loss: 3.0600\n",
      "Epoch 7952, Train Loss: 3.0037, Test Loss: 3.0554\n",
      "Epoch 7953, Train Loss: 3.0104, Test Loss: 3.0528\n",
      "Epoch 7954, Train Loss: 2.9997, Test Loss: 3.0550\n",
      "Epoch 7955, Train Loss: 3.0072, Test Loss: 3.0578\n",
      "Epoch 7956, Train Loss: 3.0044, Test Loss: 3.0629\n",
      "Epoch 7957, Train Loss: 3.0133, Test Loss: 3.0636\n",
      "Epoch 7958, Train Loss: 3.0071, Test Loss: 3.0694\n",
      "Epoch 7959, Train Loss: 3.0175, Test Loss: 3.0611\n",
      "Epoch 7960, Train Loss: 3.0095, Test Loss: 3.0634\n",
      "Epoch 7961, Train Loss: 3.0087, Test Loss: 3.0637\n",
      "Epoch 7962, Train Loss: 3.0052, Test Loss: 3.0589\n",
      "Epoch 7963, Train Loss: 3.0169, Test Loss: 3.0539\n",
      "Epoch 7964, Train Loss: 3.0105, Test Loss: 3.0503\n",
      "Epoch 7965, Train Loss: 3.0008, Test Loss: 3.0593\n",
      "Epoch 7966, Train Loss: 3.0086, Test Loss: 3.0591\n",
      "Epoch 7967, Train Loss: 3.0081, Test Loss: 3.0679\n",
      "Epoch 7968, Train Loss: 3.0155, Test Loss: 3.0721\n",
      "Epoch 7969, Train Loss: 3.0163, Test Loss: 3.0652\n",
      "Epoch 7970, Train Loss: 3.0063, Test Loss: 3.0528\n",
      "Epoch 7971, Train Loss: 3.0045, Test Loss: 3.0615\n",
      "Epoch 7972, Train Loss: 3.0184, Test Loss: 3.0493\n",
      "Epoch 7973, Train Loss: 3.0063, Test Loss: 3.0573\n",
      "Epoch 7974, Train Loss: 3.0003, Test Loss: 3.0642\n",
      "Epoch 7975, Train Loss: 3.0081, Test Loss: 3.0562\n",
      "Epoch 7976, Train Loss: 3.0022, Test Loss: 3.0524\n",
      "Epoch 7977, Train Loss: 3.0011, Test Loss: 3.0614\n",
      "Epoch 7978, Train Loss: 3.0127, Test Loss: 3.0584\n",
      "Epoch 7979, Train Loss: 2.9960, Test Loss: 3.0575\n",
      "Epoch 7980, Train Loss: 3.0015, Test Loss: 3.0551\n",
      "Epoch 7981, Train Loss: 3.0069, Test Loss: 3.0496\n",
      "Epoch 7982, Train Loss: 2.9970, Test Loss: 3.0526\n",
      "Epoch 7983, Train Loss: 3.0077, Test Loss: 3.0542\n",
      "Epoch 7984, Train Loss: 2.9965, Test Loss: 3.0550\n",
      "Epoch 7985, Train Loss: 3.0010, Test Loss: 3.0598\n",
      "Epoch 7986, Train Loss: 3.0066, Test Loss: 3.0556\n",
      "Epoch 7987, Train Loss: 2.9979, Test Loss: 3.0576\n",
      "Epoch 7988, Train Loss: 3.0125, Test Loss: 3.0513\n",
      "Epoch 7989, Train Loss: 3.0057, Test Loss: 3.0571\n",
      "Epoch 7990, Train Loss: 3.0019, Test Loss: 3.0670\n",
      "Epoch 7991, Train Loss: 3.0133, Test Loss: 3.0668\n",
      "Epoch 7992, Train Loss: 3.0110, Test Loss: 3.0609\n",
      "Epoch 7993, Train Loss: 3.0024, Test Loss: 3.0603\n",
      "Epoch 7994, Train Loss: 3.0098, Test Loss: 3.0520\n",
      "Epoch 7995, Train Loss: 3.0023, Test Loss: 3.0582\n",
      "Epoch 7996, Train Loss: 3.0067, Test Loss: 3.0610\n",
      "Epoch 7997, Train Loss: 3.0010, Test Loss: 3.0630\n",
      "Epoch 7998, Train Loss: 3.0091, Test Loss: 3.0565\n",
      "Epoch 7999, Train Loss: 3.0044, Test Loss: 3.0579\n",
      "Epoch 8000, Train Loss: 3.0031, Test Loss: 3.0504\n",
      "Epoch 8001, Train Loss: 2.9986, Test Loss: 3.0552\n",
      "Epoch 8002, Train Loss: 3.0117, Test Loss: 3.0632\n",
      "Epoch 8003, Train Loss: 3.0096, Test Loss: 3.0588\n",
      "Epoch 8004, Train Loss: 3.0031, Test Loss: 3.0606\n",
      "Epoch 8005, Train Loss: 3.0079, Test Loss: 3.0559\n",
      "Epoch 8006, Train Loss: 3.0036, Test Loss: 3.0575\n",
      "Epoch 8007, Train Loss: 3.0050, Test Loss: 3.0505\n",
      "Epoch 8008, Train Loss: 2.9966, Test Loss: 3.0567\n",
      "Epoch 8009, Train Loss: 3.0126, Test Loss: 3.0621\n",
      "Epoch 8010, Train Loss: 3.0080, Test Loss: 3.0537\n",
      "Epoch 8011, Train Loss: 2.9974, Test Loss: 3.0597\n",
      "Epoch 8012, Train Loss: 3.0099, Test Loss: 3.0599\n",
      "Epoch 8013, Train Loss: 3.0122, Test Loss: 3.0594\n",
      "Epoch 8014, Train Loss: 3.0006, Test Loss: 3.0672\n",
      "Epoch 8015, Train Loss: 3.0129, Test Loss: 3.0578\n",
      "Epoch 8016, Train Loss: 3.0007, Test Loss: 3.0528\n",
      "Epoch 8017, Train Loss: 3.0076, Test Loss: 3.0531\n",
      "Epoch 8018, Train Loss: 3.0096, Test Loss: 3.0648\n",
      "Epoch 8019, Train Loss: 3.0111, Test Loss: 3.0580\n",
      "Epoch 8020, Train Loss: 2.9996, Test Loss: 3.0606\n",
      "Epoch 8021, Train Loss: 3.0027, Test Loss: 3.0608\n",
      "Epoch 8022, Train Loss: 3.0056, Test Loss: 3.0604\n",
      "Epoch 8023, Train Loss: 3.0061, Test Loss: 3.0561\n",
      "Epoch 8024, Train Loss: 3.0001, Test Loss: 3.0497\n",
      "Epoch 8025, Train Loss: 2.9985, Test Loss: 3.0551\n",
      "Epoch 8026, Train Loss: 3.0083, Test Loss: 3.0547\n",
      "Epoch 8027, Train Loss: 3.0024, Test Loss: 3.0562\n",
      "Epoch 8028, Train Loss: 3.0012, Test Loss: 3.0523\n",
      "Epoch 8029, Train Loss: 2.9957, Test Loss: 3.0503\n",
      "Epoch 8030, Train Loss: 2.9957, Test Loss: 3.0532\n",
      "Epoch 8031, Train Loss: 3.0022, Test Loss: 3.0597\n",
      "Epoch 8032, Train Loss: 2.9960, Test Loss: 3.0603\n",
      "Epoch 8033, Train Loss: 2.9988, Test Loss: 3.0546\n",
      "Epoch 8034, Train Loss: 2.9995, Test Loss: 3.0504\n",
      "Epoch 8035, Train Loss: 2.9987, Test Loss: 3.0501\n",
      "Epoch 8036, Train Loss: 2.9996, Test Loss: 3.0514\n",
      "Epoch 8037, Train Loss: 2.9984, Test Loss: 3.0531\n",
      "Epoch 8038, Train Loss: 2.9970, Test Loss: 3.0554\n",
      "Epoch 8039, Train Loss: 3.0011, Test Loss: 3.0578\n",
      "Epoch 8040, Train Loss: 3.0025, Test Loss: 3.0590\n",
      "Epoch 8041, Train Loss: 3.0080, Test Loss: 3.0549\n",
      "Epoch 8042, Train Loss: 3.0038, Test Loss: 3.0596\n",
      "Epoch 8043, Train Loss: 3.0053, Test Loss: 3.0600\n",
      "Epoch 8044, Train Loss: 3.0039, Test Loss: 3.0573\n",
      "Epoch 8045, Train Loss: 3.0033, Test Loss: 3.0516\n",
      "Epoch 8046, Train Loss: 3.0046, Test Loss: 3.0528\n",
      "Epoch 8047, Train Loss: 3.0089, Test Loss: 3.0531\n",
      "Epoch 8048, Train Loss: 3.0007, Test Loss: 3.0584\n",
      "Epoch 8049, Train Loss: 3.0099, Test Loss: 3.0572\n",
      "Epoch 8050, Train Loss: 2.9949, Test Loss: 3.0640\n",
      "Epoch 8051, Train Loss: 3.0068, Test Loss: 3.0578\n",
      "Epoch 8052, Train Loss: 3.0030, Test Loss: 3.0490\n",
      "Epoch 8053, Train Loss: 2.9971, Test Loss: 3.0559\n",
      "Epoch 8054, Train Loss: 3.0059, Test Loss: 3.0557\n",
      "Epoch 8055, Train Loss: 3.0081, Test Loss: 3.0588\n",
      "Epoch 8056, Train Loss: 2.9998, Test Loss: 3.0661\n",
      "Epoch 8057, Train Loss: 3.0174, Test Loss: 3.0548\n",
      "Epoch 8058, Train Loss: 3.0002, Test Loss: 3.0617\n",
      "Epoch 8059, Train Loss: 3.0059, Test Loss: 3.0548\n",
      "Epoch 8060, Train Loss: 3.0055, Test Loss: 3.0517\n",
      "Epoch 8061, Train Loss: 3.0074, Test Loss: 3.0544\n",
      "Epoch 8062, Train Loss: 2.9994, Test Loss: 3.0611\n",
      "Epoch 8063, Train Loss: 3.0061, Test Loss: 3.0563\n",
      "Epoch 8064, Train Loss: 3.0125, Test Loss: 3.0594\n",
      "Epoch 8065, Train Loss: 3.0048, Test Loss: 3.0595\n",
      "Epoch 8066, Train Loss: 3.0128, Test Loss: 3.0532\n",
      "Epoch 8067, Train Loss: 3.0039, Test Loss: 3.0564\n",
      "Epoch 8068, Train Loss: 3.0028, Test Loss: 3.0605\n",
      "Epoch 8069, Train Loss: 3.0080, Test Loss: 3.0582\n",
      "Epoch 8070, Train Loss: 3.0050, Test Loss: 3.0544\n",
      "Epoch 8071, Train Loss: 3.0022, Test Loss: 3.0556\n",
      "Epoch 8072, Train Loss: 3.0138, Test Loss: 3.0523\n",
      "Epoch 8073, Train Loss: 3.0042, Test Loss: 3.0583\n",
      "Epoch 8074, Train Loss: 3.0015, Test Loss: 3.0677\n",
      "Epoch 8075, Train Loss: 3.0083, Test Loss: 3.0596\n",
      "Epoch 8076, Train Loss: 3.0092, Test Loss: 3.0588\n",
      "Epoch 8077, Train Loss: 3.0137, Test Loss: 3.0611\n",
      "Epoch 8078, Train Loss: 3.0130, Test Loss: 3.0663\n",
      "Epoch 8079, Train Loss: 3.0150, Test Loss: 3.0641\n",
      "Epoch 8080, Train Loss: 3.0056, Test Loss: 3.0757\n",
      "Epoch 8081, Train Loss: 3.0179, Test Loss: 3.0821\n",
      "Epoch 8082, Train Loss: 3.0256, Test Loss: 3.0643\n",
      "Epoch 8083, Train Loss: 3.0099, Test Loss: 3.0520\n",
      "Epoch 8084, Train Loss: 3.0081, Test Loss: 3.0594\n",
      "Epoch 8085, Train Loss: 3.0078, Test Loss: 3.0582\n",
      "Epoch 8086, Train Loss: 3.0094, Test Loss: 3.0545\n",
      "Epoch 8087, Train Loss: 3.0050, Test Loss: 3.0574\n",
      "Epoch 8088, Train Loss: 3.0079, Test Loss: 3.0567\n",
      "Epoch 8089, Train Loss: 3.0096, Test Loss: 3.0548\n",
      "Epoch 8090, Train Loss: 3.0025, Test Loss: 3.0567\n",
      "Epoch 8091, Train Loss: 3.0125, Test Loss: 3.0524\n",
      "Epoch 8092, Train Loss: 2.9978, Test Loss: 3.0557\n",
      "Epoch 8093, Train Loss: 3.0035, Test Loss: 3.0560\n",
      "Epoch 8094, Train Loss: 3.0010, Test Loss: 3.0536\n",
      "Epoch 8095, Train Loss: 3.0032, Test Loss: 3.0567\n",
      "Epoch 8096, Train Loss: 2.9994, Test Loss: 3.0639\n",
      "Epoch 8097, Train Loss: 3.0042, Test Loss: 3.0583\n",
      "Epoch 8098, Train Loss: 3.0012, Test Loss: 3.0559\n",
      "Epoch 8099, Train Loss: 3.0029, Test Loss: 3.0585\n",
      "Epoch 8100, Train Loss: 3.0037, Test Loss: 3.0555\n",
      "Epoch 8101, Train Loss: 3.0011, Test Loss: 3.0521\n",
      "Epoch 8102, Train Loss: 3.0020, Test Loss: 3.0579\n",
      "Epoch 8103, Train Loss: 3.0097, Test Loss: 3.0610\n",
      "Epoch 8104, Train Loss: 3.0020, Test Loss: 3.0591\n",
      "Epoch 8105, Train Loss: 3.0068, Test Loss: 3.0600\n",
      "Epoch 8106, Train Loss: 3.0077, Test Loss: 3.0660\n",
      "Epoch 8107, Train Loss: 3.0084, Test Loss: 3.0583\n",
      "Epoch 8108, Train Loss: 2.9990, Test Loss: 3.0552\n",
      "Epoch 8109, Train Loss: 3.0030, Test Loss: 3.0496\n",
      "Epoch 8110, Train Loss: 2.9961, Test Loss: 3.0558\n",
      "Epoch 8111, Train Loss: 3.0045, Test Loss: 3.0553\n",
      "Epoch 8112, Train Loss: 2.9997, Test Loss: 3.0581\n",
      "Epoch 8113, Train Loss: 3.0006, Test Loss: 3.0568\n",
      "Epoch 8114, Train Loss: 2.9992, Test Loss: 3.0621\n",
      "Epoch 8115, Train Loss: 3.0097, Test Loss: 3.0536\n",
      "Epoch 8116, Train Loss: 3.0006, Test Loss: 3.0509\n",
      "Epoch 8117, Train Loss: 3.0000, Test Loss: 3.0494\n",
      "Epoch 8118, Train Loss: 3.0000, Test Loss: 3.0514\n",
      "Epoch 8119, Train Loss: 2.9998, Test Loss: 3.0564\n",
      "Epoch 8120, Train Loss: 3.0005, Test Loss: 3.0533\n",
      "Epoch 8121, Train Loss: 3.0011, Test Loss: 3.0541\n",
      "Epoch 8122, Train Loss: 3.0086, Test Loss: 3.0582\n",
      "Epoch 8123, Train Loss: 3.0112, Test Loss: 3.0569\n",
      "Epoch 8124, Train Loss: 3.0031, Test Loss: 3.0577\n",
      "Epoch 8125, Train Loss: 3.0006, Test Loss: 3.0561\n",
      "Epoch 8126, Train Loss: 3.0025, Test Loss: 3.0543\n",
      "Epoch 8127, Train Loss: 2.9956, Test Loss: 3.0527\n",
      "Epoch 8128, Train Loss: 2.9991, Test Loss: 3.0529\n",
      "Epoch 8129, Train Loss: 3.0047, Test Loss: 3.0539\n",
      "Epoch 8130, Train Loss: 3.0024, Test Loss: 3.0643\n",
      "Epoch 8131, Train Loss: 3.0004, Test Loss: 3.0769\n",
      "Epoch 8132, Train Loss: 3.0189, Test Loss: 3.0591\n",
      "Epoch 8133, Train Loss: 3.0008, Test Loss: 3.0522\n",
      "Epoch 8134, Train Loss: 3.0093, Test Loss: 3.0572\n",
      "Epoch 8135, Train Loss: 3.0151, Test Loss: 3.0598\n",
      "Epoch 8136, Train Loss: 3.0083, Test Loss: 3.0683\n",
      "Epoch 8137, Train Loss: 3.0113, Test Loss: 3.0656\n",
      "Epoch 8138, Train Loss: 3.0115, Test Loss: 3.0714\n",
      "Epoch 8139, Train Loss: 3.0157, Test Loss: 3.0549\n",
      "Epoch 8140, Train Loss: 3.0065, Test Loss: 3.0570\n",
      "Epoch 8141, Train Loss: 3.0037, Test Loss: 3.0616\n",
      "Epoch 8142, Train Loss: 3.0062, Test Loss: 3.0585\n",
      "Epoch 8143, Train Loss: 3.0036, Test Loss: 3.0585\n",
      "Epoch 8144, Train Loss: 3.0006, Test Loss: 3.0591\n",
      "Epoch 8145, Train Loss: 3.0077, Test Loss: 3.0595\n",
      "Epoch 8146, Train Loss: 3.0066, Test Loss: 3.0545\n",
      "Epoch 8147, Train Loss: 3.0103, Test Loss: 3.0548\n",
      "Epoch 8148, Train Loss: 3.0032, Test Loss: 3.0571\n",
      "Epoch 8149, Train Loss: 3.0107, Test Loss: 3.0546\n",
      "Epoch 8150, Train Loss: 3.0060, Test Loss: 3.0602\n",
      "Epoch 8151, Train Loss: 3.0038, Test Loss: 3.0681\n",
      "Epoch 8152, Train Loss: 3.0203, Test Loss: 3.0711\n",
      "Epoch 8153, Train Loss: 3.0186, Test Loss: 3.0718\n",
      "Epoch 8154, Train Loss: 3.0137, Test Loss: 3.0609\n",
      "Epoch 8155, Train Loss: 3.0134, Test Loss: 3.0697\n",
      "Epoch 8156, Train Loss: 3.0523, Test Loss: 3.0880\n",
      "Epoch 8157, Train Loss: 3.0354, Test Loss: 3.1095\n",
      "Epoch 8158, Train Loss: 3.0455, Test Loss: 3.0711\n",
      "Epoch 8159, Train Loss: 3.0182, Test Loss: 3.0760\n",
      "Epoch 8160, Train Loss: 3.0586, Test Loss: 3.0570\n",
      "Epoch 8161, Train Loss: 3.0075, Test Loss: 3.0736\n",
      "Epoch 8162, Train Loss: 3.0135, Test Loss: 3.0726\n",
      "Epoch 8163, Train Loss: 3.0182, Test Loss: 3.0632\n",
      "Epoch 8164, Train Loss: 3.0229, Test Loss: 3.0695\n",
      "Epoch 8165, Train Loss: 3.0305, Test Loss: 3.0580\n",
      "Epoch 8166, Train Loss: 3.0053, Test Loss: 3.0785\n",
      "Epoch 8167, Train Loss: 3.0204, Test Loss: 3.0820\n",
      "Epoch 8168, Train Loss: 3.0291, Test Loss: 3.0577\n",
      "Epoch 8169, Train Loss: 3.0158, Test Loss: 3.0621\n",
      "Epoch 8170, Train Loss: 3.0141, Test Loss: 3.0666\n",
      "Epoch 8171, Train Loss: 3.0192, Test Loss: 3.0646\n",
      "Epoch 8172, Train Loss: 3.0180, Test Loss: 3.0491\n",
      "Epoch 8173, Train Loss: 3.0032, Test Loss: 3.0579\n",
      "Epoch 8174, Train Loss: 3.0152, Test Loss: 3.0614\n",
      "Epoch 8175, Train Loss: 3.0146, Test Loss: 3.0678\n",
      "Epoch 8176, Train Loss: 3.0184, Test Loss: 3.0575\n",
      "Epoch 8177, Train Loss: 3.0091, Test Loss: 3.0597\n",
      "Epoch 8178, Train Loss: 3.0177, Test Loss: 3.0555\n",
      "Epoch 8179, Train Loss: 3.0162, Test Loss: 3.0582\n",
      "Epoch 8180, Train Loss: 3.0182, Test Loss: 3.0656\n",
      "Epoch 8181, Train Loss: 3.0143, Test Loss: 3.0707\n",
      "Epoch 8182, Train Loss: 3.0100, Test Loss: 3.0695\n",
      "Epoch 8183, Train Loss: 3.0168, Test Loss: 3.0621\n",
      "Epoch 8184, Train Loss: 3.0006, Test Loss: 3.0710\n",
      "Epoch 8185, Train Loss: 3.0136, Test Loss: 3.0565\n",
      "Epoch 8186, Train Loss: 3.0064, Test Loss: 3.0608\n",
      "Epoch 8187, Train Loss: 3.0095, Test Loss: 3.0765\n",
      "Epoch 8188, Train Loss: 3.0161, Test Loss: 3.0792\n",
      "Epoch 8189, Train Loss: 3.0152, Test Loss: 3.0741\n",
      "Epoch 8190, Train Loss: 3.0141, Test Loss: 3.0711\n",
      "Epoch 8191, Train Loss: 3.0217, Test Loss: 3.0640\n",
      "Epoch 8192, Train Loss: 3.0069, Test Loss: 3.0803\n",
      "Epoch 8193, Train Loss: 3.0284, Test Loss: 3.0749\n",
      "Epoch 8194, Train Loss: 3.0304, Test Loss: 3.0549\n",
      "Epoch 8195, Train Loss: 3.0101, Test Loss: 3.0668\n",
      "Epoch 8196, Train Loss: 3.0247, Test Loss: 3.0708\n",
      "Epoch 8197, Train Loss: 3.0286, Test Loss: 3.0617\n",
      "Epoch 8198, Train Loss: 3.0059, Test Loss: 3.0716\n",
      "Epoch 8199, Train Loss: 3.0266, Test Loss: 3.0607\n",
      "Epoch 8200, Train Loss: 3.0103, Test Loss: 3.0722\n",
      "Epoch 8201, Train Loss: 3.0222, Test Loss: 3.0531\n",
      "Epoch 8202, Train Loss: 3.0005, Test Loss: 3.0698\n",
      "Epoch 8203, Train Loss: 3.0353, Test Loss: 3.0513\n",
      "Epoch 8204, Train Loss: 3.0020, Test Loss: 3.0731\n",
      "Epoch 8205, Train Loss: 3.0224, Test Loss: 3.0658\n",
      "Epoch 8206, Train Loss: 3.0161, Test Loss: 3.0549\n",
      "Epoch 8207, Train Loss: 3.0136, Test Loss: 3.0547\n",
      "Epoch 8208, Train Loss: 3.0068, Test Loss: 3.0652\n",
      "Epoch 8209, Train Loss: 3.0166, Test Loss: 3.0538\n",
      "Epoch 8210, Train Loss: 3.0164, Test Loss: 3.0622\n",
      "Epoch 8211, Train Loss: 3.0135, Test Loss: 3.0606\n",
      "Epoch 8212, Train Loss: 3.0025, Test Loss: 3.0606\n",
      "Epoch 8213, Train Loss: 3.0184, Test Loss: 3.0576\n",
      "Epoch 8214, Train Loss: 3.0158, Test Loss: 3.0662\n",
      "Epoch 8215, Train Loss: 3.0286, Test Loss: 3.0630\n",
      "Epoch 8216, Train Loss: 3.0112, Test Loss: 3.0566\n",
      "Epoch 8217, Train Loss: 3.0125, Test Loss: 3.0556\n",
      "Epoch 8218, Train Loss: 3.0189, Test Loss: 3.0618\n",
      "Epoch 8219, Train Loss: 3.0155, Test Loss: 3.0690\n",
      "Epoch 8220, Train Loss: 3.0246, Test Loss: 3.0593\n",
      "Epoch 8221, Train Loss: 3.0075, Test Loss: 3.0732\n",
      "Epoch 8222, Train Loss: 3.0289, Test Loss: 3.0708\n",
      "Epoch 8223, Train Loss: 3.0240, Test Loss: 3.0708\n",
      "Epoch 8224, Train Loss: 3.0278, Test Loss: 3.0579\n",
      "Epoch 8225, Train Loss: 3.0160, Test Loss: 3.0923\n",
      "Epoch 8226, Train Loss: 3.0375, Test Loss: 3.0841\n",
      "Epoch 8227, Train Loss: 3.0268, Test Loss: 3.0852\n",
      "Epoch 8228, Train Loss: 3.0314, Test Loss: 3.0708\n",
      "Epoch 8229, Train Loss: 3.0235, Test Loss: 3.0721\n",
      "Epoch 8230, Train Loss: 3.0250, Test Loss: 3.0686\n",
      "Epoch 8231, Train Loss: 3.0282, Test Loss: 3.0637\n",
      "Epoch 8232, Train Loss: 3.0336, Test Loss: 3.0921\n",
      "Epoch 8233, Train Loss: 3.0340, Test Loss: 3.0672\n",
      "Epoch 8234, Train Loss: 3.0202, Test Loss: 3.0885\n",
      "Epoch 8235, Train Loss: 3.0526, Test Loss: 3.0883\n",
      "Epoch 8236, Train Loss: 3.0341, Test Loss: 3.0694\n",
      "Epoch 8237, Train Loss: 3.0300, Test Loss: 3.0686\n",
      "Epoch 8238, Train Loss: 3.0371, Test Loss: 3.0689\n",
      "Epoch 8239, Train Loss: 3.0254, Test Loss: 3.0691\n",
      "Epoch 8240, Train Loss: 3.0253, Test Loss: 3.0731\n",
      "Epoch 8241, Train Loss: 3.0271, Test Loss: 3.0760\n",
      "Epoch 8242, Train Loss: 3.0248, Test Loss: 3.0586\n",
      "Epoch 8243, Train Loss: 3.0121, Test Loss: 3.0634\n",
      "Epoch 8244, Train Loss: 3.0230, Test Loss: 3.0656\n",
      "Epoch 8245, Train Loss: 3.0169, Test Loss: 3.0616\n",
      "Epoch 8246, Train Loss: 3.0239, Test Loss: 3.0519\n",
      "Epoch 8247, Train Loss: 3.0121, Test Loss: 3.0674\n",
      "Epoch 8248, Train Loss: 3.0239, Test Loss: 3.0690\n",
      "Epoch 8249, Train Loss: 3.0193, Test Loss: 3.0566\n",
      "Epoch 8250, Train Loss: 3.0109, Test Loss: 3.0551\n",
      "Epoch 8251, Train Loss: 3.0145, Test Loss: 3.0665\n",
      "Epoch 8252, Train Loss: 3.0254, Test Loss: 3.0674\n",
      "Epoch 8253, Train Loss: 3.0193, Test Loss: 3.0705\n",
      "Epoch 8254, Train Loss: 3.0197, Test Loss: 3.0590\n",
      "Epoch 8255, Train Loss: 3.0049, Test Loss: 3.0538\n",
      "Epoch 8256, Train Loss: 3.0034, Test Loss: 3.0550\n",
      "Epoch 8257, Train Loss: 3.0121, Test Loss: 3.0631\n",
      "Epoch 8258, Train Loss: 3.0077, Test Loss: 3.0664\n",
      "Epoch 8259, Train Loss: 3.0048, Test Loss: 3.0773\n",
      "Epoch 8260, Train Loss: 3.0166, Test Loss: 3.0698\n",
      "Epoch 8261, Train Loss: 3.0133, Test Loss: 3.0549\n",
      "Epoch 8262, Train Loss: 3.0046, Test Loss: 3.0517\n",
      "Epoch 8263, Train Loss: 3.0083, Test Loss: 3.0606\n",
      "Epoch 8264, Train Loss: 3.0120, Test Loss: 3.0587\n",
      "Epoch 8265, Train Loss: 3.0028, Test Loss: 3.0576\n",
      "Epoch 8266, Train Loss: 3.0003, Test Loss: 3.0621\n",
      "Epoch 8267, Train Loss: 3.0095, Test Loss: 3.0526\n",
      "Epoch 8268, Train Loss: 2.9992, Test Loss: 3.0572\n",
      "Epoch 8269, Train Loss: 3.0081, Test Loss: 3.0552\n",
      "Epoch 8270, Train Loss: 3.0066, Test Loss: 3.0558\n",
      "Epoch 8271, Train Loss: 3.0104, Test Loss: 3.0630\n",
      "Epoch 8272, Train Loss: 3.0151, Test Loss: 3.0662\n",
      "Epoch 8273, Train Loss: 3.0109, Test Loss: 3.0631\n",
      "Epoch 8274, Train Loss: 3.0066, Test Loss: 3.0531\n",
      "Epoch 8275, Train Loss: 3.0081, Test Loss: 3.0560\n",
      "Epoch 8276, Train Loss: 3.0105, Test Loss: 3.0613\n",
      "Epoch 8277, Train Loss: 3.0011, Test Loss: 3.0647\n",
      "Epoch 8278, Train Loss: 3.0035, Test Loss: 3.0724\n",
      "Epoch 8279, Train Loss: 3.0070, Test Loss: 3.0589\n",
      "Epoch 8280, Train Loss: 3.0027, Test Loss: 3.0507\n",
      "Epoch 8281, Train Loss: 3.0033, Test Loss: 3.0554\n",
      "Epoch 8282, Train Loss: 3.0103, Test Loss: 3.0519\n",
      "Epoch 8283, Train Loss: 3.0011, Test Loss: 3.0525\n",
      "Epoch 8284, Train Loss: 2.9991, Test Loss: 3.0616\n",
      "Epoch 8285, Train Loss: 3.0085, Test Loss: 3.0637\n",
      "Epoch 8286, Train Loss: 3.0043, Test Loss: 3.0600\n",
      "Epoch 8287, Train Loss: 3.0120, Test Loss: 3.0514\n",
      "Epoch 8288, Train Loss: 3.0034, Test Loss: 3.0504\n",
      "Epoch 8289, Train Loss: 3.0027, Test Loss: 3.0507\n",
      "Epoch 8290, Train Loss: 3.0031, Test Loss: 3.0505\n",
      "Epoch 8291, Train Loss: 2.9989, Test Loss: 3.0602\n",
      "Epoch 8292, Train Loss: 3.0030, Test Loss: 3.0652\n",
      "Epoch 8293, Train Loss: 3.0129, Test Loss: 3.0577\n",
      "Epoch 8294, Train Loss: 2.9968, Test Loss: 3.0474\n",
      "Epoch 8295, Train Loss: 2.9973, Test Loss: 3.0503\n",
      "Epoch 8296, Train Loss: 3.0079, Test Loss: 3.0566\n",
      "Epoch 8297, Train Loss: 3.0076, Test Loss: 3.0555\n",
      "Epoch 8298, Train Loss: 2.9987, Test Loss: 3.0661\n",
      "Epoch 8299, Train Loss: 3.0121, Test Loss: 3.0604\n",
      "Epoch 8300, Train Loss: 3.0075, Test Loss: 3.0618\n",
      "Epoch 8301, Train Loss: 3.0064, Test Loss: 3.0562\n",
      "Epoch 8302, Train Loss: 3.0120, Test Loss: 3.0556\n",
      "Epoch 8303, Train Loss: 3.0041, Test Loss: 3.0549\n",
      "Epoch 8304, Train Loss: 3.0046, Test Loss: 3.0584\n",
      "Epoch 8305, Train Loss: 3.0000, Test Loss: 3.0626\n",
      "Epoch 8306, Train Loss: 3.0040, Test Loss: 3.0526\n",
      "Epoch 8307, Train Loss: 3.0003, Test Loss: 3.0482\n",
      "Epoch 8308, Train Loss: 2.9995, Test Loss: 3.0486\n",
      "Epoch 8309, Train Loss: 3.0006, Test Loss: 3.0529\n",
      "Epoch 8310, Train Loss: 2.9994, Test Loss: 3.0576\n",
      "Epoch 8311, Train Loss: 3.0035, Test Loss: 3.0562\n",
      "Epoch 8312, Train Loss: 2.9991, Test Loss: 3.0613\n",
      "Epoch 8313, Train Loss: 3.0119, Test Loss: 3.0561\n",
      "Epoch 8314, Train Loss: 3.0019, Test Loss: 3.0601\n",
      "Epoch 8315, Train Loss: 3.0042, Test Loss: 3.0599\n",
      "Epoch 8316, Train Loss: 3.0041, Test Loss: 3.0614\n",
      "Epoch 8317, Train Loss: 3.0129, Test Loss: 3.0558\n",
      "Epoch 8318, Train Loss: 3.0044, Test Loss: 3.0603\n",
      "Epoch 8319, Train Loss: 3.0071, Test Loss: 3.0572\n",
      "Epoch 8320, Train Loss: 3.0072, Test Loss: 3.0547\n",
      "Epoch 8321, Train Loss: 3.0052, Test Loss: 3.0655\n",
      "Epoch 8322, Train Loss: 3.0088, Test Loss: 3.0581\n",
      "Epoch 8323, Train Loss: 3.0059, Test Loss: 3.0527\n",
      "Epoch 8324, Train Loss: 3.0044, Test Loss: 3.0590\n",
      "Epoch 8325, Train Loss: 3.0175, Test Loss: 3.0570\n",
      "Epoch 8326, Train Loss: 3.0050, Test Loss: 3.0540\n",
      "Epoch 8327, Train Loss: 3.0026, Test Loss: 3.0577\n",
      "Epoch 8328, Train Loss: 3.0033, Test Loss: 3.0631\n",
      "Epoch 8329, Train Loss: 3.0109, Test Loss: 3.0637\n",
      "Epoch 8330, Train Loss: 3.0041, Test Loss: 3.0632\n",
      "Epoch 8331, Train Loss: 3.0161, Test Loss: 3.0593\n",
      "Epoch 8332, Train Loss: 3.0184, Test Loss: 3.0653\n",
      "Epoch 8333, Train Loss: 3.0150, Test Loss: 3.0715\n",
      "Epoch 8334, Train Loss: 3.0195, Test Loss: 3.0630\n",
      "Epoch 8335, Train Loss: 3.0074, Test Loss: 3.0618\n",
      "Epoch 8336, Train Loss: 3.0096, Test Loss: 3.0625\n",
      "Epoch 8337, Train Loss: 3.0153, Test Loss: 3.0576\n",
      "Epoch 8338, Train Loss: 3.0114, Test Loss: 3.0551\n",
      "Epoch 8339, Train Loss: 3.0070, Test Loss: 3.0602\n",
      "Epoch 8340, Train Loss: 3.0117, Test Loss: 3.0562\n",
      "Epoch 8341, Train Loss: 3.0048, Test Loss: 3.0541\n",
      "Epoch 8342, Train Loss: 3.0032, Test Loss: 3.0562\n",
      "Epoch 8343, Train Loss: 3.0062, Test Loss: 3.0532\n",
      "Epoch 8344, Train Loss: 3.0033, Test Loss: 3.0562\n",
      "Epoch 8345, Train Loss: 3.0027, Test Loss: 3.0553\n",
      "Epoch 8346, Train Loss: 3.0010, Test Loss: 3.0581\n",
      "Epoch 8347, Train Loss: 3.0122, Test Loss: 3.0561\n",
      "Epoch 8348, Train Loss: 3.0017, Test Loss: 3.0526\n",
      "Epoch 8349, Train Loss: 2.9996, Test Loss: 3.0482\n",
      "Epoch 8350, Train Loss: 2.9973, Test Loss: 3.0478\n",
      "Epoch 8351, Train Loss: 2.9969, Test Loss: 3.0492\n",
      "Epoch 8352, Train Loss: 2.9956, Test Loss: 3.0501\n",
      "Epoch 8353, Train Loss: 3.0003, Test Loss: 3.0515\n",
      "Epoch 8354, Train Loss: 2.9967, Test Loss: 3.0486\n",
      "Epoch 8355, Train Loss: 2.9942, Test Loss: 3.0491\n",
      "Epoch 8356, Train Loss: 2.9978, Test Loss: 3.0521\n",
      "Epoch 8357, Train Loss: 2.9979, Test Loss: 3.0537\n",
      "Epoch 8358, Train Loss: 3.0010, Test Loss: 3.0546\n",
      "Epoch 8359, Train Loss: 2.9999, Test Loss: 3.0544\n",
      "Epoch 8360, Train Loss: 3.0001, Test Loss: 3.0516\n",
      "Epoch 8361, Train Loss: 3.0003, Test Loss: 3.0612\n",
      "Epoch 8362, Train Loss: 3.0063, Test Loss: 3.0552\n",
      "Epoch 8363, Train Loss: 2.9994, Test Loss: 3.0601\n",
      "Epoch 8364, Train Loss: 3.0050, Test Loss: 3.0581\n",
      "Epoch 8365, Train Loss: 2.9957, Test Loss: 3.0592\n",
      "Epoch 8366, Train Loss: 3.0058, Test Loss: 3.0525\n",
      "Epoch 8367, Train Loss: 2.9952, Test Loss: 3.0607\n",
      "Epoch 8368, Train Loss: 3.0060, Test Loss: 3.0515\n",
      "Epoch 8369, Train Loss: 3.0019, Test Loss: 3.0512\n",
      "Epoch 8370, Train Loss: 3.0103, Test Loss: 3.0507\n",
      "Epoch 8371, Train Loss: 2.9993, Test Loss: 3.0551\n",
      "Epoch 8372, Train Loss: 3.0066, Test Loss: 3.0543\n",
      "Epoch 8373, Train Loss: 2.9978, Test Loss: 3.0604\n",
      "Epoch 8374, Train Loss: 3.0140, Test Loss: 3.0644\n",
      "Epoch 8375, Train Loss: 3.0096, Test Loss: 3.0572\n",
      "Epoch 8376, Train Loss: 3.0037, Test Loss: 3.0555\n",
      "Epoch 8377, Train Loss: 3.0106, Test Loss: 3.0550\n",
      "Epoch 8378, Train Loss: 3.0037, Test Loss: 3.0598\n",
      "Epoch 8379, Train Loss: 3.0101, Test Loss: 3.0582\n",
      "Epoch 8380, Train Loss: 2.9988, Test Loss: 3.0558\n",
      "Epoch 8381, Train Loss: 3.0063, Test Loss: 3.0537\n",
      "Epoch 8382, Train Loss: 3.0081, Test Loss: 3.0589\n",
      "Epoch 8383, Train Loss: 3.0086, Test Loss: 3.0626\n",
      "Epoch 8384, Train Loss: 3.0076, Test Loss: 3.0641\n",
      "Epoch 8385, Train Loss: 3.0071, Test Loss: 3.0642\n",
      "Epoch 8386, Train Loss: 3.0092, Test Loss: 3.0476\n",
      "Epoch 8387, Train Loss: 2.9972, Test Loss: 3.0560\n",
      "Epoch 8388, Train Loss: 3.0090, Test Loss: 3.0592\n",
      "Epoch 8389, Train Loss: 3.0068, Test Loss: 3.0517\n",
      "Epoch 8390, Train Loss: 3.0002, Test Loss: 3.0555\n",
      "Epoch 8391, Train Loss: 3.0053, Test Loss: 3.0579\n",
      "Epoch 8392, Train Loss: 3.0009, Test Loss: 3.0657\n",
      "Epoch 8393, Train Loss: 3.0041, Test Loss: 3.0651\n",
      "Epoch 8394, Train Loss: 3.0126, Test Loss: 3.0540\n",
      "Epoch 8395, Train Loss: 3.0054, Test Loss: 3.0633\n",
      "Epoch 8396, Train Loss: 3.0234, Test Loss: 3.0570\n",
      "Epoch 8397, Train Loss: 3.0098, Test Loss: 3.0596\n",
      "Epoch 8398, Train Loss: 3.0105, Test Loss: 3.0586\n",
      "Epoch 8399, Train Loss: 3.0065, Test Loss: 3.0544\n",
      "Epoch 8400, Train Loss: 2.9985, Test Loss: 3.0557\n",
      "Epoch 8401, Train Loss: 3.0064, Test Loss: 3.0562\n",
      "Epoch 8402, Train Loss: 3.0049, Test Loss: 3.0530\n",
      "Epoch 8403, Train Loss: 3.0080, Test Loss: 3.0505\n",
      "Epoch 8404, Train Loss: 2.9960, Test Loss: 3.0574\n",
      "Epoch 8405, Train Loss: 2.9978, Test Loss: 3.0647\n",
      "Epoch 8406, Train Loss: 3.0069, Test Loss: 3.0598\n",
      "Epoch 8407, Train Loss: 3.0042, Test Loss: 3.0540\n",
      "Epoch 8408, Train Loss: 3.0014, Test Loss: 3.0549\n",
      "Epoch 8409, Train Loss: 2.9995, Test Loss: 3.0541\n",
      "Epoch 8410, Train Loss: 3.0031, Test Loss: 3.0569\n",
      "Epoch 8411, Train Loss: 3.0014, Test Loss: 3.0567\n",
      "Epoch 8412, Train Loss: 3.0038, Test Loss: 3.0541\n",
      "Epoch 8413, Train Loss: 2.9994, Test Loss: 3.0592\n",
      "Epoch 8414, Train Loss: 3.0067, Test Loss: 3.0552\n",
      "Epoch 8415, Train Loss: 2.9996, Test Loss: 3.0580\n",
      "Epoch 8416, Train Loss: 3.0129, Test Loss: 3.0526\n",
      "Epoch 8417, Train Loss: 3.0021, Test Loss: 3.0558\n",
      "Epoch 8418, Train Loss: 3.0044, Test Loss: 3.0616\n",
      "Epoch 8419, Train Loss: 3.0013, Test Loss: 3.0658\n",
      "Epoch 8420, Train Loss: 3.0080, Test Loss: 3.0583\n",
      "Epoch 8421, Train Loss: 3.0023, Test Loss: 3.0551\n",
      "Epoch 8422, Train Loss: 3.0035, Test Loss: 3.0582\n",
      "Epoch 8423, Train Loss: 3.0081, Test Loss: 3.0550\n",
      "Epoch 8424, Train Loss: 3.0034, Test Loss: 3.0567\n",
      "Epoch 8425, Train Loss: 3.0098, Test Loss: 3.0661\n",
      "Epoch 8426, Train Loss: 3.0121, Test Loss: 3.0687\n",
      "Epoch 8427, Train Loss: 3.0080, Test Loss: 3.0682\n",
      "Epoch 8428, Train Loss: 3.0079, Test Loss: 3.0618\n",
      "Epoch 8429, Train Loss: 3.0006, Test Loss: 3.0566\n",
      "Epoch 8430, Train Loss: 3.0078, Test Loss: 3.0535\n",
      "Epoch 8431, Train Loss: 3.0078, Test Loss: 3.0545\n",
      "Epoch 8432, Train Loss: 3.0094, Test Loss: 3.0548\n",
      "Epoch 8433, Train Loss: 3.0022, Test Loss: 3.0675\n",
      "Epoch 8434, Train Loss: 3.0073, Test Loss: 3.0742\n",
      "Epoch 8435, Train Loss: 3.0178, Test Loss: 3.0569\n",
      "Epoch 8436, Train Loss: 3.0007, Test Loss: 3.0603\n",
      "Epoch 8437, Train Loss: 3.0167, Test Loss: 3.0605\n",
      "Epoch 8438, Train Loss: 3.0088, Test Loss: 3.0671\n",
      "Epoch 8439, Train Loss: 3.0137, Test Loss: 3.0674\n",
      "Epoch 8440, Train Loss: 3.0118, Test Loss: 3.0633\n",
      "Epoch 8441, Train Loss: 3.0078, Test Loss: 3.0583\n",
      "Epoch 8442, Train Loss: 3.0083, Test Loss: 3.0607\n",
      "Epoch 8443, Train Loss: 3.0135, Test Loss: 3.0579\n",
      "Epoch 8444, Train Loss: 3.0059, Test Loss: 3.0556\n",
      "Epoch 8445, Train Loss: 3.0008, Test Loss: 3.0663\n",
      "Epoch 8446, Train Loss: 3.0051, Test Loss: 3.0665\n",
      "Epoch 8447, Train Loss: 3.0027, Test Loss: 3.0696\n",
      "Epoch 8448, Train Loss: 3.0123, Test Loss: 3.0582\n",
      "Epoch 8449, Train Loss: 3.0015, Test Loss: 3.0514\n",
      "Epoch 8450, Train Loss: 2.9984, Test Loss: 3.0546\n",
      "Epoch 8451, Train Loss: 3.0045, Test Loss: 3.0587\n",
      "Epoch 8452, Train Loss: 3.0012, Test Loss: 3.0595\n",
      "Epoch 8453, Train Loss: 3.0015, Test Loss: 3.0558\n",
      "Epoch 8454, Train Loss: 2.9962, Test Loss: 3.0525\n",
      "Epoch 8455, Train Loss: 3.0005, Test Loss: 3.0479\n",
      "Epoch 8456, Train Loss: 3.0000, Test Loss: 3.0464\n",
      "Epoch 8457, Train Loss: 2.9977, Test Loss: 3.0533\n",
      "Epoch 8458, Train Loss: 3.0016, Test Loss: 3.0546\n",
      "Epoch 8459, Train Loss: 2.9994, Test Loss: 3.0530\n",
      "Epoch 8460, Train Loss: 2.9957, Test Loss: 3.0504\n",
      "Epoch 8461, Train Loss: 2.9961, Test Loss: 3.0505\n",
      "Epoch 8462, Train Loss: 2.9978, Test Loss: 3.0527\n",
      "Epoch 8463, Train Loss: 2.9989, Test Loss: 3.0568\n",
      "Epoch 8464, Train Loss: 3.0052, Test Loss: 3.0595\n",
      "Epoch 8465, Train Loss: 2.9990, Test Loss: 3.0615\n",
      "Epoch 8466, Train Loss: 3.0034, Test Loss: 3.0521\n",
      "Epoch 8467, Train Loss: 2.9990, Test Loss: 3.0497\n",
      "Epoch 8468, Train Loss: 2.9983, Test Loss: 3.0483\n",
      "Epoch 8469, Train Loss: 3.0010, Test Loss: 3.0516\n",
      "Epoch 8470, Train Loss: 3.0003, Test Loss: 3.0526\n",
      "Epoch 8471, Train Loss: 2.9979, Test Loss: 3.0568\n",
      "Epoch 8472, Train Loss: 3.0022, Test Loss: 3.0568\n",
      "Epoch 8473, Train Loss: 3.0037, Test Loss: 3.0521\n",
      "Epoch 8474, Train Loss: 3.0002, Test Loss: 3.0534\n",
      "Epoch 8475, Train Loss: 3.0010, Test Loss: 3.0490\n",
      "Epoch 8476, Train Loss: 2.9997, Test Loss: 3.0500\n",
      "Epoch 8477, Train Loss: 2.9966, Test Loss: 3.0527\n",
      "Epoch 8478, Train Loss: 3.0006, Test Loss: 3.0534\n",
      "Epoch 8479, Train Loss: 2.9975, Test Loss: 3.0568\n",
      "Epoch 8480, Train Loss: 2.9989, Test Loss: 3.0559\n",
      "Epoch 8481, Train Loss: 3.0013, Test Loss: 3.0524\n",
      "Epoch 8482, Train Loss: 2.9976, Test Loss: 3.0506\n",
      "Epoch 8483, Train Loss: 3.0020, Test Loss: 3.0501\n",
      "Epoch 8484, Train Loss: 2.9968, Test Loss: 3.0549\n",
      "Epoch 8485, Train Loss: 2.9937, Test Loss: 3.0574\n",
      "Epoch 8486, Train Loss: 2.9960, Test Loss: 3.0586\n",
      "Epoch 8487, Train Loss: 2.9991, Test Loss: 3.0526\n",
      "Epoch 8488, Train Loss: 2.9997, Test Loss: 3.0518\n",
      "Epoch 8489, Train Loss: 2.9973, Test Loss: 3.0533\n",
      "Epoch 8490, Train Loss: 3.0041, Test Loss: 3.0525\n",
      "Epoch 8491, Train Loss: 2.9996, Test Loss: 3.0601\n",
      "Epoch 8492, Train Loss: 2.9970, Test Loss: 3.0566\n",
      "Epoch 8493, Train Loss: 2.9985, Test Loss: 3.0580\n",
      "Epoch 8494, Train Loss: 3.0005, Test Loss: 3.0543\n",
      "Epoch 8495, Train Loss: 3.0025, Test Loss: 3.0563\n",
      "Epoch 8496, Train Loss: 3.0029, Test Loss: 3.0626\n",
      "Epoch 8497, Train Loss: 3.0060, Test Loss: 3.0584\n",
      "Epoch 8498, Train Loss: 3.0059, Test Loss: 3.0583\n",
      "Epoch 8499, Train Loss: 3.0016, Test Loss: 3.0598\n",
      "Epoch 8500, Train Loss: 3.0114, Test Loss: 3.0546\n",
      "Epoch 8501, Train Loss: 3.0040, Test Loss: 3.0620\n",
      "Epoch 8502, Train Loss: 3.0075, Test Loss: 3.0591\n",
      "Epoch 8503, Train Loss: 3.0041, Test Loss: 3.0553\n",
      "Epoch 8504, Train Loss: 3.0020, Test Loss: 3.0545\n",
      "Epoch 8505, Train Loss: 2.9998, Test Loss: 3.0549\n",
      "Epoch 8506, Train Loss: 2.9991, Test Loss: 3.0544\n",
      "Epoch 8507, Train Loss: 3.0020, Test Loss: 3.0572\n",
      "Epoch 8508, Train Loss: 2.9983, Test Loss: 3.0568\n",
      "Epoch 8509, Train Loss: 3.0019, Test Loss: 3.0554\n",
      "Epoch 8510, Train Loss: 3.0026, Test Loss: 3.0510\n",
      "Epoch 8511, Train Loss: 2.9999, Test Loss: 3.0544\n",
      "Epoch 8512, Train Loss: 3.0112, Test Loss: 3.0552\n",
      "Epoch 8513, Train Loss: 3.0067, Test Loss: 3.0585\n",
      "Epoch 8514, Train Loss: 3.0039, Test Loss: 3.0656\n",
      "Epoch 8515, Train Loss: 3.0066, Test Loss: 3.0587\n",
      "Epoch 8516, Train Loss: 3.0058, Test Loss: 3.0531\n",
      "Epoch 8517, Train Loss: 2.9985, Test Loss: 3.0505\n",
      "Epoch 8518, Train Loss: 3.0000, Test Loss: 3.0501\n",
      "Epoch 8519, Train Loss: 3.0041, Test Loss: 3.0524\n",
      "Epoch 8520, Train Loss: 3.0019, Test Loss: 3.0576\n",
      "Epoch 8521, Train Loss: 3.0005, Test Loss: 3.0659\n",
      "Epoch 8522, Train Loss: 3.0021, Test Loss: 3.0644\n",
      "Epoch 8523, Train Loss: 3.0042, Test Loss: 3.0519\n",
      "Epoch 8524, Train Loss: 3.0036, Test Loss: 3.0506\n",
      "Epoch 8525, Train Loss: 3.0059, Test Loss: 3.0520\n",
      "Epoch 8526, Train Loss: 2.9984, Test Loss: 3.0583\n",
      "Epoch 8527, Train Loss: 3.0040, Test Loss: 3.0602\n",
      "Epoch 8528, Train Loss: 3.0012, Test Loss: 3.0697\n",
      "Epoch 8529, Train Loss: 3.0087, Test Loss: 3.0605\n",
      "Epoch 8530, Train Loss: 3.0126, Test Loss: 3.0559\n",
      "Epoch 8531, Train Loss: 3.0025, Test Loss: 3.0641\n",
      "Epoch 8532, Train Loss: 3.0118, Test Loss: 3.0678\n",
      "Epoch 8533, Train Loss: 3.0170, Test Loss: 3.0557\n",
      "Epoch 8534, Train Loss: 3.0116, Test Loss: 3.0524\n",
      "Epoch 8535, Train Loss: 3.0038, Test Loss: 3.0657\n",
      "Epoch 8536, Train Loss: 3.0110, Test Loss: 3.0635\n",
      "Epoch 8537, Train Loss: 2.9994, Test Loss: 3.0607\n",
      "Epoch 8538, Train Loss: 3.0049, Test Loss: 3.0520\n",
      "Epoch 8539, Train Loss: 2.9995, Test Loss: 3.0514\n",
      "Epoch 8540, Train Loss: 2.9998, Test Loss: 3.0520\n",
      "Epoch 8541, Train Loss: 3.0016, Test Loss: 3.0544\n",
      "Epoch 8542, Train Loss: 2.9996, Test Loss: 3.0616\n",
      "Epoch 8543, Train Loss: 2.9996, Test Loss: 3.0613\n",
      "Epoch 8544, Train Loss: 3.0034, Test Loss: 3.0556\n",
      "Epoch 8545, Train Loss: 2.9991, Test Loss: 3.0533\n",
      "Epoch 8546, Train Loss: 3.0021, Test Loss: 3.0522\n",
      "Epoch 8547, Train Loss: 3.0005, Test Loss: 3.0498\n",
      "Epoch 8548, Train Loss: 2.9945, Test Loss: 3.0546\n",
      "Epoch 8549, Train Loss: 3.0015, Test Loss: 3.0596\n",
      "Epoch 8550, Train Loss: 3.0051, Test Loss: 3.0472\n",
      "Epoch 8551, Train Loss: 2.9976, Test Loss: 3.0529\n",
      "Epoch 8552, Train Loss: 3.0043, Test Loss: 3.0561\n",
      "Epoch 8553, Train Loss: 3.0016, Test Loss: 3.0632\n",
      "Epoch 8554, Train Loss: 3.0007, Test Loss: 3.0623\n",
      "Epoch 8555, Train Loss: 3.0043, Test Loss: 3.0505\n",
      "Epoch 8556, Train Loss: 2.9991, Test Loss: 3.0501\n",
      "Epoch 8557, Train Loss: 3.0033, Test Loss: 3.0539\n",
      "Epoch 8558, Train Loss: 3.0054, Test Loss: 3.0571\n",
      "Epoch 8559, Train Loss: 3.0033, Test Loss: 3.0595\n",
      "Epoch 8560, Train Loss: 3.0049, Test Loss: 3.0628\n",
      "Epoch 8561, Train Loss: 3.0128, Test Loss: 3.0605\n",
      "Epoch 8562, Train Loss: 3.0064, Test Loss: 3.0561\n",
      "Epoch 8563, Train Loss: 3.0040, Test Loss: 3.0529\n",
      "Epoch 8564, Train Loss: 3.0007, Test Loss: 3.0580\n",
      "Epoch 8565, Train Loss: 2.9985, Test Loss: 3.0534\n",
      "Epoch 8566, Train Loss: 2.9962, Test Loss: 3.0592\n",
      "Epoch 8567, Train Loss: 3.0033, Test Loss: 3.0549\n",
      "Epoch 8568, Train Loss: 3.0045, Test Loss: 3.0551\n",
      "Epoch 8569, Train Loss: 2.9991, Test Loss: 3.0506\n",
      "Epoch 8570, Train Loss: 3.0009, Test Loss: 3.0546\n",
      "Epoch 8571, Train Loss: 3.0050, Test Loss: 3.0521\n",
      "Epoch 8572, Train Loss: 2.9959, Test Loss: 3.0579\n",
      "Epoch 8573, Train Loss: 3.0032, Test Loss: 3.0555\n",
      "Epoch 8574, Train Loss: 2.9987, Test Loss: 3.0591\n",
      "Epoch 8575, Train Loss: 3.0095, Test Loss: 3.0572\n",
      "Epoch 8576, Train Loss: 3.0063, Test Loss: 3.0571\n",
      "Epoch 8577, Train Loss: 3.0046, Test Loss: 3.0624\n",
      "Epoch 8578, Train Loss: 3.0087, Test Loss: 3.0530\n",
      "Epoch 8579, Train Loss: 3.0041, Test Loss: 3.0553\n",
      "Epoch 8580, Train Loss: 3.0110, Test Loss: 3.0651\n",
      "Epoch 8581, Train Loss: 3.0083, Test Loss: 3.0636\n",
      "Epoch 8582, Train Loss: 2.9981, Test Loss: 3.0628\n",
      "Epoch 8583, Train Loss: 3.0040, Test Loss: 3.0589\n",
      "Epoch 8584, Train Loss: 3.0049, Test Loss: 3.0571\n",
      "Epoch 8585, Train Loss: 3.0065, Test Loss: 3.0515\n",
      "Epoch 8586, Train Loss: 3.0028, Test Loss: 3.0516\n",
      "Epoch 8587, Train Loss: 3.0032, Test Loss: 3.0525\n",
      "Epoch 8588, Train Loss: 3.0078, Test Loss: 3.0617\n",
      "Epoch 8589, Train Loss: 3.0002, Test Loss: 3.0638\n",
      "Epoch 8590, Train Loss: 3.0049, Test Loss: 3.0597\n",
      "Epoch 8591, Train Loss: 3.0135, Test Loss: 3.0559\n",
      "Epoch 8592, Train Loss: 3.0014, Test Loss: 3.0523\n",
      "Epoch 8593, Train Loss: 3.0020, Test Loss: 3.0549\n",
      "Epoch 8594, Train Loss: 3.0034, Test Loss: 3.0539\n",
      "Epoch 8595, Train Loss: 3.0074, Test Loss: 3.0573\n",
      "Epoch 8596, Train Loss: 3.0116, Test Loss: 3.0514\n",
      "Epoch 8597, Train Loss: 3.0005, Test Loss: 3.0540\n",
      "Epoch 8598, Train Loss: 3.0045, Test Loss: 3.0520\n",
      "Epoch 8599, Train Loss: 3.0092, Test Loss: 3.0524\n",
      "Epoch 8600, Train Loss: 3.0049, Test Loss: 3.0557\n",
      "Epoch 8601, Train Loss: 2.9966, Test Loss: 3.0594\n",
      "Epoch 8602, Train Loss: 3.0055, Test Loss: 3.0544\n",
      "Epoch 8603, Train Loss: 2.9969, Test Loss: 3.0480\n",
      "Epoch 8604, Train Loss: 2.9984, Test Loss: 3.0501\n",
      "Epoch 8605, Train Loss: 2.9969, Test Loss: 3.0536\n",
      "Epoch 8606, Train Loss: 2.9988, Test Loss: 3.0539\n",
      "Epoch 8607, Train Loss: 3.0021, Test Loss: 3.0570\n",
      "Epoch 8608, Train Loss: 2.9974, Test Loss: 3.0579\n",
      "Epoch 8609, Train Loss: 3.0011, Test Loss: 3.0531\n",
      "Epoch 8610, Train Loss: 2.9970, Test Loss: 3.0535\n",
      "Epoch 8611, Train Loss: 3.0066, Test Loss: 3.0488\n",
      "Epoch 8612, Train Loss: 2.9977, Test Loss: 3.0503\n",
      "Epoch 8613, Train Loss: 3.0020, Test Loss: 3.0536\n",
      "Epoch 8614, Train Loss: 3.0006, Test Loss: 3.0628\n",
      "Epoch 8615, Train Loss: 3.0097, Test Loss: 3.0563\n",
      "Epoch 8616, Train Loss: 3.0078, Test Loss: 3.0512\n",
      "Epoch 8617, Train Loss: 3.0011, Test Loss: 3.0515\n",
      "Epoch 8618, Train Loss: 3.0099, Test Loss: 3.0528\n",
      "Epoch 8619, Train Loss: 3.0109, Test Loss: 3.0523\n",
      "Epoch 8620, Train Loss: 2.9968, Test Loss: 3.0614\n",
      "Epoch 8621, Train Loss: 3.0097, Test Loss: 3.0602\n",
      "Epoch 8622, Train Loss: 3.0092, Test Loss: 3.0636\n",
      "Epoch 8623, Train Loss: 3.0133, Test Loss: 3.0646\n",
      "Epoch 8624, Train Loss: 3.0172, Test Loss: 3.0703\n",
      "Epoch 8625, Train Loss: 3.0171, Test Loss: 3.0626\n",
      "Epoch 8626, Train Loss: 3.0017, Test Loss: 3.0580\n",
      "Epoch 8627, Train Loss: 3.0256, Test Loss: 3.0584\n",
      "Epoch 8628, Train Loss: 3.0092, Test Loss: 3.0724\n",
      "Epoch 8629, Train Loss: 3.0240, Test Loss: 3.0642\n",
      "Epoch 8630, Train Loss: 3.0172, Test Loss: 3.0730\n",
      "Epoch 8631, Train Loss: 3.0261, Test Loss: 3.0630\n",
      "Epoch 8632, Train Loss: 3.0253, Test Loss: 3.0732\n",
      "Epoch 8633, Train Loss: 3.0183, Test Loss: 3.0676\n",
      "Epoch 8634, Train Loss: 3.0160, Test Loss: 3.0586\n",
      "Epoch 8635, Train Loss: 3.0180, Test Loss: 3.0649\n",
      "Epoch 8636, Train Loss: 3.0317, Test Loss: 3.0669\n",
      "Epoch 8637, Train Loss: 3.0094, Test Loss: 3.0643\n",
      "Epoch 8638, Train Loss: 3.0200, Test Loss: 3.0623\n",
      "Epoch 8639, Train Loss: 3.0209, Test Loss: 3.0615\n",
      "Epoch 8640, Train Loss: 3.0124, Test Loss: 3.0585\n",
      "Epoch 8641, Train Loss: 3.0101, Test Loss: 3.0659\n",
      "Epoch 8642, Train Loss: 3.0147, Test Loss: 3.0637\n",
      "Epoch 8643, Train Loss: 3.0090, Test Loss: 3.0549\n",
      "Epoch 8644, Train Loss: 3.0056, Test Loss: 3.0572\n",
      "Epoch 8645, Train Loss: 3.0102, Test Loss: 3.0527\n",
      "Epoch 8646, Train Loss: 3.0032, Test Loss: 3.0505\n",
      "Epoch 8647, Train Loss: 2.9989, Test Loss: 3.0561\n",
      "Epoch 8648, Train Loss: 3.0026, Test Loss: 3.0565\n",
      "Epoch 8649, Train Loss: 2.9992, Test Loss: 3.0535\n",
      "Epoch 8650, Train Loss: 2.9959, Test Loss: 3.0512\n",
      "Epoch 8651, Train Loss: 2.9999, Test Loss: 3.0508\n",
      "Epoch 8652, Train Loss: 3.0033, Test Loss: 3.0522\n",
      "Epoch 8653, Train Loss: 2.9975, Test Loss: 3.0537\n",
      "Epoch 8654, Train Loss: 2.9975, Test Loss: 3.0553\n",
      "Epoch 8655, Train Loss: 2.9981, Test Loss: 3.0528\n",
      "Epoch 8656, Train Loss: 2.9984, Test Loss: 3.0494\n",
      "Epoch 8657, Train Loss: 2.9969, Test Loss: 3.0513\n",
      "Epoch 8658, Train Loss: 2.9994, Test Loss: 3.0559\n",
      "Epoch 8659, Train Loss: 3.0033, Test Loss: 3.0552\n",
      "Epoch 8660, Train Loss: 3.0021, Test Loss: 3.0549\n",
      "Epoch 8661, Train Loss: 3.0026, Test Loss: 3.0541\n",
      "Epoch 8662, Train Loss: 2.9977, Test Loss: 3.0515\n",
      "Epoch 8663, Train Loss: 2.9984, Test Loss: 3.0480\n",
      "Epoch 8664, Train Loss: 2.9953, Test Loss: 3.0507\n",
      "Epoch 8665, Train Loss: 3.0011, Test Loss: 3.0484\n",
      "Epoch 8666, Train Loss: 2.9943, Test Loss: 3.0501\n",
      "Epoch 8667, Train Loss: 2.9980, Test Loss: 3.0521\n",
      "Epoch 8668, Train Loss: 2.9993, Test Loss: 3.0509\n",
      "Epoch 8669, Train Loss: 2.9963, Test Loss: 3.0507\n",
      "Epoch 8670, Train Loss: 3.0004, Test Loss: 3.0482\n",
      "Epoch 8671, Train Loss: 3.0003, Test Loss: 3.0555\n",
      "Epoch 8672, Train Loss: 3.0017, Test Loss: 3.0539\n",
      "Epoch 8673, Train Loss: 3.0005, Test Loss: 3.0548\n",
      "Epoch 8674, Train Loss: 2.9983, Test Loss: 3.0566\n",
      "Epoch 8675, Train Loss: 3.0011, Test Loss: 3.0535\n",
      "Epoch 8676, Train Loss: 2.9978, Test Loss: 3.0505\n",
      "Epoch 8677, Train Loss: 2.9994, Test Loss: 3.0547\n",
      "Epoch 8678, Train Loss: 2.9981, Test Loss: 3.0600\n",
      "Epoch 8679, Train Loss: 3.0028, Test Loss: 3.0546\n",
      "Epoch 8680, Train Loss: 2.9965, Test Loss: 3.0507\n",
      "Epoch 8681, Train Loss: 3.0044, Test Loss: 3.0524\n",
      "Epoch 8682, Train Loss: 2.9990, Test Loss: 3.0612\n",
      "Epoch 8683, Train Loss: 3.0059, Test Loss: 3.0563\n",
      "Epoch 8684, Train Loss: 2.9992, Test Loss: 3.0553\n",
      "Epoch 8685, Train Loss: 2.9977, Test Loss: 3.0596\n",
      "Epoch 8686, Train Loss: 3.0032, Test Loss: 3.0496\n",
      "Epoch 8687, Train Loss: 2.9929, Test Loss: 3.0538\n",
      "Epoch 8688, Train Loss: 2.9990, Test Loss: 3.0541\n",
      "Epoch 8689, Train Loss: 3.0045, Test Loss: 3.0547\n",
      "Epoch 8690, Train Loss: 3.0016, Test Loss: 3.0502\n",
      "Epoch 8691, Train Loss: 2.9949, Test Loss: 3.0548\n",
      "Epoch 8692, Train Loss: 3.0061, Test Loss: 3.0535\n",
      "Epoch 8693, Train Loss: 2.9958, Test Loss: 3.0641\n",
      "Epoch 8694, Train Loss: 3.0001, Test Loss: 3.0585\n",
      "Epoch 8695, Train Loss: 2.9984, Test Loss: 3.0515\n",
      "Epoch 8696, Train Loss: 2.9964, Test Loss: 3.0533\n",
      "Epoch 8697, Train Loss: 3.0012, Test Loss: 3.0513\n",
      "Epoch 8698, Train Loss: 2.9986, Test Loss: 3.0518\n",
      "Epoch 8699, Train Loss: 2.9949, Test Loss: 3.0558\n",
      "Epoch 8700, Train Loss: 2.9998, Test Loss: 3.0555\n",
      "Epoch 8701, Train Loss: 2.9970, Test Loss: 3.0523\n",
      "Epoch 8702, Train Loss: 2.9988, Test Loss: 3.0529\n",
      "Epoch 8703, Train Loss: 2.9947, Test Loss: 3.0528\n",
      "Epoch 8704, Train Loss: 2.9983, Test Loss: 3.0516\n",
      "Epoch 8705, Train Loss: 2.9996, Test Loss: 3.0524\n",
      "Epoch 8706, Train Loss: 2.9980, Test Loss: 3.0571\n",
      "Epoch 8707, Train Loss: 3.0001, Test Loss: 3.0522\n",
      "Epoch 8708, Train Loss: 2.9951, Test Loss: 3.0563\n",
      "Epoch 8709, Train Loss: 3.0069, Test Loss: 3.0553\n",
      "Epoch 8710, Train Loss: 2.9994, Test Loss: 3.0567\n",
      "Epoch 8711, Train Loss: 3.0024, Test Loss: 3.0556\n",
      "Epoch 8712, Train Loss: 3.0017, Test Loss: 3.0581\n",
      "Epoch 8713, Train Loss: 2.9978, Test Loss: 3.0583\n",
      "Epoch 8714, Train Loss: 3.0003, Test Loss: 3.0549\n",
      "Epoch 8715, Train Loss: 2.9995, Test Loss: 3.0529\n",
      "Epoch 8716, Train Loss: 2.9985, Test Loss: 3.0528\n",
      "Epoch 8717, Train Loss: 2.9999, Test Loss: 3.0561\n",
      "Epoch 8718, Train Loss: 3.0043, Test Loss: 3.0520\n",
      "Epoch 8719, Train Loss: 3.0050, Test Loss: 3.0516\n",
      "Epoch 8720, Train Loss: 3.0007, Test Loss: 3.0538\n",
      "Epoch 8721, Train Loss: 3.0018, Test Loss: 3.0649\n",
      "Epoch 8722, Train Loss: 3.0061, Test Loss: 3.0546\n",
      "Epoch 8723, Train Loss: 3.0030, Test Loss: 3.0561\n",
      "Epoch 8724, Train Loss: 3.0101, Test Loss: 3.0538\n",
      "Epoch 8725, Train Loss: 3.0064, Test Loss: 3.0513\n",
      "Epoch 8726, Train Loss: 2.9995, Test Loss: 3.0530\n",
      "Epoch 8727, Train Loss: 3.0071, Test Loss: 3.0517\n",
      "Epoch 8728, Train Loss: 2.9969, Test Loss: 3.0506\n",
      "Epoch 8729, Train Loss: 2.9990, Test Loss: 3.0544\n",
      "Epoch 8730, Train Loss: 3.0060, Test Loss: 3.0524\n",
      "Epoch 8731, Train Loss: 2.9971, Test Loss: 3.0515\n",
      "Epoch 8732, Train Loss: 3.0005, Test Loss: 3.0488\n",
      "Epoch 8733, Train Loss: 2.9998, Test Loss: 3.0505\n",
      "Epoch 8734, Train Loss: 2.9977, Test Loss: 3.0536\n",
      "Epoch 8735, Train Loss: 2.9981, Test Loss: 3.0571\n",
      "Epoch 8736, Train Loss: 2.9993, Test Loss: 3.0503\n",
      "Epoch 8737, Train Loss: 3.0034, Test Loss: 3.0554\n",
      "Epoch 8738, Train Loss: 3.0031, Test Loss: 3.0514\n",
      "Epoch 8739, Train Loss: 3.0065, Test Loss: 3.0528\n",
      "Epoch 8740, Train Loss: 3.0047, Test Loss: 3.0616\n",
      "Epoch 8741, Train Loss: 3.0062, Test Loss: 3.0601\n",
      "Epoch 8742, Train Loss: 3.0087, Test Loss: 3.0527\n",
      "Epoch 8743, Train Loss: 2.9961, Test Loss: 3.0528\n",
      "Epoch 8744, Train Loss: 3.0052, Test Loss: 3.0547\n",
      "Epoch 8745, Train Loss: 3.0009, Test Loss: 3.0608\n",
      "Epoch 8746, Train Loss: 3.0030, Test Loss: 3.0605\n",
      "Epoch 8747, Train Loss: 2.9993, Test Loss: 3.0627\n",
      "Epoch 8748, Train Loss: 3.0122, Test Loss: 3.0539\n",
      "Epoch 8749, Train Loss: 2.9997, Test Loss: 3.0512\n",
      "Epoch 8750, Train Loss: 3.0049, Test Loss: 3.0550\n",
      "Epoch 8751, Train Loss: 3.0005, Test Loss: 3.0551\n",
      "Epoch 8752, Train Loss: 2.9991, Test Loss: 3.0590\n",
      "Epoch 8753, Train Loss: 3.0106, Test Loss: 3.0543\n",
      "Epoch 8754, Train Loss: 2.9998, Test Loss: 3.0585\n",
      "Epoch 8755, Train Loss: 3.0061, Test Loss: 3.0536\n",
      "Epoch 8756, Train Loss: 3.0038, Test Loss: 3.0575\n",
      "Epoch 8757, Train Loss: 3.0041, Test Loss: 3.0536\n",
      "Epoch 8758, Train Loss: 2.9986, Test Loss: 3.0524\n",
      "Epoch 8759, Train Loss: 3.0004, Test Loss: 3.0602\n",
      "Epoch 8760, Train Loss: 3.0097, Test Loss: 3.0589\n",
      "Epoch 8761, Train Loss: 3.0115, Test Loss: 3.0629\n",
      "Epoch 8762, Train Loss: 3.0115, Test Loss: 3.0671\n",
      "Epoch 8763, Train Loss: 3.0095, Test Loss: 3.0700\n",
      "Epoch 8764, Train Loss: 3.0197, Test Loss: 3.0573\n",
      "Epoch 8765, Train Loss: 3.0062, Test Loss: 3.0658\n",
      "Epoch 8766, Train Loss: 3.0151, Test Loss: 3.0599\n",
      "Epoch 8767, Train Loss: 3.0075, Test Loss: 3.0562\n",
      "Epoch 8768, Train Loss: 2.9999, Test Loss: 3.0608\n",
      "Epoch 8769, Train Loss: 3.0118, Test Loss: 3.0590\n",
      "Epoch 8770, Train Loss: 3.0062, Test Loss: 3.0601\n",
      "Epoch 8771, Train Loss: 3.0108, Test Loss: 3.0578\n",
      "Epoch 8772, Train Loss: 3.0109, Test Loss: 3.0610\n",
      "Epoch 8773, Train Loss: 3.0088, Test Loss: 3.0708\n",
      "Epoch 8774, Train Loss: 3.0143, Test Loss: 3.0649\n",
      "Epoch 8775, Train Loss: 3.0147, Test Loss: 3.0607\n",
      "Epoch 8776, Train Loss: 3.0132, Test Loss: 3.0583\n",
      "Epoch 8777, Train Loss: 3.0081, Test Loss: 3.0628\n",
      "Epoch 8778, Train Loss: 3.0108, Test Loss: 3.0573\n",
      "Epoch 8779, Train Loss: 3.0116, Test Loss: 3.0673\n",
      "Epoch 8780, Train Loss: 3.0093, Test Loss: 3.0687\n",
      "Epoch 8781, Train Loss: 3.0226, Test Loss: 3.0681\n",
      "Epoch 8782, Train Loss: 3.0136, Test Loss: 3.0700\n",
      "Epoch 8783, Train Loss: 3.0152, Test Loss: 3.0658\n",
      "Epoch 8784, Train Loss: 3.0188, Test Loss: 3.0531\n",
      "Epoch 8785, Train Loss: 3.0146, Test Loss: 3.0802\n",
      "Epoch 8786, Train Loss: 3.0216, Test Loss: 3.0643\n",
      "Epoch 8787, Train Loss: 3.0067, Test Loss: 3.0691\n",
      "Epoch 8788, Train Loss: 3.0291, Test Loss: 3.0609\n",
      "Epoch 8789, Train Loss: 3.0095, Test Loss: 3.0798\n",
      "Epoch 8790, Train Loss: 3.0286, Test Loss: 3.0569\n",
      "Epoch 8791, Train Loss: 3.0133, Test Loss: 3.0570\n",
      "Epoch 8792, Train Loss: 3.0124, Test Loss: 3.0575\n",
      "Epoch 8793, Train Loss: 3.0090, Test Loss: 3.0707\n",
      "Epoch 8794, Train Loss: 3.0347, Test Loss: 3.0640\n",
      "Epoch 8795, Train Loss: 3.0251, Test Loss: 3.0700\n",
      "Epoch 8796, Train Loss: 3.0224, Test Loss: 3.0734\n",
      "Epoch 8797, Train Loss: 3.0142, Test Loss: 3.0904\n",
      "Epoch 8798, Train Loss: 3.0237, Test Loss: 3.0777\n",
      "Epoch 8799, Train Loss: 3.0203, Test Loss: 3.0644\n",
      "Epoch 8800, Train Loss: 3.0160, Test Loss: 3.0627\n",
      "Epoch 8801, Train Loss: 3.0066, Test Loss: 3.0548\n",
      "Epoch 8802, Train Loss: 3.0106, Test Loss: 3.0502\n",
      "Epoch 8803, Train Loss: 3.0097, Test Loss: 3.0644\n",
      "Epoch 8804, Train Loss: 3.0048, Test Loss: 3.0750\n",
      "Epoch 8805, Train Loss: 3.0238, Test Loss: 3.0731\n",
      "Epoch 8806, Train Loss: 3.0147, Test Loss: 3.0636\n",
      "Epoch 8807, Train Loss: 3.0157, Test Loss: 3.0580\n",
      "Epoch 8808, Train Loss: 3.0140, Test Loss: 3.0484\n",
      "Epoch 8809, Train Loss: 2.9968, Test Loss: 3.0612\n",
      "Epoch 8810, Train Loss: 3.0168, Test Loss: 3.0590\n",
      "Epoch 8811, Train Loss: 3.0163, Test Loss: 3.0613\n",
      "Epoch 8812, Train Loss: 3.0208, Test Loss: 3.0559\n",
      "Epoch 8813, Train Loss: 3.0113, Test Loss: 3.0566\n",
      "Epoch 8814, Train Loss: 3.0170, Test Loss: 3.0518\n",
      "Epoch 8815, Train Loss: 3.0032, Test Loss: 3.0626\n",
      "Epoch 8816, Train Loss: 3.0169, Test Loss: 3.0547\n",
      "Epoch 8817, Train Loss: 3.0045, Test Loss: 3.0688\n",
      "Epoch 8818, Train Loss: 3.0208, Test Loss: 3.0607\n",
      "Epoch 8819, Train Loss: 3.0119, Test Loss: 3.0689\n",
      "Epoch 8820, Train Loss: 3.0164, Test Loss: 3.0648\n",
      "Epoch 8821, Train Loss: 3.0120, Test Loss: 3.0518\n",
      "Epoch 8822, Train Loss: 3.0085, Test Loss: 3.0587\n",
      "Epoch 8823, Train Loss: 3.0125, Test Loss: 3.0611\n",
      "Epoch 8824, Train Loss: 3.0101, Test Loss: 3.0631\n",
      "Epoch 8825, Train Loss: 3.0127, Test Loss: 3.0701\n",
      "Epoch 8826, Train Loss: 3.0045, Test Loss: 3.0695\n",
      "Epoch 8827, Train Loss: 3.0074, Test Loss: 3.0525\n",
      "Epoch 8828, Train Loss: 3.0136, Test Loss: 3.0557\n",
      "Epoch 8829, Train Loss: 3.0176, Test Loss: 3.0607\n",
      "Epoch 8830, Train Loss: 3.0168, Test Loss: 3.0689\n",
      "Epoch 8831, Train Loss: 3.0113, Test Loss: 3.0707\n",
      "Epoch 8832, Train Loss: 3.0132, Test Loss: 3.0615\n",
      "Epoch 8833, Train Loss: 3.0123, Test Loss: 3.0508\n",
      "Epoch 8834, Train Loss: 3.0097, Test Loss: 3.0597\n",
      "Epoch 8835, Train Loss: 3.0158, Test Loss: 3.0596\n",
      "Epoch 8836, Train Loss: 3.0067, Test Loss: 3.0639\n",
      "Epoch 8837, Train Loss: 3.0139, Test Loss: 3.0569\n",
      "Epoch 8838, Train Loss: 3.0018, Test Loss: 3.0666\n",
      "Epoch 8839, Train Loss: 3.0071, Test Loss: 3.0623\n",
      "Epoch 8840, Train Loss: 3.0090, Test Loss: 3.0575\n",
      "Epoch 8841, Train Loss: 3.0122, Test Loss: 3.0575\n",
      "Epoch 8842, Train Loss: 3.0038, Test Loss: 3.0583\n",
      "Epoch 8843, Train Loss: 3.0012, Test Loss: 3.0647\n",
      "Epoch 8844, Train Loss: 3.0105, Test Loss: 3.0609\n",
      "Epoch 8845, Train Loss: 3.0017, Test Loss: 3.0610\n",
      "Epoch 8846, Train Loss: 3.0061, Test Loss: 3.0608\n",
      "Epoch 8847, Train Loss: 3.0091, Test Loss: 3.0633\n",
      "Epoch 8848, Train Loss: 3.0131, Test Loss: 3.0553\n",
      "Epoch 8849, Train Loss: 3.0136, Test Loss: 3.0586\n",
      "Epoch 8850, Train Loss: 3.0118, Test Loss: 3.0651\n",
      "Epoch 8851, Train Loss: 3.0084, Test Loss: 3.0596\n",
      "Epoch 8852, Train Loss: 2.9986, Test Loss: 3.0519\n",
      "Epoch 8853, Train Loss: 3.0003, Test Loss: 3.0526\n",
      "Epoch 8854, Train Loss: 3.0028, Test Loss: 3.0563\n",
      "Epoch 8855, Train Loss: 3.0122, Test Loss: 3.0515\n",
      "Epoch 8856, Train Loss: 3.0052, Test Loss: 3.0535\n",
      "Epoch 8857, Train Loss: 2.9975, Test Loss: 3.0650\n",
      "Epoch 8858, Train Loss: 3.0096, Test Loss: 3.0660\n",
      "Epoch 8859, Train Loss: 3.0096, Test Loss: 3.0612\n",
      "Epoch 8860, Train Loss: 3.0028, Test Loss: 3.0522\n",
      "Epoch 8861, Train Loss: 2.9983, Test Loss: 3.0548\n",
      "Epoch 8862, Train Loss: 2.9996, Test Loss: 3.0537\n",
      "Epoch 8863, Train Loss: 3.0043, Test Loss: 3.0603\n",
      "Epoch 8864, Train Loss: 3.0075, Test Loss: 3.0597\n",
      "Epoch 8865, Train Loss: 3.0033, Test Loss: 3.0672\n",
      "Epoch 8866, Train Loss: 3.0090, Test Loss: 3.0732\n",
      "Epoch 8867, Train Loss: 3.0115, Test Loss: 3.0702\n",
      "Epoch 8868, Train Loss: 3.0163, Test Loss: 3.0705\n",
      "Epoch 8869, Train Loss: 3.0146, Test Loss: 3.0605\n",
      "Epoch 8870, Train Loss: 3.0028, Test Loss: 3.0594\n",
      "Epoch 8871, Train Loss: 3.0024, Test Loss: 3.0583\n",
      "Epoch 8872, Train Loss: 3.0062, Test Loss: 3.0639\n",
      "Epoch 8873, Train Loss: 3.0190, Test Loss: 3.0573\n",
      "Epoch 8874, Train Loss: 3.0042, Test Loss: 3.0631\n",
      "Epoch 8875, Train Loss: 3.0056, Test Loss: 3.0620\n",
      "Epoch 8876, Train Loss: 3.0101, Test Loss: 3.0597\n",
      "Epoch 8877, Train Loss: 3.0202, Test Loss: 3.0599\n",
      "Epoch 8878, Train Loss: 3.0020, Test Loss: 3.0746\n",
      "Epoch 8879, Train Loss: 3.0134, Test Loss: 3.0735\n",
      "Epoch 8880, Train Loss: 3.0109, Test Loss: 3.0716\n",
      "Epoch 8881, Train Loss: 3.0170, Test Loss: 3.0627\n",
      "Epoch 8882, Train Loss: 3.0105, Test Loss: 3.0536\n",
      "Epoch 8883, Train Loss: 2.9999, Test Loss: 3.0647\n",
      "Epoch 8884, Train Loss: 3.0117, Test Loss: 3.0601\n",
      "Epoch 8885, Train Loss: 3.0080, Test Loss: 3.0557\n",
      "Epoch 8886, Train Loss: 3.0052, Test Loss: 3.0548\n",
      "Epoch 8887, Train Loss: 3.0107, Test Loss: 3.0666\n",
      "Epoch 8888, Train Loss: 3.0080, Test Loss: 3.0674\n",
      "Epoch 8889, Train Loss: 3.0121, Test Loss: 3.0612\n",
      "Epoch 8890, Train Loss: 3.0070, Test Loss: 3.0657\n",
      "Epoch 8891, Train Loss: 3.0138, Test Loss: 3.0598\n",
      "Epoch 8892, Train Loss: 3.0120, Test Loss: 3.0586\n",
      "Epoch 8893, Train Loss: 3.0302, Test Loss: 3.0534\n",
      "Epoch 8894, Train Loss: 3.0038, Test Loss: 3.0624\n",
      "Epoch 8895, Train Loss: 3.0161, Test Loss: 3.0652\n",
      "Epoch 8896, Train Loss: 3.0206, Test Loss: 3.0591\n",
      "Epoch 8897, Train Loss: 3.0143, Test Loss: 3.0559\n",
      "Epoch 8898, Train Loss: 3.0082, Test Loss: 3.0523\n",
      "Epoch 8899, Train Loss: 3.0010, Test Loss: 3.0564\n",
      "Epoch 8900, Train Loss: 3.0186, Test Loss: 3.0626\n",
      "Epoch 8901, Train Loss: 3.0179, Test Loss: 3.0571\n",
      "Epoch 8902, Train Loss: 2.9989, Test Loss: 3.0659\n",
      "Epoch 8903, Train Loss: 3.0099, Test Loss: 3.0627\n",
      "Epoch 8904, Train Loss: 3.0048, Test Loss: 3.0657\n",
      "Epoch 8905, Train Loss: 3.0122, Test Loss: 3.0534\n",
      "Epoch 8906, Train Loss: 3.0059, Test Loss: 3.0531\n",
      "Epoch 8907, Train Loss: 3.0057, Test Loss: 3.0514\n",
      "Epoch 8908, Train Loss: 3.0105, Test Loss: 3.0566\n",
      "Epoch 8909, Train Loss: 3.0093, Test Loss: 3.0590\n",
      "Epoch 8910, Train Loss: 3.0050, Test Loss: 3.0621\n",
      "Epoch 8911, Train Loss: 3.0095, Test Loss: 3.0596\n",
      "Epoch 8912, Train Loss: 2.9980, Test Loss: 3.0554\n",
      "Epoch 8913, Train Loss: 3.0000, Test Loss: 3.0576\n",
      "Epoch 8914, Train Loss: 3.0027, Test Loss: 3.0525\n",
      "Epoch 8915, Train Loss: 3.0033, Test Loss: 3.0512\n",
      "Epoch 8916, Train Loss: 3.0009, Test Loss: 3.0479\n",
      "Epoch 8917, Train Loss: 2.9959, Test Loss: 3.0521\n",
      "Epoch 8918, Train Loss: 3.0083, Test Loss: 3.0530\n",
      "Epoch 8919, Train Loss: 2.9964, Test Loss: 3.0594\n",
      "Epoch 8920, Train Loss: 3.0017, Test Loss: 3.0578\n",
      "Epoch 8921, Train Loss: 3.0055, Test Loss: 3.0550\n",
      "Epoch 8922, Train Loss: 3.0015, Test Loss: 3.0521\n",
      "Epoch 8923, Train Loss: 3.0008, Test Loss: 3.0550\n",
      "Epoch 8924, Train Loss: 3.0025, Test Loss: 3.0603\n",
      "Epoch 8925, Train Loss: 3.0123, Test Loss: 3.0512\n",
      "Epoch 8926, Train Loss: 2.9941, Test Loss: 3.0610\n",
      "Epoch 8927, Train Loss: 3.0078, Test Loss: 3.0582\n",
      "Epoch 8928, Train Loss: 3.0034, Test Loss: 3.0523\n",
      "Epoch 8929, Train Loss: 3.0048, Test Loss: 3.0516\n",
      "Epoch 8930, Train Loss: 2.9995, Test Loss: 3.0555\n",
      "Epoch 8931, Train Loss: 2.9984, Test Loss: 3.0536\n",
      "Epoch 8932, Train Loss: 2.9974, Test Loss: 3.0530\n",
      "Epoch 8933, Train Loss: 2.9978, Test Loss: 3.0495\n",
      "Epoch 8934, Train Loss: 3.0000, Test Loss: 3.0554\n",
      "Epoch 8935, Train Loss: 3.0038, Test Loss: 3.0550\n",
      "Epoch 8936, Train Loss: 3.0020, Test Loss: 3.0620\n",
      "Epoch 8937, Train Loss: 3.0107, Test Loss: 3.0708\n",
      "Epoch 8938, Train Loss: 3.0090, Test Loss: 3.0718\n",
      "Epoch 8939, Train Loss: 3.0127, Test Loss: 3.0667\n",
      "Epoch 8940, Train Loss: 3.0164, Test Loss: 3.0598\n",
      "Epoch 8941, Train Loss: 3.0071, Test Loss: 3.0607\n",
      "Epoch 8942, Train Loss: 3.0161, Test Loss: 3.0560\n",
      "Epoch 8943, Train Loss: 3.0125, Test Loss: 3.0561\n",
      "Epoch 8944, Train Loss: 3.0112, Test Loss: 3.0598\n",
      "Epoch 8945, Train Loss: 3.0118, Test Loss: 3.0646\n",
      "Epoch 8946, Train Loss: 3.0149, Test Loss: 3.0670\n",
      "Epoch 8947, Train Loss: 3.0110, Test Loss: 3.0569\n",
      "Epoch 8948, Train Loss: 3.0013, Test Loss: 3.0664\n",
      "Epoch 8949, Train Loss: 3.0132, Test Loss: 3.0532\n",
      "Epoch 8950, Train Loss: 3.0043, Test Loss: 3.0548\n",
      "Epoch 8951, Train Loss: 3.0108, Test Loss: 3.0508\n",
      "Epoch 8952, Train Loss: 3.0018, Test Loss: 3.0543\n",
      "Epoch 8953, Train Loss: 3.0013, Test Loss: 3.0633\n",
      "Epoch 8954, Train Loss: 3.0033, Test Loss: 3.0703\n",
      "Epoch 8955, Train Loss: 3.0103, Test Loss: 3.0646\n",
      "Epoch 8956, Train Loss: 3.0047, Test Loss: 3.0557\n",
      "Epoch 8957, Train Loss: 3.0114, Test Loss: 3.0529\n",
      "Epoch 8958, Train Loss: 3.0088, Test Loss: 3.0648\n",
      "Epoch 8959, Train Loss: 3.0115, Test Loss: 3.0689\n",
      "Epoch 8960, Train Loss: 3.0071, Test Loss: 3.0628\n",
      "Epoch 8961, Train Loss: 3.0018, Test Loss: 3.0540\n",
      "Epoch 8962, Train Loss: 3.0072, Test Loss: 3.0587\n",
      "Epoch 8963, Train Loss: 3.0047, Test Loss: 3.0612\n",
      "Epoch 8964, Train Loss: 3.0055, Test Loss: 3.0539\n",
      "Epoch 8965, Train Loss: 3.0037, Test Loss: 3.0535\n",
      "Epoch 8966, Train Loss: 2.9989, Test Loss: 3.0545\n",
      "Epoch 8967, Train Loss: 3.0018, Test Loss: 3.0667\n",
      "Epoch 8968, Train Loss: 3.0152, Test Loss: 3.0567\n",
      "Epoch 8969, Train Loss: 3.0045, Test Loss: 3.0554\n",
      "Epoch 8970, Train Loss: 3.0110, Test Loss: 3.0472\n",
      "Epoch 8971, Train Loss: 3.0039, Test Loss: 3.0669\n",
      "Epoch 8972, Train Loss: 3.0275, Test Loss: 3.0626\n",
      "Epoch 8973, Train Loss: 3.0083, Test Loss: 3.0802\n",
      "Epoch 8974, Train Loss: 3.0240, Test Loss: 3.0687\n",
      "Epoch 8975, Train Loss: 3.0089, Test Loss: 3.0689\n",
      "Epoch 8976, Train Loss: 3.0167, Test Loss: 3.0674\n",
      "Epoch 8977, Train Loss: 3.0251, Test Loss: 3.0610\n",
      "Epoch 8978, Train Loss: 3.0092, Test Loss: 3.0759\n",
      "Epoch 8979, Train Loss: 3.0323, Test Loss: 3.0564\n",
      "Epoch 8980, Train Loss: 3.0153, Test Loss: 3.0832\n",
      "Epoch 8981, Train Loss: 3.0272, Test Loss: 3.0869\n",
      "Epoch 8982, Train Loss: 3.0351, Test Loss: 3.0815\n",
      "Epoch 8983, Train Loss: 3.0300, Test Loss: 3.0603\n",
      "Epoch 8984, Train Loss: 3.0057, Test Loss: 3.0578\n",
      "Epoch 8985, Train Loss: 3.0140, Test Loss: 3.0602\n",
      "Epoch 8986, Train Loss: 3.0281, Test Loss: 3.0506\n",
      "Epoch 8987, Train Loss: 3.0045, Test Loss: 3.0536\n",
      "Epoch 8988, Train Loss: 3.0035, Test Loss: 3.0632\n",
      "Epoch 8989, Train Loss: 3.0095, Test Loss: 3.0666\n",
      "Epoch 8990, Train Loss: 3.0102, Test Loss: 3.0614\n",
      "Epoch 8991, Train Loss: 3.0094, Test Loss: 3.0606\n",
      "Epoch 8992, Train Loss: 3.0059, Test Loss: 3.0583\n",
      "Epoch 8993, Train Loss: 3.0015, Test Loss: 3.0536\n",
      "Epoch 8994, Train Loss: 3.0079, Test Loss: 3.0493\n",
      "Epoch 8995, Train Loss: 3.0023, Test Loss: 3.0535\n",
      "Epoch 8996, Train Loss: 3.0021, Test Loss: 3.0593\n",
      "Epoch 8997, Train Loss: 2.9990, Test Loss: 3.0585\n",
      "Epoch 8998, Train Loss: 3.0003, Test Loss: 3.0562\n",
      "Epoch 8999, Train Loss: 3.0000, Test Loss: 3.0535\n",
      "Epoch 9000, Train Loss: 2.9992, Test Loss: 3.0553\n",
      "Epoch 9001, Train Loss: 3.0063, Test Loss: 3.0534\n",
      "Epoch 9002, Train Loss: 3.0002, Test Loss: 3.0569\n",
      "Epoch 9003, Train Loss: 3.0031, Test Loss: 3.0525\n",
      "Epoch 9004, Train Loss: 2.9982, Test Loss: 3.0578\n",
      "Epoch 9005, Train Loss: 3.0103, Test Loss: 3.0563\n",
      "Epoch 9006, Train Loss: 3.0030, Test Loss: 3.0525\n",
      "Epoch 9007, Train Loss: 2.9974, Test Loss: 3.0541\n",
      "Epoch 9008, Train Loss: 3.0025, Test Loss: 3.0514\n",
      "Epoch 9009, Train Loss: 2.9975, Test Loss: 3.0520\n",
      "Epoch 9010, Train Loss: 2.9961, Test Loss: 3.0519\n",
      "Epoch 9011, Train Loss: 2.9944, Test Loss: 3.0509\n",
      "Epoch 9012, Train Loss: 2.9944, Test Loss: 3.0524\n",
      "Epoch 9013, Train Loss: 2.9974, Test Loss: 3.0537\n",
      "Epoch 9014, Train Loss: 2.9985, Test Loss: 3.0538\n",
      "Epoch 9015, Train Loss: 2.9982, Test Loss: 3.0550\n",
      "Epoch 9016, Train Loss: 3.0009, Test Loss: 3.0534\n",
      "Epoch 9017, Train Loss: 2.9947, Test Loss: 3.0510\n",
      "Epoch 9018, Train Loss: 2.9948, Test Loss: 3.0564\n",
      "Epoch 9019, Train Loss: 3.0028, Test Loss: 3.0466\n",
      "Epoch 9020, Train Loss: 2.9944, Test Loss: 3.0536\n",
      "Epoch 9021, Train Loss: 3.0009, Test Loss: 3.0478\n",
      "Epoch 9022, Train Loss: 2.9977, Test Loss: 3.0518\n",
      "Epoch 9023, Train Loss: 2.9981, Test Loss: 3.0574\n",
      "Epoch 9024, Train Loss: 2.9974, Test Loss: 3.0581\n",
      "Epoch 9025, Train Loss: 3.0072, Test Loss: 3.0561\n",
      "Epoch 9026, Train Loss: 2.9997, Test Loss: 3.0540\n",
      "Epoch 9027, Train Loss: 2.9970, Test Loss: 3.0535\n",
      "Epoch 9028, Train Loss: 2.9963, Test Loss: 3.0501\n",
      "Epoch 9029, Train Loss: 2.9961, Test Loss: 3.0534\n",
      "Epoch 9030, Train Loss: 3.0024, Test Loss: 3.0550\n",
      "Epoch 9031, Train Loss: 2.9994, Test Loss: 3.0588\n",
      "Epoch 9032, Train Loss: 3.0014, Test Loss: 3.0596\n",
      "Epoch 9033, Train Loss: 3.0017, Test Loss: 3.0610\n",
      "Epoch 9034, Train Loss: 3.0036, Test Loss: 3.0545\n",
      "Epoch 9035, Train Loss: 2.9993, Test Loss: 3.0493\n",
      "Epoch 9036, Train Loss: 3.0014, Test Loss: 3.0525\n",
      "Epoch 9037, Train Loss: 2.9997, Test Loss: 3.0516\n",
      "Epoch 9038, Train Loss: 3.0014, Test Loss: 3.0504\n",
      "Epoch 9039, Train Loss: 2.9962, Test Loss: 3.0506\n",
      "Epoch 9040, Train Loss: 2.9981, Test Loss: 3.0537\n",
      "Epoch 9041, Train Loss: 3.0014, Test Loss: 3.0481\n",
      "Epoch 9042, Train Loss: 3.0000, Test Loss: 3.0540\n",
      "Epoch 9043, Train Loss: 3.0116, Test Loss: 3.0570\n",
      "Epoch 9044, Train Loss: 3.0032, Test Loss: 3.0596\n",
      "Epoch 9045, Train Loss: 2.9980, Test Loss: 3.0574\n",
      "Epoch 9046, Train Loss: 3.0002, Test Loss: 3.0521\n",
      "Epoch 9047, Train Loss: 3.0031, Test Loss: 3.0500\n",
      "Epoch 9048, Train Loss: 2.9988, Test Loss: 3.0504\n",
      "Epoch 9049, Train Loss: 3.0035, Test Loss: 3.0497\n",
      "Epoch 9050, Train Loss: 2.9960, Test Loss: 3.0586\n",
      "Epoch 9051, Train Loss: 3.0026, Test Loss: 3.0569\n",
      "Epoch 9052, Train Loss: 3.0026, Test Loss: 3.0577\n",
      "Epoch 9053, Train Loss: 3.0067, Test Loss: 3.0558\n",
      "Epoch 9054, Train Loss: 3.0087, Test Loss: 3.0551\n",
      "Epoch 9055, Train Loss: 3.0010, Test Loss: 3.0614\n",
      "Epoch 9056, Train Loss: 3.0035, Test Loss: 3.0596\n",
      "Epoch 9057, Train Loss: 3.0005, Test Loss: 3.0630\n",
      "Epoch 9058, Train Loss: 3.0134, Test Loss: 3.0539\n",
      "Epoch 9059, Train Loss: 3.0079, Test Loss: 3.0566\n",
      "Epoch 9060, Train Loss: 3.0054, Test Loss: 3.0623\n",
      "Epoch 9061, Train Loss: 3.0197, Test Loss: 3.0528\n",
      "Epoch 9062, Train Loss: 3.0082, Test Loss: 3.0727\n",
      "Epoch 9063, Train Loss: 3.0241, Test Loss: 3.0634\n",
      "Epoch 9064, Train Loss: 3.0107, Test Loss: 3.0641\n",
      "Epoch 9065, Train Loss: 3.0171, Test Loss: 3.0527\n",
      "Epoch 9066, Train Loss: 2.9996, Test Loss: 3.0573\n",
      "Epoch 9067, Train Loss: 3.0137, Test Loss: 3.0530\n",
      "Epoch 9068, Train Loss: 3.0048, Test Loss: 3.0548\n",
      "Epoch 9069, Train Loss: 3.0018, Test Loss: 3.0643\n",
      "Epoch 9070, Train Loss: 3.0174, Test Loss: 3.0598\n",
      "Epoch 9071, Train Loss: 3.0124, Test Loss: 3.0555\n",
      "Epoch 9072, Train Loss: 3.0085, Test Loss: 3.0597\n",
      "Epoch 9073, Train Loss: 3.0057, Test Loss: 3.0707\n",
      "Epoch 9074, Train Loss: 3.0078, Test Loss: 3.0645\n",
      "Epoch 9075, Train Loss: 3.0126, Test Loss: 3.0637\n",
      "Epoch 9076, Train Loss: 3.0092, Test Loss: 3.0702\n",
      "Epoch 9077, Train Loss: 3.0127, Test Loss: 3.0560\n",
      "Epoch 9078, Train Loss: 3.0059, Test Loss: 3.0512\n",
      "Epoch 9079, Train Loss: 3.0031, Test Loss: 3.0518\n",
      "Epoch 9080, Train Loss: 2.9991, Test Loss: 3.0528\n",
      "Epoch 9081, Train Loss: 3.0033, Test Loss: 3.0543\n",
      "Epoch 9082, Train Loss: 3.0019, Test Loss: 3.0545\n",
      "Epoch 9083, Train Loss: 3.0007, Test Loss: 3.0522\n",
      "Epoch 9084, Train Loss: 2.9940, Test Loss: 3.0508\n",
      "Epoch 9085, Train Loss: 3.0079, Test Loss: 3.0503\n",
      "Epoch 9086, Train Loss: 3.0006, Test Loss: 3.0518\n",
      "Epoch 9087, Train Loss: 3.0013, Test Loss: 3.0599\n",
      "Epoch 9088, Train Loss: 3.0039, Test Loss: 3.0529\n",
      "Epoch 9089, Train Loss: 2.9972, Test Loss: 3.0552\n",
      "Epoch 9090, Train Loss: 2.9980, Test Loss: 3.0514\n",
      "Epoch 9091, Train Loss: 3.0052, Test Loss: 3.0578\n",
      "Epoch 9092, Train Loss: 3.0017, Test Loss: 3.0645\n",
      "Epoch 9093, Train Loss: 3.0078, Test Loss: 3.0683\n",
      "Epoch 9094, Train Loss: 2.9986, Test Loss: 3.0589\n",
      "Epoch 9095, Train Loss: 3.0025, Test Loss: 3.0582\n",
      "Epoch 9096, Train Loss: 3.0064, Test Loss: 3.0603\n",
      "Epoch 9097, Train Loss: 3.0097, Test Loss: 3.0637\n",
      "Epoch 9098, Train Loss: 3.0108, Test Loss: 3.0596\n",
      "Epoch 9099, Train Loss: 3.0051, Test Loss: 3.0582\n",
      "Epoch 9100, Train Loss: 3.0088, Test Loss: 3.0549\n",
      "Epoch 9101, Train Loss: 3.0150, Test Loss: 3.0581\n",
      "Epoch 9102, Train Loss: 3.0117, Test Loss: 3.0750\n",
      "Epoch 9103, Train Loss: 3.0241, Test Loss: 3.0646\n",
      "Epoch 9104, Train Loss: 3.0149, Test Loss: 3.0594\n",
      "Epoch 9105, Train Loss: 3.0158, Test Loss: 3.0718\n",
      "Epoch 9106, Train Loss: 3.0291, Test Loss: 3.0632\n",
      "Epoch 9107, Train Loss: 3.0103, Test Loss: 3.0698\n",
      "Epoch 9108, Train Loss: 3.0195, Test Loss: 3.0573\n",
      "Epoch 9109, Train Loss: 3.0159, Test Loss: 3.0721\n",
      "Epoch 9110, Train Loss: 3.0326, Test Loss: 3.0649\n",
      "Epoch 9111, Train Loss: 3.0194, Test Loss: 3.0710\n",
      "Epoch 9112, Train Loss: 3.0138, Test Loss: 3.0777\n",
      "Epoch 9113, Train Loss: 3.0250, Test Loss: 3.0680\n",
      "Epoch 9114, Train Loss: 3.0300, Test Loss: 3.0610\n",
      "Epoch 9115, Train Loss: 3.0064, Test Loss: 3.0606\n",
      "Epoch 9116, Train Loss: 3.0102, Test Loss: 3.0562\n",
      "Epoch 9117, Train Loss: 3.0102, Test Loss: 3.0606\n",
      "Epoch 9118, Train Loss: 3.0086, Test Loss: 3.0614\n",
      "Epoch 9119, Train Loss: 3.0055, Test Loss: 3.0620\n",
      "Epoch 9120, Train Loss: 3.0046, Test Loss: 3.0579\n",
      "Epoch 9121, Train Loss: 3.0095, Test Loss: 3.0586\n",
      "Epoch 9122, Train Loss: 2.9984, Test Loss: 3.0547\n",
      "Epoch 9123, Train Loss: 2.9943, Test Loss: 3.0592\n",
      "Epoch 9124, Train Loss: 3.0074, Test Loss: 3.0547\n",
      "Epoch 9125, Train Loss: 2.9972, Test Loss: 3.0493\n",
      "Epoch 9126, Train Loss: 2.9951, Test Loss: 3.0517\n",
      "Epoch 9127, Train Loss: 2.9934, Test Loss: 3.0557\n",
      "Epoch 9128, Train Loss: 2.9951, Test Loss: 3.0582\n",
      "Epoch 9129, Train Loss: 3.0042, Test Loss: 3.0550\n",
      "Epoch 9130, Train Loss: 3.0008, Test Loss: 3.0528\n",
      "Epoch 9131, Train Loss: 2.9966, Test Loss: 3.0522\n",
      "Epoch 9132, Train Loss: 3.0002, Test Loss: 3.0508\n",
      "Epoch 9133, Train Loss: 2.9982, Test Loss: 3.0503\n",
      "Epoch 9134, Train Loss: 2.9953, Test Loss: 3.0552\n",
      "Epoch 9135, Train Loss: 3.0031, Test Loss: 3.0548\n",
      "Epoch 9136, Train Loss: 2.9957, Test Loss: 3.0599\n",
      "Epoch 9137, Train Loss: 3.0017, Test Loss: 3.0638\n",
      "Epoch 9138, Train Loss: 3.0040, Test Loss: 3.0586\n",
      "Epoch 9139, Train Loss: 2.9983, Test Loss: 3.0554\n",
      "Epoch 9140, Train Loss: 3.0030, Test Loss: 3.0521\n",
      "Epoch 9141, Train Loss: 2.9988, Test Loss: 3.0528\n",
      "Epoch 9142, Train Loss: 3.0042, Test Loss: 3.0577\n",
      "Epoch 9143, Train Loss: 3.0067, Test Loss: 3.0590\n",
      "Epoch 9144, Train Loss: 2.9957, Test Loss: 3.0577\n",
      "Epoch 9145, Train Loss: 3.0053, Test Loss: 3.0535\n",
      "Epoch 9146, Train Loss: 2.9993, Test Loss: 3.0522\n",
      "Epoch 9147, Train Loss: 3.0047, Test Loss: 3.0493\n",
      "Epoch 9148, Train Loss: 2.9972, Test Loss: 3.0530\n",
      "Epoch 9149, Train Loss: 3.0010, Test Loss: 3.0545\n",
      "Epoch 9150, Train Loss: 2.9978, Test Loss: 3.0539\n",
      "Epoch 9151, Train Loss: 3.0003, Test Loss: 3.0510\n",
      "Epoch 9152, Train Loss: 2.9987, Test Loss: 3.0578\n",
      "Epoch 9153, Train Loss: 3.0065, Test Loss: 3.0522\n",
      "Epoch 9154, Train Loss: 2.9989, Test Loss: 3.0543\n",
      "Epoch 9155, Train Loss: 3.0085, Test Loss: 3.0564\n",
      "Epoch 9156, Train Loss: 2.9958, Test Loss: 3.0674\n",
      "Epoch 9157, Train Loss: 3.0198, Test Loss: 3.0573\n",
      "Epoch 9158, Train Loss: 2.9977, Test Loss: 3.0602\n",
      "Epoch 9159, Train Loss: 3.0097, Test Loss: 3.0474\n",
      "Epoch 9160, Train Loss: 2.9990, Test Loss: 3.0590\n",
      "Epoch 9161, Train Loss: 3.0171, Test Loss: 3.0564\n",
      "Epoch 9162, Train Loss: 2.9972, Test Loss: 3.0701\n",
      "Epoch 9163, Train Loss: 3.0040, Test Loss: 3.0673\n",
      "Epoch 9164, Train Loss: 3.0007, Test Loss: 3.0627\n",
      "Epoch 9165, Train Loss: 3.0137, Test Loss: 3.0570\n",
      "Epoch 9166, Train Loss: 2.9984, Test Loss: 3.0520\n",
      "Epoch 9167, Train Loss: 3.0005, Test Loss: 3.0498\n",
      "Epoch 9168, Train Loss: 2.9977, Test Loss: 3.0524\n",
      "Epoch 9169, Train Loss: 3.0005, Test Loss: 3.0513\n",
      "Epoch 9170, Train Loss: 2.9930, Test Loss: 3.0487\n",
      "Epoch 9171, Train Loss: 2.9955, Test Loss: 3.0506\n",
      "Epoch 9172, Train Loss: 2.9977, Test Loss: 3.0527\n",
      "Epoch 9173, Train Loss: 2.9957, Test Loss: 3.0546\n",
      "Epoch 9174, Train Loss: 2.9971, Test Loss: 3.0525\n",
      "Epoch 9175, Train Loss: 2.9992, Test Loss: 3.0518\n",
      "Epoch 9176, Train Loss: 3.0039, Test Loss: 3.0530\n",
      "Epoch 9177, Train Loss: 3.0011, Test Loss: 3.0511\n",
      "Epoch 9178, Train Loss: 3.0034, Test Loss: 3.0537\n",
      "Epoch 9179, Train Loss: 2.9944, Test Loss: 3.0609\n",
      "Epoch 9180, Train Loss: 3.0035, Test Loss: 3.0505\n",
      "Epoch 9181, Train Loss: 2.9969, Test Loss: 3.0519\n",
      "Epoch 9182, Train Loss: 3.0002, Test Loss: 3.0521\n",
      "Epoch 9183, Train Loss: 2.9964, Test Loss: 3.0563\n",
      "Epoch 9184, Train Loss: 2.9984, Test Loss: 3.0536\n",
      "Epoch 9185, Train Loss: 2.9974, Test Loss: 3.0549\n",
      "Epoch 9186, Train Loss: 2.9918, Test Loss: 3.0579\n",
      "Epoch 9187, Train Loss: 3.0025, Test Loss: 3.0500\n",
      "Epoch 9188, Train Loss: 2.9921, Test Loss: 3.0519\n",
      "Epoch 9189, Train Loss: 3.0071, Test Loss: 3.0517\n",
      "Epoch 9190, Train Loss: 3.0000, Test Loss: 3.0535\n",
      "Epoch 9191, Train Loss: 3.0064, Test Loss: 3.0572\n",
      "Epoch 9192, Train Loss: 3.0040, Test Loss: 3.0674\n",
      "Epoch 9193, Train Loss: 3.0021, Test Loss: 3.0621\n",
      "Epoch 9194, Train Loss: 2.9952, Test Loss: 3.0605\n",
      "Epoch 9195, Train Loss: 3.0031, Test Loss: 3.0601\n",
      "Epoch 9196, Train Loss: 2.9992, Test Loss: 3.0668\n",
      "Epoch 9197, Train Loss: 3.0055, Test Loss: 3.0600\n",
      "Epoch 9198, Train Loss: 3.0019, Test Loss: 3.0535\n",
      "Epoch 9199, Train Loss: 3.0073, Test Loss: 3.0518\n",
      "Epoch 9200, Train Loss: 3.0021, Test Loss: 3.0580\n",
      "Epoch 9201, Train Loss: 3.0066, Test Loss: 3.0671\n",
      "Epoch 9202, Train Loss: 3.0008, Test Loss: 3.0751\n",
      "Epoch 9203, Train Loss: 3.0112, Test Loss: 3.0636\n",
      "Epoch 9204, Train Loss: 3.0093, Test Loss: 3.0541\n",
      "Epoch 9205, Train Loss: 2.9945, Test Loss: 3.0559\n",
      "Epoch 9206, Train Loss: 3.0109, Test Loss: 3.0591\n",
      "Epoch 9207, Train Loss: 3.0067, Test Loss: 3.0570\n",
      "Epoch 9208, Train Loss: 2.9961, Test Loss: 3.0586\n",
      "Epoch 9209, Train Loss: 2.9987, Test Loss: 3.0644\n",
      "Epoch 9210, Train Loss: 3.0063, Test Loss: 3.0592\n",
      "Epoch 9211, Train Loss: 3.0026, Test Loss: 3.0569\n",
      "Epoch 9212, Train Loss: 3.0023, Test Loss: 3.0575\n",
      "Epoch 9213, Train Loss: 2.9990, Test Loss: 3.0567\n",
      "Epoch 9214, Train Loss: 3.0021, Test Loss: 3.0612\n",
      "Epoch 9215, Train Loss: 3.0053, Test Loss: 3.0600\n",
      "Epoch 9216, Train Loss: 3.0034, Test Loss: 3.0590\n",
      "Epoch 9217, Train Loss: 2.9979, Test Loss: 3.0566\n",
      "Epoch 9218, Train Loss: 3.0052, Test Loss: 3.0572\n",
      "Epoch 9219, Train Loss: 3.0080, Test Loss: 3.0512\n",
      "Epoch 9220, Train Loss: 2.9968, Test Loss: 3.0522\n",
      "Epoch 9221, Train Loss: 2.9986, Test Loss: 3.0550\n",
      "Epoch 9222, Train Loss: 2.9995, Test Loss: 3.0578\n",
      "Epoch 9223, Train Loss: 3.0023, Test Loss: 3.0562\n",
      "Epoch 9224, Train Loss: 3.0070, Test Loss: 3.0625\n",
      "Epoch 9225, Train Loss: 3.0084, Test Loss: 3.0677\n",
      "Epoch 9226, Train Loss: 3.0095, Test Loss: 3.0548\n",
      "Epoch 9227, Train Loss: 3.0001, Test Loss: 3.0556\n",
      "Epoch 9228, Train Loss: 3.0040, Test Loss: 3.0544\n",
      "Epoch 9229, Train Loss: 3.0074, Test Loss: 3.0613\n",
      "Epoch 9230, Train Loss: 3.0010, Test Loss: 3.0664\n",
      "Epoch 9231, Train Loss: 3.0188, Test Loss: 3.0563\n",
      "Epoch 9232, Train Loss: 3.0058, Test Loss: 3.0762\n",
      "Epoch 9233, Train Loss: 3.0212, Test Loss: 3.0687\n",
      "Epoch 9234, Train Loss: 3.0028, Test Loss: 3.0665\n",
      "Epoch 9235, Train Loss: 3.0125, Test Loss: 3.0575\n",
      "Epoch 9236, Train Loss: 3.0008, Test Loss: 3.0564\n",
      "Epoch 9237, Train Loss: 3.0028, Test Loss: 3.0592\n",
      "Epoch 9238, Train Loss: 3.0051, Test Loss: 3.0549\n",
      "Epoch 9239, Train Loss: 2.9939, Test Loss: 3.0584\n",
      "Epoch 9240, Train Loss: 3.0022, Test Loss: 3.0573\n",
      "Epoch 9241, Train Loss: 2.9976, Test Loss: 3.0581\n",
      "Epoch 9242, Train Loss: 2.9988, Test Loss: 3.0589\n",
      "Epoch 9243, Train Loss: 3.0001, Test Loss: 3.0581\n",
      "Epoch 9244, Train Loss: 2.9976, Test Loss: 3.0564\n",
      "Epoch 9245, Train Loss: 2.9991, Test Loss: 3.0537\n",
      "Epoch 9246, Train Loss: 2.9931, Test Loss: 3.0576\n",
      "Epoch 9247, Train Loss: 3.0030, Test Loss: 3.0565\n",
      "Epoch 9248, Train Loss: 2.9997, Test Loss: 3.0539\n",
      "Epoch 9249, Train Loss: 2.9965, Test Loss: 3.0536\n",
      "Epoch 9250, Train Loss: 2.9991, Test Loss: 3.0526\n",
      "Epoch 9251, Train Loss: 2.9976, Test Loss: 3.0504\n",
      "Epoch 9252, Train Loss: 2.9984, Test Loss: 3.0537\n",
      "Epoch 9253, Train Loss: 3.0026, Test Loss: 3.0613\n",
      "Epoch 9254, Train Loss: 2.9990, Test Loss: 3.0628\n",
      "Epoch 9255, Train Loss: 2.9964, Test Loss: 3.0635\n",
      "Epoch 9256, Train Loss: 3.0060, Test Loss: 3.0568\n",
      "Epoch 9257, Train Loss: 2.9980, Test Loss: 3.0644\n",
      "Epoch 9258, Train Loss: 3.0101, Test Loss: 3.0578\n",
      "Epoch 9259, Train Loss: 3.0064, Test Loss: 3.0572\n",
      "Epoch 9260, Train Loss: 3.0085, Test Loss: 3.0600\n",
      "Epoch 9261, Train Loss: 3.0074, Test Loss: 3.0679\n",
      "Epoch 9262, Train Loss: 3.0044, Test Loss: 3.0730\n",
      "Epoch 9263, Train Loss: 3.0044, Test Loss: 3.0551\n",
      "Epoch 9264, Train Loss: 3.0014, Test Loss: 3.0573\n",
      "Epoch 9265, Train Loss: 3.0179, Test Loss: 3.0529\n",
      "Epoch 9266, Train Loss: 2.9991, Test Loss: 3.0631\n",
      "Epoch 9267, Train Loss: 3.0142, Test Loss: 3.0648\n",
      "Epoch 9268, Train Loss: 3.0145, Test Loss: 3.0528\n",
      "Epoch 9269, Train Loss: 3.0001, Test Loss: 3.0561\n",
      "Epoch 9270, Train Loss: 2.9990, Test Loss: 3.0548\n",
      "Epoch 9271, Train Loss: 3.0056, Test Loss: 3.0532\n",
      "Epoch 9272, Train Loss: 3.0070, Test Loss: 3.0532\n",
      "Epoch 9273, Train Loss: 3.0021, Test Loss: 3.0576\n",
      "Epoch 9274, Train Loss: 2.9952, Test Loss: 3.0668\n",
      "Epoch 9275, Train Loss: 3.0159, Test Loss: 3.0548\n",
      "Epoch 9276, Train Loss: 3.0029, Test Loss: 3.0637\n",
      "Epoch 9277, Train Loss: 3.0077, Test Loss: 3.0737\n",
      "Epoch 9278, Train Loss: 3.0178, Test Loss: 3.0658\n",
      "Epoch 9279, Train Loss: 3.0176, Test Loss: 3.0799\n",
      "Epoch 9280, Train Loss: 3.0241, Test Loss: 3.0752\n",
      "Epoch 9281, Train Loss: 3.0128, Test Loss: 3.0832\n",
      "Epoch 9282, Train Loss: 3.0360, Test Loss: 3.0589\n",
      "Epoch 9283, Train Loss: 3.0048, Test Loss: 3.0650\n",
      "Epoch 9284, Train Loss: 3.0128, Test Loss: 3.0711\n",
      "Epoch 9285, Train Loss: 3.0174, Test Loss: 3.0708\n",
      "Epoch 9286, Train Loss: 3.0300, Test Loss: 3.0693\n",
      "Epoch 9287, Train Loss: 3.0261, Test Loss: 3.0698\n",
      "Epoch 9288, Train Loss: 3.0133, Test Loss: 3.0770\n",
      "Epoch 9289, Train Loss: 3.0173, Test Loss: 3.0805\n",
      "Epoch 9290, Train Loss: 3.0187, Test Loss: 3.0772\n",
      "Epoch 9291, Train Loss: 3.0378, Test Loss: 3.0679\n",
      "Epoch 9292, Train Loss: 3.0172, Test Loss: 3.0650\n",
      "Epoch 9293, Train Loss: 3.0146, Test Loss: 3.0829\n",
      "Epoch 9294, Train Loss: 3.0236, Test Loss: 3.0925\n",
      "Epoch 9295, Train Loss: 3.0257, Test Loss: 3.0888\n",
      "Epoch 9296, Train Loss: 3.0411, Test Loss: 3.0838\n",
      "Epoch 9297, Train Loss: 3.0300, Test Loss: 3.0843\n",
      "Epoch 9298, Train Loss: 3.0194, Test Loss: 3.0745\n",
      "Epoch 9299, Train Loss: 3.0145, Test Loss: 3.0841\n",
      "Epoch 9300, Train Loss: 3.0499, Test Loss: 3.0762\n",
      "Epoch 9301, Train Loss: 3.0145, Test Loss: 3.0792\n",
      "Epoch 9302, Train Loss: 3.0199, Test Loss: 3.0749\n",
      "Epoch 9303, Train Loss: 3.0153, Test Loss: 3.0652\n",
      "Epoch 9304, Train Loss: 3.0183, Test Loss: 3.0683\n",
      "Epoch 9305, Train Loss: 3.0138, Test Loss: 3.0658\n",
      "Epoch 9306, Train Loss: 3.0189, Test Loss: 3.0638\n",
      "Epoch 9307, Train Loss: 3.0126, Test Loss: 3.0670\n",
      "Epoch 9308, Train Loss: 3.0215, Test Loss: 3.0751\n",
      "Epoch 9309, Train Loss: 3.0376, Test Loss: 3.0704\n",
      "Epoch 9310, Train Loss: 3.0136, Test Loss: 3.0679\n",
      "Epoch 9311, Train Loss: 3.0164, Test Loss: 3.0660\n",
      "Epoch 9312, Train Loss: 3.0119, Test Loss: 3.0668\n",
      "Epoch 9313, Train Loss: 3.0192, Test Loss: 3.0636\n",
      "Epoch 9314, Train Loss: 3.0236, Test Loss: 3.0555\n",
      "Epoch 9315, Train Loss: 3.0073, Test Loss: 3.0576\n",
      "Epoch 9316, Train Loss: 3.0128, Test Loss: 3.0572\n",
      "Epoch 9317, Train Loss: 3.0082, Test Loss: 3.0612\n",
      "Epoch 9318, Train Loss: 3.0198, Test Loss: 3.0574\n",
      "Epoch 9319, Train Loss: 3.0053, Test Loss: 3.0574\n",
      "Epoch 9320, Train Loss: 3.0083, Test Loss: 3.0548\n",
      "Epoch 9321, Train Loss: 3.0015, Test Loss: 3.0603\n",
      "Epoch 9322, Train Loss: 3.0072, Test Loss: 3.0624\n",
      "Epoch 9323, Train Loss: 3.0172, Test Loss: 3.0632\n",
      "Epoch 9324, Train Loss: 3.0049, Test Loss: 3.0677\n",
      "Epoch 9325, Train Loss: 3.0045, Test Loss: 3.0693\n",
      "Epoch 9326, Train Loss: 3.0221, Test Loss: 3.0612\n",
      "Epoch 9327, Train Loss: 3.0073, Test Loss: 3.0628\n",
      "Epoch 9328, Train Loss: 3.0082, Test Loss: 3.0623\n",
      "Epoch 9329, Train Loss: 3.0174, Test Loss: 3.0517\n",
      "Epoch 9330, Train Loss: 3.0146, Test Loss: 3.0547\n",
      "Epoch 9331, Train Loss: 3.0101, Test Loss: 3.0606\n",
      "Epoch 9332, Train Loss: 3.0151, Test Loss: 3.0675\n",
      "Epoch 9333, Train Loss: 3.0101, Test Loss: 3.0704\n",
      "Epoch 9334, Train Loss: 3.0131, Test Loss: 3.0605\n",
      "Epoch 9335, Train Loss: 3.0049, Test Loss: 3.0523\n",
      "Epoch 9336, Train Loss: 3.0059, Test Loss: 3.0495\n",
      "Epoch 9337, Train Loss: 3.0108, Test Loss: 3.0570\n",
      "Epoch 9338, Train Loss: 3.0095, Test Loss: 3.0627\n",
      "Epoch 9339, Train Loss: 3.0107, Test Loss: 3.0589\n",
      "Epoch 9340, Train Loss: 3.0099, Test Loss: 3.0500\n",
      "Epoch 9341, Train Loss: 2.9971, Test Loss: 3.0658\n",
      "Epoch 9342, Train Loss: 3.0216, Test Loss: 3.0559\n",
      "Epoch 9343, Train Loss: 3.0122, Test Loss: 3.0525\n",
      "Epoch 9344, Train Loss: 3.0039, Test Loss: 3.0546\n",
      "Epoch 9345, Train Loss: 3.0020, Test Loss: 3.0592\n",
      "Epoch 9346, Train Loss: 3.0055, Test Loss: 3.0534\n",
      "Epoch 9347, Train Loss: 2.9961, Test Loss: 3.0524\n",
      "Epoch 9348, Train Loss: 2.9935, Test Loss: 3.0534\n",
      "Epoch 9349, Train Loss: 3.0010, Test Loss: 3.0572\n",
      "Epoch 9350, Train Loss: 3.0026, Test Loss: 3.0560\n",
      "Epoch 9351, Train Loss: 2.9999, Test Loss: 3.0585\n",
      "Epoch 9352, Train Loss: 2.9993, Test Loss: 3.0567\n",
      "Epoch 9353, Train Loss: 3.0057, Test Loss: 3.0502\n",
      "Epoch 9354, Train Loss: 3.0035, Test Loss: 3.0506\n",
      "Epoch 9355, Train Loss: 2.9964, Test Loss: 3.0501\n",
      "Epoch 9356, Train Loss: 2.9965, Test Loss: 3.0535\n",
      "Epoch 9357, Train Loss: 3.0013, Test Loss: 3.0544\n",
      "Epoch 9358, Train Loss: 2.9963, Test Loss: 3.0579\n",
      "Epoch 9359, Train Loss: 2.9990, Test Loss: 3.0551\n",
      "Epoch 9360, Train Loss: 3.0003, Test Loss: 3.0505\n",
      "Epoch 9361, Train Loss: 2.9977, Test Loss: 3.0497\n",
      "Epoch 9362, Train Loss: 3.0007, Test Loss: 3.0545\n",
      "Epoch 9363, Train Loss: 3.0040, Test Loss: 3.0576\n",
      "Epoch 9364, Train Loss: 3.0027, Test Loss: 3.0522\n",
      "Epoch 9365, Train Loss: 3.0026, Test Loss: 3.0563\n",
      "Epoch 9366, Train Loss: 3.0003, Test Loss: 3.0627\n",
      "Epoch 9367, Train Loss: 2.9996, Test Loss: 3.0598\n",
      "Epoch 9368, Train Loss: 3.0001, Test Loss: 3.0578\n",
      "Epoch 9369, Train Loss: 3.0162, Test Loss: 3.0520\n",
      "Epoch 9370, Train Loss: 2.9985, Test Loss: 3.0598\n",
      "Epoch 9371, Train Loss: 3.0098, Test Loss: 3.0536\n",
      "Epoch 9372, Train Loss: 3.0009, Test Loss: 3.0576\n",
      "Epoch 9373, Train Loss: 3.0086, Test Loss: 3.0494\n",
      "Epoch 9374, Train Loss: 2.9965, Test Loss: 3.0545\n",
      "Epoch 9375, Train Loss: 3.0032, Test Loss: 3.0552\n",
      "Epoch 9376, Train Loss: 3.0000, Test Loss: 3.0623\n",
      "Epoch 9377, Train Loss: 3.0078, Test Loss: 3.0622\n",
      "Epoch 9378, Train Loss: 3.0039, Test Loss: 3.0539\n",
      "Epoch 9379, Train Loss: 2.9991, Test Loss: 3.0520\n",
      "Epoch 9380, Train Loss: 2.9996, Test Loss: 3.0624\n",
      "Epoch 9381, Train Loss: 3.0127, Test Loss: 3.0594\n",
      "Epoch 9382, Train Loss: 3.0044, Test Loss: 3.0587\n",
      "Epoch 9383, Train Loss: 3.0007, Test Loss: 3.0677\n",
      "Epoch 9384, Train Loss: 3.0114, Test Loss: 3.0589\n",
      "Epoch 9385, Train Loss: 3.0004, Test Loss: 3.0542\n",
      "Epoch 9386, Train Loss: 3.0051, Test Loss: 3.0567\n",
      "Epoch 9387, Train Loss: 3.0062, Test Loss: 3.0629\n",
      "Epoch 9388, Train Loss: 3.0050, Test Loss: 3.0541\n",
      "Epoch 9389, Train Loss: 2.9951, Test Loss: 3.0535\n",
      "Epoch 9390, Train Loss: 3.0031, Test Loss: 3.0520\n",
      "Epoch 9391, Train Loss: 3.0014, Test Loss: 3.0522\n",
      "Epoch 9392, Train Loss: 2.9987, Test Loss: 3.0566\n",
      "Epoch 9393, Train Loss: 3.0049, Test Loss: 3.0626\n",
      "Epoch 9394, Train Loss: 3.0093, Test Loss: 3.0561\n",
      "Epoch 9395, Train Loss: 3.0055, Test Loss: 3.0553\n",
      "Epoch 9396, Train Loss: 3.0061, Test Loss: 3.0554\n",
      "Epoch 9397, Train Loss: 3.0000, Test Loss: 3.0514\n",
      "Epoch 9398, Train Loss: 2.9977, Test Loss: 3.0465\n",
      "Epoch 9399, Train Loss: 2.9991, Test Loss: 3.0515\n",
      "Epoch 9400, Train Loss: 2.9973, Test Loss: 3.0530\n",
      "Epoch 9401, Train Loss: 2.9981, Test Loss: 3.0551\n",
      "Epoch 9402, Train Loss: 3.0054, Test Loss: 3.0531\n",
      "Epoch 9403, Train Loss: 2.9986, Test Loss: 3.0510\n",
      "Epoch 9404, Train Loss: 3.0008, Test Loss: 3.0561\n",
      "Epoch 9405, Train Loss: 3.0000, Test Loss: 3.0524\n",
      "Epoch 9406, Train Loss: 2.9984, Test Loss: 3.0528\n",
      "Epoch 9407, Train Loss: 2.9952, Test Loss: 3.0511\n",
      "Epoch 9408, Train Loss: 2.9989, Test Loss: 3.0525\n",
      "Epoch 9409, Train Loss: 3.0029, Test Loss: 3.0500\n",
      "Epoch 9410, Train Loss: 2.9961, Test Loss: 3.0538\n",
      "Epoch 9411, Train Loss: 2.9961, Test Loss: 3.0523\n",
      "Epoch 9412, Train Loss: 2.9953, Test Loss: 3.0533\n",
      "Epoch 9413, Train Loss: 2.9961, Test Loss: 3.0487\n",
      "Epoch 9414, Train Loss: 2.9914, Test Loss: 3.0482\n",
      "Epoch 9415, Train Loss: 2.9995, Test Loss: 3.0473\n",
      "Epoch 9416, Train Loss: 2.9928, Test Loss: 3.0472\n",
      "Epoch 9417, Train Loss: 2.9930, Test Loss: 3.0463\n",
      "Epoch 9418, Train Loss: 2.9967, Test Loss: 3.0470\n",
      "Epoch 9419, Train Loss: 2.9947, Test Loss: 3.0494\n",
      "Epoch 9420, Train Loss: 2.9954, Test Loss: 3.0519\n",
      "Epoch 9421, Train Loss: 2.9954, Test Loss: 3.0517\n",
      "Epoch 9422, Train Loss: 3.0017, Test Loss: 3.0536\n",
      "Epoch 9423, Train Loss: 2.9989, Test Loss: 3.0546\n",
      "Epoch 9424, Train Loss: 2.9969, Test Loss: 3.0538\n",
      "Epoch 9425, Train Loss: 2.9963, Test Loss: 3.0519\n",
      "Epoch 9426, Train Loss: 2.9936, Test Loss: 3.0548\n",
      "Epoch 9427, Train Loss: 2.9987, Test Loss: 3.0534\n",
      "Epoch 9428, Train Loss: 2.9934, Test Loss: 3.0553\n",
      "Epoch 9429, Train Loss: 3.0011, Test Loss: 3.0504\n",
      "Epoch 9430, Train Loss: 2.9942, Test Loss: 3.0524\n",
      "Epoch 9431, Train Loss: 2.9936, Test Loss: 3.0502\n",
      "Epoch 9432, Train Loss: 2.9944, Test Loss: 3.0493\n",
      "Epoch 9433, Train Loss: 2.9965, Test Loss: 3.0500\n",
      "Epoch 9434, Train Loss: 2.9930, Test Loss: 3.0504\n",
      "Epoch 9435, Train Loss: 2.9896, Test Loss: 3.0568\n",
      "Epoch 9436, Train Loss: 2.9952, Test Loss: 3.0554\n",
      "Epoch 9437, Train Loss: 2.9977, Test Loss: 3.0571\n",
      "Epoch 9438, Train Loss: 2.9957, Test Loss: 3.0540\n",
      "Epoch 9439, Train Loss: 2.9997, Test Loss: 3.0522\n",
      "Epoch 9440, Train Loss: 2.9958, Test Loss: 3.0521\n",
      "Epoch 9441, Train Loss: 2.9993, Test Loss: 3.0502\n",
      "Epoch 9442, Train Loss: 2.9966, Test Loss: 3.0537\n",
      "Epoch 9443, Train Loss: 3.0015, Test Loss: 3.0521\n",
      "Epoch 9444, Train Loss: 2.9958, Test Loss: 3.0514\n",
      "Epoch 9445, Train Loss: 2.9981, Test Loss: 3.0517\n",
      "Epoch 9446, Train Loss: 2.9952, Test Loss: 3.0552\n",
      "Epoch 9447, Train Loss: 2.9975, Test Loss: 3.0523\n",
      "Epoch 9448, Train Loss: 2.9955, Test Loss: 3.0549\n",
      "Epoch 9449, Train Loss: 3.0004, Test Loss: 3.0506\n",
      "Epoch 9450, Train Loss: 3.0015, Test Loss: 3.0538\n",
      "Epoch 9451, Train Loss: 2.9992, Test Loss: 3.0535\n",
      "Epoch 9452, Train Loss: 2.9962, Test Loss: 3.0602\n",
      "Epoch 9453, Train Loss: 3.0048, Test Loss: 3.0617\n",
      "Epoch 9454, Train Loss: 3.0039, Test Loss: 3.0520\n",
      "Epoch 9455, Train Loss: 2.9985, Test Loss: 3.0541\n",
      "Epoch 9456, Train Loss: 2.9994, Test Loss: 3.0505\n",
      "Epoch 9457, Train Loss: 2.9972, Test Loss: 3.0525\n",
      "Epoch 9458, Train Loss: 2.9928, Test Loss: 3.0569\n",
      "Epoch 9459, Train Loss: 2.9957, Test Loss: 3.0561\n",
      "Epoch 9460, Train Loss: 3.0005, Test Loss: 3.0526\n",
      "Epoch 9461, Train Loss: 2.9960, Test Loss: 3.0531\n",
      "Epoch 9462, Train Loss: 3.0037, Test Loss: 3.0505\n",
      "Epoch 9463, Train Loss: 2.9966, Test Loss: 3.0543\n",
      "Epoch 9464, Train Loss: 3.0002, Test Loss: 3.0533\n",
      "Epoch 9465, Train Loss: 2.9945, Test Loss: 3.0536\n",
      "Epoch 9466, Train Loss: 2.9977, Test Loss: 3.0498\n",
      "Epoch 9467, Train Loss: 2.9974, Test Loss: 3.0479\n",
      "Epoch 9468, Train Loss: 2.9927, Test Loss: 3.0490\n",
      "Epoch 9469, Train Loss: 2.9961, Test Loss: 3.0555\n",
      "Epoch 9470, Train Loss: 2.9954, Test Loss: 3.0590\n",
      "Epoch 9471, Train Loss: 3.0023, Test Loss: 3.0535\n",
      "Epoch 9472, Train Loss: 3.0056, Test Loss: 3.0492\n",
      "Epoch 9473, Train Loss: 3.0001, Test Loss: 3.0513\n",
      "Epoch 9474, Train Loss: 2.9975, Test Loss: 3.0515\n",
      "Epoch 9475, Train Loss: 3.0035, Test Loss: 3.0522\n",
      "Epoch 9476, Train Loss: 2.9952, Test Loss: 3.0546\n",
      "Epoch 9477, Train Loss: 3.0021, Test Loss: 3.0544\n",
      "Epoch 9478, Train Loss: 2.9939, Test Loss: 3.0563\n",
      "Epoch 9479, Train Loss: 2.9961, Test Loss: 3.0547\n",
      "Epoch 9480, Train Loss: 3.0010, Test Loss: 3.0495\n",
      "Epoch 9481, Train Loss: 2.9959, Test Loss: 3.0481\n",
      "Epoch 9482, Train Loss: 2.9971, Test Loss: 3.0526\n",
      "Epoch 9483, Train Loss: 3.0021, Test Loss: 3.0550\n",
      "Epoch 9484, Train Loss: 3.0013, Test Loss: 3.0523\n",
      "Epoch 9485, Train Loss: 2.9979, Test Loss: 3.0528\n",
      "Epoch 9486, Train Loss: 2.9991, Test Loss: 3.0541\n",
      "Epoch 9487, Train Loss: 2.9953, Test Loss: 3.0556\n",
      "Epoch 9488, Train Loss: 3.0020, Test Loss: 3.0594\n",
      "Epoch 9489, Train Loss: 3.0111, Test Loss: 3.0580\n",
      "Epoch 9490, Train Loss: 3.0034, Test Loss: 3.0537\n",
      "Epoch 9491, Train Loss: 3.0003, Test Loss: 3.0551\n",
      "Epoch 9492, Train Loss: 3.0050, Test Loss: 3.0535\n",
      "Epoch 9493, Train Loss: 3.0192, Test Loss: 3.0573\n",
      "Epoch 9494, Train Loss: 3.0074, Test Loss: 3.0604\n",
      "Epoch 9495, Train Loss: 3.0091, Test Loss: 3.0666\n",
      "Epoch 9496, Train Loss: 3.0130, Test Loss: 3.0619\n",
      "Epoch 9497, Train Loss: 2.9991, Test Loss: 3.0626\n",
      "Epoch 9498, Train Loss: 3.0110, Test Loss: 3.0484\n",
      "Epoch 9499, Train Loss: 2.9991, Test Loss: 3.0508\n",
      "Epoch 9500, Train Loss: 3.0060, Test Loss: 3.0543\n",
      "Epoch 9501, Train Loss: 3.0027, Test Loss: 3.0539\n",
      "Epoch 9502, Train Loss: 3.0023, Test Loss: 3.0540\n",
      "Epoch 9503, Train Loss: 2.9968, Test Loss: 3.0536\n",
      "Epoch 9504, Train Loss: 2.9986, Test Loss: 3.0569\n",
      "Epoch 9505, Train Loss: 2.9993, Test Loss: 3.0544\n",
      "Epoch 9506, Train Loss: 2.9938, Test Loss: 3.0543\n",
      "Epoch 9507, Train Loss: 2.9993, Test Loss: 3.0570\n",
      "Epoch 9508, Train Loss: 3.0017, Test Loss: 3.0508\n",
      "Epoch 9509, Train Loss: 2.9967, Test Loss: 3.0527\n",
      "Epoch 9510, Train Loss: 3.0051, Test Loss: 3.0506\n",
      "Epoch 9511, Train Loss: 2.9987, Test Loss: 3.0504\n",
      "Epoch 9512, Train Loss: 2.9957, Test Loss: 3.0486\n",
      "Epoch 9513, Train Loss: 2.9939, Test Loss: 3.0546\n",
      "Epoch 9514, Train Loss: 2.9966, Test Loss: 3.0560\n",
      "Epoch 9515, Train Loss: 2.9968, Test Loss: 3.0540\n",
      "Epoch 9516, Train Loss: 2.9975, Test Loss: 3.0519\n",
      "Epoch 9517, Train Loss: 2.9967, Test Loss: 3.0507\n",
      "Epoch 9518, Train Loss: 2.9944, Test Loss: 3.0471\n",
      "Epoch 9519, Train Loss: 2.9944, Test Loss: 3.0481\n",
      "Epoch 9520, Train Loss: 2.9950, Test Loss: 3.0498\n",
      "Epoch 9521, Train Loss: 2.9943, Test Loss: 3.0503\n",
      "Epoch 9522, Train Loss: 2.9936, Test Loss: 3.0520\n",
      "Epoch 9523, Train Loss: 2.9973, Test Loss: 3.0516\n",
      "Epoch 9524, Train Loss: 2.9980, Test Loss: 3.0524\n",
      "Epoch 9525, Train Loss: 2.9979, Test Loss: 3.0511\n",
      "Epoch 9526, Train Loss: 2.9929, Test Loss: 3.0511\n",
      "Epoch 9527, Train Loss: 2.9955, Test Loss: 3.0465\n",
      "Epoch 9528, Train Loss: 2.9944, Test Loss: 3.0522\n",
      "Epoch 9529, Train Loss: 3.0122, Test Loss: 3.0508\n",
      "Epoch 9530, Train Loss: 2.9996, Test Loss: 3.0610\n",
      "Epoch 9531, Train Loss: 3.0037, Test Loss: 3.0624\n",
      "Epoch 9532, Train Loss: 3.0013, Test Loss: 3.0616\n",
      "Epoch 9533, Train Loss: 3.0075, Test Loss: 3.0575\n",
      "Epoch 9534, Train Loss: 3.0056, Test Loss: 3.0508\n",
      "Epoch 9535, Train Loss: 2.9952, Test Loss: 3.0563\n",
      "Epoch 9536, Train Loss: 3.0090, Test Loss: 3.0494\n",
      "Epoch 9537, Train Loss: 3.0035, Test Loss: 3.0578\n",
      "Epoch 9538, Train Loss: 3.0094, Test Loss: 3.0573\n",
      "Epoch 9539, Train Loss: 3.0053, Test Loss: 3.0669\n",
      "Epoch 9540, Train Loss: 3.0082, Test Loss: 3.0673\n",
      "Epoch 9541, Train Loss: 3.0136, Test Loss: 3.0625\n",
      "Epoch 9542, Train Loss: 3.0049, Test Loss: 3.0587\n",
      "Epoch 9543, Train Loss: 3.0085, Test Loss: 3.0595\n",
      "Epoch 9544, Train Loss: 3.0176, Test Loss: 3.0556\n",
      "Epoch 9545, Train Loss: 3.0032, Test Loss: 3.0578\n",
      "Epoch 9546, Train Loss: 3.0070, Test Loss: 3.0541\n",
      "Epoch 9547, Train Loss: 2.9988, Test Loss: 3.0534\n",
      "Epoch 9548, Train Loss: 2.9989, Test Loss: 3.0576\n",
      "Epoch 9549, Train Loss: 3.0045, Test Loss: 3.0515\n",
      "Epoch 9550, Train Loss: 2.9981, Test Loss: 3.0516\n",
      "Epoch 9551, Train Loss: 2.9951, Test Loss: 3.0545\n",
      "Epoch 9552, Train Loss: 2.9969, Test Loss: 3.0533\n",
      "Epoch 9553, Train Loss: 2.9989, Test Loss: 3.0500\n",
      "Epoch 9554, Train Loss: 2.9983, Test Loss: 3.0502\n",
      "Epoch 9555, Train Loss: 2.9967, Test Loss: 3.0508\n",
      "Epoch 9556, Train Loss: 2.9936, Test Loss: 3.0521\n",
      "Epoch 9557, Train Loss: 2.9985, Test Loss: 3.0494\n",
      "Epoch 9558, Train Loss: 2.9934, Test Loss: 3.0509\n",
      "Epoch 9559, Train Loss: 3.0027, Test Loss: 3.0529\n",
      "Epoch 9560, Train Loss: 2.9945, Test Loss: 3.0542\n",
      "Epoch 9561, Train Loss: 2.9983, Test Loss: 3.0517\n",
      "Epoch 9562, Train Loss: 2.9966, Test Loss: 3.0563\n",
      "Epoch 9563, Train Loss: 3.0059, Test Loss: 3.0512\n",
      "Epoch 9564, Train Loss: 2.9971, Test Loss: 3.0543\n",
      "Epoch 9565, Train Loss: 2.9999, Test Loss: 3.0481\n",
      "Epoch 9566, Train Loss: 2.9936, Test Loss: 3.0553\n",
      "Epoch 9567, Train Loss: 3.0028, Test Loss: 3.0554\n",
      "Epoch 9568, Train Loss: 2.9982, Test Loss: 3.0596\n",
      "Epoch 9569, Train Loss: 3.0063, Test Loss: 3.0497\n",
      "Epoch 9570, Train Loss: 2.9935, Test Loss: 3.0514\n",
      "Epoch 9571, Train Loss: 3.0017, Test Loss: 3.0545\n",
      "Epoch 9572, Train Loss: 2.9989, Test Loss: 3.0589\n",
      "Epoch 9573, Train Loss: 3.0090, Test Loss: 3.0691\n",
      "Epoch 9574, Train Loss: 3.0087, Test Loss: 3.0746\n",
      "Epoch 9575, Train Loss: 3.0105, Test Loss: 3.0692\n",
      "Epoch 9576, Train Loss: 3.0126, Test Loss: 3.0562\n",
      "Epoch 9577, Train Loss: 3.0050, Test Loss: 3.0613\n",
      "Epoch 9578, Train Loss: 3.0250, Test Loss: 3.0605\n",
      "Epoch 9579, Train Loss: 3.0114, Test Loss: 3.0729\n",
      "Epoch 9580, Train Loss: 3.0180, Test Loss: 3.0539\n",
      "Epoch 9581, Train Loss: 3.0092, Test Loss: 3.0668\n",
      "Epoch 9582, Train Loss: 3.0177, Test Loss: 3.0546\n",
      "Epoch 9583, Train Loss: 2.9990, Test Loss: 3.0538\n",
      "Epoch 9584, Train Loss: 3.0019, Test Loss: 3.0560\n",
      "Epoch 9585, Train Loss: 3.0102, Test Loss: 3.0680\n",
      "Epoch 9586, Train Loss: 3.0072, Test Loss: 3.0767\n",
      "Epoch 9587, Train Loss: 3.0133, Test Loss: 3.0688\n",
      "Epoch 9588, Train Loss: 3.0061, Test Loss: 3.0623\n",
      "Epoch 9589, Train Loss: 3.0075, Test Loss: 3.0536\n",
      "Epoch 9590, Train Loss: 3.0169, Test Loss: 3.0548\n",
      "Epoch 9591, Train Loss: 3.0063, Test Loss: 3.0629\n",
      "Epoch 9592, Train Loss: 3.0064, Test Loss: 3.0618\n",
      "Epoch 9593, Train Loss: 3.0109, Test Loss: 3.0654\n",
      "Epoch 9594, Train Loss: 3.0053, Test Loss: 3.0700\n",
      "Epoch 9595, Train Loss: 3.0073, Test Loss: 3.0634\n",
      "Epoch 9596, Train Loss: 3.0166, Test Loss: 3.0561\n",
      "Epoch 9597, Train Loss: 3.0051, Test Loss: 3.0575\n",
      "Epoch 9598, Train Loss: 3.0101, Test Loss: 3.0676\n",
      "Epoch 9599, Train Loss: 3.0082, Test Loss: 3.0631\n",
      "Epoch 9600, Train Loss: 3.0066, Test Loss: 3.0637\n",
      "Epoch 9601, Train Loss: 3.0074, Test Loss: 3.0596\n",
      "Epoch 9602, Train Loss: 3.0051, Test Loss: 3.0631\n",
      "Epoch 9603, Train Loss: 3.0118, Test Loss: 3.0574\n",
      "Epoch 9604, Train Loss: 3.0099, Test Loss: 3.0515\n",
      "Epoch 9605, Train Loss: 2.9998, Test Loss: 3.0560\n",
      "Epoch 9606, Train Loss: 3.0164, Test Loss: 3.0596\n",
      "Epoch 9607, Train Loss: 3.0045, Test Loss: 3.0657\n",
      "Epoch 9608, Train Loss: 3.0135, Test Loss: 3.0655\n",
      "Epoch 9609, Train Loss: 3.0088, Test Loss: 3.0560\n",
      "Epoch 9610, Train Loss: 3.0168, Test Loss: 3.0574\n",
      "Epoch 9611, Train Loss: 3.0121, Test Loss: 3.0556\n",
      "Epoch 9612, Train Loss: 3.0087, Test Loss: 3.0821\n",
      "Epoch 9613, Train Loss: 3.0159, Test Loss: 3.0686\n",
      "Epoch 9614, Train Loss: 3.0066, Test Loss: 3.0571\n",
      "Epoch 9615, Train Loss: 3.0104, Test Loss: 3.0513\n",
      "Epoch 9616, Train Loss: 3.0006, Test Loss: 3.0568\n",
      "Epoch 9617, Train Loss: 3.0052, Test Loss: 3.0580\n",
      "Epoch 9618, Train Loss: 3.0106, Test Loss: 3.0564\n",
      "Epoch 9619, Train Loss: 3.0135, Test Loss: 3.0545\n",
      "Epoch 9620, Train Loss: 3.0009, Test Loss: 3.0582\n",
      "Epoch 9621, Train Loss: 3.0074, Test Loss: 3.0537\n",
      "Epoch 9622, Train Loss: 3.0012, Test Loss: 3.0504\n",
      "Epoch 9623, Train Loss: 2.9999, Test Loss: 3.0471\n",
      "Epoch 9624, Train Loss: 2.9998, Test Loss: 3.0490\n",
      "Epoch 9625, Train Loss: 2.9961, Test Loss: 3.0520\n",
      "Epoch 9626, Train Loss: 2.9942, Test Loss: 3.0564\n",
      "Epoch 9627, Train Loss: 3.0005, Test Loss: 3.0533\n",
      "Epoch 9628, Train Loss: 2.9992, Test Loss: 3.0473\n",
      "Epoch 9629, Train Loss: 2.9944, Test Loss: 3.0497\n",
      "Epoch 9630, Train Loss: 3.0000, Test Loss: 3.0572\n",
      "Epoch 9631, Train Loss: 3.0048, Test Loss: 3.0545\n",
      "Epoch 9632, Train Loss: 3.0009, Test Loss: 3.0495\n",
      "Epoch 9633, Train Loss: 2.9984, Test Loss: 3.0506\n",
      "Epoch 9634, Train Loss: 2.9946, Test Loss: 3.0556\n",
      "Epoch 9635, Train Loss: 2.9998, Test Loss: 3.0547\n",
      "Epoch 9636, Train Loss: 2.9987, Test Loss: 3.0537\n",
      "Epoch 9637, Train Loss: 3.0012, Test Loss: 3.0530\n",
      "Epoch 9638, Train Loss: 3.0008, Test Loss: 3.0544\n",
      "Epoch 9639, Train Loss: 3.0005, Test Loss: 3.0521\n",
      "Epoch 9640, Train Loss: 2.9998, Test Loss: 3.0547\n",
      "Epoch 9641, Train Loss: 2.9972, Test Loss: 3.0531\n",
      "Epoch 9642, Train Loss: 2.9934, Test Loss: 3.0519\n",
      "Epoch 9643, Train Loss: 2.9960, Test Loss: 3.0517\n",
      "Epoch 9644, Train Loss: 3.0005, Test Loss: 3.0538\n",
      "Epoch 9645, Train Loss: 3.0021, Test Loss: 3.0535\n",
      "Epoch 9646, Train Loss: 2.9959, Test Loss: 3.0493\n",
      "Epoch 9647, Train Loss: 2.9962, Test Loss: 3.0488\n",
      "Epoch 9648, Train Loss: 2.9994, Test Loss: 3.0550\n",
      "Epoch 9649, Train Loss: 3.0013, Test Loss: 3.0581\n",
      "Epoch 9650, Train Loss: 2.9996, Test Loss: 3.0647\n",
      "Epoch 9651, Train Loss: 2.9998, Test Loss: 3.0564\n",
      "Epoch 9652, Train Loss: 2.9967, Test Loss: 3.0549\n",
      "Epoch 9653, Train Loss: 3.0016, Test Loss: 3.0535\n",
      "Epoch 9654, Train Loss: 2.9976, Test Loss: 3.0509\n",
      "Epoch 9655, Train Loss: 2.9986, Test Loss: 3.0499\n",
      "Epoch 9656, Train Loss: 3.0000, Test Loss: 3.0485\n",
      "Epoch 9657, Train Loss: 2.9968, Test Loss: 3.0528\n",
      "Epoch 9658, Train Loss: 2.9950, Test Loss: 3.0562\n",
      "Epoch 9659, Train Loss: 2.9958, Test Loss: 3.0537\n",
      "Epoch 9660, Train Loss: 2.9963, Test Loss: 3.0514\n",
      "Epoch 9661, Train Loss: 3.0008, Test Loss: 3.0556\n",
      "Epoch 9662, Train Loss: 2.9999, Test Loss: 3.0580\n",
      "Epoch 9663, Train Loss: 3.0034, Test Loss: 3.0562\n",
      "Epoch 9664, Train Loss: 2.9980, Test Loss: 3.0604\n",
      "Epoch 9665, Train Loss: 3.0102, Test Loss: 3.0480\n",
      "Epoch 9666, Train Loss: 2.9991, Test Loss: 3.0494\n",
      "Epoch 9667, Train Loss: 3.0002, Test Loss: 3.0522\n",
      "Epoch 9668, Train Loss: 3.0026, Test Loss: 3.0581\n",
      "Epoch 9669, Train Loss: 3.0032, Test Loss: 3.0520\n",
      "Epoch 9670, Train Loss: 2.9960, Test Loss: 3.0567\n",
      "Epoch 9671, Train Loss: 3.0014, Test Loss: 3.0552\n",
      "Epoch 9672, Train Loss: 3.0035, Test Loss: 3.0577\n",
      "Epoch 9673, Train Loss: 3.0008, Test Loss: 3.0499\n",
      "Epoch 9674, Train Loss: 2.9939, Test Loss: 3.0569\n",
      "Epoch 9675, Train Loss: 3.0073, Test Loss: 3.0535\n",
      "Epoch 9676, Train Loss: 3.0046, Test Loss: 3.0566\n",
      "Epoch 9677, Train Loss: 3.0088, Test Loss: 3.0579\n",
      "Epoch 9678, Train Loss: 3.0012, Test Loss: 3.0555\n",
      "Epoch 9679, Train Loss: 3.0019, Test Loss: 3.0522\n",
      "Epoch 9680, Train Loss: 3.0028, Test Loss: 3.0518\n",
      "Epoch 9681, Train Loss: 3.0005, Test Loss: 3.0529\n",
      "Epoch 9682, Train Loss: 3.0016, Test Loss: 3.0535\n",
      "Epoch 9683, Train Loss: 3.0016, Test Loss: 3.0523\n",
      "Epoch 9684, Train Loss: 2.9963, Test Loss: 3.0562\n",
      "Epoch 9685, Train Loss: 3.0030, Test Loss: 3.0541\n",
      "Epoch 9686, Train Loss: 3.0010, Test Loss: 3.0520\n",
      "Epoch 9687, Train Loss: 2.9982, Test Loss: 3.0506\n",
      "Epoch 9688, Train Loss: 2.9950, Test Loss: 3.0503\n",
      "Epoch 9689, Train Loss: 2.9949, Test Loss: 3.0511\n",
      "Epoch 9690, Train Loss: 2.9977, Test Loss: 3.0539\n",
      "Epoch 9691, Train Loss: 2.9964, Test Loss: 3.0534\n",
      "Epoch 9692, Train Loss: 2.9962, Test Loss: 3.0550\n",
      "Epoch 9693, Train Loss: 2.9995, Test Loss: 3.0564\n",
      "Epoch 9694, Train Loss: 3.0002, Test Loss: 3.0559\n",
      "Epoch 9695, Train Loss: 2.9929, Test Loss: 3.0547\n",
      "Epoch 9696, Train Loss: 3.0008, Test Loss: 3.0473\n",
      "Epoch 9697, Train Loss: 2.9948, Test Loss: 3.0520\n",
      "Epoch 9698, Train Loss: 3.0012, Test Loss: 3.0541\n",
      "Epoch 9699, Train Loss: 3.0031, Test Loss: 3.0534\n",
      "Epoch 9700, Train Loss: 2.9958, Test Loss: 3.0547\n",
      "Epoch 9701, Train Loss: 2.9995, Test Loss: 3.0521\n",
      "Epoch 9702, Train Loss: 2.9988, Test Loss: 3.0558\n",
      "Epoch 9703, Train Loss: 2.9966, Test Loss: 3.0556\n",
      "Epoch 9704, Train Loss: 2.9990, Test Loss: 3.0522\n",
      "Epoch 9705, Train Loss: 2.9988, Test Loss: 3.0526\n",
      "Epoch 9706, Train Loss: 3.0029, Test Loss: 3.0515\n",
      "Epoch 9707, Train Loss: 2.9983, Test Loss: 3.0522\n",
      "Epoch 9708, Train Loss: 2.9964, Test Loss: 3.0537\n",
      "Epoch 9709, Train Loss: 2.9974, Test Loss: 3.0520\n",
      "Epoch 9710, Train Loss: 2.9996, Test Loss: 3.0469\n",
      "Epoch 9711, Train Loss: 2.9944, Test Loss: 3.0500\n",
      "Epoch 9712, Train Loss: 2.9990, Test Loss: 3.0544\n",
      "Epoch 9713, Train Loss: 2.9977, Test Loss: 3.0540\n",
      "Epoch 9714, Train Loss: 2.9985, Test Loss: 3.0527\n",
      "Epoch 9715, Train Loss: 2.9957, Test Loss: 3.0505\n",
      "Epoch 9716, Train Loss: 2.9928, Test Loss: 3.0531\n",
      "Epoch 9717, Train Loss: 2.9961, Test Loss: 3.0534\n",
      "Epoch 9718, Train Loss: 2.9982, Test Loss: 3.0545\n",
      "Epoch 9719, Train Loss: 2.9999, Test Loss: 3.0503\n",
      "Epoch 9720, Train Loss: 3.0011, Test Loss: 3.0497\n",
      "Epoch 9721, Train Loss: 2.9961, Test Loss: 3.0503\n",
      "Epoch 9722, Train Loss: 2.9948, Test Loss: 3.0491\n",
      "Epoch 9723, Train Loss: 2.9952, Test Loss: 3.0487\n",
      "Epoch 9724, Train Loss: 2.9989, Test Loss: 3.0495\n",
      "Epoch 9725, Train Loss: 2.9995, Test Loss: 3.0522\n",
      "Epoch 9726, Train Loss: 2.9953, Test Loss: 3.0532\n",
      "Epoch 9727, Train Loss: 2.9950, Test Loss: 3.0486\n",
      "Epoch 9728, Train Loss: 2.9959, Test Loss: 3.0481\n",
      "Epoch 9729, Train Loss: 2.9951, Test Loss: 3.0509\n",
      "Epoch 9730, Train Loss: 2.9974, Test Loss: 3.0510\n",
      "Epoch 9731, Train Loss: 2.9907, Test Loss: 3.0567\n",
      "Epoch 9732, Train Loss: 3.0003, Test Loss: 3.0537\n",
      "Epoch 9733, Train Loss: 2.9952, Test Loss: 3.0551\n",
      "Epoch 9734, Train Loss: 3.0023, Test Loss: 3.0535\n",
      "Epoch 9735, Train Loss: 2.9939, Test Loss: 3.0515\n",
      "Epoch 9736, Train Loss: 2.9984, Test Loss: 3.0507\n",
      "Epoch 9737, Train Loss: 3.0024, Test Loss: 3.0530\n",
      "Epoch 9738, Train Loss: 3.0052, Test Loss: 3.0552\n",
      "Epoch 9739, Train Loss: 2.9986, Test Loss: 3.0531\n",
      "Epoch 9740, Train Loss: 2.9944, Test Loss: 3.0532\n",
      "Epoch 9741, Train Loss: 2.9969, Test Loss: 3.0529\n",
      "Epoch 9742, Train Loss: 3.0002, Test Loss: 3.0525\n",
      "Epoch 9743, Train Loss: 2.9976, Test Loss: 3.0553\n",
      "Epoch 9744, Train Loss: 2.9953, Test Loss: 3.0542\n",
      "Epoch 9745, Train Loss: 3.0004, Test Loss: 3.0505\n",
      "Epoch 9746, Train Loss: 2.9947, Test Loss: 3.0567\n",
      "Epoch 9747, Train Loss: 3.0028, Test Loss: 3.0523\n",
      "Epoch 9748, Train Loss: 2.9987, Test Loss: 3.0509\n",
      "Epoch 9749, Train Loss: 3.0000, Test Loss: 3.0489\n",
      "Epoch 9750, Train Loss: 3.0002, Test Loss: 3.0548\n",
      "Epoch 9751, Train Loss: 3.0009, Test Loss: 3.0715\n",
      "Epoch 9752, Train Loss: 3.0172, Test Loss: 3.0554\n",
      "Epoch 9753, Train Loss: 3.0003, Test Loss: 3.0643\n",
      "Epoch 9754, Train Loss: 3.0151, Test Loss: 3.0562\n",
      "Epoch 9755, Train Loss: 3.0025, Test Loss: 3.0625\n",
      "Epoch 9756, Train Loss: 3.0039, Test Loss: 3.0622\n",
      "Epoch 9757, Train Loss: 3.0057, Test Loss: 3.0496\n",
      "Epoch 9758, Train Loss: 3.0017, Test Loss: 3.0578\n",
      "Epoch 9759, Train Loss: 3.0162, Test Loss: 3.0566\n",
      "Epoch 9760, Train Loss: 3.0027, Test Loss: 3.0599\n",
      "Epoch 9761, Train Loss: 2.9990, Test Loss: 3.0687\n",
      "Epoch 9762, Train Loss: 3.0135, Test Loss: 3.0594\n",
      "Epoch 9763, Train Loss: 2.9966, Test Loss: 3.0552\n",
      "Epoch 9764, Train Loss: 2.9975, Test Loss: 3.0501\n",
      "Epoch 9765, Train Loss: 3.0031, Test Loss: 3.0501\n",
      "Epoch 9766, Train Loss: 3.0055, Test Loss: 3.0512\n",
      "Epoch 9767, Train Loss: 3.0005, Test Loss: 3.0570\n",
      "Epoch 9768, Train Loss: 3.0033, Test Loss: 3.0533\n",
      "Epoch 9769, Train Loss: 2.9983, Test Loss: 3.0516\n",
      "Epoch 9770, Train Loss: 2.9952, Test Loss: 3.0555\n",
      "Epoch 9771, Train Loss: 3.0058, Test Loss: 3.0513\n",
      "Epoch 9772, Train Loss: 2.9952, Test Loss: 3.0569\n",
      "Epoch 9773, Train Loss: 3.0027, Test Loss: 3.0568\n",
      "Epoch 9774, Train Loss: 2.9959, Test Loss: 3.0637\n",
      "Epoch 9775, Train Loss: 3.0001, Test Loss: 3.0626\n",
      "Epoch 9776, Train Loss: 3.0028, Test Loss: 3.0481\n",
      "Epoch 9777, Train Loss: 2.9983, Test Loss: 3.0512\n",
      "Epoch 9778, Train Loss: 3.0021, Test Loss: 3.0517\n",
      "Epoch 9779, Train Loss: 3.0018, Test Loss: 3.0632\n",
      "Epoch 9780, Train Loss: 3.0083, Test Loss: 3.0626\n",
      "Epoch 9781, Train Loss: 3.0029, Test Loss: 3.0562\n",
      "Epoch 9782, Train Loss: 2.9981, Test Loss: 3.0536\n",
      "Epoch 9783, Train Loss: 3.0007, Test Loss: 3.0540\n",
      "Epoch 9784, Train Loss: 3.0070, Test Loss: 3.0557\n",
      "Epoch 9785, Train Loss: 3.0015, Test Loss: 3.0555\n",
      "Epoch 9786, Train Loss: 3.0024, Test Loss: 3.0559\n",
      "Epoch 9787, Train Loss: 3.0058, Test Loss: 3.0547\n",
      "Epoch 9788, Train Loss: 2.9992, Test Loss: 3.0586\n",
      "Epoch 9789, Train Loss: 3.0099, Test Loss: 3.0486\n",
      "Epoch 9790, Train Loss: 2.9934, Test Loss: 3.0499\n",
      "Epoch 9791, Train Loss: 2.9978, Test Loss: 3.0513\n",
      "Epoch 9792, Train Loss: 3.0025, Test Loss: 3.0505\n",
      "Epoch 9793, Train Loss: 3.0015, Test Loss: 3.0485\n",
      "Epoch 9794, Train Loss: 2.9970, Test Loss: 3.0546\n",
      "Epoch 9795, Train Loss: 3.0000, Test Loss: 3.0538\n",
      "Epoch 9796, Train Loss: 2.9960, Test Loss: 3.0513\n",
      "Epoch 9797, Train Loss: 2.9994, Test Loss: 3.0568\n",
      "Epoch 9798, Train Loss: 2.9985, Test Loss: 3.0555\n",
      "Epoch 9799, Train Loss: 3.0033, Test Loss: 3.0538\n",
      "Epoch 9800, Train Loss: 2.9975, Test Loss: 3.0483\n",
      "Epoch 9801, Train Loss: 2.9979, Test Loss: 3.0474\n",
      "Epoch 9802, Train Loss: 3.0000, Test Loss: 3.0472\n",
      "Epoch 9803, Train Loss: 2.9963, Test Loss: 3.0487\n",
      "Epoch 9804, Train Loss: 2.9951, Test Loss: 3.0525\n",
      "Epoch 9805, Train Loss: 2.9976, Test Loss: 3.0534\n",
      "Epoch 9806, Train Loss: 2.9943, Test Loss: 3.0525\n",
      "Epoch 9807, Train Loss: 3.0003, Test Loss: 3.0501\n",
      "Epoch 9808, Train Loss: 2.9964, Test Loss: 3.0501\n",
      "Epoch 9809, Train Loss: 2.9990, Test Loss: 3.0504\n",
      "Epoch 9810, Train Loss: 2.9936, Test Loss: 3.0523\n",
      "Epoch 9811, Train Loss: 3.0041, Test Loss: 3.0531\n",
      "Epoch 9812, Train Loss: 2.9949, Test Loss: 3.0681\n",
      "Epoch 9813, Train Loss: 3.0115, Test Loss: 3.0507\n",
      "Epoch 9814, Train Loss: 2.9967, Test Loss: 3.0555\n",
      "Epoch 9815, Train Loss: 3.0030, Test Loss: 3.0543\n",
      "Epoch 9816, Train Loss: 3.0010, Test Loss: 3.0671\n",
      "Epoch 9817, Train Loss: 3.0048, Test Loss: 3.0517\n",
      "Epoch 9818, Train Loss: 2.9904, Test Loss: 3.0506\n",
      "Epoch 9819, Train Loss: 2.9924, Test Loss: 3.0514\n",
      "Epoch 9820, Train Loss: 2.9989, Test Loss: 3.0490\n",
      "Epoch 9821, Train Loss: 2.9966, Test Loss: 3.0518\n",
      "Epoch 9822, Train Loss: 2.9977, Test Loss: 3.0515\n",
      "Epoch 9823, Train Loss: 2.9927, Test Loss: 3.0547\n",
      "Epoch 9824, Train Loss: 2.9964, Test Loss: 3.0531\n",
      "Epoch 9825, Train Loss: 2.9928, Test Loss: 3.0548\n",
      "Epoch 9826, Train Loss: 2.9920, Test Loss: 3.0556\n",
      "Epoch 9827, Train Loss: 2.9998, Test Loss: 3.0516\n",
      "Epoch 9828, Train Loss: 2.9936, Test Loss: 3.0537\n",
      "Epoch 9829, Train Loss: 3.0025, Test Loss: 3.0515\n",
      "Epoch 9830, Train Loss: 2.9923, Test Loss: 3.0649\n",
      "Epoch 9831, Train Loss: 2.9972, Test Loss: 3.0626\n",
      "Epoch 9832, Train Loss: 3.0001, Test Loss: 3.0536\n",
      "Epoch 9833, Train Loss: 2.9990, Test Loss: 3.0539\n",
      "Epoch 9834, Train Loss: 3.0060, Test Loss: 3.0527\n",
      "Epoch 9835, Train Loss: 2.9994, Test Loss: 3.0698\n",
      "Epoch 9836, Train Loss: 3.0152, Test Loss: 3.0555\n",
      "Epoch 9837, Train Loss: 2.9984, Test Loss: 3.0617\n",
      "Epoch 9838, Train Loss: 2.9993, Test Loss: 3.0631\n",
      "Epoch 9839, Train Loss: 3.0118, Test Loss: 3.0528\n",
      "Epoch 9840, Train Loss: 3.0001, Test Loss: 3.0616\n",
      "Epoch 9841, Train Loss: 3.0104, Test Loss: 3.0527\n",
      "Epoch 9842, Train Loss: 2.9966, Test Loss: 3.0629\n",
      "Epoch 9843, Train Loss: 3.0052, Test Loss: 3.0633\n",
      "Epoch 9844, Train Loss: 3.0085, Test Loss: 3.0545\n",
      "Epoch 9845, Train Loss: 2.9989, Test Loss: 3.0587\n",
      "Epoch 9846, Train Loss: 3.0075, Test Loss: 3.0577\n",
      "Epoch 9847, Train Loss: 3.0055, Test Loss: 3.0607\n",
      "Epoch 9848, Train Loss: 3.0125, Test Loss: 3.0500\n",
      "Epoch 9849, Train Loss: 2.9986, Test Loss: 3.0620\n",
      "Epoch 9850, Train Loss: 3.0041, Test Loss: 3.0729\n",
      "Epoch 9851, Train Loss: 3.0055, Test Loss: 3.0674\n",
      "Epoch 9852, Train Loss: 3.0114, Test Loss: 3.0524\n",
      "Epoch 9853, Train Loss: 3.0052, Test Loss: 3.0578\n",
      "Epoch 9854, Train Loss: 3.0146, Test Loss: 3.0558\n",
      "Epoch 9855, Train Loss: 2.9970, Test Loss: 3.0659\n",
      "Epoch 9856, Train Loss: 3.0126, Test Loss: 3.0547\n",
      "Epoch 9857, Train Loss: 3.0045, Test Loss: 3.0541\n",
      "Epoch 9858, Train Loss: 3.0075, Test Loss: 3.0502\n",
      "Epoch 9859, Train Loss: 2.9987, Test Loss: 3.0561\n",
      "Epoch 9860, Train Loss: 3.0015, Test Loss: 3.0574\n",
      "Epoch 9861, Train Loss: 3.0021, Test Loss: 3.0498\n",
      "Epoch 9862, Train Loss: 2.9956, Test Loss: 3.0559\n",
      "Epoch 9863, Train Loss: 3.0055, Test Loss: 3.0516\n",
      "Epoch 9864, Train Loss: 2.9972, Test Loss: 3.0536\n",
      "Epoch 9865, Train Loss: 3.0035, Test Loss: 3.0523\n",
      "Epoch 9866, Train Loss: 2.9946, Test Loss: 3.0536\n",
      "Epoch 9867, Train Loss: 2.9974, Test Loss: 3.0482\n",
      "Epoch 9868, Train Loss: 2.9955, Test Loss: 3.0488\n",
      "Epoch 9869, Train Loss: 3.0024, Test Loss: 3.0494\n",
      "Epoch 9870, Train Loss: 3.0014, Test Loss: 3.0504\n",
      "Epoch 9871, Train Loss: 2.9949, Test Loss: 3.0523\n",
      "Epoch 9872, Train Loss: 2.9970, Test Loss: 3.0544\n",
      "Epoch 9873, Train Loss: 2.9963, Test Loss: 3.0512\n",
      "Epoch 9874, Train Loss: 3.0031, Test Loss: 3.0492\n",
      "Epoch 9875, Train Loss: 2.9990, Test Loss: 3.0562\n",
      "Epoch 9876, Train Loss: 3.0050, Test Loss: 3.0517\n",
      "Epoch 9877, Train Loss: 2.9962, Test Loss: 3.0551\n",
      "Epoch 9878, Train Loss: 3.0053, Test Loss: 3.0538\n",
      "Epoch 9879, Train Loss: 2.9974, Test Loss: 3.0593\n",
      "Epoch 9880, Train Loss: 3.0011, Test Loss: 3.0571\n",
      "Epoch 9881, Train Loss: 3.0010, Test Loss: 3.0547\n",
      "Epoch 9882, Train Loss: 2.9966, Test Loss: 3.0592\n",
      "Epoch 9883, Train Loss: 3.0023, Test Loss: 3.0508\n",
      "Epoch 9884, Train Loss: 3.0027, Test Loss: 3.0508\n",
      "Epoch 9885, Train Loss: 2.9978, Test Loss: 3.0515\n",
      "Epoch 9886, Train Loss: 3.0017, Test Loss: 3.0547\n",
      "Epoch 9887, Train Loss: 2.9979, Test Loss: 3.0622\n",
      "Epoch 9888, Train Loss: 3.0050, Test Loss: 3.0587\n",
      "Epoch 9889, Train Loss: 2.9993, Test Loss: 3.0618\n",
      "Epoch 9890, Train Loss: 2.9985, Test Loss: 3.0581\n",
      "Epoch 9891, Train Loss: 2.9975, Test Loss: 3.0593\n",
      "Epoch 9892, Train Loss: 3.0017, Test Loss: 3.0583\n",
      "Epoch 9893, Train Loss: 2.9984, Test Loss: 3.0541\n",
      "Epoch 9894, Train Loss: 3.0002, Test Loss: 3.0480\n",
      "Epoch 9895, Train Loss: 2.9967, Test Loss: 3.0522\n",
      "Epoch 9896, Train Loss: 2.9953, Test Loss: 3.0563\n",
      "Epoch 9897, Train Loss: 3.0022, Test Loss: 3.0571\n",
      "Epoch 9898, Train Loss: 2.9998, Test Loss: 3.0543\n",
      "Epoch 9899, Train Loss: 2.9976, Test Loss: 3.0571\n",
      "Epoch 9900, Train Loss: 3.0077, Test Loss: 3.0605\n",
      "Epoch 9901, Train Loss: 3.0041, Test Loss: 3.0610\n",
      "Epoch 9902, Train Loss: 3.0051, Test Loss: 3.0487\n",
      "Epoch 9903, Train Loss: 2.9950, Test Loss: 3.0521\n",
      "Epoch 9904, Train Loss: 2.9999, Test Loss: 3.0612\n",
      "Epoch 9905, Train Loss: 2.9992, Test Loss: 3.0625\n",
      "Epoch 9906, Train Loss: 3.0041, Test Loss: 3.0592\n",
      "Epoch 9907, Train Loss: 3.0001, Test Loss: 3.0590\n",
      "Epoch 9908, Train Loss: 3.0064, Test Loss: 3.0580\n",
      "Epoch 9909, Train Loss: 3.0035, Test Loss: 3.0555\n",
      "Epoch 9910, Train Loss: 2.9974, Test Loss: 3.0544\n",
      "Epoch 9911, Train Loss: 2.9973, Test Loss: 3.0495\n",
      "Epoch 9912, Train Loss: 2.9995, Test Loss: 3.0497\n",
      "Epoch 9913, Train Loss: 3.0088, Test Loss: 3.0556\n",
      "Epoch 9914, Train Loss: 2.9955, Test Loss: 3.0567\n",
      "Epoch 9915, Train Loss: 2.9943, Test Loss: 3.0547\n",
      "Epoch 9916, Train Loss: 3.0005, Test Loss: 3.0542\n",
      "Epoch 9917, Train Loss: 3.0060, Test Loss: 3.0585\n",
      "Epoch 9918, Train Loss: 2.9980, Test Loss: 3.0550\n",
      "Epoch 9919, Train Loss: 3.0040, Test Loss: 3.0530\n",
      "Epoch 9920, Train Loss: 3.0044, Test Loss: 3.0550\n",
      "Epoch 9921, Train Loss: 3.0049, Test Loss: 3.0529\n",
      "Epoch 9922, Train Loss: 2.9984, Test Loss: 3.0542\n",
      "Epoch 9923, Train Loss: 3.0042, Test Loss: 3.0514\n",
      "Epoch 9924, Train Loss: 2.9972, Test Loss: 3.0576\n",
      "Epoch 9925, Train Loss: 3.0011, Test Loss: 3.0573\n",
      "Epoch 9926, Train Loss: 3.0017, Test Loss: 3.0596\n",
      "Epoch 9927, Train Loss: 3.0087, Test Loss: 3.0552\n",
      "Epoch 9928, Train Loss: 2.9955, Test Loss: 3.0614\n",
      "Epoch 9929, Train Loss: 3.0004, Test Loss: 3.0597\n",
      "Epoch 9930, Train Loss: 3.0019, Test Loss: 3.0575\n",
      "Epoch 9931, Train Loss: 3.0019, Test Loss: 3.0667\n",
      "Epoch 9932, Train Loss: 3.0086, Test Loss: 3.0620\n",
      "Epoch 9933, Train Loss: 3.0147, Test Loss: 3.0501\n",
      "Epoch 9934, Train Loss: 3.0036, Test Loss: 3.0598\n",
      "Epoch 9935, Train Loss: 3.0074, Test Loss: 3.0702\n",
      "Epoch 9936, Train Loss: 3.0065, Test Loss: 3.0599\n",
      "Epoch 9937, Train Loss: 2.9999, Test Loss: 3.0587\n",
      "Epoch 9938, Train Loss: 3.0024, Test Loss: 3.0569\n",
      "Epoch 9939, Train Loss: 3.0021, Test Loss: 3.0570\n",
      "Epoch 9940, Train Loss: 3.0028, Test Loss: 3.0525\n",
      "Epoch 9941, Train Loss: 2.9979, Test Loss: 3.0503\n",
      "Epoch 9942, Train Loss: 3.0018, Test Loss: 3.0493\n",
      "Epoch 9943, Train Loss: 3.0074, Test Loss: 3.0553\n",
      "Epoch 9944, Train Loss: 3.0031, Test Loss: 3.0567\n",
      "Epoch 9945, Train Loss: 3.0065, Test Loss: 3.0570\n",
      "Epoch 9946, Train Loss: 3.0003, Test Loss: 3.0636\n",
      "Epoch 9947, Train Loss: 3.0062, Test Loss: 3.0616\n",
      "Epoch 9948, Train Loss: 3.0082, Test Loss: 3.0593\n",
      "Epoch 9949, Train Loss: 3.0080, Test Loss: 3.0532\n",
      "Epoch 9950, Train Loss: 3.0034, Test Loss: 3.0497\n",
      "Epoch 9951, Train Loss: 2.9961, Test Loss: 3.0543\n",
      "Epoch 9952, Train Loss: 3.0081, Test Loss: 3.0599\n",
      "Epoch 9953, Train Loss: 3.0071, Test Loss: 3.0549\n",
      "Epoch 9954, Train Loss: 3.0017, Test Loss: 3.0500\n",
      "Epoch 9955, Train Loss: 2.9966, Test Loss: 3.0559\n",
      "Epoch 9956, Train Loss: 2.9983, Test Loss: 3.0714\n",
      "Epoch 9957, Train Loss: 3.0022, Test Loss: 3.0690\n",
      "Epoch 9958, Train Loss: 3.0019, Test Loss: 3.0567\n",
      "Epoch 9959, Train Loss: 2.9984, Test Loss: 3.0522\n",
      "Epoch 9960, Train Loss: 2.9998, Test Loss: 3.0502\n",
      "Epoch 9961, Train Loss: 2.9959, Test Loss: 3.0537\n",
      "Epoch 9962, Train Loss: 2.9918, Test Loss: 3.0556\n",
      "Epoch 9963, Train Loss: 3.0010, Test Loss: 3.0531\n",
      "Epoch 9964, Train Loss: 2.9994, Test Loss: 3.0518\n",
      "Epoch 9965, Train Loss: 2.9967, Test Loss: 3.0541\n",
      "Epoch 9966, Train Loss: 2.9953, Test Loss: 3.0560\n",
      "Epoch 9967, Train Loss: 2.9985, Test Loss: 3.0500\n",
      "Epoch 9968, Train Loss: 2.9971, Test Loss: 3.0507\n",
      "Epoch 9969, Train Loss: 2.9991, Test Loss: 3.0498\n",
      "Epoch 9970, Train Loss: 2.9914, Test Loss: 3.0545\n",
      "Epoch 9971, Train Loss: 2.9997, Test Loss: 3.0491\n",
      "Epoch 9972, Train Loss: 2.9922, Test Loss: 3.0500\n",
      "Epoch 9973, Train Loss: 2.9990, Test Loss: 3.0520\n",
      "Epoch 9974, Train Loss: 3.0017, Test Loss: 3.0509\n",
      "Epoch 9975, Train Loss: 2.9968, Test Loss: 3.0497\n",
      "Epoch 9976, Train Loss: 2.9979, Test Loss: 3.0541\n",
      "Epoch 9977, Train Loss: 2.9999, Test Loss: 3.0503\n",
      "Epoch 9978, Train Loss: 2.9953, Test Loss: 3.0501\n",
      "Epoch 9979, Train Loss: 2.9977, Test Loss: 3.0520\n",
      "Epoch 9980, Train Loss: 2.9943, Test Loss: 3.0532\n",
      "Epoch 9981, Train Loss: 2.9972, Test Loss: 3.0510\n",
      "Epoch 9982, Train Loss: 2.9955, Test Loss: 3.0507\n",
      "Epoch 9983, Train Loss: 2.9938, Test Loss: 3.0511\n",
      "Epoch 9984, Train Loss: 2.9961, Test Loss: 3.0490\n",
      "Epoch 9985, Train Loss: 2.9910, Test Loss: 3.0506\n",
      "Epoch 9986, Train Loss: 2.9983, Test Loss: 3.0501\n",
      "Epoch 9987, Train Loss: 2.9951, Test Loss: 3.0494\n",
      "Epoch 9988, Train Loss: 2.9978, Test Loss: 3.0556\n",
      "Epoch 9989, Train Loss: 3.0034, Test Loss: 3.0543\n",
      "Epoch 9990, Train Loss: 2.9948, Test Loss: 3.0542\n",
      "Epoch 9991, Train Loss: 2.9958, Test Loss: 3.0509\n",
      "Epoch 9992, Train Loss: 2.9947, Test Loss: 3.0486\n",
      "Epoch 9993, Train Loss: 2.9896, Test Loss: 3.0478\n",
      "Epoch 9994, Train Loss: 2.9916, Test Loss: 3.0496\n",
      "Epoch 9995, Train Loss: 2.9943, Test Loss: 3.0500\n",
      "Epoch 9996, Train Loss: 2.9947, Test Loss: 3.0491\n",
      "Epoch 9997, Train Loss: 2.9935, Test Loss: 3.0495\n",
      "Epoch 9998, Train Loss: 2.9910, Test Loss: 3.0509\n",
      "Epoch 9999, Train Loss: 2.9940, Test Loss: 3.0498\n",
      "Epoch 10000, Train Loss: 3.0009, Test Loss: 3.0567\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(10000):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Model prediction\n",
    "    output = model(graph_data.x, graph_data.edge_index)[graph_data.train_mask]\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(output, graph_data.y[graph_data.train_mask])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    # Evaluate on test data\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_output = model(graph_data.x, graph_data.edge_index)[graph_data.test_mask]\n",
    "        test_loss = criterion(test_output, graph_data.y[graph_data.test_mask])\n",
    "        test_losses.append(test_loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {loss.item():.4f}, Test Loss: {test_loss.item():.4f}\")\n",
    "\n",
    "# Plot Loss Curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-Class Accuracy: 30.86%\n"
     ]
    }
   ],
   "source": [
    "def multiclass_accuracy(preds, labels):\n",
    "    preds = preds.argmax(dim=1)  # Convert softmax output to class index\n",
    "    labels = labels.argmax(dim=1) if labels.dim() > 1 else labels  # Convert one-hot labels to class indices if necessary\n",
    "    correct = (preds == labels).sum().item()\n",
    "    total = labels.numel()\n",
    "    return correct / total\n",
    "\n",
    "\n",
    "# Evaluate Accuracy\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_output = model(graph_data.x, graph_data.edge_index)[graph_data.test_mask]\n",
    "    test_labels = graph_data.y[graph_data.test_mask]\n",
    "\n",
    "    acc = multiclass_accuracy(test_output, test_labels)\n",
    "    print(f\"Multi-Class Accuracy: {acc * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
